## Abstract

Large multimodal models (LMM) have recently shown
encouraging progress with visual instruction tuning. In this
paper, we present the first systematic study to investigate
the design choices of LMMs in a controlled setting under
the LLaVA framework. We show that the fully-connected
vision-language connector in LLaVA is surprisingly power-
ful and data-efficient. With simple modifications to LLaVA,
namely, using CLIP-ViT-L-336px with an MLP projection
and adding academic-task-oriented VQA data with response
formatting prompts, we establish stronger baselines that
achieve state-of-the-art across 11 benchmarks. Our final
13B checkpoint uses merely 1.2M publicly available data,
and finishes full training in ∼1 day on a single 8-A100 node.
Furthermore, we present some early exploration of open
problems in LMMs, including scaling to higher resolution
inputs, compositional capabilities, and model hallucination,
etc. We hope this makes state-of-the-art LMM research more
accessible. Code and model will be publicly available.

## 1. Introduction

Large multimodal models (LMMs) have become increasingly popular in the research community, as they are the key building blocks towards general-purpose assistants [2, 30, 43]. Recent studies on LMMs are converging on a central concept known as visual instruction tuning [36]. The results are promising, *e.g*. LLaVA [36] and MiniGPT-4 [62] demonstrate impressive results on natural instruction-following and visual reasoning capabilities. To better understand the capability of LMMs, multiple benchmarks [17, 27, 34, 37, 55] have been proposed. Recent works further demonstrate improved performance by scaling up the pretraining data [3, 14, 54], instruction-following data [14, 18, 29, 58], visual encoders [3], or language models [39], respectively. The LLaVA architecture is also leveraged in different downstream tasks and domains, including region-level [8, 56] and pixel-level [26, 50] understanding, biomedical assistants [31], image generation [5], adversarial studies [6, 59].

However, despite many benchmarks and developments, it still remains unclear what the best recipe is to train LMMs towards the goal of general-purpose assistants. For example, LLaVA [36] excels in conversational-style visual reasoning and even outperforms later approaches like Instruct-
BLIP [14] on such benchmarks [55], while InstructBLIP
excels in traditional VQA benchmarks that demands singleword or short answers. Given the significant differences in the model architecture and training data between them, the root cause of the disparity in their capabilities remains elusive, despite conjectures [37, 55]: the amount of training data, the usage of resamplers like Qformer [32], *etc*. To this end, we present the first systematic study to investigate the design choices of LMMs in a controlled setting. Our study originates from LLaVA and builds a road map by carefully making effective contributions from the perspectives of the input, model, and data.

First, we unveil that the fully-connected vision-language connector in LLaVA is surprisingly powerful and dataefficient, and we establish stronger and more feasible baselines built upon the LLaVA framework. We report that two simple improvements, namely, an MLP cross-modal connector and incorporating academic task related data such as VQA, are orthogonal to the framework of LLaVA, and when used with LLaVA, lead to better multimodal understanding capabilities. In contrast to InstructBLIP [14] or Qwen-VL [3], which trains specially designed visual resamplers on hundreds of millions or even billions of image-text paired data, LLaVA uses one of the simplest architecture design for LMMs and requires only training a simple fullyconnected projection layer on merely 600K image-text pairs.

Our final model can finish training in ∼1 day on a single
8-A100 machine and achieves state-of-the-art results on a wide range of benchmarks. Moreover, unlike Qwen-VL [3]
that includes in-house data in training, LLaVA utilizes only publicly available data.

Next, we delve into an early exploration of other open problems of large multimodal models. Our findings include:
(1) Scaling to high-resolution image inputs. We show that LLaVA's architecture is versatile in scaling to higher resolutions by simply dividing images into grids and maintains its data efficiency; with the increased resolution, it improves the model's detailed perception capabilities and reduces hallucination. (2) Compositional capabilities. We find that large multimodal models are capable of generalizing to compositional capabilities. For example, training on long-form language reasoning together with shorter visual reasoning can improve the model's writing capability for multimodal questions. (3) Data efficiency. We show that randomly downsampling LLaVA's training data mixture by up to 75% does not significantly decrease the model's performance, suggesting that the possibility of a more sophisticated dataset compression strategy can further improve LLaVA's already efficient training pipeline. (4) Data scaling.

We provide empirical evidence for the scaling of data granularity in conjunction with the model's capability is crucial for an improved capability without introducing artifacts like hallucination.

In sum, we perform a systematic study on the training of large multimodal models, and introduce a simple yet effective approach to balance the multitask learning and effective scaling for large multimodal models. Our improved baselines, LLaVA-1.5, uses only *public* data, achieves the state-of-the-art on a broad range of 11 tasks, and is significantly more data-efficient than previous approaches. By rethinking the conventional approaches and exploring the open problems in visual instruction tuning, we pave the way for more robust and capable systems for LMMs. We hope these improved and easily-reproducible baselines will provide a reference for future research in open-source LMMs.

## 3. Approach 3.1. Preliminaries

As the seminal work of visual instruction tuning, LLaVA [36]
showcases commendable proficiency in visual reasoning capabilities, surpassing even more recent models on diverse benchmarks [4, 55] for real-life visual instruction-following tasks. LLaVA uses a single linear layer to project the visual features to language space, and optimizes the whole LLM for visual instruction tuning. However, LLaVA falls short on academic benchmarks that typically require shortform answers (e.g. single-word), and tends to answer *yes* for yes/no questions due to the lack of such data in the training distribution.

On the other hand, InstructBLIP [14] is the pioneer to incorporate academic-task-oriented datasets like VQA-v2 [19] along with LLaVA-Instruct [36], and demonstrates improved performance on VQA benchmarks. It pretrains Qformer [32]
on 129M image-text pairs and only finetunes the instructionaware Qformer for visual instruction tuning. However, recent studies [7, 55] show that it does not perform as well as LLaVA on engaging in real-life visual conversation tasks. More specifically, as shown in Table 1a, it can overfit to VQA training sets with short-answers, even on requests that require detailed responses.

## 6. Conclusion

In this paper, we take a step towards demystifying the design of large multimodal models, and propose a simple, effective, and data-efficient baseline, LLaVA-1.5, for large multimodal models. In addition, we explore the open problems in visual instruction tuning, scale LMMs to higher resolutions, and present some intriguing findings in terms of model hallucination and compositional capabilities for LMMs. We hope these improved and easily-reproducible baselines as well as the new findings will provide a reference for future research in open-source LMM.

Limitations. Despite the promising results demonstrated by LLaVA-1.5, it still has limitations including prolonged training for high-resolution images, lack of multiple-image understanding, limited problem solving capabilities in certain fields. It is not exempt from producing hallucinations, and should be used with caution in critical applications (e.g.

medical). See appendix for a detailed discussion.

Acknowledgements. This work was supported in part by NSF CAREER IIS2150012, and Institute of Information &
communications Technology Planning & Evaluation(IITP)
grants funded by the Korea government(MSIT) (No. 2022-
0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS- 2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training).

