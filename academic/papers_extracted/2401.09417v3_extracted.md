## Abstract

Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling.

Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k
*Equal contribution
1School of EIC, Huazhong University of Science & Technology
2Institute of Artificial Intelligence, Huazhong University of Science & Technology 3Horizon Robotics
4Beijing Academy of Artificial Intelligence. Correspondence to:
Xinggang Wang <xgwang@hust.edu.cn>.

Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).

semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8× faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248×1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code and models are released at https://github.com/hustvl/Vim

## 1. Introduction

Recent research advancements have led to a surge of interest in the state space model (SSM). Originating from the classic Kalman filter model (Kalman, 1960), modern SSMs excel at capturing long-range dependencies and benefit from parallel training. Some SSM-based methods, such as the linear state-space layers (LSSL) (Gu et al., 2021b), structured state space sequence model (S4) (Gu et al., 2021a), diagonal state space (DSS) (Gupta et al., 2022), and S4D (Gu et al., 2022), are proposed to process sequence data across a wide range of tasks and modalities, particularly on modeling long-range dependencies. They are efficient in processing long sequences because of convolutional computation and near-linear computation. 2-D SSM (Baron et al., 2023), SGConvNeXt (Li et al., 2022b), and ConvSSM (Smith et al.,
2023a) combine SSM with CNN or Transformer architecture to process 2-D data. The recent work, Mamba (Gu &
Dao, 2023), incorporates time-varying parameters into the SSM and proposes a hardware-aware algorithm to enable very efficient training and inference. The superior scaling performance of Mamba indicates that it is a promising alternative to Transformer in language modeling. Nevertheless, a generic pure-SSM-based backbone network has not been explored for processing visual data, such as images and videos.

Vision Transformers (ViTs) have achieved great success in visual representation learning, excelling in large-scale self-supervised pre-training and high performance on downstream tasks. Compared with convolutional neural networks, the core advantage lies in that ViT can provide each image patch with data/patch-dependent global context through selfattention. This differs from convolutional networks that use the same parameters, *i.e.*, the convolutional filters, for all positions. Another advantage is the modality-agnostic modeling by treating an image as a sequence of patches without
2D inductive bias, which makes it the preferred architecture for multimodal applications (Bavishi et al., 2023; Li et al.,
2023; Liu et al., 2023). At the same time, the self-attention mechanism in Transformers poses challenges in terms of speed and memory usage when dealing with long-range visual dependencies, *e.g.*, processing high-resolution images.

Motivated by the success of Mamba in language modeling, it is appealing that we can also transfer this success from language to vision, *i.e.*, to design a generic and efficient visual backbone with the advanced SSM method.

However, there are two challenges for Mamba, *i.e.*, unidirectional modeling and lack of positional awareness. To address these challenges, we propose the Vision Mamba
(Vim) model, which incorporates the bidirectional SSMs for data-dependent global visual context modeling and position embeddings for location-aware visual recognition. We first split the input image into patches and linearly project them as vectors to Vim. Image patches are treated as the sequence data in Vim blocks, which efficiently compresses the visual representation with the proposed bidirectional selective state space. Furthermore, the position embedding in Vim block provides the awareness for spatial information, which enables Vim to be more robust in dense prediction tasks. In the current stage, we train the Vim model on the supervised image classification task using the ImageNet dataset and then use the pretrained Vim as the backbone to perform sequential visual representation learning for downstream dense prediction tasks, *i.e.*, semantic segmentation, object detection, and instance segmentation. Like Transformers, Vim can be pretrained on large-scale unsupervised visual data for better visual representation. Thanks to the better efficiency of Mamba, the large-scale pretraining of Vim can be achieved with lower computational cost.

Compared with other SSM-based models for vision tasks, Vim is a pure-SSM-based method and models images in a sequence manner, which is more promising for a generic and efficient backbone. Thanks to the bidirectional compressing modeling with positional awareness, Vim is the first pure- SSM-based model to handle dense prediction tasks. Compared with the most convincing Transformer-based model, i.e., DeiT (Touvron et al., 2021a), Vim achieves superior performance on ImageNet classification. Furthermore, Vim is more efficient in terms of GPU memory and inference time for high-resolution images. The efficiency in terms of memory and speed empowers Vim to directly perform sequential visual representation learning without relying on
2D priors (such as the 2D local window in ViTDet (Li et al., 2022c)) for high-resolution visual understanding tasks while achieving higher accuracy than DeiT. Our main contributions can be summarized as follows:

- We propose Vision Mamba (Vim), which incorporates
bidirectional SSM for data-dependent global visual
context modeling and position embeddings for locationaware visual understanding.
- Without the need of attention, the proposed Vim has
the same modeling power as ViT while it only has subquadratic-time computation and linear memory
complexity. Specifically, Vim is 2.8× faster than DeiT
and saves 86.8% GPU memory when performing batch inference to extract features on images at the resolution
of 1248×1248.
- We conduct extensive experiments on ImageNet classification and dense prediction downstream tasks. The
results demonstrate that Vim achieves superior performance compared to the well-established and highlyoptimized plain vision Transformer, *i.e.*, DeiT.

## 3. Method

The goal of Vision Mamba (Vim) is to introduce the advanced state space model (SSM), *i.e.*, Mamba (Gu & Dao,
2023), to computer vision. This section begins with a description of the preliminaries of SSM. It is followed by an overview of Vim. We then detail how the Vim block processes input token sequences and proceed to illustrate the architecture details of Vim. The section concludes with an analysis of the efficiency of the proposed Vim.

## 3.1. Preliminaries

The SSM-based models, i.e., structured state space sequence
models (S4) and Mamba are inspired by the continuous
system, which maps a 1-D function or sequence x(t) ∈
R 7→y(t) ∈R through a hidden state h(t) ∈RN. This
system uses A ∈RN×N as the evolution parameter and
B ∈RN×1, C ∈R1×N as the projection parameters. The
continuous system works as follows: h′(t) = Ah(t) +
Bx(t) and y(t) = Ch(t).

The S4 and Mamba are the discrete versions of the con-
tinuous system, which include a timescale parameter ∆
to transform the continuous parameters A, B to discrete
parameters A, B. The commonly used method for trans-
formation is zero-order hold (ZOH), which is defined as
follows:

$$\begin{array}{l}\mathbf{A}=\exp{(\mathbf{\Delta A})},\\ \mathbf{B}=(\mathbf{\Delta A})^{-1}(\exp{(\mathbf{\Delta A})}-\mathbf{I})\cdot\mathbf{\Delta B}.\end{array}\tag{1}$$
After the discretization of A, B, the discretized version using a step size ∆can be rewritten as:

$$h_{t}=\mathbf{A}h_{t-1}+\mathbf{B}x_{t},\tag{2}$$ $$y_{t}=\mathbf{C}h_{t}.$$

At last, the models compute output through a global convolution.

$$\overline{\mathbf{K}}=(\mathbf{C}\overline{\mathbf{B}},\mathbf{C}\overline{\mathbf{A}}\overline{\mathbf{B}},\ldots,\mathbf{C}\overline{\mathbf{A}}^{t-1}\overline{\mathbf{B}}),\tag{3}$$ $$\mathbf{y}=\mathbf{x}*\overline{\mathbf{K}},$$
where M is the length of the input sequence x, and K ∈RM
is a structured convolutional kernel.

## 4.4. Ablation Study

Bidirectional SSM. We ablate the key bidirectional design of Vim, using ImageNet-1K classification and the Segmenter (Strudel et al., 2021) semantic segmentation framework on ADE20K. To fully evaluate the power of learned representation on ImageNet, we use a simple Segmenter head with only 2 layers to perform transfer learning on semantic segmentation. We study the following bidirectional strategies. *None*: We directly adopt the Mamba block to

Bidirectional strategy
ImageNet top-1 acc.
ADE20K
mIoU
None
73.2
32.3
Bidirectional Layer
70.9
33.6
Bidirectional SSM
72.8
33.2
Bidirectional SSM + Conv1d
73.9
35.9

process visual sequence with only the forward direction.

Bidirectional Sequence: During training, we randomly flip the visual sequence. This works like data augmentation.

Bidirectional Block: We pair the stacked blocks. The first block of each pair processes visual sequence in the forward direction and the second block of each pair processes in the backward direction. *Bidirectional SSM*: We add an extra SSM for each block to process the visual sequence in the backward direction. *Bidirectional SSM + Conv1d*: Based on Bidirectional SSM, we further add a backward Conv1d before the backward SSM (Fig. 2).

As shown in Tab. 4, directly adopting the Mamba block achieves good performance in classification. However, the unnatural unidirectional manner poses challenges in downstream dense prediction. Specifically, the preliminary bidirectional strategy of using Bidirectional Block achieves 7
points lower top-1 accuracy on classification. Yet, it outperforms the vanilla unidirectional Mamba block by 1.3 mIoU
on semantic segmentation. By adding extra backward SSM
and Conv1d, we achieve superior classification accuracy
(73.9 top-1 acc *vs.* 73.2 top-1 acc) and exceptional segmentation superiority (35.9 mIoU *vs.* 32.3 mIoU). We use the strategy of Bidirectional SSM + Conv1d as the default setting in our Vim block.

Classification Design. We ablate the classification design of Vim, benchmarking on ImageNet-1K classification. We study the following classification strategies. *Mean pool*:
We adopt mean pooling on the output feature from the last Vim block and perform classification on this pooled feature.

Max pool: We first adapt the classification head on each token of the visual sequence and then perform max pooling on the sequence to get the classification prediction result.

Head class token: Following DeiT (Touvron et al., 2021b), we concatenate the class token at the head of the visual sequence and perform classification. *Double class token*:
Based on the head class token strategy, we additionally add a class token at the tail of the visual sequence. Middle class token: We add a class token at the middle of the visual sequence and then perform classification on the final middle class token.

| Classification strategy   |
|---------------------------|
| Mean pool                 |
| 73.9                      |
| Max pool                  |
| 73.4                      |
| Head class token          |
| 75.2                      |
| Double class token        |
| 74.3                      |
| Middle class token        |
| 76.1                      |

As shown in Tab. 5, experiments show that the middle class token strategy can fully exploit the recurrent nature of SSM
and the central object prior in ImageNet, demonstrating the best top-1 accuracy of 76.1.

