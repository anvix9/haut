## Abstract

The year 1948 witnessed the historic moment of the birth of classic information theory (CIT).

Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function H(U), channel capacity C = maxp(x) I(X; Y ) and rate-distortion function R(D) =
minp(ˆx|x):Ed(x,ˆx)≤D I(X; ˆX). Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework of semantic information theory (SIT). We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information. Stemming from this core concept, synonymous mapping f, we introduce the measures of semantic information, such as semantic entropy Hs( ˜U), up/down semantic mutual information Is( ˜X; ˜Y ) (Is( ˜X; ˜Y )), semantic capacity Cs = maxfxy maxp(x) Is( ˜X; ˜Y ), and semantic rate-distortion function Rs(D) = min{fx,fˆx} minp(ˆx|x):Eds(˜x,ˆ˜x)≤D Is( ˜X; ˆ˜X). Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is, Hs( ˜U) ≤H(U), Cs ≥C and Rs(D) ≤R(D). All these works composite the basis of semantic information theory. In addition, we discuss the semantic information measures in the continuous case.

Especially, for the band-limited Gaussian channel, we obtain a new channel capacity formula, Cs =

B log
   h
   S4 
      1 +
         P
        N0B
           i
            , where the average synonymous length S indicates the identification ability of

information. In summary, the theoretic framework of SIT proposed in this paper is a natural extension of CIT and may reveal great performance potential for future communication.

K. Niu and P. Zhang are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China (e-mail: {niukai, pzhang}@bupt.edu.cn).

Synonymous mapping, Semantic entropy, Semantic relative entropy, Up/Down semantic mutual information, Semantic channel capacity, Semantic distortion, Semantic rate distortion function, Semantically typical set, Synonymous typical set, Semantically jointly typical set, Jointly typical decoding, Jointly typical encoding, Synonymous length, Maximum likelihood group decoding, Semantic source channel coding.

## I. Introduction

Classic information theory (CIT), established by C. E. Shannon [1] in 1948, was a great achievement in the modern information and communication field. As shown in Fig. 1, the classic communication system includes source, encoder, channel with noise, decoder and destination. This theory is concerned with the uncertainty of information and introduces four critical measures, such as entropy, mutual information, channel capacity, and rate-distortion function to evaluate the performance of information processing and transmission. Especially, three famous coding theorems, such as, lossless/lossy source coding theorem and channel coding theorem, reveal the fundamental limitation of data compression and information transmission. Over the past 70 years or so, people developed many advanced techniques to approach these theoretical limits. For the lossless source coding, Huffman coding and arithmetic coding are the representative optimal coding methods can achieve the source entropy. Similarly, for the channel coding, polar code, as a great breakthrough [23], is the first constructive capacity-achieving coding scheme. Correspondingly, for the lossy source coding, some modern coding schemes, such as BPG (Better Portable Graphics) standard and H. 265/266 standard, can approach the rate-distortion lower bounds of image and video sources. It follows that information and communication technologies guided by CIT have approached the theoretical limitation and the performance improvement of modern communication systems encounters a lot of bottlenecks.

Essentially, Weaver [3], just one year after Shannon published the seminal paper on information theory, pointed out that communication involves problems at three levels as follows:
"**LEVEL A**. How accurately can the symbols of communication be transmitted? (The technical problem.)
LEVEL B. How precisely do the transmitted symbols convey the desired meaning? (The semantic problem.)

| Channel   |
|-----------|
| Noise     |

## C. Semantic Source Coding Method

Similar to the classic source coding, the variable length coding is desired for semantic source coding. Thus we also obtain the semantic version of Kraft inequality as following.

Theorem 16. (Semantic Kraft Inequality): Given a discrete random variable U ∈U = {ui}N
i=1,

the corresponding semantic variable ˜U ∈˜U = {˜uis}
                                                  ˜
                                                  N
                                                  is=1, and the synonymous mapping f : ˜U →

U. For any prefix code over an alphabet of size F exists if and only if the codeword length

l1, l2, · · · , l ˜
               N satisfies
                                                           ˜
                                                          N
                                                        X

is=1 F −lis ≤1. (99)
The proof is similar to that of classic Kraft inequality and omitted. It should be noted that the semantic Kraft inequality has the same form as that of classic counterpart. However, since the semantic prefix code is performed over a synonymous set rather than a single syntactic symbol, the number of codewords is less than classic prefix code, that is, ˜N ≤N.

Syntactic symbol
u1
u2
u3
u4
Probability
1 2
1 4
1 8
1 8
Syn. HC
0
10
110
111

Furthermore, we can obtain the average code length of optimal semantic source code as follows.

Theorem 17. Given the syntactic source distribution p(u) and the synonymous mapping f :
˜U →U, let l∗
           1, l∗
             2, · · · , l∗
                    ˜
                   N denote the optimal code lengths with an F-ary alphabet, then the

expected length ¯L∗of the optimal semantic code satisfies

$$\frac{H_{s}(\tilde{U})}{\log F}\leq\tilde{L}^{*}<\frac{H_{s}(\tilde{U})}{\log F}+1.\tag{100}$$

Proof: Assign the code length as lis =
                                    l
                                     −logF
                                            P

i∈Uis p(ui)
                m
                  . Similar to the classic version,

by semantic Kraft inequality, we have

$$\frac{H_{s}(\tilde{U})}{\log F}\leq\bar{L}^{s}\leq\sum_{i_{s}=1}^{\tilde{N}}p(\hat{u}_{i_{s}})l_{i_{s}}<\frac{H_{s}(\tilde{U})}{\log F}+1.\tag{101}$$
So we prove the theorem.

□
Example 4. (Semantic Huffman Coding): Now we describe an example of semantic Huffman coding. For a syntactic source U with four symbols u1, u2, u3, u4, the probability distribution is listed in Table VII. The information entropy is H(U) = 1.75 bits. By using Huffman coding, the codewords are shown in Table VII and the average code length is ¯L = 1.75 bits = H(U).

If we give a synonymous mapping f, that is, ˜u1 →{u1}, ˜u2 →{u2}, ˜u3 →{u3, u4}, the probability distribution of semantic source ˜U is listed in Table VIII. So the semantic entropy is calculated as Hs( ˜U) = 1.5 sebits. Correspondingly, by the semantic Huffman codewords listed in Table VIII, the average code length is ¯Ls = 1.5 sebits = Hs( ˜U). Distinctly, due to synonymous mapping, the average code length of semantic Huffman code is smaller than that of traditional Huffman code.

Given a syntactic sequence u = (u1u1u3u4u2u3u2), by Table VII, the syntactic Huffman coding is x = (001101111011010). On the other hand, by Table VIII, the semantic Huffman coding is

Semantic symbol
˜u1 →{u1}
˜u2 →{u2}
˜u3 →{u3, u4}
Probability
1 2
1 4
1 4
Sem. HC
0
10
11

xs = (001111101110)*. Hence, the length of syntactic coding is* L(x) = 15 bits and the length of semantic coding is L(xs) = 12 sebits so that the latter is smaller than the former, i.e., L(xs) < L(x). Certainly, since the decoder can select arbitrary symbol from the set {u3, u4}
when decoding ˜u3*, the result may be* ˆu = (u1u1u3u3u2u4u2). Although such decoding sequence is different from the original one u = (u1u1u3u4u2u3u2) in the syntactic sense, the semantic information of the decoding results still keeps the same since u3 and u4 have the same meaning.

Remark 8. For the method of semantic source coding, we have two kinds of design thought.

The first kind thought is to modify the traditional source coding, such as Huffman, arithmetic or universal coding. By using an elaborate synonymous mapping, these coding methods can be devised to further improve the compression efficiency. For the second thought, based on the deep learning method, we can construct a neural network model to perform semantic source coding. In this model, the synonymous mapping and semantic coding can be integrated and optimized to approach the theoretic limitation.

## C. Semantic Channel Coding Method

We now investigate the semantic channel coding method. Given a
                                                              
                                                               2n(R+Rs), n
                                                                          
                                                                           channel code

with length n and semantic rate R, the codebook C can be divided into 2nR synonymous codeword

groups Cs(is), is ∈{1, 2, · · · , 2nR} and each group has 2nRs synonymous codewords. Consider

the synonymous mapping, we propose a new method, named as maximum likelihood group (MLG) decoding algorithm to decode this semantic code. The basic idea is to calculate the likelihood probability of the received signal on a synonymous group and compare all the group likelihood probabilities so as to select a group with the maximum probability as the final decoding result.

Definition 23. Assume one codeword xn ∈Cs(is) is transmitted on the discrete memoryless

channel with the transition probability p(yn|xn), the group likelihood probability is defined as

$$P(y^{n}|{\cal C}_{s}(i_{s}))\stackrel{{\triangle}}{{=}}\prod_{l=1}^{2^{nR_{s}}}p(y^{n}|x^{n}(i_{s},l)).\tag{145}$$
So the maximum likelihood group decoding rule is written as

$$\dot{i}_{s}=\arg\max_{i_{s}}P(y^{n}|{\cal C}_{s}(i_{s}))\tag{146}$$ $$=\arg\max_{i_{s}}\prod_{l=1}^{2^{nR_{s}}}p(y^{n}|x^{n}(i_{s},l)).$$
Equivalently, this rule can also presented as a logarithmic version,

$$\dot{i}_{s}=\arg\max_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\ln p(y^{n}|x^{n}(i_{s},l)).\tag{147}$$
Hence, we can calculate all the group likelihood probabilities and select one group with the maximum probability as the final decision. The index ˆis indicates the estimation of semantic information ˆ˜xn.

Next, we discuss the MLG decoding in the additive white Gaussian noise (AWGN) channel.

When a signal is transmitted over the AWGN channel, the received signal can be represented by an equivalent low-pass signal sampled at time k:

$y_{k}=s_{k}+z_{k}$ (148)
where sk =

±√Es
	
is the binary phase shifted key (BPSK) signal, zk is a sample of a zeromean complex Gaussian noise process with variance σ2 = N0/2. Let Es be the symbol energy and N0 denote the single-sided power spectral density of the additive white noise. So the symbol signal-to-noise ratio (SNR) is defined as Es N0.

Assume one codeword xn(is, l) is mapped into a transmitted signal vector sn(is, l) =
√Es (1n −2xn(is, l)) where 1n is the all-one vector with the length of n, by using MLG rule, we can write

$$\hat{i}_{s}=\arg\max_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\ln p(y^{n}|x^{n}(i_{s},l))\tag{149}$$ $$=\arg\max_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\ln\left[\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{\|y^{n}-s^{n}(i_{s},l)\|^{2}}{2\sigma^{2}}}\right]$$ $$=\arg\min_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\left\|y^{n}-s^{n}(i_{s},l)\right\|^{2}$$ $$=\arg\min_{i_{s}}d_{\rm E}^{2}\left(y^{n},{\cal C}_{s}(i_{s})\right),$$
where d2
E (yn, Cs(is)) = P2nRs l=1 ∥yn −sn(is, l)∥2 is the squared Euclidian distance between the receive vector yn and the code group Cs(is). Thus the MLG rule in AWGN channel is transformed into the minimum distance grouping decoding rule.

Now we investigate the group-wise error probability (GEP) of semantic channel code.

Theorem 22. Given a semantic channel code C with equipartition code groups Cs, the GEP
between Cs(is) and Cs(js) is upper bounded by

$$P\left(\mathcal{C}_{s}(i_{s})\rightarrow\mathcal{C}_{s}(j_{s})\right)\leq\exp\left\{-d_{GH}(\mathcal{C}_{s}(i_{s}),\mathcal{C}_{s}(j_{s}))\frac{E_{s}}{N_{0}}\right\},\tag{150}$$
where dGH denotes the group Hamming distance which is defined as following dGH(Cs(is), Cs(js)) =

$$\min_{m}\left[\sum_{l=1}^{2^{nR_{k}}}d_{H}(x^{n}(i_{s},m),x^{n}(j_{s},l))-\sum_{l=1,l\neq m}^{2^{nR_{k}}}d_{H}(x^{n}(i_{s},m),x^{n}(i_{s},l))\right]^{2}\tag{151}$$ $$\frac{\parallel\sum_{l=1}^{2^{nR_{k}}}\left(x^{n}(j_{s},l)-x^{n}(i_{s},l)\right)\parallel^{2}}{\parallel\sum_{l=1}^{2^{nR_{k}}}\left(x^{n}(j_{s},l)-x^{n}(i_{s},l)\right)\parallel^{2}}.$$
Proof: Assume one codeword xn(1, 1) in the code group Cs(1) is transmitted, the received signal vector can be represented as follows

$y^{n}=s^{n}(1,1)+z^{n}$, (152)
where zn ∼N(0, σ2I) is the Gaussian noise vector.

Suppose a codeword xn(js, l) ∈Cs(js), js ̸= 1 is mapped into the signal vector sn(js, l). By using the MLG rule, if a group-wise error occurs, the Euclidian distance between the received vector and the transmitted signal vector group satisfy the inequality

$$d_{\rm E}^{2}\left(y^{n},{\cal C}_{s}(i_{s})\right)>d_{\rm E}^{2}\left(y^{n},{\cal C}_{s}(j_{s})\right).\tag{153}$$
Substituting (149) and (152) into this inequality, we have

$$\sum_{l=1}^{2^{nRs}}\|y^{n}-s^{n}(1,l)\|^{2}$$ $$>\sum_{l=1}^{2^{nRs}}\|y^{n}-s^{n}(j_{s},l)\|^{2}\Rightarrow\tag{154}$$ $$\|z^{n}\|^{2}+\sum_{l=2}^{2^{nRs}}\|s^{n}(1,1)-s^{n}(1,l)+z^{n}\|^{2}$$ $$>\sum_{l=1}^{2^{nRs}}\|s^{n}(1,1)-s^{n}(j_{s},l)+z^{n}\|^{2}\,.$$
After some manipulations, the error decision region can be written as

$$\mathcal{H}=\left\{z^{n}:\left|\sum_{l=1}^{2^{nRs}}\left(s^{n}(j_{s},l)-s^{n}(1,l)\right)\right|\,\left(z^{n}\right)^{T}>\right.$$ $$\left.\frac{1}{2}\left|\sum_{l=1}^{2^{nRs}}\left\|s^{n}(1,1)-s^{n}(j_{s},l)\right\|^{2}\right.\right.\tag{155}$$ $$\left.-\sum_{l=2}^{2^{nRs}}\left\|s^{n}(1,1)-s^{n}(1,l)\right\|^{2}\right\}.$$
Let d2
E (sn(1, 1), Cs(js)) = P2nRs l=1 ∥sn(1, 1) −sn(js, l)∥2 denote the distance between the transmit vector sn(1, 1) and the code group Cs(js) and d2
E (sn(1, 1), Cs(1)) = P2nRs l=2 ∥sn(1, 1) −sn(1, l)∥2
denote the inner distance of code group Cs(1).

So the codeword-to-group error probability can be derived as

$$P\left(x^{n}(1,1)\rightarrow\mathcal{C}_{s}(j_{s})\right)$$ $$=Q\,\left|\,\sqrt{\frac{\left(d_{\mathbb{E}}^{2}\left(s^{n}(1,1),\mathcal{C}_{s}(j_{s})\right)-d_{\mathbb{E}}^{2}\left(s^{n}(1,1),\mathcal{C}_{s}(1)\right)\right)^{2}}{\left\|\sum_{l=1}^{2^{n}B_{s}}\left(s^{n}(j_{s},l)-s^{n}(1,l)\right)\right\|^{2}2N_{0}}}\right|\,,\tag{156}$$
where Q(x) =
1
√

$\frac{1}{2\pi}\int_{x}^{\infty}e^{-t^{2}/2}dt$ is the tail distribution function of the standard normal distribution.

Furthermore, due to (sn(1, 1) −sn(js, l)) = 2√Es (xn(js, l) −xn(1, 1)), then we have ∥sn(1, 1) −sn(js, l)∥2

= 4EsdH(xn(1, 1), xn(js, l)). Additionally, we can derive that
                                                  

P2nRs
                                                     l=1 (sn(js, l) −sn(1, l))
                                                                        

                                                                         2
                                                                           =

4Es
   

P2nRs
      l=1 (xn(js, l) −xn(1, l))
                           

                            2
                             = 4Es∥∆(Cs(js), Cs(1))∥2. Thus the error probability can be

further written as

$$P\left(x^{n}(1,1)\rightarrow{\cal C}_{s}(j_{s})\right)=Q\left[\begin{array}{c}/\\ d_{\rm GH}(x^{n}(1,1),{\cal C}_{s}(j_{s}))\frac{2E_{s}}{N_{0}}\end{array}\right]\,,\tag{157}$$
where dGH(xn(1, 1), Cs(js)) =[P2nRs l=1 dH(xn(1, 1), xn(js, l))
−P2nRs l=2 dH(xn(1, 1), xn(1, l))]2/∥∆(Cs(js), Cs(1))∥2 denotes the codeword-to-group Hamming distance.

Furthermore, using the inequality Q(x) ≤e−x2
2 , the codeword-to-group error probability can be upper bounded by

$$P\left(x^{n}(1,1)\to{\cal C}_{s}(j_{s})\right)\leq e^{-d_{\rm OH}(x^{n}(1,1),{\cal C}_{s}(j_{s}))}\frac{E_{s}}{N_{0}}\,.\tag{158}$$
Averaging over all the codewords of the group Cs(1), we obtain the upper bound of GEP as follows

$$P\left(\mathcal{C}_{s}(1)\rightarrow\mathcal{C}_{s}(j_{s})\right)\leq\sum_{l=1}^{2^{nR_{s}}}\frac{1}{2^{nR_{s}}}e^{-d_{\mathrm{GH}}(x^{n}(1,l),\mathcal{C}_{s}(j_{s}))}\frac{E_{s}}{N_{0}}\tag{159}$$ $$\leq\exp\left\{-d_{\mathrm{GH}}(\mathcal{C}_{s}(1),\mathcal{C}_{s}(j_{s}))\frac{E_{s}}{N_{0}}\right\}\,.$$
So we complete the proof.

□
In the ML decoding, the minimum Hamming distance dH,min determines the error performance of one linear channel code. Similarly, in the MLG decoding, the minimum group Hamming distance dGH,min = min dGH(Cs(is), Cs(js)) dominates the performance of semantic channel code.

Example 5. We now give an example of semantic code constructed based on (7,4) Hamming code with synonymous mapping and MLG decoding. The codebook is shown in Table IX. All the sixteen codewords are divided into eight code groups and each group has two synonymous codewords. For an instance, Cs(1) *has two codewords* (0000000) *and* (1101000) and its semantic sequence is (000). So this code can be regarded as a (7,3) semantic Hamming code with code rate R = 3
7.

7 and Rs = 1
By using ML decoding, the union bound of the error probability is

$$P_{e}\leq\sum_{d=d_{H_{min}}}^{n}A_{d}Q\left(\begin{array}{c}/_{2}\frac{E_{s}}{N_{0}}\\ \end{array}\right)\leq\sum_{d=d_{H_{min}}}^{n}A_{d}e^{-d\frac{E_{s}}{N_{0}}}.\tag{160}$$
Since the minimum Hamming distance of this code is d*H,min* = 3 and distance spectrum is
{A3 = 8, A4 = 6, A7 = 1}, the error probability of ML decoding is upper bounded by

$$P_{e}^{\it ML}\leq8e^{-3\frac{E_{\pi}}{N_{0}}}+6e^{-4\frac{E_{\pi}}{N_{0}}}+e^{-7\frac{E_{\pi}}{N_{0}}}.\tag{161}$$
Let {Ad1,d2} denote the group distance spectrum and d1 and d2 mean the codeword-to-group Hamming distance. By using MLG decoding, the union bound of the error probability is

" ! !#  r  r Ad1,d2 n X Q + Q Pe ≤ 2 2d1 Es N0 2d2 Es N0 d1,d2=dGH,min (162) Ad1,d2 n X ≤  e−d1 Es N0 + e−d2 Es N0  . 2 d1,d2=dGH,min
So the minimum group Hamming distance of this code is d*GH,min* = 2 and group distance spectrum is {A2,2 = 6, A4,4 = 1}. The corresponding upper bound of the MLG decoding is

$$P_{e}^{MLG}\leq6e^{-2\frac{E_{x}}{N_{0}}}+e^{-4\frac{E_{x}}{N_{0}}}.\tag{163}$$
Compare with (161) and (163), we find that the minimum distance of semantic Hamming code is decreased. However, for a long code length and well-designed synonymous mapping, the error performance of MLG decoding will be better than that using ML decoding.

Remark 10. From the viewpoint of practical application, semantic channel codes are a new kind of channel codes. Synonymous mapping provides a valuable idea for the construction and decoding of semantic channel codes. Unlike the traditional channel codes, semantic channel codes should optimize the minimum group Hamming distance. How to design an optimal synonymous mapping to cleverly partition the code group is significant for the design of semantic codes. Non-equipartition mapping may be more flexible than the equipartition mapping. On the other hand, the optimal decoding of semantic codes is the MLG rule rather ML rule. However,

Index is
Semantic sequence
Hamming code group Cs(is)
1
000
{0000000, 1101000}
2
001
{0110100, 1011100}
3
010
{1110010, 0011010}
4
011
{1000110, 0101110}
5
100
{1010001, 0111001}
6
101
{1100101, 0001101}
7
110
{0100011, 1001011}
8
111
{0010111, 1111111}

due to the exponent complexity of MLG decoding algorithm, it is not practical for application. So we should pursuit lower complexity decoding algorithms for the semantic channel codes in the future.

## Xi. Conclusions

In this paper, we develop an information-theoretic framework of semantic communication.

We start from the synonym, a fundamental property of semantic information, to build the semantic information measures including semantic entropy, up/down semantic mutual information, semantic channel capacity, and semantic rate distortion function. Then we extend the asymptotic equipartition property to the semantic sense and introduce the synonymous typical set to prove three significant coding theorems, that is, semantic source coding theorem, semantic channel coding theorem, and semantic rate distortion coding theorem. Additionally, we investigate the semantic information measures in the continuous case and derive the semantic capacity of Gaussian channel and semantic rate distortion of Gaussian source. All these works uncover the critical features of semantic communication and constitute the theoretic basis of semantic information theory.

For the theoretic analysis, the semantic information theory needs further development. In this paper, we only consider the semantic information measure and the fundamental limitation in the discrete or continuous memoryless case. In the future, we can further investigate the measure and limitation of semantic information in various memory source or channel cases, such as stationary and ergodic process (e.g. Markov process) or non-stationary non-ergodic process. Strong asymptotic equipartition property and strong typicality in the semantic sense should be further explored. On the other hand, the analysis of semantic capacity or semantic rate distortion with finite block length may also be an interesting research topic. In addition, in various multiuser communication scenarios, such as multiple access, broadcasting, relay etc., we can further analyze and derive the corresponding measure and performance limit of semantic information.

Guided by the classic information theory, in the past seventy years, the source coding and channel coding techniques have approached the theoretic limitation. On the contrary, the semantic information theory paves a new way for the coding techniques. From the viewpoint of semantic processing, with the help of synonymous mapping, the lossless source coding has much space to improve and the existing coding methods can be further modified and polished. The construction of semantic channel codes may be centered on the group Hamming distance and the optimization of decoding algorithms will be concentrated on the group decoding so that the information transmission techniques will usher in a new era that surpasses the classic limitation and approaches the semantic capacity. By the optimization of synonymous mapping, the classic lossy source coding techniques, such as vector quantization, prediction coding, and transform coding, will demonstrate new advantages to further improve the compression efficiency. Briefly, the performance bottleneck of classic communication will be broken and the traditional communication will naturally evolve to the semantic communication.

For the new coding techniques based on deep learning (DL), the semantic information theory will lift its mystery veil and provide a systematic design and optimization tool. The synonymous mapping will provide a reasonable explanation for the semantic information extracted by the deep neural network. The basic structures of mainstream DL models, such as convolutional neural networks, transformer model, variational auto-encoder and so on, may be analyzed and optimized based on the semantic information measures. Furthermore, the system architecture of semantic communication based on deep learning can be simplified or optimized guided by the semantic information theory.

In summary, the theoretic framework proposed in this paper may help understanding the essential features of semantic information and shed light on some ambiguity problems in semantic communication. We believe that the semantic information theory will uncover a new chapter of information theory and have a profound impact on many fields such as communication, signal detection and estimation, deep learning and machine learning, and integrated sensing and communication etc.

