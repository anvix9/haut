## Abstract

We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for cu-
* Equal contributions.

rating video data.
                   In this paper, we identify and evalu-
ate three different stages for successful training of video
LDMs: text-to-image pretraining, video pretraining, and
high-quality video finetuning.
                               Furthermore, we demon-
strate the necessity of a well-curated pretraining dataset
for generating high-quality videos and present a system-
atic curation process to train a strong base model, includ-
ing captioning and filtering strategies.
                                        We then explore
the impact of finetuning our base model on high-quality
data and train a text-to-video model that is competitive with
closed-source video generation. We also show that our base

model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/ Stability-AI/generative-models.

## 1. Introduction

Driven by advances in generative image modeling with diffusion models [38, 68, 71, 76], there has been significant recent progress on generative video models both in research [9, 42, 82, 95] and real-world applications [54, 74] Broadly, these models are either trained from scratch [41] or finetuned (partially or fully) from pretrained image models with additional temporal layers inserted [9, 32, 43, 82]. Training is often carried out on a mix of image and video datasets [41].

While research around improvements in video modeling has primarily focused on the exact arrangement of the spatial and temporal layers [9, 41, 43, 82], none of the aforementioned works investigate the influence of data selection. This is surprising, especially since the significant impact of the training data distribution on generative models is undisputed [13, 105]. Moreover, for generative image modeling, it is known that pretraining on a large and diverse dataset and finetuning on a smaller but higher quality dataset significantly improves the performance [13, 71]. Since many previous approaches to video modeling have successfully drawn on techniques from the image domain [9, 42, 43], it is noteworthy that the effect of data and training strategies, i.e., the separation of video pretraining at lower resolutions and high-quality finetuning, has yet to be studied. This work directly addresses these previously uncharted territories.

We believe that the significant contribution of data selection is heavily underrepresented in today's video research landscape despite being well-recognized among practitioners when training video models at scale. Thus, in contrast to previous works, we draw on simple latent video diffusion baselines [9] for which we fix architecture and training scheme and assess the effect of *data curation*. To this end, we first identify three different video training stages that we find crucial for good performance: text-to-image pretraining, *video pretraining* on a large dataset at low resolution, and high-resolution *video finetuning* on a much smaller dataset with higher-quality videos. Borrowing from largescale image model training [13, 64, 66], we introduce a systematic approach to curate video data at scale and present an empirical study on the effect of data curation during video pretraining. Our main findings imply that pretraining on well-curated datasets leads to significant performance improvements that persist after high-quality finetuning.

A general motion and multi-view prior Drawing on these findings, we apply our proposed curation scheme to a large video dataset comprising roughly 600 million samples and train a strong pretrained text-to-video base model, which provides a general motion representation. We exploit this and finetune the base model on a smaller, high-quality dataset for high-resolution downstream tasks such as textto-video (see Figure 1, top row) and image-to-video, where we predict a sequence of frames from a single conditioning image (see Figure 1, mid rows). Human preference studies reveal that the resulting model outperforms state-of-the-art image-to-video models.

Furthermore, we also demonstrate that our model provides a strong multi-view prior and can serve as a base to finetune a multi-view diffusion model that generates multiple consistent views of an object in a feedforward manner and outperforms specialized novel view synthesis methods such as Zero123XL [14, 57] and SyncDreamer [58]. Finally, we demonstrate that our model allows for explicit motion control by specifically prompting the temporal layers with motion cues and also via training LoRA- modules [32, 45] on datasets resembling specific motions only, which can be efficiently plugged into the model. To summarize, our core contributions are threefold: (i) We present a systematic data curation workflow to turn a large uncurated video collection into a quality dataset for generative video modeling. Using this workflow, we (ii) train state-of-the-art text-to-video and image-to-video models, outperforming all prior models. Finally, we (iii) probe the strong prior of motion and 3D understanding in our models by conducting domain-specific experiments. Specifically, we provide evidence that pretrained video diffusion models can be turned into strong multi-view generators, which may help overcome the data scarcity typically observed in the 3D domain [14].

## 2. Background

Most recent works on video generation rely on diffusion models [38, 84, 87] to jointly synthesize multiple consistent frames from text- or image-conditioning.

Diffusion models implement an iterative refinement process by learning to gradually denoise a sample from a normal distribution and have been successfully applied to highresolution text-to-image [13, 64, 68, 71, 75] and video synthesis [9, 29, 41, 82, 95].

In this work, we follow this paradigm and train a latent [71, 92] video diffusion model [9, 23] on our video dataset.

We provide a brief overview of related works which utilize latent video diffusion models (Video-LDMs)
in the following paragraph; a full discussion that includes approaches using GANs [10, 30] and autoregressive models [43] can be found in App. B. An introduction to diffusion models can be found in App. D. Latent Video Diffusion Models Video-LDMs [9, 31, 32,
35, 97] train the main generative model in a latent space of reduced computational complexity [22, 71]. Most related works make use of a pretrained text-to-image model and insert temporal mixing layers of various forms [1, 9, 29, 31, 32] into the pretrained architecture. Ge et al. [29] additionally relies on temporally correlated noise to increase temporal consistency and ease the learning task. In this work, we follow the architecture proposed in Blattmann et al. [9] and insert temporal convolution and attention layers after every spatial convolution and attention layer. In contrast to works that only train temporal layers [9, 32] or are completely training-free [52, 114], we finetune the full model. For textto-video synthesis in particular, most works directly condition the model on a text prompt [9, 97] or make use of an additional text-to-image prior [23, 82].

In our work, we follow the former approach and show that the resulting model is a strong general motion prior, which can easily be finetuned into an image-to-video or multi-view synthesis model.

Additionally, we introduce micro-conditioning [64] on frame rate.

We also employ the EDM-framework [51] and significantly shift the noise schedule towards higher noise values, which we find to be essential for high-resolution finetuning. See Section 4 for a detailed discussion of the latter.

Data Curation Pretraining on large-scale datasets [80]
is an essential ingredient for powerful models in several tasks such as discriminative text-image [66, 105] and language [27, 63, 67] modeling.

By leveraging efficient language-image representations such as CLIP [47, 66, 105], data curation has similarly been successfully applied for generative image modeling [13, 64, 80].

However, discussions on such data curation strategies have largely been missing in the video generation literature [41, 43, 82, 94], and processing and filtering strategies have been introduced in an ad-hoc manner. Among the publicly accessible video datasets, WebVid-10M [7] dataset has been a popular choice [9, 82, 115] despite being watermarked and suboptimal in size.

Additionally, WebVid-10M is often used in combination with image data [80], to enable joint image-video training.

However, this amplifies the difficulty of separating the effects of image and video data on the final model. To address these shortcomings, this work presents a systematic study of methods for video data curation and further introduces a general three-stage training strategy for generative video models, producing a state-ofthe-art model.

## 5. Conclusion

We present *Stable Video Diffusion* (SVD), a latent video diffusion model for high-resolution, state-of-the-art text-tovideo and image-to-video synthesis. To construct its pretraining dataset, we conduct a systematic data selection and scaling study, and propose a method to curate vast amounts of video data and turn large and noisy video collection into suitable datasets for generative video models. Furthermore, we introduce three distinct stages of video model training which we separately analyze to assess their impact on the final model performance. *Stable Video Diffusion* provides a powerful video representation from which we finetune video models for state-of-the-art image-to-video synthesis and other highly relevant applications such as LoRAs for camera control. Finally we provide a pioneering study on multi-view finetuning of video diffusion models and show that SVD constitutes a strong 3D prior, which obtains stateof-the-art results in multi-view synthesis while using only a fraction of the compute of previous methods.

We hope these findings will be broadly useful in the generative video modeling literature.

A discussion on our work's broader impact and limitations can be found in App. A.

