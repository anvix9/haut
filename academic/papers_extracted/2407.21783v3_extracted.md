## 1 Introduction

Foundation models are general models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks. They form the basis of many modern AI systems.

The development of modern foundation models consists of two main stages: **(1)** a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next-word prediction or captioning and **(2)** a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities (for example, coding and reasoning).

In this paper, we present a new set of foundation models for language, called **Llama 3**. The Llama 3 Herd of models natively supports multilinguality, coding, reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. Each member of the herd is listed in Table 1. All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity.

We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process:

- **Data.** Compared to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity and
quality of the data we use for pre-training and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data and the development of more rigorous quality assurance and filtering approaches for post-training data. We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2.
- **Scale.** We train a model at far larger scale than previous Llama models: our flagship language model was
pre-trained using 3.8 × 1025 FLOPs, almost 50× more than the largest version of Llama 2. Specifically,
we pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens. As expected per

| Finetuned               | Multilingual   | Long context   | Tool use   |
|-------------------------|----------------|----------------|------------|
| Llama 3 8B              |                |                |            |
| ✗                       | ✗              |                |            |
| 1                       |                |                |            |
| ✗                       | ✗              |                |            |
| April 2024              |                |                |            |
| Llama 3 8B Instruct     |                |                |            |
| ✓                       | ✗              | ✗              | ✗          |
| April 2024              |                |                |            |
| Llama 3 70B             |                |                |            |
| ✗                       | ✗              |                |            |
| 1                       |                |                |            |
| ✗                       | ✗              |                |            |
| April 2024              |                |                |            |
| Llama 3 70B Instruct    |                |                |            |
| ✓                       | ✗              | ✗              | ✗          |
| April 2024              |                |                |            |
| Llama 3.1 8B            |                |                |            |
| ✗                       | ✓              | ✓              | ✗          |
| July 2024               |                |                |            |
| Llama 3.1 8B Instruct   |                |                |            |
| ✓                       | ✓              | ✓              | ✓          |
| July 2024               |                |                |            |
| Llama 3.1 70B           |                |                |            |
| ✗                       | ✓              | ✓              | ✗          |
| July 2024               |                |                |            |
| Llama 3.1 70B Instruct  |                |                |            |
| ✓                       | ✓              | ✓              | ✓          |
| July 2024               |                |                |            |
| Llama 3.1 405B          |                |                |            |
| ✗                       | ✓              | ✓              | ✗          |
| July 2024               |                |                |            |
| Llama 3.1 405B Instruct |                |                |            |
| ✓                       | ✓              | ✓              | ✓          |
| July 2024               |                |                |            |

scaling laws for foundation models, our flagship model outperforms smaller models trained using the same procedure. While our scaling laws suggest our flagship model is an approximately compute-optimal size for our training budget, we also train our smaller models for much longer than is compute-optimal.

The resulting models perform better than compute-optimal models at the same inference budget. We use the flagship model to further improve the quality of those smaller models during post-training.

- **Managing complexity.** We make design choices that seek to maximize our ability to scale the model
development process. For example, we opt for a standard dense Transformer model architecture (Vaswani et al., 2017) with minor adaptations, rather than for a mixture-of-experts model (Shazeer et al., 2017) to maximize training stability. Similarly, we adopt a relatively simple post-training procedure based on supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO; Rafailov et al. (2023)) as opposed to more complex reinforcement learning algorithms (Ouyang et al.,
2022; Schulman et al., 2017) that tend to be less stable and harder to scale.
The result of our work is Llama 3: a herd of three multilingual1 language models with 8B, 70B, and 405B
parameters. We evaluate the performance of Llama 3 on a plethora of benchmark datasets that span a wide range of language understanding tasks. In addition, we perform extensive human evaluations that compare Llama 3 with competing models. An overview of the performance of the flagship Llama 3 model on key benchmarks is presented in Table 2. Our experimental evaluation suggests that our flagship model performs on par with leading language models such as GPT-4 (OpenAI, 2023a) across a variety of tasks, and is close to matching the state-of-the-art. Our smaller models are best-in-class, outperforming alternative models with similar numbers of parameters (Bai et al., 2023; Jiang et al., 2023). Llama 3 also delivers a much better balance between helpfulness and harmlessness than its predecessor (Touvron et al., 2023b). We present a detailed analysis of the safety of Llama 3 in Section 5.4.

We are publicly releasing all three Llama 3 models under an updated version of the Llama 3 Community License;
see https://llama.meta.com. This includes pre-trained and post-trained versions of our 405B parameter language model and a new version of our Llama Guard model (Inan et al., 2023) for input and output safety.

We hope that the open release of a flagship model will spur a wave of innovation in the research community, and accelerate a responsible path towards the development of artificial general intelligence (AGI).

As part of the Llama 3 development process we also develop multimodal extensions to the models, enabling image recognition, video recognition, and speech understanding capabilities. These models are still under active development and not yet ready for release. In addition to our language modeling results, the paper presents results of our initial experiments with those multimodal models.

Llama 3 405B
GPT-4o
Claude 3.5 Sonnet
Llama 3 8B
Gemma 2 9B
Mistral 7B
Llama 3 70B
Mixtral 8x22B
GPT 3.5 Turbo
Nemotron 4 340B
GPT-4 (0125)
Category
Benchmark
General
MMLU (5-shot)
69.4
72.3
61.1
83.6
76.9
70.7
87.3
82.6
85.1
89.1
89.9
MMLU (0-shot, CoT)
73.0
72.3△
60.5
86.0
79.9
69.8
88.6
78.7◁
85.4
88.7
88.3
MMLU-Pro (5-shot, CoT)
48.3
–
36.9
66.4
56.3
49.2
73.3
62.7
64.8
74.0
77.0
IFEval
80.4
73.6
57.6
87.5
72.7
69.9
88.6
85.1
84.3
85.6
88.0
Code
HumanEval (0-shot)
72.6
54.3
40.2
80.5
75.6
68.0
89.0
73.2
86.6
90.2
92.0
MBPP EvalPlus (0-shot)
72.8
71.7
49.5
86.0
78.6
82.0
88.6
72.8
83.6
87.8
90.5
Math
GSM8K (8-shot, CoT)
84.5
76.7
53.2
95.1
88.2
81.6
96.8
92.3♢
94.2
96.1
96.4♢
MATH (0-shot, CoT)
51.9
44.3
13.0
68.0
54.1
43.1
73.8
41.1
64.5
76.6
71.1
Reasoning
ARC Challenge (0-shot)
83.4
87.6
74.2
94.8
88.7
83.7
96.9
94.6
96.4
96.7
96.7
GPQA (0-shot, CoT)
32.8
–
28.8
46.7
33.3
30.8
51.1
–
41.4
53.6
59.4
Tool use
BFCL
76.1
–
60.4
84.8
–
85.9
88.5
86.5
88.3
80.5
90.2
Nexus
38.5
30.0
24.7
56.7
48.5
37.2
58.7
–
50.3
56.1
45.7
Long context
ZeroSCROLLS/QuALITY
81.0
–
–
90.5
–
–
95.2
–
95.2
90.5
90.5
InfiniteBench/En.MC
65.1
–
–
78.2
–
–
83.4
–
72.1
82.5
–
NIH/Multi-needle
98.8
–
–
97.5
–
–
98.1
–
100.0
100.0
90.8
Multilingual
MGSM (0-shot, CoT)
68.9
53.2
29.9
86.9
71.1
51.4
91.6
–
85.9
90.5
91.6

## 2 General Overview

The model architecture of Llama 3 is illustrated in Figure 1. The development of our Llama 3 language models comprises two main stages:

- **Language model pre-training.** We start by converting a large, multilingual text corpus to discrete tokens
and pre-training a large language model (LLM) on the resulting data to perform next-token prediction. In the language model pre-training stage, the model learns the structure of language and obtains large amounts of knowledge about the world from the text it is "reading". To do this effectively, pre-training is performed at massive scale: we pre-train a model with 405B parameters on 15.6T tokens using a context window of 8K tokens. This standard pre-training stage is followed by a continued pre-training stage that increases the supported context window to 128K tokens. See Section 3 for details.
- **Language model post-training.** The pre-trained language model has a rich understanding of language
but it does not yet follow instructions or behave in the way we would expect an assistant to. We align the model with human feedback in several rounds, each of which involves supervised finetuning
(SFT) on instruction tuning data and Direct Preference Optimization (DPO; Rafailov et al., 2024).
At this post-training2 stage, we also integrate new capabilities, such as tool-use, and observe strong
improvements in other areas, such as coding and reasoning. See Section 4 for details. Finally, safety mitigations are also incorporated into the model at the post-training stage, the details of which are described in Section 5.4.
The resulting models have a rich set of capabilities. They can answer questions in at least eight languages, write high-quality code, solve complex reasoning problems, and use tools out-of-the-box or in a zero-shot way.

We also perform experiments in which we add image, video, and speech capabilities to Llama 3 using a compositional approach. The approach we study comprises the three additional stages illustrated in Figure 28:

- **Multi-modal encoder pre-training.** We train separate encoders for images and speech. We train our
image encoder on large amounts of image-text pairs. This teaches the model the relation between visual content and the description of that content in natural language. Our speech encoder is trained using a
self-supervised approach that masks out parts of the speech inputs and tries to reconstruct the masked out parts via a discrete-token representation. As a result, the model learns the structure of speech signals. See Section 7 for details on the image encoder and Section 8 for details on the speech encoder.

- **Vision adapter training.** We train an adapter that integrates the pre-trained image encoder into the
pre-trained language model. The adapter consists of a series of cross-attention layers that feed imageencoder representations into the language model. The adapter is trained on text-image pairs. This aligns the image representations with the language representations. During adapter training, we also update the parameters of the image encoder but we intentionally do not update the language-model parameters. We also train a video adapter on top of the image adapter on paired video-text data. This enables the model to aggregate information across frames. See Section 7 for details.
- **Speech adapter training.** Finally, we integrate the speech encoder into the model via an adapter that
converts speech encodings into token representations that can be fed directly into the finetuned language model. The parameters of the adapter and encoder are jointly updated in a supervised finetuning stage to enable high-quality speech understanding. We do not change the language model during speech adapter training. We also integrate a text-to-speech system. See Section 8 for details.
Our multimodal experiments lead to models that can recognize the content of images and videos, and support interaction via a speech interface. These models are still under development and not yet ready for release.

## 10 Conclusion

In many ways, the development of high-quality foundation models is still in its infancy. Our experience in developing Llama 3 suggests that substantial further improvements of these models are on the horizon.

Throughout the development of the Llama 3 model family, we found that a strong focus on high-quality data, scale, and simplicity consistently yielded the best results. In preliminary experiments, we explored more complex model architectures and training recipes but did not find the benefits of such approaches to outweigh the additional complexity they introduce in model development. Developing a flagship foundation model such as Llama 3 involves overcoming a plethora of deep technical problems but also requires clever organizational decisions. For example, to ensure Llama 3 is not accidentally overfitted on commonly used benchmarks, our pre-training data was procured and processed by a separate team that was strongly incentivized to prevent contamination of that pre-training data with external benchmarks.

As another example, we ensure that our human evaluations remain trustworthy by allowing only a small set of researchers who do not contribute to model development to perform and access these evaluations. While such organizational decisions are rarely discussed in technical papers, we found them to be pivotal to the successful development of the Llama 3 family of models.

We shared the details of our development process because we believe this will: **(1)** help the larger research community understand the key factors of foundation model development and **(2)** contribute to a more informed debate about the future of foundation models in the general public. We also shared preliminary experiments with integrating multimodal capabilities into Llama 3. While these models are still under active development and not yet ready for release, we hope sharing our results early will accelerate research in this direction.

Following the positive outcomes of the detailed safety analyses presented in this paper, we publicly release our Llama 3 language models in order to accelerate the development of AI systems for a plethora of societally relevant use cases and enable the research community to scrutinize our models and identify ways to make these models better and safer. We believe that the public release of foundation models plays a key role in the responsible development of such models, and we hope that the release of Llama 3 encourages the industry to embrace the open, responsible development of AGI.

