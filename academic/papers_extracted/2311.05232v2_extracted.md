## 1 Introduction

Recently, the emergence of large language models (LLMs) [383], exemplified by LLaMA [299, 300], Claude [9], Gemini [7, 259] and GPT-4 [232], has ushered in a significant paradigm shift in natural language processing (NLP), achieving unprecedented progress in language understanding [116, 124], generation [373, 393] and reasoning [57, 151, 250, 326, 354]. Furthermore, the extensive factual knowledge encoded within LLMs has demonstrated considerable advancements in leveraging LLMs for information seeking [6, 246], potentially reshaping the landscape of information retrieval systems [394]. Nevertheless, in tandem with these remarkable advancements, concerns have arisen about the tendency of LLMs to generate hallucinations [15, 105], resulting in seemingly plausible yet factually unsupported content. Further compounding this issue is the capability of LLMs to generate highly convincing and human-like responses [265], which makes detecting these hallucinations particularly challenging, thereby complicating the practical deployment of LLMs, especially realworld information retrieval (IR) systems that have integrated into our daily lives like chatbots
[8, 231], search engines [4, 214], and recommender systems [97, 171]. Given that the information provided by these systems can directly influence decision-making, any misleading information has the potential to spread false beliefs, or even cause harm.

Notably, hallucinations in conventional natural language generation (NLG) tasks have been extensively studied [125, 136], with hallucinations defined as generated content that is either nonsensical or unfaithful to the provided source content. These hallucinations are categorized into two types: *intrinsic hallucination*, where the generated output contradicts the source content, and extrinsic hallucination, where the generated output cannot be verified from the source. However, given their remarkable versatility across tasks [15, 30], understanding hallucinations in LLMs presents a unique challenge compared to models tailored for specific tasks. Besides, as LLMs typically function as open-ended systems, the scope of hallucination encompasses a broader concept, predominantly manifesting factual errors. This shift necessitates a reevaluation and adjustment of the existing taxonomy of hallucinations, aiming to enhance its adaptability in the evolving landscape of LLMs.

In this survey, we propose a redefined taxonomy of hallucination tailored specifically for applications involving LLMs. We categorize hallucination into two primary types: factuality hallucination and faithfulness hallucination. *Factuality hallucination* emphasizes the discrepancy between generated content and verifiable real-world facts, typically manifesting as factual inconsistencies.

Conversely, *faithfulness hallucination* captures the divergence of generated content from user input or the lack of self-consistency within the generated content. This category is further subdivided into instruction inconsistency, where the content deviates from the user's original instruction; context inconsistency, highlighting discrepancies from the provided context; and logical inconsistency, pointing out internal contradictions within the content. Such categorization refines our understanding of hallucinations in LLMs, aligning it closely with their contemporary usage.

Delving into the underlying causes of hallucinations in LLMs is essential not merely for enhancing the comprehension of these phenomena but also for informing strategies aimed at alleviating them. Recognizing the multifaceted sources of LLM hallucinations, our survey identifies potential contributors into three main aspects: data, training, and inference stages. This categorization allows us to span a broad spectrum of factors, providing a holistic view of the origins and mechanisms by which hallucinations may arise within LLM systems. Furthermore, we comprehensively outline a variety of effective detection methods specifically devised for detecting hallucinations in LLMs, as well as an exhaustive overview of benchmarks related to LLM hallucinations, serving as appropriate testbeds to assess the extent of hallucinations generated by LLMs and the efficacy of detection methods. Beyond evaluation, significant efforts have been undertaken to mitigate hallucinations of LLMs. These initiatives are comprehensively surveyed in our study, in accordance with the corresponding causes, spanning from data-related, training-related, and inference-related approaches. In addition, the effectiveness of retrieval-augmented generation (RAG) in mitigating hallucinations has garnered tremendous attention within the field. Despite the considerable potential of RAG, current systems inherently face limitations and even suffer from hallucinations. Accordingly, our survey undertakes an in-depth analysis of these challenges, aiming to provide valuable insights aimed at developing more robust RAG systems. We also highlight several promising avenues for future research, such as hallucinations in large vision-language models and understanding of knowledge boundaries in LLM hallucinations, paving the way for forthcoming research in the field.

Comparing with Existing Surveys. As hallucination stands out as a major challenge in generative AI, numerous research [136, 192, 258, 298, 312, 376] has been directed towards hallucinations.

While these contributions have explored LLM hallucination from various perspectives and provided valuable insights, our survey seeks to delineate their distinct contributions and the comprehensive scope they encompass. Ji et al. [136] primarily shed light on hallucinations in pre-trained models for NLG tasks, leaving LLMs outside their discussion purview. Tonmoy et al. [298] mainly focused on discussing the mitigation strategies combating LLM hallucinations. Besides, Liu et al. [192] took a broader view of LLM trustworthiness without delving into specific hallucination phenomena, whereas Wang et al. [312] provided an in-depth look at factuality in LLMs. However, our work narrows down to a critical subset of trustworthiness challenges, specifically addressing factuality and extending the discussion to include faithfulness hallucinations. To the best of our knowledge, Zhang et al. [376] presented research closely aligned with ours, detailing LLM hallucination taxonomies, evaluation benchmarks, and mitigation strategies. However, our survey sets itself apart through a unique taxonomy and organizational structure. We present a detailed, layered classification of hallucinations and conduct a more comprehensive analysis of the causes of hallucinations. Crucially, our proposed mitigation strategies are directly tied to these causes, offering a targeted and coherent framework for addressing LLM hallucinations.

Organization of this Survey. In this survey, we present a comprehensive overview of the latest developments in LLM hallucinations, as shown in Fig 1. We commence by constructing a taxonomy of hallucinations in the realm of LLM (§2). Subsequently, we analyze factors contributing to LLM
hallucinations in depth (§3), followed by a review of various strategies and benchmarks employed for the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches designed to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by current RAG systems (§6) and delineate potential pathways for forthcoming research (§7).

2
DEFINITIONS
For the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a succinct introduction to LLMs (§2.1), delineating the scope of this survey. Subsequently, we delve into the training stages of LLMs (§2.2), as a thorough understanding of the training mechanisms contributes significantly to elucidating the origins of hallucinations. Lastly, we expound upon the concept of hallucinations in LLMs (§2.3), further categorizing it into two distinct types. 2.1
Large Language Models Before delving into the causes of hallucination, we first introduce the concept of LLMs. Typically, LLMs refer to a series of general-purpose models that leverage the transformer-based language model architecture and undergo extensive training on massive textual corpora with notable examples including GPT-3 [29], PaLM [54], LLaMA [300], GPT-4 [232] and Gemini [259]. By scaling the amount of data and model capacity, LLMs raise amazing emergent abilities, typically including in-context learning (ICL) [29], chain-of-thought prompting [326] and instruction following [244].

Misinformation and
e.g. Bender et al. [20], Lee et al. [159], Lin et al. [182]
Knowledge Boundary
e.g. Katz et al. [149], Onoe et al. [230], Singhal et al. [279]
Inferior Alignment Data
e.g. Gekhman et al. [98], Li et al. [168]
Hallucination from Pre-training
e.g. Li et al. [180], Liu et al. [183], Wang and Sennrich [313]
Hallucination from SFT
e.g. Schulman [269], Yang et al. [341], Zhang et al. [362]
Hallucination from RLHF
e.g. Cotra [64], Perez et al. [245], Sharma et al. [274], Wei et al. [327]
Imperfect Decoding Strategies
e.g. Holtzman et al. [118], Stahlberg and Byrne [283]
Over-confidence
e.g. Chen et al. [45, 46], Liu et al. [193], Miao et al. [212]
Softmax Bottleneck
e.g. Chang and McCallum [38], Miao et al. [212]
Reasoning Failure
e.g. Berglund et al. [22], Zheng et al. [386] e.g. Dhuliawala et al. [74], Manakul et al. [205], Min et al. [216] e.g. Fabbri et al. [80], Maynez et al. [208], Scialom et al. [271] e.g. TruthfulQA [182], HalluQA [49], HaluEval-2.0 [168] e.g. SelfCheckGPT-Wikibio [213], HaluEval [169], FELM [42]
Data Filtering
e.g. Abbas et al. [1], Gunasekar et al. [107], Touvron et al. [300] e.g. Dai et al. [67], Huang et al. [127], Mitchell et al. [219] e.g. Gao et al. [94], Ram et al. [255], Yu et al. [358] e.g. Li et al. [180], Liu et al. [183, 189], Shi et al. [276] e.g. Rimsky [264], Sharma et al. [274], Wei et al. [327] e.g. Chuang et al. [59], Lee et al. [160], Li et al. [172] e.g. Chang et al. [36], Shi et al. [275], Wan et al. [309]

2.2
Training Stages of Large Language Models The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities.

2.2.1
Pre-training. Pre-training is widely acknowledged as a foundational stage for LLM to acquire knowledge and capabilities [388]. During this phase, LLMs engage in autoregressive prediction of subsequent tokens within sequences. Through self-supervised training on extensive textual corpora, LLMs acquire knowledge of language syntax, world knowledge, and reasoning abilities, thereby laying a solid groundwork for further fine-tuning. Besides, recent research [72, 291] suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of LLMs lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world.

2.2.2
Supervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pre-training primarily optimizes for completion.

Consequently, pre-trained LLMs fundamentally serve as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses. To bridge this gap, SFT [370] has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies [60, 129]
have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities.

2.2.3
Reinforcement Learning from Human Feedback. While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences.

Among various methods that utilize human feedback, RLHF stands out as an representative solution for aligning with human preferences through reinforcement learning [55, 233, 285]. Typically, RLHF employs a preference model [26] trained to predict preference rankings given a prompt alongside a pair of human-labeled responses. To align with human preferences, RLHF optimizes the LLM to generate outputs that maximize the reward provided by the trained preference model, typically employing a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) [270].

Such integration of human feedback into the training loop has proven effective in enhancing the alignment of LLMs, guiding them toward producing high-quality and harmless responses.

2.3
Hallucinations in Large Language Models The concept of hallucination traces its roots to the fields of pathology and psychology and is defined as *the perception of an entity or event that is absent in reality* [202]. Within the realm of NLP, hallucination is typically referred to as a phenomenon in which the generated content appears nonsensical or unfaithful to the provided source content [89, 208]. This concept bears a loose resemblance to the phenomenon of hallucination observed in human psychology. Generally, hallucinations in natural language generation tasks can be categorized into two primary types:
intrinsic hallucination and *extrinsic hallucination* [126, 136, 174].

Specifically, *intrinsic hallucinations* pertain to the model outputs that directly conflict with the provided source context. On the other hand, *extrinsic hallucinations* involve outputs that cannot be verified using the provided source context or external knowledge bases. This means the generated text is neither supported by nor directly contradicts the available information, rendering the output unverifiable and potentially misleading.

However, in the era of LLMs, the versatile capabilities of these models have facilitated their widespread use across diverse fields, highlighting limitations in existing task-specific categorization paradigms. Considering that LLMs place a significant emphasis on user-centric interactions and prioritize alignment with user directives, coupled with the fact that their hallucinations predominantly surface at factual levels, we introduce a more granular taxonomy building upon the foundational work by Ji et al. [136]. This refined taxonomy seeks to encapsulate the distinct intricacies associated with LLM hallucinations. To provide a more intuitive illustration of our definition of LLM
hallucination, we present examples for each type of hallucination in Table 1, namely factuality hallucination and *faithfulness hallucination*.

2.3.1
Factuality Hallucination. The emergence of LLMs marks a significant shift from traditional task-specific toolkits to AI assistants that have a heightened focus on open-domain interactions.

output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM hallucination.

.
Category
Type
Example Dialog
Explanation
Instruction: What are Thomas Edison's main contributions
to science and technology?
Factual Contradiction
The response is factually incorrect. In reality, Edison
improved the light bulb, building on earlier designs by others, and **Alexander Graham Bell inven**-
ted the telephone.
Response: Thomas Edison developed the first practical telephone and invented the light bulb.
Instruction: What are the major environmental impacts of
the construction of the Eiffel Tower?
Factuality Hallucination
Factual Fabrication
Response: The construction of the Eiffel Tower in 1889 led
to the extinction of the Parisian tiger, a species that played a crucial role in the region's ecosystem. Additionally, it is widely recognized as the event that sparked the global green architecture movement.
The response erroneously states that the construction of the Eiffel Tower led to the extinction of the 'Parisian tiger' and sparked the global green architecture movement. In fact, there is no evidence of a 'Parisian tiger' ever existing, making this a fabricated claim.
Moreover, attributing the origins of green architecture to the Eiffel Tower is an exaggeration, as this
movement has diverse roots and cannot be traced back to a single event.
Instruction: Translate the English question into Spanish:
"What is the capital of France?"
Instruction
Inconsistency
Response: The capital of France is Paris.
The user explicitly provided an instruction to translate
the English question into Spanish. Instead of performing the translation as instructed, the LLM chose to answer the question.
Faithfulness Hallucination
Retrieved context: The Nile is a major north-flowing river
in northeastern Africa. It's traditionally considered the longest river in the world. Originating in the Great Lakes region of central Africa, it flows through several countries before emptying into the Mediterranean Sea.
Context Inconsistency
The summary incorrectly states that the Nile originates from the "mountain ranges of central Africa", while the paragraph mentions that the Nile originates in the
Great Lakes region of central Africa.
Instruction: Summarize the retrieval document above.
Response: The Nile originates from the mountain ranges
of central Africa and flows into the Mediterranean.
Instruction: Solve the equation 2x + 3 = 11 step by step.
Logical Inconsistency

The first step correctly isolates the term with x to get
2x = 8. However, the next step inaccurately divides 8 by 2 to yield a result of x = 3, which is inconsistent with the earlier reasoning.

of the equation is x=3.

This shift is primarily attributed to their vast parametric factual knowledge. However, existing LLMs occasionally exhibit tendencies to produce outputs that are either inconsistent with real-world facts or unverifiable [168], posing challenges to the trustworthiness of artificial intelligence. In this context, we categorize these factuality hallucinations into two primary types:
Factual Contradiction refers to situations where the LLM's output contains facts that can be grounded in real-world information, but present contradictions. This type of hallucination occurs most frequently and arises from diverse sources, encompassing the LLM's capture, storage, and expression of factual knowledge. Depending on the error type of contradictions, it can be further divided into two subcategories: *entity-error hallucination* and *relation-error hallucination*.

- **Entity-error hallucination** refers to the situations where the generated text of LLMs
contains erroneous entities. As shown in Table 1, when asked about "the inventor of the
telephone", the model erroneously states *"Thomas Edison"*, conflicting with the real fact that it
was *"Alexander Graham Bell"*.
- **Relation-error hallucination** refers to instances where the generated text of LLMs contains
wrong relations between entities. As shown in Table 1, when inquired about "the inventor of
the light bulb", the model incorrectly claims *"Thomas Edison"*, despite the fact that he improved
upon existing designs and did not invent it.
Factual Fabrication refers to instances where the LLM's output contains facts that are unverifiable against established real-world knowledge. This can be further divided into unverifiability hallucination and *overclaim hallucination*.

- **Unverifiability hallucination** pertains to statements that are entirely non-existent or
cannot be verified using available sources. As shown in Table 1, when asked about "the major environmental impacts of the construction of the Eiffel Tower", the model incorrectly states that
"the construction led to the extinction of the Parisian tiger", a species that does not exist and
thus, this claim cannot be substantiated by any historical or biological record.
- **Overclaim hallucination** involves claims that lack universal validity due to subjective biases.
As shown in Table 1, the model claims that "the Eiffel Tower's construction is widely recognized
as the event that sparked the global green architecture movement." This is an overclaim, as
there is no broad consensus or substantial evidence to support the statement.
2.3.2
Faithfulness Hallucination. LLMs are inherently trained to align with user instructions. As the use of LLMs shifts towards more user-centric applications, ensuring their consistency with user-provided instructions and contextual information becomes increasingly vital. Furthermore, LLM's faithfulness is also reflected in the logical consistency of its generated content. From this perspective, we categorize three subtypes of faithfulness hallucinations:
Instruction inconsistency refers to the LLM's outputs that deviate from a user's directive.

While some deviations might serve safety guidelines, the inconsistencies here signify unintentional misalignment with non-malicious user instructions. As described in Table 1, the user's actual intention is translation, However, the LLM erroneously deviated from the user's instruction and performed a question-answering task instead.

Context inconsistency points to instances where the LLM's output is unfaithful with the user's provided contextual information. For example, as shown in Table 1, the user mentioned the Nile's source being in the Great Lakes region of central Africa, yet the LLM's response contradicted the context.

Logical inconsistency underscores when LLM outputs exhibit internal logical contradictions, often observed in reasoning tasks. This manifests as inconsistency both among the reasoning steps themselves and between the steps and the final answer. For example, as shown in Table 1, while the reasoning step of dividing both sides of the equation by 2 is correct, the final answer of x=4 is inconsistent with the reasoning chain, leading to an incorrect result.

3
HALLUCINATION CAUSES
LLM hallucinations have multifaceted origins, spanning the entire spectrum of LLMs' capability acquisition process. In this section, we delve into the root causes of hallucinations in LLMs, primarily categorized into three key aspects: (1) *Data* (§3.1), (2) *Training* (§3.2), and (3) *Inference* (§3.3).

