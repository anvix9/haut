## Abstract

We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, ZEPHYR-
7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation.

In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHF-
based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.

## 1 Introduction

Smaller, open large language models (LLMs) have greatly increased in ability in recent years, from early GPT-2-like models (Wang & Komatsuzaki, 2021) to accurate and compact models (Touvron et al., 2023; Penedo et al., 2023; Jiang et al., 2023) that are trained on significantly more tokens than the "compute-optimal" amount suggested by the Chincilla scaling laws (De Vries, 2023). In addition, researchers have shown that these models can be further trained through distilled supervised fine-tuning (dSFT) based on proprietary models to increase their accuracy (Taori et al., 2023). In this approach, the output of a more capable teacher model is used as supervised data for the student model. Distillation has proven to be an effective tool for improving open models on a range of different tasks (Chiang et al., 2023); however, it does not reach the performance of the teacher models (Gudibande et al., 2023). Users have noted that these models are not "intent aligned", i.e. they do not behave in a manner that aligns with human users' preferences. This property often leads to outputs that do not provide correct responses to queries.

Intention alignment has been difficult to quantify, but recent work has led to the development of benchmarks like MT-Bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023) that specifically target this behavior. These benchmarks yield scores that correlate closely with human ratings of model outputs and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation. This motivates careful collection of human feedback for alignment, often at enormous cost at scale, such as in LLAMA2-CHAT (Touvron et al., 2023).

In this work, we consider the problem of aligning a small open LLM entirely through distillation. The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective (Rafailov et al., 2023). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling compared to using other approaches like proximal preference optimization (PPO) (Schulman et al., 2017). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB). To validate this approach, we construct ZEPHYR-7B, an aligned version of Mistral-7B (Jiang et al., 2023). We first use dSFT, based on the UltraChat (Ding et al., 2023) dataset. Next we use the AI feedback data collected in the UltraFeedback dataset (Cui et al., 2023). Finally, we apply dDPO based on this feedback data. Experiments show that this 7B parameter model can achieve performance comparable to 70B-parameter chat models aligned with human feedback. Results show improvements both in terms of standard academic benchmarks as well as benchmarks that take into account conversational capabilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available at https://github.com/huggingface/alignment-handbook.

We note an important caveat for these results. We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice (Bai et al., 2022). As distillation only works with the output of publicly available models this is technically more challenging to do because of added challenges in curating that type of synthetic data, and is an important subject for future work.

## 3 Method

The goal of this work is to align an open-source large-language model to the intent of the user.

Throughout the work we assume access to a larger teacher model πT which can be queried by prompted generation. Our goal is be to produce a student model πθ and our approach follows similar stages as InstructGPT (Ouyang et al., 2022) as shown in Figure 2.

Distilled Supervised Fine-Tuning (dSFT)
Starting with a raw LLM, we first need to train it to respond to user prompts. This step is traditionally done through supervised fine tuning (SFT) on a dataset of high-quality instructions and responses (Chung et al., 2022; Sanh et al., 2021). Given access to a teacher language models, we can instead have the model generate instructions and responses (Taori et al., 2023), and train the model directly on these. We refer to this as distilled SFT (dSFT).

Approaches to dFFT follow the self-instr protocol (Wang et al., 2023). Let $x_{1}^{0},\ldots,x_{n}^{0}$ be a set of seed prompts, constructed to represent a diverse set of logical domains. A dataset is constructed through iterative self-propping where the teacher is used to both respond to an instruction and refine the instruction based on the response. For each $x^{0}$, we first sample response $y^{0}\sim\pi_{\Gamma}(\cdot|x^{0})$, and then refine by sampling a new instruction (using a prompt for refinement), $x^{1}\sim\pi_{\Gamma}(\cdot|x^{0},y^{0})$. The end point is a final dataset, $\mathcal{C}=\{(x_{1},\,y_{1}),\ldots,(x_{J},\,y_{J})\}$. Distillation is performed by $\mathsf{SFT}$,

$$\pi_{\mathsf{dFT}}=\max_{\pi}\underset{(x,y)\sim\mathcal{C}}{\mathbb{E}}\log\pi(y|x)$$
AI Feedback through Preferences (AIF)
Human feedback (HF) can provide additional signal to align LLMs. Human feedback is typically given through preferences on the quality of LLM responses (Ouyang et al., 2022). For distillation, we instead use AI preferences from the teacher model on generated outputs from other models. We follow the approach of UltraFeedback (Cui et al., 2023) which uses the teacher to provide preferences on model outputs. As with SFT, the system starts with a set of prompts x1*, . . . , x*J. Each prompt x is fed to a collection of four models π1*, . . . , π*4, e.g. Claude, Falcon, Llama, etc, each of which yield a response y1 ∼π1(·|x)*, . . . , y*4 ∼π4(·|x). These responses are then fed to the teacher model, e.g. GPT-4, which gives a score for the response s1 ∼πT (·|x, y1), . . . , s4 ∼πT (·|*x, y*4).

After collecting the scores for a prompt x, we save the highest scoring response as yw and a random lower scoring prompt as yl. The final feedback dataset D consists of a set of these triples (x, yw, yl).

Distilled Direct Preference Optimization (dDPO)
The goal of the final step is to refine the πdSFT
by maximizing the likelihood of ranking the preferred yw over yl in a preference model. The preference model is determined by a reward function rθ(*x, y*) which utilizes the student language model
πθ. Past work using AI feedback has primarily focused on using RL methods such as proximal policy optimization (PPO) to optimize θ with respect to this reward. These approaches optimize θ
by first training the reward and then sampling from the current policy to compute updates. Direct preference optimization (DPO) uses a simpler approach to directly optimize the preference model from the static data (Rafailov et al., 2023). The key observation is to derive the optimal reward function in terms of the optimal LLM policy π∗and the original LLM policy πdSFT. Under an appropriate choice of preference model they show, for constant β and partition function Z that,

$$r^{*}(x,y)=\beta{\frac{\pi_{*}(y|x)}{\pi_{\mathrm{dsfT}}(y|x)}}+\beta\log Z(x)$$
By plugging this function of the reward into the preference model, the authors show that the objective can be written as,

$$\pi_{\theta}=\max_{\pi}\underset{(x,y_{w},y_{l})}{\mathbb{E}}\log\sigma\left(\beta\log\frac{\pi(y_{w}|x)}{\pi_{\text{dSFT}}(y_{w}|x)}-\beta\log\frac{\pi(y_{l}|x)}{\pi_{\text{dSFT}}(y_{l}|x)}\right).\tag{1}$$
While this term looks complex, we note that it implies a simple training procedure. Starting with the dSFT version of the model, we iterate through each AIF triple (x, yw, yl).

1. Compute the probability for (*x, y*w) and (*x, y*l) from the dSFT model (forward-only).
2. Compute the probability for (*x, y*w) and (*x, y*l) from the dDPO model.
3. Compute Eq 1 and backpropagate to update. Repeat.

