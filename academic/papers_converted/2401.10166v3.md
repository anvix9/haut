4
2
0
2

y
a
M
6
2

]

V
C
.
s
c
[

3
v
6
6
1
0
1
.
1
0
4
2
:
v
i
X
r
a

VMamba: Visual State Space Model

Yue Liu
UCAS
liuyue171@mails.ucas.ac.cn

Yunjie Tian
UCAS
tianyunjie19@mails.ucas.ac.cn

Yuzhong Zhao
UCAS
zhaoyuzhong20@mails.ucas.ac.cn

Hongtian Yu
UCAS
yuhongtian17@mails.ucas.ac.cn

Lingxi Xie
Huawei Inc.
198808xc@gmail.com

Yaowei Wang
Pengcheng Lab.
wangyw@pcl.ac.cn

Qixiang Ye
UCAS
qxye@ucas.ac.cn

Yunfan Liu
UCAS
yunfan.liu@ucas.ac.cn

Abstract

Designing computationally efficient network architectures persists as an ongoing
necessity in computer vision. In this paper, we transplant Mamba, a state-space lan-
guage model, into VMamba, a vision backbone that works in linear time complexity.
At the core of VMamba lies a stack of Visual State-Space (VSS) blocks with the
2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D
helps bridge the gap between the ordered nature of 1D selective scan and the non-
sequential structure of 2D vision data, which facilitates the gathering of contextual
information from various sources and perspectives. Based on the VSS blocks, we
develop a family of VMamba architectures and accelerate them through a succes-
sion of architectural and implementation enhancements. Extensive experiments
showcase VMamba‚Äôs promising performance across diverse visual perception tasks,
highlighting its advantages in input scaling efficiency compared to existing bench-
mark models. Source code is available at https://github.com/MzeroMiko/VMamba.

1

Introduction

Visual representation learning stands as a fundamental research area in computer vision, which has
witnessed remarkable progress in the era of deep learning. To represent complex patterns in vision
data, two primary categories of backbone networks, i.e., Convolution Neural Networks (CNNs) [49,
28, 30, 54, 38] and Vision Transformers (ViTs) [13, 37, 58, 68], have been proposed and extensively
utilized in a variety of visual tasks. Compared to CNNs, ViTs generally demonstrate superior learning
capabilities on large-scale data due to the integration of the self-attention mechanism [59, 13].
However, the quadratic complexity of self-attention w.r.t. the number of tokens introduces substantial
computational overhead in downstream tasks involving large spatial resolutions.

To tackle this challenge, considerable efforts have been made to enhance the efficiency of attention
computation [55, 37, 12]. However, existing approaches either impose limitations on the size of
the effective receptive field [37] or experience evident performance degradation across diverse

Preprint. Under review.

Figure 1: Comparison of correlation establishment between image patches via (a) self-attention and
(b) the proposed 2D-Selective-Scan (SS2D). Red boxes indicate the query image patch, with patch
opacity representing the degree of information loss.

tasks [31, 62]. This motivates us to develop a novel architecture for vision data, preserving the
inherent advantages of the vanilla self-attention mechanism, i.e., global receptive fields and dynamic
weighting parameters [23].

Recently, Mamba [17], a novel State Space Model (SSM) [17, 44, 61] in the field natural language
processing (NLP), has emerged as a highly promising approach for long sequence modeling with
linear complexity. Drawing inspiration from this advancement, we introduce VMamba, a vision
backbone integrating SSM-based blocks to facilitate efficient visual representation learning. However,
the core algorithm of Mamba, i.e., the parallelized selective scan operation, is essentially designed
for processing one-dimensional sequential data. This poses a challenge when attempting to adapt it
for processing vision data, which inherently lacks a sequential arrangement of visual components. To
address this issue, we propose 2D Selective Scan (SS2D), a four-way scanning mechanism tailored
for spatial domain traversal. In contrast to the self-attention mechanism (Figure 1 (a)), SS2D ensures
that each image patch gains contextual knowledge exclusively through a compressed hidden state
computed along the corresponding scanning path (Figure 1 (b)), thereby reducing the computational
complexity from quadratic to linear.

Upon the VSS blocks, we develop a family of VMamba architectures (i.e., VMamba-Tiny/Small/Base)
and accelerate them through a series of architectural enhancements and implementation optimiza-
tions. Compared to benchmark vision models built on CNN (ConvNeXt [38]), ViT (Swin [37]
and HiViT [68]), and SSM (S4ND [45] and Vim [71]), VMamba consistently achieves superior
image classification accuracy on ImageNet-1K [9] across model scales. Specifically, VMamba-Base
achieves a top-1 accuracy of 83.9%, surpassing Swin by +0.4%, with a throughput exceeding that of
Swin by a substantial margin over 40% (646 vs. 458). The superiority of VMamba extends across
various downstream tasks, with VMamba-Tiny/Small/Base achieving 47.3%/48.7%/49.2% mAP in
object detection on COCO [34] (1√ó training schedule). This outperforms Swin by 4.6%/3.9%/2.3%
and ConvNeXt by 3.1%/3.3%/2.2%, respectively. As for single-scale semantic segmentation on
ADE20K [70], VMamba-Tiny/Small/Base achieves 47.9%/50.6%/51.0% mIoU, which surpasses
Swin by 3.4%/3.0%/2.9% and ConvNeXt by 1.9%/1.9%/1.9%, respectively. Furthermore, unlike
ViT-based models, which experience quadratic growth in computational complexity with the number
of input tokens, VMamba exhibits linear growth in FLOPs while maintaining comparable performance.
This underscores its state-of-the-art input scalability.

The contributions of this study are summarized as follows:

‚Ä¢ We propose VMamba, an SSM-based vision backbone network for visual representation
learning with linear time complexity. A series of improvements in architectural design and
implementation details are adopted to improve the inference speed of VMamba.

‚Ä¢ We introduce 2D Selective Scan (SS2D) to bridge the gap between 1D array scanning and

2D plane traversal, facilitating the extension of selective SSM to process vision data.

‚Ä¢ Without bells and whistles, VMamba demonstrates promising performance across a range
of visual tasks, including image classification, object detection, and semantic segmentation.
It also exhibits remarkable adaptability w.r.t. the length of the input sequence, showcasing
linear growth in computational complexity.

2

(a) Self-Attention(b) 2D-Selective-Scan (SS2D)2 Related Work

Convolutional Neural Networks (CNNs). Starting from AlexNet [32], significant efforts have
been dedicated to enhancing the modeling capability [49, 53, 28, 30] and computational efficiency [29,
54, 66, 47] of CNN-based models across a spectrum of visual tasks. More sophisticated operators,
such as the depth-wise convolution [29] and deformable convolution [5, 72], have been proposed for
increasing the flexibility and efficacy of CNNs. Recently, drawing inspiration from the success of
Transformers [60], modern CNNs [38] have demonstrated promising performance by incorporating
long-range dependencies [11, 48, 35] and dynamic weights [24] into their architectural design.

Vision Transformers (ViTs). As the most representative pioneering work, ViT [13] explores the
effectiveness of vision models based on vanilla Transformer architecture and reveals the significance
of large-scale pre-training in improving the performance in image classification. To mitigate ViT‚Äôs
reliance on extremely large datasets, DeiT [58] introduces a teacher-student distillation strategy to
transfer knowledge encapsulated in CNN models to ViTs, showcasing the importance of the inductive
bias in visual perception. Following this line of thought, subsequent studies propose hierarchical
ViTs [37, 12, 63, 40, 68, 56, 6, 10, 69, 1].

Another line of research focuses specifically on enhancing the computational efficiency of the self-
attention mechanism, which serves as the cornerstone of ViTs. By formulating self-attention as the
linear dot-product of kernel feature maps, Linear Attention [31] leverages the associativity property
of matrix products to reduce the computational complexity from quadratic to linear. GLA [67]
further introduces a hardware-efficient variant of linear attention, which balances memory movement
against parallelizability. RWKV [46] also leverages the linear attention mechanism to combine
the parallelizable training of transformers with the efficient inference of recurrent neural networks
(RNNs). RetNet [52] integrates an additional gating mechanism to enable a parallelizable computation
path as an alternative to recurrence, and RMT [15] extends it for visual representation learning by
incorporating the temporal decay mechanism into the spatial domain.

State Space Models (SSMs).
Despite the widespread adoption of ViT architectures in vision
tasks, the quadratic complexity of self-attention presents a significant challenge when dealing with
long input sequences, (e.g., high-resolution images). Among the attempts to improve the scaling
efficiency [8, 7, 46, 52, 42], SSMs have emerged as compelling alternatives to Transformers, attracting
significant attention from the community. Gu et al. [21] demonstrated the potential of SSM-based
models in handling the long-range dependency with the HiPPO initialization [18]. To improve
the practical feasibility, S4 [20] proposed to normalize the parameter matrices into a diagonal
structure. Subsequently, various structured SSM models have emerged, each introducing different
architectural enhancements. These include models with complex-diagonal structures [22, 19], support
for multiple-input multiple-output [50], decomposition of diagonal plus low-rank operations [25], and
selection mechanisms [17]. These advancements have also been integrated into larger representation
models [44, 42, 16], further demonstrating the versatility and scalability of structured state space
models in various applications. While these models primarily focus on applying SSMs to long-range
and sequential data like text and speech, there has been limited exploration of SSMs‚Äô application to
vision data with two-dimensional structures.

3 Preliminaries

Formulation of SSMs. Originating from the Kalman filter [33], SSMs can be regarded as linear
time-invariant (LTI) systems that map the input stimulation u(t) ‚àà R to response y(t) ‚àà R through
the hidden state h(t) ‚àà RN . Concretely, continuous-time SSMs can be formulated as linear ordinary
differential equations (ODEs) as follows,

h‚Ä≤(t) = Ah(t) + Bu(t),
y(t) = Ch(t) + Du(t),

(1)

where A ‚àà RN √óN , B ‚àà RN √ó1, C ‚àà R1√óN , and D ‚àà R1 are the weighting parameters.

Discretization of SSM. To be integrated into deep models, continuous-time SSMs must undergo
discretization in advance. Concretely, consider the time interval [ta, tb],the analytic solution to the

3

hidden state variable h(t)|t=tb can be expressed as

h(tb) = eA(tb‚àíta)h(ta) + eA(tb‚àíta)

(cid:90) tb

ta

B(œÑ )u(œÑ )e‚àíA(œÑ ‚àíta) dœÑ.

By sampling with the time-scale parameter ‚àÜ (i.e., dœÑ |ti+1

ti = ‚àÜi), h(tb) can be discretized by

hb = eA(‚àÜa+...+‚àÜb‚àí1)

ha +

(cid:32)

Biuie‚àíA(‚àÜa+...+‚àÜi)‚àÜi

,

(cid:33)

b‚àí1
(cid:88)

i=a

(2)

(3)

where [a, b] is the corresponding discrete step interval. Notably, this formulation approximates the
result obtained by the zero-order hold (ZOH) method, which is frequently utilized in the literature of
SSM-based models (please refer to Appendix A for detailed proof).

Selective Scan Mechanism. To tackle the limitation of LTI SSMs (Eq. 1) in capturing the contextual
information, Gu et al. [17] propose a novel parameterization method for SSMs that integrates an
input-dependent selection mechanism (referred to as S6). However, in the case of selective SSMs, the
time-varying weighting parameters present a challenge for efficient computation of hidden states,
as convolutions do not accommodate dynamic weights and are consequently rendered inapplicable.
Nevertheless, as the recurrence relation of hb in Eq. 3 can be derived, the response yb can still be
efficiently computed using associative scan algorithms [2, 43, 51] with linear complexity (the detailed
explanation is deferred to Appendix B).

4 VMamba: Visual State Space Model

4.1 Network Architecture

We develop VMamba at three scales: VMamba-Tiny, VMamba-Small, and VMamba-Base (referred
to as VMamba-T, VMamba-S, and VMamba-B, respectively). An overview of the architecture of
VMamba-T is illustrated in Figure 3 (a), and detailed configurations are provided in Appendix E.
The input image I ‚àà RH√óW √ó3 is first partitioned into patches by a stem module, resulting in a 2D
feature map with the spatial dimension of H/4 √ó W/4. Subsequently, multiple network stages are
employed to create hierarchical representations with resolutions of H/8 √ó W/8, H/16 √ó W/16, and
H/32 √ó W/32. Each stage comprises a down-sampling layer (except for the first stage) followed by
a stack of Visual State Space (VSS) blocks.

The VSS blocks serve as the visual counterpart to Mamba blocks [17] (Figure 3 (b)) for representation
learning. The initial architecture of VSS blocks (referred to as the ‚Äòvanilla VSS Block‚Äô in Figure 3
(c)) is formulated by substituting the S6 module, which stands as the core of Mamba in concurrently
achieving global receptive fields, dynamic weights (i.e., selectivity), and linear complexity, with the
newly proposed 2D-Selective-Scan (SS2D) module (to be introduced in the following subsection).
To further enhance computational efficiency, we eliminate the entire multiplicative branch (encircled
by the red box in Figure 3 (c)), as the effect of the gating mechanism has been achieved by the
selectivity of SS2D. Consequently, the resulting VSS block (depicted in Figure 3 (d)) consists of a
single network branch with two residual modules, mimicking the architecture of a vanilla Transformer
block [60]. Throughout this paper, all results are obtained using VMamba models built with VSS
blocks in this architecture.

4.2

2D-Selective-Scan for Vision Data (SS2D)

While the sequential nature of the scanning operation in S6 aligns well with NLP tasks involving
temporal data, it poses a significant challenge when applied to vision data, which is inherently non-
sequential and encompasses spatial information (e.g., local texture and global structure). To address
this issue, S4ND [45] reformulates SSM with convolutional operations, directly extending the kernel
from 1D to 2D through outer-product. However, such modification prevents the weights from being
input-independent, resulting in a limited capacity for capturing contextual information. Therefore, we
adhere to the selective scan approach [17] for input processing, and propose the 2D-Selective-Scan
(SS2D) module to adapt S6 to vision data without compromising its advantages.

4

Figure 2: Illustration of 2D-Selective-Scan (SS2D). Input patches are traversed along four different
scanning paths (Cross-Scan), and each sequence is independently processed by distinct S6 blocks.
Subsequently, the results are merged to construct a 2D feature map as the final output (Cross-Merge).

Figure 3: Left: Illustration of (a) the overall architecture of VMamba, and (b) - (d) the structure of
Mamba and VSS blocks. Right: Comparison of variants of VMamba and benchmark methods in
classification accuracy and computational efficiency.

As illustrated in Figure 2, data forwarding in SS2D involves three steps: cross-scan, selective
scanning with S6 blocks, and cross-merge. Given the input data, SS2D first unfolds input patches
into sequences along four distinct traversal paths (i.e., Cross-Scan), processes each patch sequence
using a separate S6 block in parallel, and subsequently reshapes and merges the resultant sequences
to form the output map (i.e., Cross-Merge). By adopting complementary 1D traversal paths, SS2D
enables each pixel in the image to effectively integrate information from all other pixels in different
directions, thereby facilitating the establishment of global receptive fields in the 2D space.

4.3 Accelerating VMamba

As shown in Figure 3 (e), the VMamba-T model with vanilla VSS blocks (referred to as ‚ÄòVanilla
VMamba‚Äô) achieves a throughput of 426 images/s and consists of 22.9M parameters with 5.6G
FLOPs. Despite the state-of-the-art 82.2% classification accuracy (in the tiny-level, outperforms
Swin-T [37] by 0.9%), the low throughput and high memory overhead pose significant challenges for
the practical deployment of VMamba.

In this subsection, we outline our efforts to enhance its inference speed, primarily focusing on
advancements in both implementation details and architectural design. We evaluate the models
through image classification on ImageNet-1K. The impact of each progressive improvement is
summarized as follows, where (%, img/s) denote the gain of top-1 accuracy on ImageNet-1K and
inference throughput, respectively. This will be further discussed in Appendix E.

Step (a) (+0.0%, +41 img/s) by re-implementing Cross-Scan and Cross-Merge in Triton.

5

Cross-scanùë•ùë¶ùêµ,ùê∂Œîùê¥,ùê∑‚Ñéùë°=ùëíŒîùê¥‚Ñéùë°‚àí1+Œîùêµùë•ùë°ùë¶ùë°=ùê∂‚Ñéùë°+ùê∑ùë•ùë°ùë¶=[ùë¶1,ùë¶2,‚Ä¶ùë¶ùêø]S6 blocksLinearLinearEmbedding123456789Input Patches1‚Ä¶2389147‚Ä¶6998721‚Ä¶96341‚Ä¶1‚Ä¶2389147‚Ä¶6998721‚Ä¶96341‚Ä¶Cross-merge123456789Output PatchesSS2DBlock(c) The Vanilla VSS BlockLinearSS2DLNLNLinearLinear(b) Mamba BlockLinearS6LinearLinearSiLUSiLUSiLUSiLUùêª32√óùëä32√óùê∂4ùêª√óùëä√ó3Patch PartitionVSSBlockStage 1DownsamplingVSSBlockStage 2ùêª8√óùëä8√óùê∂2DownsamplingVSSBlockStage 3√óùêø3ùêª16√óùëä16√óùê∂3DownsamplingVSSBlockStage 4ùêª4√óùëä4√óùê∂1(a) Architecture of VMamba(d) VSS BlockInputImageLNlinearSS2DLNLNFFNlinearDWConvSiLU√óùêø1√óùêø2√óùêø4RMSNormDWConv1dDWConvMultiplicative Branch1340117981363846442646782.282.5ImplementationalVanilla VMamba82.2Step (a)CSM in Triton82.2Step (b)f16 in & f32 out82.2Step (c)Einsum‚ÜíLinearTensor layout82.2Step (d)+ MLP, -DWConvFewer Layers81.6Step (e)ssm-ratio: 2‚Üò1No Skip Branch,  More Layers  Step (f)d_state: 16‚Üò1ssm-ratio: 1‚Üó2, + DWConv818382ConvNeXt-TSwin-T82.181.3ArchitecturalImageNet Top-1 Acc. (%)Performance4.94.95.65.65.65.65.6GFLOPsThroughput4.511984.51244(e) Performance Comparison ùêª4√óùëä4√óùê∂1168682.6Step (g)ssm-ratio: 2‚Üò1More Layers4.9Step (b) (+0.0%, ‚àí3 img/s) by adjusting the CUDA implementation of the selective scan to ac-
commodate float16 input and float32 output. This remarkably enhances the training
efficiency (throughput from 165 to 184), despite slight speed fluctuation at test time.

Step (c) (+0.0%, +174 img/s) by substituting the relatively slow einsum in selective scan with a
linear transformation (i.e., torch.nn.functional.linear). We also adopt the tensor
layout of (B, C, H, W) to eliminate unnecessary data permutations.

Step (d) (‚àí0.6%, +175 img/s) by introducing MLP into VMamba due to its computational efficiency.
We also discard the DWConv (depth-wise convolutional [24]) layers and change the layer
configuration from [2,2,9,2] to [2,2,2,2] to lower FLOPs.

Step (e) (+0.6%, +366 img/s) by discarding the entire multiplicative branch, adopting the VSS block
illustrate in Figure 3 (d), and reducing the parameter ssm-ratio (the feature expansion
factor) from 2.0 to 1.0. This allows us to raise the layer numbers to [2,2,5,2] while
reducing the FLOPs.

Step (f) (+0.3%, +161 img/s) by reducing the parameter d_state (the SSM state dimension) from
16.0 to 1.0. This allows us to raise ssm-ratio back to 2.0 and introduce the DWConv layers
without increasing the FLOPs.

Step (g) (+0.1%, +346 img/s) by reducing the ssm-ratio to 1.0 while changing the layer configu-

ration from [2,2,5,2] to [2,2,8,2].

5 Experiments

In this section, we conduct a series of experiments to evaluate the performance of VMamba and
compare it with popular benchmark models across various visual tasks. We also validate the effec-
tiveness of the proposed 2D feature map traversing method by comparing it to alternative approaches.
Additionally, we analyze the characteristics of VMamba by visualizing its effective receptive field
(ERF) and activation map, and examining its scalability with increasingly long input sequences. We
primarily followed the hyper-parameter settings and experimental configurations of Swin [37]. Please
refer to Appendix E and F for detailed experiment settings, and Appendix H for more ablations. All
experiments are conducted on a server with 8 √ó NVIDIA Tesla-A100 GPUs.

5.1

Image Classification

We evaluate VMamba‚Äôs performance in image classification on ImageNet-1K [9], and the comparison
results against benchmark methods are summarized in Table 1. With similar FLOPs, VMamba-T
achieves a top-1 accuracy of 82.6%, outperforming DeiT-S by 2.8% and Swin-T by 1.3%. Notably,
VMamba maintains its performance advantage at both Small and Base scale. For instance, VMamba-B
achieves a top-1 accuracy of 83.9%, surpassing DeiT-B by 2.1% and Swin-B by 0.4%.

In terms of computational efficiency, VMamba-T achieves a throughput of 1,686 images/s, which is
either better or comparable to state-of-the-art methods. This advantage persists across VMamba-S
and VMamba-B, with a throughput of 877 images/s and 646 images/s, respectively. Compared to
SSM-based models, the throughput of VMamba-T is 1.47√ó higher than S4ND-Conv-T [45] and
1.08√ó higher than Vim-S [71], while maintaining a performance lead over them by a clear margin of
0.4% and 2.1%, respectively.

5.2 Downstream Tasks

In this sub-section, we assess the performance of VMamba on downstream tasks, including ob-
ject detection and instance segmentation on MSCOCO2017 [34], and semantic segmentation on
ADE20K [70]. The training framework is developed based on the MMDetection [3] and MMSegme-
nation [4] library, and we follow [36] to use the Mask-RCNN [27] and UperNet [65] as the detection
and segmentation network respectively.

Object Detection and Instance Segmentation. The results on MSCOCO are reported in Table 2.
VMamba maintains superiority in both box and mask Average Precision (APb and APm), irrespective
of the training schedule employed. With the 12-epoch fine-tuning schedule, VMamba-T/S/B achieves
object detection mAPs of 47.3%/48.7%/49.2%, surpassing Swin-T/S/B by 4.6%/3.9%/2.3%

6

Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100
GPU and an AMD EPYC 7542 CPU, using the toolkit released by [64], following the protocol
proposed in [37]. All images are of size 224 √ó 224.

Model

Params FLOPs

(M)

(G)

TP.
(img/s)

Top-1
(%)

Model

Params FLOPs TP. Top-1
(%)

(img/s)

(M)
(G)
ConvNet-Based

DeiT-S [58]
DeiT-B [58]
HiViT-T [68]
HiViT-S [68]
HiViT-B [68]
Swin-T [37]
Swin-S [37]
Swin-B [37]
XCiT-S24 [1]
XCiT-M24 [1]

Transformer-Based
22M 4.6G
86M 17.5G
19M 4.6G
38M 9.1G
66M 15.9G
28M 4.5G
50M 8.7G
88M 15.4G
48M 9.2G
84M 16.2G

1761
503
1393
712
456
1244
718
458
671
423

79.8
81.8
82.1
83.5
83.8
81.3
83.0
83.5
82.6
82.7

ConvNeXt-T [38]
ConvNeXt-S [38]
ConvNeXt-B [38]

29M 4.5G 1198
50M 8.7G 684
89M 15.4G 436

SSM-Based

683
-
S4ND-Conv-T [45] 30M
397
-
89M
S4ND-ViT-B [45]
Vim-S [71]
26M
811
-
30M 4.9G 1686
VMamba-T
50M 8.7G 877
VMamba-S
89M 15.4G 646
VMamba-B

82.1
83.1
83.8

82.2
80.4
80.5
82.6
83.6
83.9

Table 2: Left: Results of object detection and instance segmentation on MSCOCO. AP b and AP m
denote box AP and mask AP, respectively. FLOPs are calculated with input size 1280 √ó 800. The
notation ‚Äò1√ó‚Äô refers to models fine-tuned for 12 epochs, and ‚Äò3√óMS‚Äô denotes multi-scale training
for 36 epochs. Right: Results of semantic segmentation on ADE20K. FLOPs are calculated with an
input size of 512 √ó 2048. ‚ÄòSS‚Äô and ‚ÄòMS‚Äô denote single-scale and multi-scale testing, respectively.

APb APm
Backbone
39.3
42.7
Swin-T
40.1
44.2
ConvNeXt-T
42.7
47.3
VMamba-T
40.9
44.8
Swin-S
41.8
45.4
ConvNeXt-S
43.7
48.7
VMamba-S
42.3
46.9
Swin-B
42.7
ConvNeXt-B 47.0
44.1
49.2
VMamba-B

Mask R-CNN 1√ó schedule
Params
48M
48M
50M
69M
70M
70M
107M
108M
108M

Mask R-CNN 3√ó MS schedule

Swin-T
ConvNeXt-T
NAT-T
VMamba-T
Swin-S
ConvNeXt-S
NAT-S
VMamba-S

46.0
46.2
47.7
48.8
48.2
47.9
48.4
49.9

41.6
41.7
42.6
43.7
43.2
42.9
43.2
44.2

48M
48M
48M
50M
69M
70M
70M
70M

FLOPs
267G
262G
271G
354G
348G
349G
496G
486G
485G

267G
262G
258G
271G
354G
348G
330G
349G

ADE20K with crop size 512

Backbone

ResNet-50
DeiT-S + MLN
Swin-T
ConvNeXt-T
NAT-T
Vim-S
VMamba-T
ResNet-101
DeiT-B + MLN
Swin-S
ConvNeXt-S
NAT-S
VMamba-S
Swin-B
ConvNeXt-B
NAT-B
RepLKNet-31B
VMamba-B

mIOU
(SS)
42.1
43.8
44.5
46.0
47.1
44.9
47.9
43.8
45.5
47.6
48.7
48.0
50.6
48.1
49.1
48.5
49.9
51.0

mIOU
(MS)
42.8
45.1
45.8
46.7
48.4
-
48.8
44.9
47.2
49.5
49.6
49.5
51.2
49.7
49.9
49.7
50.6
51.6

Params

FLOPs

953G
67M
1217G
58M
945G
60M
939G
60M
934G
58M
-
46M
949G
62M
86M
1030G
144M 2007G
1039G
81M
1027G
82M
1010G
82M
82M
1028G
121M 1188G
122M 1170G
123M 1137G
112M 1170G
122M 1170G

mAP, and ConvNeXt-T/S/B by 3.1%/3.3%/2.2% mAP, respectively. With the same configura-
tion, the instance segmentation mAPs achieved by VMamba-T/S/B outperforms Swin-T/S/B by
3.4%/2.8%/1.8% mAP, and ConvNeXt-T/S/B by 2.6%/1.9%/1.4% mAP, respectively. Moreover,
VMamba‚Äôs advantages persist under the 36-epoch fine-tuning schedule with multi-scale training,
showcasing its potential to achieve promising performance in downstream tasks with dense prediction.

Semantic Segmentation. Consistent with previous experiments, VMamba demonstrates superior
performance in semantic segmentation on ADE20K with a comparable amount of parameters. As
shown in Table 2, VMamba-T is 3.4% mIoU higher than Swin-T and 1.9% higher than ConvNeXt-T
in the Single-Scale (SS) setting, and the advantage persists with Multi-Scale (MS) input. For models
at the Small and Base level, VMamba-S/B outperforms NAT-S/B [26] by 2.6%/2.5% mIoU under the
SS setting, and 1.7%/1.9% mIoU under the MS setting.

7

Figure 4: Illustration of the adaptability of VMamba to (a) downstream tasks, and (b) input image
with progressively increased resolutions. Swin-T‚àó denotes Swin-T tested with scaled window sizes.

Figure 5: Illustration of the activation map for query patches indicated by red stars. Visualization
results in (b) and (c) are obtained by combining the activation map of each scanning path in SS2D.

Discussion Experiment results in this subsection demonstrate VMamba‚Äôs adaptability to object
detection, instance segmentation, and semantic segmentation. In Figure 4 (a), we compare VMamba‚Äôs
performance with Swin and ConvNeXt, highlighting its advantages in addressing downstream tasks
with comparable classification accuracy on ImageNet-1K. VMamba also exhibits greater tolerance to
changes in input resolution, with linear growth in FLOPs (see Figure 4 (b)) and modest accuracy drop
(further discussed in Appendix G), making it more effective and efficient compared to ViT-based
methods when adapting to downstream tasks with inputs of larger spatial resolutions. This aligns
with Mamba‚Äôs advanced capability in efficient long sequence modeling [17].

5.3 Analysis and Discussion

Relationship between SS2D and Self-Attention. To formulate the response Y within the time
interval [a, b] of length T , we denote the corresponding SSM-related variables ui ‚äô ‚àÜi ‚àà R1√óDv ,
Bi ‚àà R1√óDk , and Ci ‚àà R1√óDk as V ‚àà RT √óDv , K ‚àà RT √óDk , and Q ‚àà RT √óDk , respectively.
Therefore, the j-th dimension of Y, i.e., Y(j) ‚àà RT √ó1, can be expressed as

Y(j) =

(cid:104)

Q ‚äô w(j)(cid:105)

ha

(j) +

(cid:34)

(cid:16)

Q ‚äô w(j)(cid:17) (cid:18) K
w(j)

(cid:19)‚ä§

(cid:35)

‚äô M

V(j),

(4)

where ha ‚àà RDk is the hidden state at step a, M denotes the temporal mask matrix of size T √ó T
with the lower triangular part set to 1 and elsewhere 0, and ‚äô denotes element-wise product. Detailed
derivations are deferred to Appendix C.

In Eq. 4, the matrix multiplication process involving Q, K, and V closely resembles the mecha-
nism of self-attention, despite the inclusion of w. Concretely, the formulation of each element in
w := [w1; . . . ; wT ] ‚àà RT √óDk√óDv , i.e., wi ‚àà RDk√óDv , can be written as wi = (cid:81)i
a‚àí1+j ,
representing the cumulative attention weight at step i computed along the scanning path.

j=1 eA‚àÜ‚ä§

Visualization of Activation Maps. To gain an intuitive and in-depth understanding of SS2D, we
further visualize the attention values in QK‚ä§ and (Q ‚äô w) (K/w)‚ä§ corresponding to a specific

8

(a)ùëèùê¥ùëÉùëèon COCOùê¥ùëÉùëöon COCOmIoUon ADE20KTop-1 acc. on ImageNet-1KTop-1 acc. on ImageNet-1KTop-1 acc. on ImageNet-1KFLOPs (G)Resolution of input image(b) Activation map of(a) Input Image(c) Activation map of(ùë∏‚®Äùíò)(ùë≤/ùíò)ùëª(d) Activation map of (ùë∏‚®Äùíò)(ùë≤/ùíò)ùëª for individual scanning pathùë∏ùë≤ùëªFigure 6: Comparison of Effective Receptive Fields (ERF) [41] between VMamba and other bench-
mark models. Pixels with higher intensity indicate larger responses regarding the central pixel.

query patch within foreground objects (referred to as the ‚Äòactivation map‚Äô). As shown in Figure 5 (b),
the activation map of QK‚ä§ demonstrates the effectiveness of SS2D in capturing and retaining the
traversed information, with all previously scanned tokens in the foreground region being activated.
Furthermore, the inclusion of w leads to activation maps that are more focused on the neighborhood
of query patches (Figure 5 (c)), which is consistent with the temporal weighting effect inherent to
the formulation of w. Nevertheless, the selective scan mechanism enables VMamba to accumulate
the history along the scanning path, thereby facilitating the establishment of long-term dependencies
across image patches. This is evident in the sub-figure encircled with a red box (Figure 5 (d)), where
patches of the sheep far to the left (scanned in early steps) remain activated. For more visualizations
and further discussion, please refer to Appendix D.

Visualization of Effective Receptive Fields. The Effective Receptive Field (ERF) [41, 11] refers
to the region in the input space that contributes to the activation of a certain output unit. We conduct
a comparative analysis of the central pixel‚Äôs ERF across various visual backbones, both before and
after training. The results presented in Figure 6 illustrate that among the models examined, only DeiT,
HiViT, and VMamba, demonstrate global ERFs, whereas the others exhibit local ERFs, despite their
theoretical potential for global coverage. Moreover, the linear time complexity of VMamba makes it
more computationally efficient compared to DeiT and HiViT, which have quadratic costs w.r.t. the
number of input patches.

Diagnostic Study on Selective Scan Patterns. We com-
pare the proposed scanning pattern (i.e. Cross-Scan) to
three benchmark patterns, including unidirectional scan-
ning (Unidi-Scan), bidirectional scanning (Bidi-Scan), and
cascade scanning (Cascade-Scan, scanning the data row-
wise and column-wise successively). Feature dimensions
are adjusted to maintain similar architectural parameters
and FLOPs for fair comparison. As illustrated in Figure 7,
Cross-Scan outperforms other scanning patterns in both
computational efficiency and classification accuracy, high-
lighting its effectiveness in achieving 2D-Selective-Scan.
Removing the DWConv layer, which has been observed to
potentially aid the model in learning 2D spatial informa-
tion, further enhances this advantage. This emphasizes the
inherent strength of Cross-Scan in capturing 2D contextual
information through its adoption of four-way scanning.

6 Conclusion

Figure 7: Performance comparison of
different scanning patterns.

This paper presents VMamba, an efficient vision backbone network built with State Space Models
(SSMs). VMamba integrates the benefits of selective SSMs from NLP tasks into visual data process-
ing, bridging the gap between ordered 1D scanning and non-sequential 2D traversal through the novel
SS2D module. Moreover, we have substantially improved the inference speed of VMamba through a
series of architectural and implementation refinements. The effectiveness of the VMamba family has

9

Before trainingAfter trainingResnet-50ConvNeXt-TSwin-TDeiT-SHiViT-TVMamba-T1.00.80.60.40.20.016821687998168617161719100717179001400UnidiBidiCascadeCrossTP. (img/s)w/ dwconvw/o dwconv82.26%82.49%82.42%82.60%80.88%81.80%81.71%82.25%81%82%83%UnidiBidiCascadeCrossTop-1(%)w/ dwconvw/o dwconv(a)(b)been demonstrated through extensive experiments, and the linear time complexity of VMamba makes
it advantageous for downstream tasks with large-resolution inputs.

Limitations. While VMamba demonstrates promising experimental results, there is still room
for this study to be further improved. Previous research has validated the efficacy of unsupervised
pre-training on large-scale datasets (e.g., ImageNet-21K). However, the compatibility of existing pre-
training methods with SSM-based architectures like VMamba, and the identification of pre-training
techniques specifically tailored to such models, remain unexplored. Investigating these aspects
could serve as a promising avenue for future research in architectural design. Moreover, limited
computational resources have prevented us from exploring VMamba‚Äôs architecture at the Large scale,
as well as conducting a fine-grained search of hyperparameters to further improve experimental
performance.

References

[1] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan
Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image trans-
formers. NeurIPS, 34:20014‚Äì20027, 2021.

[2] Guy E Blelloch. Prefix sums and their applications. 1990.

[3] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,
Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu
Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change
Loy, and Dahua Lin. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint
arXiv:1906.07155, 2019.

[4] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and

benchmark. https://github.com/open-mmlab/mmsegmentation, 2020.

[5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable

convolutional networks. In ICCV, pages 764‚Äì773, 2017.

[6] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for

all data sizes. NeurIPS, 34:3965‚Äì3977, 2021.

[7] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2023.

[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention: Fast and memory-

efficient exact attention with io-awareness. NeurIPS, 35:16344‚Äì16359, 2022.

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical

image database. In CVPR, pages 248‚Äì255, 2009.

[10] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual attention

vision transformers. In ECCV, pages 74‚Äì92, 2022.

[11] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31:

Revisiting large kernel design in cnns. In CVPR, pages 11963‚Äì11975, 2022.

[12] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and
Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In
CVPR, pages 12124‚Äì12134, 2022.

[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and
Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR,
2021.

[14] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function

approximation in reinforcement learning. Neural Networks, 107:3‚Äì11, 2018.

[15] Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, and Ran He. Rmt: Retentive networks meet

vision transformers. In CVPR, 2024.

[16] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry

hungry hippos: Towards language modeling with state space models. In ICLR, 2022.

10

[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint

arXiv:2312.00752, 2023.

[18] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R√©. Hippo: Recurrent memory with

optimal polynomial projections. NeurIPS, 33:1474‚Äì1487, 2020.

[19] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R√©. On the parameterization and initialization of

diagonal state space models. NeurIPS, 35:35971‚Äì35983, 2022.

[20] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state

spaces. In ICLR, 2021.

[21] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R√©. Combining
recurrent, convolutional, and continuous-time models with linear state space layers. NeurIPS, 34:572‚Äì585,
2021.

[22] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state

spaces. NeurIPS, 35:22982‚Äì22994, 2022.

[23] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. Demystifying
local vision transformer: Sparse connectivity, weight sharing, and dynamic weight. arXiv preprint
arXiv:2106.04263, 2021.

[24] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. On the connection

between local attention and dynamic depth-wise convolution. In ICLR, 2021.

[25] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela

Rus. Liquid structural state-space models. In ICLR, 2022.

[26] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer.

In CVPR, pages 6185‚Äì6194, 2023.

[27] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In ICCV, pages 2961‚Äìs2969,

2017.

[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.

In CVPR, pages 770‚Äì778, 2016.

[29] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861, 2017.

[30] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected

convolutional networks. In CVPR, pages 4700‚Äì4708, 2017.

[31] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns: Fast

autoregressive transformers with linear attention. In ICML, pages 5156‚Äì5165, 2020.

[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional

neural networks. NeurIPS, pages 1106‚Äì1114, 2012.

[33] Rudolf Emil K√°lm√°n. A new approach to linear filtering and prediction problems. Journal of Basic

Engineering, 82(1):35‚Äì45, 1960.

[34] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,
and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pages 740‚Äì755, 2014.

[35] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi K√§rkk√§inen,
Mykola Pechenizkiy, Decebal Constantin Mocanu, and Zhangyang Wang. More convnets in the 2020s:
Scaling up kernels beyond 51x51 using sparsity. In ICLR, 2023.

[36] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,
Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, pages 12009‚Äì12019,
2022.

[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012‚Äì10022, 2021.

[38] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A

convnet for the 2020s. In CVPR, pages 11976‚Äì11986, 2022.

11

[39] Ilya Loshchilov and Frank Hutter.

Decoupled weight decay regularization.

arXiv preprint

arXiv:1711.05101, 2017.

[40] Jiasen Lu, Roozbeh Mottaghi, Aniruddha Kembhavi, et al. Container: Context aggregation networks.

NeurIPS, 34:19160‚Äì19171, 2021.

[41] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in

deep convolutional neural networks. NeurIPS, 29:4898‚Äì4906, 2016.

[42] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and

Luke Zettlemoyer. Mega: Moving average equipped gated attention. In ICLR, 2022.

[43] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In ICLR,

2018.

[44] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via

gated state spaces. In ICLR, 2023.

[45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christo-
pher R√©. S4nd: Modeling images and videos as multidimensional signals with state spaces. NeurIPS,
35:2846‚Äì2861, 2022.

[46] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng,
Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. RWKV: reinventing rnns for the transformer era.
In EMNLP, pages 14048‚Äì14077, 2023.

[47] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll√°r. Designing network

design spaces. In CVPR, pages 10428‚Äì10436, 2020.

[48] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, and Jiwen Lu. Hornet: Efficient
high-order spatial interactions with recursive gated convolutions. NeurIPS, 35:10353‚Äì10366, 2022.

[49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. 2015.

[50] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence

modeling. In ICLR, 2022.

[51] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence

modeling. In ICLR, 2022.

[52] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.
Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621,
2023.

[53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages
1‚Äì9, 2015.

[54] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks.

In ICML, pages 6105‚Äì6114, 2019.

[55] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM

Computing Surveys, 55(6), 2022.

[56] Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang,
Qi Tian, and Qixiang Ye. Integrally pre-trained transformer pyramid networks. In CVPR, pages 18610‚Äì
18620, 2023.

[57] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,
Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture
for vision. Advances in neural information processing systems, 34:24261‚Äì24272, 2021.

[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√© J√©gou.
Training data-efficient image transformers & distillation through attention. In ICML, pages 10347‚Äì10357,
2021.

[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz

Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30:5998‚Äì6008, 2017.

12

[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz

Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30:5998‚Äì6008, 2017.

[61] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective

structured state-spaces for long-form video understanding. In CVPR, pages 6387‚Äì6397, 2023.

[62] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear

complexity. arXiv preprint arXiv:2006.04768, 2020.

[63] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and
Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.
In ICCV, pages 568‚Äì578, 2021.

[64] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,

2019.

[65] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene

understanding. In ECCV, pages 418‚Äì434, 2018.

[66] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal
self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.

[67] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention

transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.

[68] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler

and more efficient design of hierarchical vision transformer. In ICLR, 2023.

[69] Weixi Zhao, Weiqiang Wang, and Yunjie Tian. Graformer: Graph-oriented transformer for 3d pose

estimation. In CVPR, pages 20438‚Äì20447, 2022.

[70] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing

through ade20k dataset. In CVPR, pages 5122‚Äì5130, 2017.

[71] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision
mamba: Efficient visual representation learning with bidirectional state space model. In ICML, 2024.

[72] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better

results. In CVPR, pages 9308‚Äì9316, 2019.

13

A Discretization of State Space Models (SSMs)

In this section, we explore the correlation between the discretized formulation of State Space Models
(SSMs) obtained in Sec. 3 and those derived from the zero-order hold (ZOH) method [17], which is
frequently used in studies related to SSM.

Recall the discretized formulation of SSMs derived in Sec. 3 as follows,

hb = eA(‚àÜa+...+‚àÜb‚àí1)

ha +

(cid:32)

Biuie‚àíA(‚àÜa+...+‚àÜi)‚àÜi

.

(cid:33)

b‚àí1
(cid:88)

i=a

Let b = a + 1, then the above equation can be re-written as

ha+1 = eA‚àÜa ha + Ba‚àÜaua,

(5)

(6)

where Aa := eA‚àÜa is exactly the discretized form of the evolution matrix A obtained by ZOH, while
Ba := Ba‚àÜa is basically the first-order Taylor expansion of the discretized B acquired through
ZOH.

B Derivation of the Recurrence Relation of Selective SSMs

In this section, we derive the recurrence relation of the hidden state in selective SSMs. Given the
expression of hb shown in Eq. 5, let us denote eA(‚àÜa+...+‚àÜi‚àí1) as pi
A,a, then its recurrence relation
can be directly written as

For the second term of Eq. 5, we have

A,a = eA‚àÜi‚àí1pi‚àí1
pi
A,a.

B,a = eA(‚àÜa+...+‚àÜb‚àí1)
pb

b‚àí1
(cid:88)

i=a

Biuie‚àíA(‚àÜa+...+‚àÜi)‚àÜi

= eA‚àÜb‚àí1pb‚àí1

B,a + Bb‚àí1ub‚àí1‚àÜb‚àí1.

(7)

(8)

(9)

Therefore, with the associations derived in Eq. 7 and Eq. 8, hb = pb
B,a can be efficiently
computed in parallel using associative scan algorithms [2, 43, 51], which are supported by numerous
modern programming libraries. This approach effectively reduces the overall computational com-
plexity to linear, and VMamba further accelerates the computation by adopting a hardware-aware
implementation [17].

A,aha + pb

C Details of the relationship between SS2D and Self-attention

In this section, we clarify the relationship between SS2D and the self-attention mechanism commonly
employed in existing vision backbone models. Subsequently, visualization results are provided to
substantiate our explanation.

Let T denote the length of the sequence with indices from a to b, we define the following variables
V := [V1; . . . ; VT] ‚àà RT √óDv , where Vi := ua+i‚àí1 ‚äô ‚àÜa+i‚àí1 ‚àà R1√óDv
K := [K1; . . . ; KT] ‚àà RT √óDk , where Ki := Ba+i‚àí1 ‚àà R1√óDk
Q := [Q1; . . . ; QT] ‚àà RT √óDk , where Qi := Ca+i‚àí1 ‚àà R1√óDk

(12)

(11)

(10)

w := [w1; . . . ; wT] ‚àà RT √óDk√óDv , where wi :=

i
(cid:89)

j=1

eA‚àÜ‚ä§

a‚àí1+j ‚àà RDk√óDv

H := [ha; . . . ; hb] ‚àà RT √óDk√óDv , where hi ‚àà RDk√óDv
Y := [ya; . . . ; yb] ‚àà RT √óDv , where yi ‚àà RDv

(13)

(14)

(15)

Note that in practice, the parameter A in Eq. 1 is simplified to R1√óDk . Consequently, h‚Ä≤(t) =
Ah(t) + Bu(t) is simplified to h‚Ä≤(t) = A ‚äô h(t) + Bu(t), which is the reason why wi ‚àà RDk√óDv .

14

Based on these notations, the discretized solution of time-varying SSMs (Eq. 5) can be written as
T
(cid:88)

(cid:17)

(cid:16)

hb = wT ‚äô ha +

‚äô

Ki

‚ä§Vi

,

(16)

i=1
where ‚äô denotes the element-wise product between matrices, and the division is also elements-wise.

wT
wi

Based on the expression of the hidden state hb, the first term of the output of SSM, i.e., yb, can be
computed by

yb = QThb

= QT (wT ‚äô ha) + QT

T
(cid:88)

i=1

wT
wi

(cid:16)

‚äô

Ki

‚ä§Vi

(cid:17)

.

(17)

(18)

Here we drop the skip connection between the input and the response for simplicity. Particularly, the
j-th slice along dimension Dv of yb, denoted as yb
T
(cid:88)

(cid:19)

(j)

(cid:16)

(j)(cid:17)

yb

(j) =

QT ‚äô wT

ha

(j) +

(j) ‚àà R can be written as
(cid:18) QT ‚äô wT
wi

Ki

(j)

‚ä§

i=1
Similarly, the j-th slice along dimension Dv of the overall response Y, denoted as Y(j) ‚àà RT √ó1,
can be expressed as

‚äô Vi

(j).

(19)

Y(j) =

(cid:16)

Q ‚äô w(j)(cid:17)

ha

(j) +

(cid:34)

(cid:16)

Q ‚äô w(j)(cid:17) (cid:18) K
w(j)

(cid:19)‚ä§

(cid:35)

‚äô M

V(j),

(20)

where M := tril(T, T ) ‚àà {0, 1}T √óT denotes the temporal mask matrix with the lower triangular
portion of a T √ó T matrix set to 1 and elsewhere 0. It is evident that how matrices Q, K, and V are
multiplied in Eq. 20 closely resembles the process in the self-attention module of Vision Transformers.
Moreover, if w is in shape (T, Dk) rather than (T, Dk, Dv), then Eq. 17 and Eq. 20 reduces to the
form of Gated Linear Attention (GLA) [67], which indicates that GLA is also a special case of
Mamba.

D Visualization of Attention and Activation Maps

In the preceding subsection, we illustrated how the computational process of selective SSMs shares
similarities with self-attention mechanisms, allowing us to delve into the internal mechanism of SS2D
through the visualization of its weight matrices.

Given the input image as in Figure 8 (a), illustrations of four scanning paths in SS2D are shown
in Figure 8 (d), the visualization of corresponding attention maps by calculating QK‚ä§ and
(Q ‚äô w) (K/w)‚ä§ are shown in Figure 8 (e) and Figure 8 (g) respectively. These results underscore
the effectiveness of the proposed scanning approach (i.e., Cross-Scan) in capturing and retaining the
traversed information, as each row in a single attention map corresponds to the attention between the
current patch and all previously scanned foreground tokens impartially. Additionally, in Figure 8 (f),
we showcase the transformed activation maps, where the pixel order corresponds to that of the first
route, traversing the image row-wise from the upper-left to the bottom-right.

By rearranging the diagonal elements of the obtained attention map in the image space, we derive
the visualization results illustrated in Figure 8 (b) and Figure 8 (c) corresponding to QK‚ä§ and
(Q ‚äô w) (K/w)‚ä§ respectively. These maps illustrate the effectiveness of VMamba in accurately
distinguishing between foreground and background pixels within an image.

Moreover, given a certain selected patch as the query, we visualize the corresponding ‚Äòactivation map‚Äô
by reshaping the associated row in the attention map (computed by QK‚ä§ or (Q ‚äô w) (K/w)‚ä§)
This reflects the attention score between the query patch and all previously scanned patches. To
get the complete visualization for a query patch, we collect and combine the activation maps from
all four scanning paths in SS2D. Visualization results of the activation map for both QK‚ä§ and
(Q ‚äô w) (K/w)‚ä§ are shown in Figure 9. Moreover, we also visualize the diagonal elements
of attention maps computed by (Q ‚äô w) (K/w)‚ä§, where all foreground objects are effectively
highlighted and separated from the background.

15

Figure 8: Illustration of the attention maps obtained by SS2D.

Figure 9: Illustration of activation maps for the query patch (marked with a red star).

16

Route1Route2Route3Route4(ùëë)(ùëí)(ùëî)(ùëé)(ùëè)(ùëê)ùë∏ùë≤ùëªAttention Map of Diagonal ofDiagonal ofùë∏ùë≤ùëª(ùë∏‚äôùíò)(ùë≤ùíò)ùëªInput Image(ùëì)ùë∏ùë≤ùëªAligned Attention Map of Attention Map of (ùë∏‚äôùíò)(ùë≤ùíò)ùëªActivation Map of (ùëé)(ùëè)(ùëê)ùë∏ùë≤ùëªInput ImageActivation Map of (ùë∏‚äôùíò)(ùë≤ùíò)ùëªActivation Map of (ùëë)(ùëí)(ùëì)ùë∏ùë≤ùëªInput ImageActivation Map of (ùë∏‚äôùíò)(ùë≤ùíò)ùëª(ùëî)Diagonal of Attention Map Figure 10: Illustration of ERF maps throughout the training process of Vanilla-VMamba-T (with
EMA).

E Detailed Experiment Settings

Network Architecture. The architectural specifications of Vanilla-VMamba are outlined in Table 3,
and detailed configurations of the VMamba series are provided in Table 4. The Vanilla-VMamba
series are built with the vanilla VSS Block, which includes a multiplicative branch with no feed-
forward network (FFN) layer. In contrast, the VSS Block in the VMamba series eliminates the
multiplicative branch and introduces FFN layers. Additionally, we provide alternative architectures
for VMamba at Small and Base scales, referred to as VMamba-S[s1l20] and VMamba-B[s1l20],
respectively. The notation ‚Äòsxly‚Äô indicates that the ssm-ratio is set to x and the number of layers
in stage 3 is set to y. Consequently, the versions presented in Table 1 can also be referred to as
VMamba-S[s2l15] and VMamba-B[s2l15].

Experiment Setting. The hyper-parameters for training VMamba on ImageNet are inherited from
Swin [37], except for the parameters related to drop_path_rate and the exponential moving average
(EMA) technique. Specifically, VMamba-T/S/B models are trained from scratch for 300 epochs, with
a 20-epoch warm-up period, using a batch size of 1024. The training process utilizes the AdamW
optimizer [39] with betas set to (0.9, 0.999), a momentum of 0.9, an initial learning rate of 1 √ó 10‚àí3,
a weight decay of 0.05, and a cosine decay learning rate scheduler. Additional techniques such as
label smoothing (0.1) and EMA (decay ratio of 0.9999) are also applied. The drop_path_ratio
is set to 0.2 for Vanilla-VMamba-T, VMamba-T, 0.3 for Vanilla-VMamba-S, VMamba-S[s2l15],
VMamba-S[s1l20], 0.6 for Vanilla-VMamba-B, VMamba-B[s2l15] and 0.5 for VMamba-B[s1l20].
No other training techniques are employed.

Throughput Evaluation. Detailed performance comparisons with various models are presented in
Table 6. Throughput (referred to as TP.) was assessed on an A100 GPU paired with an AMD EPYC
7542 CPU, utilizing the toolkit provided by [64]. Following the protocol outlined in [37], we set the
batch size to 128. The training throughput (referred to as Train TP.) is tested on the same device
with mix-resolution, excluding the time consumption of optimizers. The batch size for measuring
the training throughput is set to 128 as well.

Accelerating VMamba. Table 5 provides detailed configurations of intermediate variants in the
acceleration process from Vanilla-VMamba-T to VMamba-T.

Evolution of ERF. We further generate the effective receptive field (ERF) maps throughout the
training process for Vanilla-VMamba-T. These maps intuitively illustrate how VMamba‚Äôs pattern of
ERF evolves from being predominantly local to predominantly global, epoch by epoch.

F Performance of the VMamba Family on Downstream Tasks

In this section, we present the experimental results of Vanilla-VMamba and VMamba on the
MSCOCO and ADE20k datasets. The results are summarized in Table 7 and Table 8, respectively.

For object detection and instance segmentation, we adhere to the protocol outlined by Swin [37] and
construct our models using the mmdetection framework [3]. Specifically, we utilize the AdamW
optimizer [39] and fine-tune the classification models pre-trained on ImageNet-1K for both 12 and 36
epochs. The learning rate is initialized with 1 √ó 10‚àí4 and is decreased by a factor of 10 at the 9-th
and 11-th epoch. We incorporate multi-scale training and random flipping with a batch size of 16, in
line with established practices for object detection evaluations.

17

Before Training1.00.80.60.40.20.0‚Ä¶‚Ä¶‚Ä¶‚Ä¶Epoch3Epoch6Epoch9Epoch12Epoch30After TrainingTable 3: Architectural overview of the Vanilla-VMamba series. Down-sampling is executed through
patch merging [37] operations in stages 1, 2, and 3. The term Linear refers to a linear layer. The
DWConv denotes a depth-wise convolution [24] operation. The proposed 2D-selective-scan is labeled
as SS2D.

layer name

output size

Vanilla-VMamba-T

Vanilla-VMamba-S

Vanilla-VMamba-B

stem

112√ó112

conv 4√ó4, 96, stride 4

conv 4√ó4, 96, stride 4

conv 4√ó4, 128, stride 4

stage 1

56√ó56

stage 2

28√ó28

stage 3

14√ó14

stage 4

7√ó7

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock
Linear 96 ‚Üí 2√ó96
DWConv 3√ó3, 2√ó96
SS2D, dim 2√ó96
Linear 2√ó96 ‚Üí 96
Multiplicative
Linear 2√ó96 ‚Üí 96

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock
Linear 96 ‚Üí 2√ó96
DWConv 3√ó3, 2√ó96
SS2D, dim 2√ó96
Linear 2√ó96 ‚Üí 96
Multiplicative
Linear 2√ó96 ‚Üí 96

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

patch merging ‚Üí 192

patch merging ‚Üí 192

vanilla VSSBLock
Linear 192 ‚Üí 2√ó192
DWConv 3√ó3, 2√ó192
SS2D, dim 2√ó192
Linear 2√ó192 ‚Üí 192
Multiplicative
Linear 2√ó192 ‚Üí 192

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock
Linear 192 ‚Üí 2√ó192
DWConv 3√ó3, 2√ó192
SS2D, dim 2√ó192
Linear 2√ó192 ‚Üí 192
Multiplicative
Linear 2√ó192 ‚Üí 192

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock
Linear 128 ‚Üí 2√ó128
DWConv 3√ó3, 2√ó128
SS2D, dim 2√ó128
Linear 2√ó128 ‚Üí 128
Multiplicative
Linear 2√ó128 ‚Üí 128

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

patch merging ‚Üí 256

vanilla VSSBLock
Linear 256 ‚Üí 2√ó256
DWConv 3√ó3, 2√ó256
SS2D, dim 2√ó256
Linear 2√ó256 ‚Üí 256
Multiplicative
Linear 2√ó256 ‚Üí 256

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

patch merging ‚Üí 384

patch merging ‚Üí 384

patch merging ‚Üí 512

vanilla VSSBLock
Linear 384 ‚Üí 2√ó384
DWConv 3√ó3, 2√ó384
SS2D, dim 2√ó384
Linear 2√ó384 ‚Üí 384
Multiplicative
Linear 2√ó384 ‚Üí 384

Ô£π

√ó9

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock
Linear 384 ‚Üí 2√ó384
DWConv 3√ó3, 2√ó384
SS2D, dim 2√ó384
Linear 2√ó384 ‚Üí 384
Multiplicative
Linear 2√ó384 ‚Üí 384

Ô£π

√ó27

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock
Linear 512 ‚Üí 2√ó512
DWConv 3√ó3, 2√ó512
SS2D, dim 2√ó512
Linear 2√ó512 ‚Üí 512
Multiplicative
Linear 2√ó512 ‚Üí 512

Ô£π

√ó27

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

patch merging ‚Üí 768

patch merging ‚Üí 768

patch merging ‚Üí 1024

vanilla VSSBLock
Linear 768 ‚Üí 2√ó768
DWConv 3√ó3, 2√ó768
SS2D, dim 2√ó768
Linear 2√ó768 ‚Üí 768
Multiplicative
Linear 2√ó768 ‚Üí 768

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock
Linear 768 ‚Üí 2√ó768
DWConv 3√ó3, 2√ó768
SS2D, dim 2√ó768
Linear 2√ó768 ‚Üí 768
Multiplicative
Linear 2√ó768 ‚Üí 768

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

vanilla VSSBLock

Linear 1024 ‚Üí 2√ó1024
DWConv 3√ó3, 2√ó1024
SS2D, dim 2√ó1024
Linear 2√ó1024 ‚Üí 1024
Multiplicative
Linear 2√ó1024 ‚Üí 1024

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

1√ó1

Param. (M)

FLOPs

average pool, 1000-d fc, softmax

22.9

5.63√ó109

44.4

11.23√ó109

76.3

18.02√ó109

For semantic segmentation, we follow Swin [37] and construct a UperHead [65] network on top of
the pre-trained model using the MMSegmentation library [4]. We employ the AdamW optimizer [39]
and set the learning rate to 6 √ó 10‚àí5. The fine-tuning process spans a total of 160k iterations with a
batch size of 16. The default input resolution is 512 √ó 512.

G Details of VMamba‚Äôs Scale-Up Experiments

Given Mamba‚Äôs exceptional ability in efficient long sequence modeling, we conduct experiments to
assess whether VMamba inherits this characteristic. We evaluate the computational efficiency and
classification accuracy of VMamba with progressively larger input spatial resolutions. Specifically,
following the protocol in XCiT [1], we apply VMamba trained on 224 √ó 224 input to images with
resolutions ranging from 288 √ó 288 to 768 √ó 768. We measure the generalization performance in
terms of the number of parameters, FLOPs, throughput during both training and inference, and the
top-1 classification accuracy on ImageNet-1K. We also conduct experiments under the ‚Äòlinear
tuning‚Äô setting, where only the header network, consisting of a single linear module, is fine-tuned
from random initialization using features extracted by those backbone models.

18

Table 4: Architectural overview of the VMamba series.

layer name

output size

VMamba-T

VMamba-S

VMamba-B

stem

112√ó112

conv 3√ó3 stride 2, LayerNorm, GeLU, conv 3√ó3 stride 2, LayerNorm

stage 1

56√ó56

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

VSSBLock(ssm-ratio=1, mlp-ratio=4)
Ô£Æ

Ô£π

VSSBlock(ssm-ratio=2, mlp-ratio=4)
Ô£Æ

Ô£π

VSSBLock(ssm-ratio=2, mlp-ratio=4)
Ô£Æ

Ô£π

Linear 96 ‚Üí ssm-ratio √ó96
DWConv 3√ó3, ssm-ratio √ó96
SS2D, dim ssm-ratio √ó96
Linear ssm-ratio √ó96 ‚Üí 96
FFN mlp-ratio √ó96

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 96 ‚Üí ssm-ratio √ó96
DWConv 3√ó3, ssm-ratio √ó96
SS2D, dim ssm-ratio √ó96
Linear ssm-ratio √ó96 ‚Üí 96
FFN mlp-ratio √ó96

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 128 ‚Üí ssm-ratio √ó128
DWConv 3√ó3, ssm-ratio √ó128
SS2D, dim ssm-ratio √ó128
Linear ssm-ratio √ó128 ‚Üí 128
FFN mlp-ratio √ó128

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

conv 3√ó3 stride 2, LayerNorm

stage 2

28√ó28

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

VSSBLock(ssm-ratio=1, mlp-ratio=4)
Ô£Æ

Ô£π

VSSBlock(ssm-ratio=2, mlp-ratio=4)
Ô£Æ

Ô£π

VSSBLock(ssm-ratio=2, mlp-ratio=4)
Ô£Æ

Ô£π

Linear 192 ‚Üí ssm-ratio √ó192
DWConv 3√ó3, ssm-ratio √ó192
SS2D, dim ssm-ratio √ó192
Linear ssm-ratio √ó192 ‚Üí 192
FFN mlp-ratio √ó192

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 192 ‚Üí ssm-ratio √ó192
DWConv 3√ó3, ssm-ratio √ó192
SS2D, dim ssm-ratio √ó192
Linear ssm-ratio √ó192 ‚Üí 192
FFN mlp-ratio √ó192

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 256 ‚Üí ssm-ratio √ó256
DWConv 3√ó3, ssm-ratio √ó256
SS2D, dim ssm-ratio √ó256
Linear ssm-ratio √ó256 ‚Üí 256
FFN mlp-ratio √ó256

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

stage 3

14√ó14

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

VSSBLock(ssm-ratio=1, mlp-ratio=4)
Ô£Æ

Ô£π

Linear 384 ‚Üí ssm-ratio √ó384
DWConv 3√ó3, ssm-ratio √ó384
SS2D, dim ssm-ratio √ó384
Linear ssm-ratio √ó384 ‚Üí 384
FFN mlp-ratio √ó384

√ó8

Ô£∫
Ô£∫
Ô£∫
Ô£ª

conv 3√ó3 stride 2, LayerNorm

VSSBlock(ssm-ratio=2, mlp-ratio=4)

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 384 ‚Üí ssm-ratio √ó384
DWConv 3√ó3, ssm-ratio √ó384
SS2D, dim ssm-ratio √ó384
Linear ssm-ratio √ó384 ‚Üí 384
FFN mlp-ratio √ó384

Ô£π

√ó15

Ô£∫
Ô£∫
Ô£∫
Ô£ª

conv 3√ó3 stride 2, LayerNorm

stage 4

7√ó7

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

VSSBLock(ssm-ratio=1, mlp-ratio=4)
Ô£Æ

Ô£π

VSSBlock(ssm-ratio=2, mlp-ratio=4)
Ô£Æ

Ô£π

Linear 768 ‚Üí ssm-ratio √ó768
DWConv 3√ó3, ssm-ratio √ó768
SS2D, dim ssm-ratio √ó768
Linear ssm-ratio √ó768 ‚Üí 768
FFN mlp-ratio √ó768

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 768 ‚Üí ssm-ratio √ó768
DWConv 3√ó3, ssm-ratio √ó768
SS2D, dim ssm-ratio √ó768
Linear ssm-ratio √ó768 ‚Üí 768
FFN mlp-ratio √ó768

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

VSSBLock(ssm-ratio=2, mlp-ratio=4)
Ô£Æ

Ô£π

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 512 ‚Üí ssm-ratio √ó512
DWConv 3√ó3, ssm-ratio √ó512
SS2D, dim ssm-ratio √ó512
Linear ssm-ratio √ó512 ‚Üí 512
FFN mlp-ratio √ó512

√ó15

Ô£∫
Ô£∫
Ô£∫
Ô£ª

VSSBLock(ssm-ratio=2, mlp-ratio=4)

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

Linear 1024 ‚Üí ssm-ratio √ó1024
DWConv 3√ó3, ssm-ratio √ó1024
SS2D, dim ssm-ratio √ó1024
Linear ssm-ratio √ó1024 ‚Üí 1024
FFN mlp-ratio √ó1024

Ô£π

√ó2

Ô£∫
Ô£∫
Ô£∫
Ô£ª

1√ó1

Param. (M)

FLOPs

30.2

4.91√ó109

average pool, 1000-d fc, softmax

50.1

8.72√ó109

88.6

15.36√ó109

Table 5: Details of accelerating VMamba.

Model

d_state ssm-ratio DWConv

Vanilla-VMamba-T
Step(a)
Step(b)
Step(c)
Step(d)
Step(e)
Step(f)
Step(g)

16
16
16
16
16
16
1
1

2.0
2.0
2.0
2.0
2.0
1.0
2.0
1.0

‚úì
‚úì
‚úì
‚úì

‚úì
‚úì

(G)

(M)

FFN

Params FLOPs

branch
‚úì
‚úì
‚úì
‚úì
‚úì

TP.
multiculative layers
(img/s)
numbers
426
22.9M 5.63G
[2,2,9,2]
467
22.9M 5.63G
[2,2,9,2]
464
22.9M 5.63G
[2,2,9,2]
638
[2,2,9,2]
22.9M 5.63G
[2,2,2,2] ‚úì 29.0M 5.63G
813
[2,2,5,2] ‚úì 26.2M 4.86G 1179
[2,2,5,2] ‚úì 30.7M 4.86G 1340
[2,2,8,2] ‚úì 30.2M 4.91G 1686

Train TP. Top-1
(%)
(img/s)
82.17
138
82.17
165
82.17
184
82.17
195
81.65
248
82.17
360
82.49
464
82.60
571

According to the results summarized in Table 9, VMamba demonstrates the most stable performance
across (i.e., modest performance drop) different input image sizes, achieving a top-1 classification
accuracy of 74.7% without fine-tuning (79.2% with linear tuning), while maintaining a relatively
high throughput of 149 images per second at an input resolution of 768 √ó 768. In comparison,
Swin [37] achieves the second-highest performance with a top-1 accuracy of 73.1% without fine-
tuning (77.5% under linear tuning) at the same input size, when using scaled window sizes (set
as the resolution divided by 32). However, its throughput significantly drops to 53 images per second.
Furthermore, ConvNeXt [38] maintains a relatively high inference speed (i.e., a throughput of 103
images per second) with the largest input resolution. However, its classification accuracy drops to
69.5% when directly tested on images of size 768 √ó 768, indicating its limited adaptability w.r.t. input
images with large spatial resolutions. The performance of Deit-S also drops dramatically, mainly due
to the use of interpolation in the absolute positional embedding.

The performance variations across different resolutions for various models are more intuitively
illustrated in Figure 11. Notably, VMamba displays a linear increase in computational complexity
(Figure 4 (b)), as measured by FLOPs, which is comparable to CNN-based architectures. This finding
aligns with the theoretical conclusions drawn from selective SSMs [17].

19

Table 6: Performance comparison on ImageNet-1K with image size 224. ‚Ä† indicates that Vim is
trained only in float32 in practical, in which case the train throughput is 232. [71].

Model

Image Params FLOPs
(M)
Size
(G)
2242
22M 4.6G
DeiT-S [58]
2242
86M 17.5G
DeiT-B [58]
2242
29M 4.5G
ConvNeXt-T [38]
2242
50M 8.7G
ConvNeXt-S [38]
2242
ConvNeXt-B [38]
89M 15.4G
2242
19M 4.6G
HiViT-T [68]
2242
38M 9.1G
HiViT-S [68]
2242
66M 15.8G
HiViT-B [68]
2242
28M 4.5G
Swin-T [37]
2242
50M 8.7G
Swin-S [37]
2242
Swin-B [37]
88M 15.5G
2242
26M 4.9G
XCiT-S12/16
2242
48M 9.2G
XCiT-S24/16
2242
84M 16.2G
XCiT-M24/16
S4ND-ConvNeXt-T [45] 2242
30M
2242
89M
S4ND-ViT-B [45]
2242
26M
Vim-S [71]
2242
23M 5.6G
Vanilla-VMamba-T
2242
44M 11.2G
Vanilla-VMamba-S
2242
76M 18.0G
Vanilla-VMamba-B
2242
30M 4.9G
VMamba-T
2242
VMamba-S[s2l15]
50M 8.7G
2242
VMamba-B[s2l15]
89M 15.4G
2242
VMamba-S[s1l20]
49M 8.6G
2242
VMamba-B[s1l20]
87M 15.2G

-
-
-

TP.
(img/s)
1761
503
1198
684
436
1393
712
456
1244
718
458
1283
671
423
683
398
811
638
359
268
1686
877
646
1106
827

Train TP. Top-1
(%)
(img/s)
79.8
2404
81.8
1032
82.1
702
83.1
445
83.8
334
82.1
1304
83.5
698
83.8
544
81.3
987
83.0
642
83.5
496
82.0
935
82.6
509
82.7
385
82.2
369
80.4
400
344‚Ä†
80.5
82.2
195
83.5
111
83.7
84
82.6
571
83.6
314
83.9
247
83.3
390
83.8
313

Figure 11: Throughtput and Top-1 accuracy change on ImageNet-1K w.r.t resolution rises. Swin-T‚àó
denotes Swin-T tested with scaled window sizes.

H Ablation Study

H.1

Influence of the Scanning Pattern

In the main submission, we validate the effectiveness of the proposed scanning pattern (referred to as
Cross-Scan) in SS2D by comparing to three alternative image traversal approaches, i.e., Unidi-Scan,
Bidi-Scan, and Cascade-Scan (Figure 12). Notably, as Unidi-Scan, Bidi-Scan, and Cross-Scan are all
implemented in Triton, they exhibit little difference in throughput. Nevertheless, the findings in
Table 10 indicate that Cross-Scan exhibits superior data modeling capacity, evident from its higher
classification accuracy. This advantage is likely attributed to the two-dimensional prior introduced
by the four-way scanning design. Nevertheless, the practical implementation of Cascade-Scan is
notably constrained by its relatively slow computational pace. This limitation primarily stems from
the inadequate compatibility between selective scanning and high-dimensional data, exacerbated by
the multi-step scanning procedure.

20

Throughput w.r.t.resolution risesTop-1 acc. on ImageNet1K w/o finetuning w.r.t.resolution rises(ùëè)(ùëê)Top-1 acc. on ImageNet1K w/ linear finetuning w.r.t.resolution risesThroughput (img/s)(ùëé)Top-1 acc. (%)Top-1 acc. (%)Table 7: Object detection and instance segmentation results on COCO dataset. The FLOPs are
calculated using inputs of size 1280 √ó 800. Here, AP b and AP m denote box AP and mask AP,
respectively. "1√ó" indicates models fine-tuned for 12 epochs, while "3√óMS" signifies the utilization
of multi-scale training for 36 epochs.

Backbone

Swin-T
ConvNeXt-T
Vanilla-VMamba-T
VMamba-T

Swin-S
ConvNeXt-S
Vanilla-VMamba-S
VMamba-S

42.7
44.2
46.5
47.3

44.8
45.4
48.2
48.7

Swin-B
46.9
47.0
ConvNeXt-B
Vanilla-VMamba-B 48.6
49.2
VMamba-B

APb APb

Mask R-CNN 1√ó schedule
75 APm APm
62.2
39.3
63.3
40.1
65.5
42.1
66.4
42.7

50 APb
46.8
48.3
50.7
52.0

65.2
66.6
68.5
69.3

50 APm
75
42.2
42.8
45.3
45.9

66.6
67.9
69.7
70.0

‚Äì
69.4
70.0
71.4

48.9
50.0
52.5
53.4

‚Äì
51.7
53.1
54.0

40.9
41.8
43.0
43.7

42.3
42.7
43.3
44.1

63.4
65.2
66.6
67.3

‚Äì
66.3
67.1
68.3

Mask R-CNN 3√ó MS schedule

Swin-T
ConvNeXt-T
Vanilla-VMamba-T
VMamba-T

Swin-S
ConvNeXt-S
Vanilla-VMamba-S
VMamba-S

46.0
46.2
48.5
48.8

48.2
47.9
49.7
49.9

68.1
67.9
70.0
70.4

69.8
70.0
70.4
70.9

50.3
50.8
52.7
53.5

52.8
52.7
54.2
54.7

41.6
41.7
43.2
43.7

43.2
42.9
44.0
44.2

65.1
65.0
66.9
67.4

67.0
66.9
67.6
68.2

Params

FLOPs

48M
48M
42M
50M

69M
70M
64M
70M

267G
262G
286G
271G

354G
348G
400G
349G

107M 496G
108M 486G
96M
540G
108M 485G

48M
48M
42M
50M

69M
70M
64M
70M

267G
262G
286G
271G

354G
348G
400G
349G

44.2
45.1
46.4
47.0

‚Äì
46.0
46.7
47.7

44.9
44.9
46.4
47.0

46.1
46.2
47.3
47.7

Figure 12: Illustration of different scanning patterns for selective scan.

Figure 13 indirectly demonstrates that among the analyzed scanning methods, only Bidi-Scan,
Cascade-Scan, and Cross-Scan showcase global ERFs. Moreover, only Cross-Scan and Cascade-Scan
exhibit two-dimensional (2D) priors. It is also noteworthy that DWConv [24] plays a significant role in
establishing 2D priors, thereby contributing to the formation of global ERFs.

21

Unidi-ScanBidi-ScanCascade-Scan Row and Col√óCross Scan+Table 8: Semantic segmentation results on ADE20K using UperNet [65]. We evaluate the performance
of semantic segmentation on the ADE20K dataset with UperNet [65]. The FLOPs are calculated with
input sizes of 512 √ó 2048. "SS" and "MS" denote single-scale and multi-scale testing, respectively.

method

crop size mIoU (SS) mIoU (MS)

Params

FLOPs

Swin-T
ConvNeXt-T
Vanilla-VMamba-T
VMamba-T

Swin-S
ConvNeXt-S
Vanilla-VMamba-S
VMamba-S

Swin-B
ConvNeXt-B
Vanilla-VMamba-B
VMamba-B

5122
5122
5122
5122

5122
5122
5122
5122

5122
5122
5122
5122

44.5
46.0
47.3
48.0

47.6
48.7
49.5
50.6

48.1
49.1
50.0
51.0

45.8
46.7
48.3
48.8

49.5
49.6
50.5
51.2

49.7
49.9
51.3
51.6

60M
60M
55M
62M

945G
939G
964G
949G

81M 1039G
82M 1027G
76M 1081G
82M 1028G

121M 1188G
122M 1170G
110M 1226G
122M 1170G

Figure 13: The visualization of ERF for models with different scanning patterns.

H.2

Influence of the Initialization Approach

In our study, we adopted the initialization scheme initially proposed for the SS2D block in S4D [19].
Therefore, it is necessary to investigate the contribution of this initialization method to the effective-
ness of VMamba. To delve deeper into this matter, we replaced the default initialization with two
alternative methods: random initialization and zero initialization.

For both initialization methods, we set the parameter D in equation 1 to a vector of all ones,
mimicking a basic skip connection (thus we have y = Ch + Du). Additionally, the weights and
biases associated with the transformation from low rank to the dimension Dv (which matches the
input size), are initialized as random vectors, in contrast to Mamba [17], which employs a more
sophisticated approach for initialization.

The main distinction between random and zero initialization lies in the parameter A in equation 5,
which is typically initialized as a HiPPO matrix in both Mamba [17, 19] and our implementation of
VMamba. Given that we selected the hyper-parameter d_state to be 1, the Mamba initialization
for log(A) can be simplified to all zeros, which aligns with zero initialization. In contrast, random
initialization assigns a random vector to log(A). We choose to initialize log(A) rather than A directly
to ensure that A remains close to the all-ones matrix when the network parameters are near zero,
which empirically helps enhance the stability of the training process.

The experiment results in Table 11 indicate that, at least for image classification with SS2D blocks,
the model‚Äôs performance is not significantly affected by the initialization method. Therefore, within
this context, the sophisticated initialization method employed in Mamba [17] can be substituted with
a simpler, more straightforward approach. We also visualize the ERF maps of models trained with

22

Before trainingAfter training1.00.80.60.40.20.0Unidi-ScanBidi-ScanCascade-ScanCross-ScanUnidi-ScanBidi-ScanCascade-ScanCross-Scanw/ DWConvw/o DWConvFigure 14: The visualization of ERF of VMamba with different initialization.

different initialization methods (see Figure 14), which intuitively reflect the robustness of SS2D
across various initialization schemes.

H.3

Influence of the d_state Parameter

Throughout this study, we primarily set the value of d_state to 1 to optimize the computational
speed of VMamba. To further explore the impact of d_state on the model‚Äôs performance, we
conduct a series of experiments.

As shown in Table 12, with all other hyper-parameters frozen, we increase d_state from 1 to 4. This
leads to a minor improvement in performance but a substantial decrease in throughput, indicating
a significant negative influence on the computational efficiency of VMamba. However, increasing
d_state to 8, while reducing ssm-ratio to maintain computational complexity, leads to improved
accuracy. However, when d_state is further increased to 16, with ssm-ratio set to 1, performance
drops.

These findings suggest that modest increases in d_state may not necessarily lead to improved
performance. Instead, selecting the optimal combination of d_state and ssm-ratio is crucial to
achieving a good trade-off between inference speed and performance.

H.4

Influence of ssm-ratio, mlp-ratio, and layer numbers

In this section, we investigate the trade-off between ssm-ratio, layer numbers, and mlp-ratio.

Experiment results shown in Table 13 suggest that reducing ssm-ratio leads to a notable decrease in
performance but significantly enhances the inference speed. Conversely, increasing layer numbers
enhances the performance while slowing down the model.

As the hyper-parameter ssm-ratio stands for the dimension employed by the SS2D module,
the trade-off between ssm-ratio and layer numbers can be interpreted as a balance between
channel-mixing and token-mixing [57]. Furthermore, we reduce mlp-ratio from 4.0 to 2.0
and progressively increase ssm-ratio to maintain constant FLOPs, as shown in Table 14. The
results presented in Tables 13 and 14 underscore the importance of an optimal combination of
ssm-ratio, mlp-ratio, and layer numbers for constructing a model that achieves a balance
between effectiveness and efficiency.

H.5

Influence of the Activation Function

In VMamba, the SiLU [14] activation function is utilized to build the SS2D block. However,
experiment results in Table 15 reveal that VMamba maintains robustness across different activation
functions. This implies that the selection of the activation function does not substantially affect the
model‚Äôs performance. Therefore, there is flexibility in choosing an appropriate activation function
based on computational constraints or other preferences.

23

Before trainingAfter trainingMamba-InitRand-InitZero-Init1.00.80.60.40.20.0Table 9: Comparison of generalizability to inputs with increased spatial resolutions. The throughput
and train throughput are measured with batch size of 32 using Pytorch 2.0, on an A100 GPU paired
with an AMD EPYC 7542 CPU. Only model forward, loss forward and backward are included in
calculating train throughput. we re-implemented the HiViT-T, as the checkpoint of HiViT-T has not
been released. ‚Ä† denotes that the batch size ‚â§ 16 due to out-of-memory (OOM).

Model

Image
Size

Param.
(M)

FLOPs
(G)

TP.
(img/s)

Train TP.
(img/s)

Top-1
acc. (%)

Top-1 acc. (%)
(w/ linear tuning)

VMamba-Tiny

VMamba-Tiny[s2l5]

Vanilla-VMamba-Tiny

Swin-Tiny

XCiT-S12/16

HiViT-Tiny

DeiT-Small

ConvNeXt-Tiny

2242
2882
3842
5122
6402
7682

2242
2882
3842
5122
6402
7682

2242
2882
3842
5122
6402
7682

2242
2882
3842
5122
6402
7682

2242
2882
3842
5122
6402
7682

2242
2882
3842
5122
6402
7682

2242
2882
3842
5122
6402
7682

2242
2882
3842
5122
6402
7682

SSM-Based

30M
4.91G
8.11G
30M
30M 14.41G
30M 25.63G
30M 40.04G
30M 57.66G

4.86G
31M
31M
8.03G
31M 14.27G
31M 25.38G
31M 39.65G
31M 57.09G

5.63G
23M
23M
9.30G
23M 16.53G
23M 29.39G
23M 45.93G
23M 66.14G

1490
947
566
340
214
149

1227
761
452
272
170
117

628
390
212
138
87
52

Transformer-Based

4.51G
28M
28M
7.60G
28M 14.05G
28M 26.65G
28M 45.00G
29M 70.72G

4.87G
26M
26M
8.05G
26M 14.31G
26M 25.44G
26M 39.75G
26M 57.24G

4.60G
19M
7.93G
19M
19M 15.21G
20M 30.56G
20M 54.83G
20M 91.41G

4.61G
22M
22M
7.99G
22M 15.52G
22M 31.80G
23M 58.17G
23M 98.70G

1142
638
316
176
88
53

1127
724
425
244
158
111

1261
750
388
186
93
55

1573
914
502
261
149
90

ConvNet-Based

4.47G
29M
7.38G
29M
29M 13.12G
29M 23.33G
29M 36.45G
29M 52.49G

1107
696
402
226
147
103

24

418
303
187
121
75
53

399
255
155
100
60
42

189
117
65
53
27
18

769
489
268
131
68
38

505
462
308
185
122
87

1041
614
333
150
71
37‚Ä†

1306
1124
697
387
244
156

614
403
240
140
90
63

82.60
82.95
82.41
80.92
78.60
74.66

82.49
82.81
82.51
81.07
79.30
76.06

82.17
82.74
82.40
81.05
78.79
75.09

81.19
81.46
80.67
78.97
76.55
73.06

81.87
82.44
81.84
79.80
76.84
72.52

81.92
82.45
81.51
79.30
76.09
71.38

80.69
80.80
78.87
74.21
68.04
60.98

82.05
82.23
81.05
78.03
74.27
69.50

82.64
83.03
82.77
81.88
80.62
79.22

82.52
82.93
82.74
82.02
81.02
79.69

82.09
82.76
82.72
81.97
80.71
79.12

81.18
81.62
81.12
80.21
78.89
77.54

81.89
82.44
82.21
80.92
79.00
76.92

81.85
82.42
81.91
80.49
78.58
76.47

80.40
80.63
79.54
76.91
73.31
69.62

81.95
82.30
81.78
80.37
78.77
76.89

Table 10: The performance of VMamba-T with different scanning patterns.

(G)

Model

Params FLOPs

TP.
(M)
(img/s)
VMamba w/ dwconv
30.2M 4.91G 1682
30.2M 4.91G 1687
998
30.2M 4.91G 1686

Unidi-Scan
Bidi-Scan
Cascade-Scan 30.2M 4.91G
Cross-Scan

VMamba w/o dwconv

30.2M 4.89G 1716
Unidi-Scan
Bidi-Scan
30.2M 4.89G 1719
Cascade-Scan 30.2M 4.90G 1007
Cross-Scan
30.2M 4.89G 1717

Train TP. Top-1
(%)
(img/s)

571
572
308
571

578
578
309
577

82.26
82.49
82.42
82.60

80.88
81.80
81.71
82.25

Table 11: The performance of VMamba-T with different initialization.

initialization

mamba
rand
zero

Params FLOPs

(M)
30.2
30.2
30.2

(G)
4.91
4.91
4.91

TP.
(img/s)
1686
1682
1683

Train TP. Top-1
(img/s)
571
570
570

acc. (%)
82.60
82.58
82.67

Table 12: The performance of VMamba-T with different d_state.

d_state ssm-ratio

1
2
4
8
16

2.0
2.0
2.0
1.5
1.0

Params FLOPs

(M)
30.7
30.8
31.0
28.6
26.3

(G)
4.86
4.98
5.22
5.04
4.87

TP.
(img/s)
1340
1269
1147
1148
1164

Train TP. Top-1
(img/s)
464
432
382
365
358

acc. (%)
82.49
82.50
82.51
82.69
82.31

Table 13: The performance of VMamba-T under different combination of ssm-ratio and layer
numbers.

ssm-ratio

2.0
1.0
1.0

layer
numbers
[2,2,5,2]
[2,2,5,2]
[2,2,8,2]

Params FLOPs

(M)
30.7
25.6
30.2

(G)
4.86
3.98
4.91

TP.
(img/s)
1340
1942
1686

Train TP. Top-1
(img/s)
464
647
571

acc. (%)
82.49
81.87
82.60

Table 14: The performance of VMamba under different combination of ssm-ratio and mlp-ratio.

mlp-ratio ssm-ratio

4.0
3.0
2.0

1.0
1.5
2.5

Params FLOPs

(M)
30.2
28.5
29.9

(G)
4.91
4.65
4.95

TP.
(img/s)
1686
1419
1075

Train TP. Top-1
(img/s)
571
473
361

acc. (%)
82.60
82.75
82.86

Table 15: The performance of VMamba-T with different activation functions in SS2D.

activation

SiLU
GELU
ReLU

Params FLOPs

(M)
30.2
30.2
30.2

(G)
4.91
4.91
4.91

TP.
(img/s)
1686
1680
1684

Train TP. Top-1
(img/s)
571
570
577

acc. (%)
82.60
82.53
82.65

25

