A Mathematical Theory of Semantic

Communication

Kai Niu, Member, IEEE, Ping Zhang, Fellow, IEEE

1

4
2
0
2

r
a

M
7
2

]
T
I
.
s
c
[

2
v
7
8
3
3
1
.
1
0
4
2
:
v
i
X
r
a

Abstract

The year 1948 witnessed the historic moment of the birth of classic information theory (CIT).

Guided by CIT, modern communication techniques have approached the theoretic limitations, such as,

entropy function H(U ), channel capacity C = maxp(x) I(X; Y ) and rate-distortion function R(D) =
minp(ˆx|x):Ed(x,ˆx)≤D I(X; ˆX). Semantic communication paves a new direction for future communication

techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework

of semantic information theory (SIT). We investigate the behavior of semantic communication and find

that synonym is the basic feature so we define the synonymous mapping between semantic information

and syntactic information. Stemming from this core concept, synonymous mapping f , we introduce
the measures of semantic information, such as semantic entropy Hs( ˜U ), up/down semantic mutual
information I s( ˜X; ˜Y ) (Is( ˜X; ˜Y )), semantic capacity Cs = maxfxy maxp(x) I s( ˜X; ˜Y ), and semantic
rate-distortion function Rs(D) = min{fx,fˆx} minp(ˆx|x):Eds(˜x,ˆ˜x)≤D Is( ˜X; ˆ˜X). Furthermore, we prove
three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that

is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion

coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is,
Hs( ˜U ) ≤ H(U ), Cs ≥ C and Rs(D) ≤ R(D). All these works composite the basis of semantic

information theory. In addition, we discuss the semantic information measures in the continuous case.

Especially, for the band-limited Gaussian channel, we obtain a new channel capacity formula, Cs =

B log

(cid:104)

S4 (cid:16)

(cid:17)(cid:105)

1 + P

N0B

, where the average synonymous length S indicates the identification ability of

information. In summary, the theoretic framework of SIT proposed in this paper is a natural extension

of CIT and may reveal great performance potential for future communication.

This work is supported by the National Natural Science Foundation of China (No. 62293481, 62071058). K. Niu and P. Zhang

are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications,

Beijing, 100876, China (e-mail: {niukai, pzhang}@bupt.edu.cn).

Index Terms

2

Synonymous mapping, Semantic entropy, Semantic relative entropy, Up/Down semantic mutual

information, Semantic channel capacity, Semantic distortion, Semantic rate distortion function, Seman-

tically typical set, Synonymous typical set, Semantically jointly typical set, Jointly typical decoding,

Jointly typical encoding, Synonymous length, Maximum likelihood group decoding, Semantic source

channel coding.

I. INTRODUCTION

Classic information theory (CIT), established by C. E. Shannon [1] in 1948, was a great

achievement in the modern information and communication field. As shown in Fig. 1, the classic

communication system includes source, encoder, channel with noise, decoder and destination.

This theory is concerned with the uncertainty of information and introduces four critical mea-

sures, such as entropy, mutual information, channel capacity, and rate-distortion function to

evaluate the performance of information processing and transmission. Especially, three famous

coding theorems, such as, lossless/lossy source coding theorem and channel coding theorem,

reveal the fundamental limitation of data compression and information transmission. Over the past

70 years or so, people developed many advanced techniques to approach these theoretical limits.

For the lossless source coding, Huffman coding and arithmetic coding are the representative

optimal coding methods can achieve the source entropy. Similarly, for the channel coding, polar

code, as a great breakthrough [23], is the first constructive capacity-achieving coding scheme.

Correspondingly, for the lossy source coding, some modern coding schemes, such as BPG (Better

Portable Graphics) standard and H. 265/266 standard, can approach the rate-distortion lower

bounds of image and video sources. It follows that information and communication technologies

guided by CIT have approached the theoretical limitation and the performance improvement of

modern communication systems encounters a lot of bottlenecks.

Essentially, Weaver [3], just one year after Shannon published the seminal paper on information

theory, pointed out that communication involves problems at three levels as follows:

“LEVEL A. How accurately can the symbols of communication be transmitted? (The technical

problem.)

LEVEL B. How precisely do the transmitted symbols convey the desired meaning? (The

semantic problem.)

3

Fig. 1. The block diagram of classic communication system.

LEVEL C. How effectively does the received meaning affect conduct in the desired way? (The

effectiveness problem.)”

Shannon [1] wrote that “semantic aspects of communication are irrelevant to the engineering

problem”. Thus, classic information theory only handles LEVEL A (technical) problem of the

information. On the contrary, date back to the time of classic information theory birth, many

works also focused on LEVEL B problem and the semantic communication theory. Carnap and

Bar-Hillel [6] and Floridi [7] considered using propositional logic sentences to express semantic

information. They introduced the semantic information entropy, which is calculated based on

logical probability [8] rather than statistical probability in CIT. Then Bao et al. [9] extended

this theoretical framework and derived the semantic source coding and semantic channel coding

theorem based on propositional logic probabilities. On the other hand, De Luca et al. [10]

[11] regarded semantic information as fuzzy variable and defined fuzzy entropy to measure the

uncertainty of semantic information. Then Wu [12] extended this work and introduced general

entropy, general conditional entropy, and general mutual information based on fuzzy variable.

However, the propositional logic or fuzzy variables are only suitable for the simple processing of

text or speech source and can not sufficiently describe semantic information of the complex data,

such as image or video source. Furthermore, although some recent works [13–16] investigated

the theoretic property of semantic information, e.g. rate-distortion function, designing semantic

communication system still lacks of systematic guiding theory.

Recently, semantic communication systems based on deep learning demonstrate excellent

performance than the traditional counterparts. Many works [17–20] investigated the design

principles and technical challenges of semantic communication. As surveyed in [21, 22], semantic

communication techniques become a hot topic in the communication community and provide a

ChannelSourceEncoderDecoderDestinationNoise4

promising methodology to break through the Shannon limits. However, semantic communication

research faces a dilemma. We can neither precisely answer what is the semantic information or

the meaning of information nor provide the fundamental limits to guide the design of the semantic

communication system. Thus, there is an urgent necessity to establish a mathematical theory of

semantic communication in order to solve these basic problems.

If we want to establish a semantic information theory, we should firstly consider the essence of

semantic information, that is, what is the meaning of information. Let’s investigate the semantic

information from the source side and the destination side respectively. Figure 2 shows some

examples of semantic information of text, speech, image and video source. In Fig. 2(a), the

words “happy, joyful, content, joyous” have the same or similar meaning, that is, all these text

data means “happy”. From the viewpoint of linguistics, these words compose the synonym of

“happy”. From the understanding of human beings, we can regard these words as having the

same semantic information. So we can use synonym alternation to generate the same meaning

presentation, as shown in the following sentences

“She appeared happy and content after receiving the good news”.

“She appeared joyful and content after receiving the good news”.

Although these two sentences have different presentations, the processing of the language center

of the brain, they have the same semantic information. Generally, such synonym phenomenon

ubiquitously exists in various language texts. Many different presentations of phrases, words, and

sentences have the same or similar meaning and compose the synonymous mapping to indicate

the same semantic information. Therefore, we conclude that synonymous mapping is a critical

feature of semantic information of text data.

Similarly, synonym phenomenon can also be observed in speech source. As shown in Fig.

2(b), a piece of speech has three presentations, that is, waveform, Mel spectrogram, and power

spectrogram. However, all these presentations indicate the same meaning “Don’t ask me to carry

an oily rag like that”. So we conclude that all these speech waveform or spectrograms compose

the synonym presentation and have the same semantic information.

We can also find the synonym objects in image source, as depicted in Fig. 2(c). After the

object-oriented segmentation, this figure is decomposed into four objects, that is, sky, lake, bears,

and ground. Intuitively, single pixel in this figure have no meaning whereas a set of many pixels

indicates some meaningful object. Furthermore, for the frame sequence shown in Fig. 2(d), by

5

Fig. 2. Semantic examples of source data.

using the object extraction method, we can obtain the meaning of this video “A jockey guides

his horse in a gallop”. From the two examples, it follows that synonymous mapping for the

semantic understanding is a popular phenomenon in image and video source.

On the other hand, in the destination side, we also observe the synonym phenomenon for

various downstream tasks. Figure 3 depicts some representative examples. For the task of

happy, joyful, content, joyous...Media Source DataSemantic Information (Meaning)happysynonymDon't ask me to carry an oily rag like that.synonym(a) Synonym in textSkyLakeBearsGroundsynonymous object-oriented segmentation(b) Synonym in speech(c) Synonym in imagetsynonymous object extractionA Jockey guides his horse in a gallop.t(d) Synonym in video6

character recognition in Fig. 3(a), different shapes and fonts of the images in each row present

the same letter. So we easily derive the meaning from these images, that is, “SEMANTIC”.

Similarly, for the task of image classification, shown in Fig. 3(b), various images stands for the

same entity or object, such as forest, staircase, ocean etc. Furthermore, Fig. 3(c) depicts the task

of obstacle detection and the marker boxes presents the pedestrians. Hence, we can conclude

that many downstream tasks involve the synonymous mapping and semantic reasoning.

Fig. 3. Semantic examples of downstream task.

Remark 1. In a word, after inspecting various examples of source and destination, we can

summarize these two rules for the semantic information processing.

(b) Image classificationForestStaircaseOceanSynonymSynonymSynonym(a) Character recognitionSEMANTICSynonymSynonymSynonymSynonymSynonymSynonymSynonymSynonym(c) Obstacle detectionPedestriansMedia DataSemantic Information (Meaning)Synonym7

(1) All the perceptible messages, such as text, speech, image, video, and so on, are syntactic

information. However, based on these messages, we can derive or reason some semantic in-

formation. There are common and stable mappings between the syntactic information and the

semantic information. These mappings can be built from the knowledge of human beings or the

configuration conditions of the application scene.

(2) Generally, the relationships between syntactic information and semantic information are

very complex. However, in most instances, the semantic information has a single meaning

whereas the presentations of source data and downstream tasks are myriads. So synonymous

mapping is a major relationship and popularly exists in various tasks of semantic reasoning.

Certainly, there may exist the ambiguity of semantic information. Nonetheless, such ambiguity

is secondary and can be removed by using multiple interactions. So in this paper, we mainly

handle the synonym of semantic information and the ambiguity will be left to future works.

In this paper, we aim to establish a systematic framework of semantic information theory as

the natural extension of classic information theory. The outline of the theoretic framework is

shown in Fig. 4.

Therefore, the contributions of this paper can be summarized as follows.

(1) We develop a systematic model for semantic communication with specific design criteria.

In this model, semantic information remains invisible but perceptible, characterized by

its prevalent synonymous features. Hence, we recognize synonymy as the fundamental

aspect of semantic information. To illustrate the connection between semantic and syntactic

information, we introduce synonymous mapping which is an one-to-many mapping from

the semantic alphabet to syntactic alphabet. Essentially, synonymous mapping constructs an

equivalent class relationship between the semantic space and the syntactic space.

(2) Stemming for a synonymous mapping fu, we introduce the semantic variable ˜U associated
with a random variable U . Essentially, the semantic variable is also a random variable. Hence,
we define the semantic entropy Hs( ˜U ) to measure the uncertainty of semantic information

and name the unit as semantic bit (sebit). Analogous to the classic information theory, this

measure can also be extended to semantic conditional/joint entropy. In addition, we prove

that all the semantic entropies are no more than their classic counterparts.

(3) We introduce the concepts of semantic relative entropy and semantic mutual information.

8

Fig. 4. The theoretic framework of semantic information.

Unlike the classic mutual information, it is noted that we use two measures to indicate
semantic mutual information, such as the up semantic mutual information I s( ˜X; ˜Y ) =
H(X)+H(Y )−Hs( ˜X, ˜Y ) and the down semantic mutual information Is( ˜X; ˜Y ) = Hs( ˜X)+
Hs( ˜Y ) − H(X, Y ). Furthermore, given a channel with the transition probability p(y|x), we
can use the semantic capacity Cs = maxfxy maxp(x) I s( ˜X; ˜Y ) to indicate the maximum
transmission rate of semantic information on the channel. Correspondingly, given a source
with the distribution p(x) and the average distortion measure constraint E
≤ D,
the semantic rate distortion Rs(D) = min{fx,f˜x} minp(ˆx|x):Eds(˜x,ˆ˜x)≤D Is( ˜X; ˆ˜X) reveals the
minimum compression rate of semantic information in the case of lossy source coding.

(cid:105)
(cid:104)
ds(˜x, ˆ˜x)

(4) We investigate the asymptotic equipartition property (AEP) in the semantic sense and intro-

duce the semantically typical set ˜A(n)
ϵ which presents all the information of semantic variable
˜U . Especially, we prove that, under the synonymous mapping, the syntactically typical set

Synonymous MappingSemantic EntropySemantic Mutual InformationUp Semantic MIDown Semantic MISemantic CapacitySemantic Rate DistortionSemantic Channel Coding TheoremSemantic Rate Distortion TheoremSemantic Source Coding TheoremGaussian Sem. entropyGaussian Sem. CapacityGaussian Sem. R(D)Lossless Semantic Source Channel Coding TheoremLossy Semantic Source Channel Coding Theorem()sHU(;)sXYI(;)sXYI2212log2seHS=(;)maxssCIXY=(;)()minssRIXYD=40log1sPCBSNB=+41log()2sPRSDD=ssHCR()ssRDRCs(),(ˆIf ,,ˆOtherwie,,)(),sssRRdxxDdxDDxIf ,lim0Otherwise,,lim1,()()nsennenPPRH→→→→If ,lim0Otherwise,,lim1,()()nsennenPPRC→→→→9

A(n)
ϵ

can be equipartitioned into many synonymous typical sets B(n)

ϵ

. By using random coding

and synonymous mapping between a semantic typical sequence to the synonymous typical set
B(n)
, we prove the lossless source coding theorem, that is, given a syntactic source U ∼ p(u)
ϵ
and the associated semantic variable ˜U , if the semantic code rate R > Hs( ˜U ), there exists a
semantic source code satisfies P (n)
e → 0 with a sufficiently large code length n, otherwise,

any codes cannot satisfy the lossless reconstruction of semantic information. Furthermore,

we extend the Kraft inequality to the semantic version and provide an example of semantic

Huffman coding. Thanks to the synonymous mapping and coding, the compression rate

of semantic lossless source coding can be further lowered and the semantic compressive

efficiency can outperform the classic source coding.

(5) Consider the problem of channel transmission, we investigate the jointly asymptotic equipar-

tition property (JAEP) of semantic version and define the corresponding jointly typical set.

Particularly, we find that, under the jointly synonymous mapping, the syntactically jointly

typical set can be evenly divided into a series of jointly synonymous typical sets. By using

random coding on jointly synonymous typical sets and jointly typical decoding, we prove

the semantic channel coding theorem. This theorem reveals that given a channel with the

transition probability p(y|x), under the jointly synonymous mapping fxy, if the code rate is
lower than the semantic capacity, i.e., R < Cs = maxfxy maxp(x) I s( ˜X, ˜Y ), there exists a
semantic channel code can satisfy the requirement of asymptotic error-free transmission, that
is P (n)

e → 0 with sufficiently large n, on the contrary, if R > Cs, the error probability of any

code can not tend to zero. Since the semantic capacity Cs is no less than the classic capacity

C, i.e., Cs ≥ C, we conclude that semantic channel coding can improve the capability of

information transmission. Furthermore, inspired by the jointly synonymous mapping, we

consider the decoding rule of semantic channel code and propose a maximum likelihood

group (MLG) decoding algorithm. Unlike traditional ML decoding, the MLG algorithm

calculates all the group likelihood probabilities and selects one synonymous codeword of

the group with the maximum likelihood probability as the final result. Hence, we define

the group Hamming distance as the construction metric and derive the group-wise error

probability to evaluate the performance of the semantic channel code.

(6) Consider the problem of lossy source coding, we define the semantic distortion and the

jointly typical set of source and reconstruction sequence. By using jointly typical encoding

10

based on synonymous typical set, we prove the semantic rate distortion coding theorem.
This theorem states that given a source X ∼ p(x) with the associated semantic source ˜X,
the synonymous mappings fx, fˆx, and the bounded semantic distortion function ds(˜x, ˆ˜x), if
the code rate R > Rs(D), there exist a sequences of semantic source codes, the semantic
distortion satisfies Eds( ˜X, ˆ˜X) < D with sufficiently large n. Conversely, if R < Rs(D),
then the semantic distortion of any code meets Eds( ˜X, ˆ˜X) > D. Since the semantic rate
distortion Rs(D) is no more than the classic counterpart R(D), that is, Rs(D) ≤ R(D),

it follows that semantic source coding can further compress the source data and achieve a

lower rate than the classic source coding.

(7) We also investigate the measure of semantic information in the continuous case. Given

a continuous random variable U ∼ p(u) and a synonymous mapping f , we define the
entropy of the associated semantic variable ˜U as Hs( ˜U ) = − (cid:82) p(u) log p(u)du − log S,
where S is named as the average synonymous length. Furthermore, we extend the semantic

conditional/joint entropy and semantic mutual information to the continuous case. Especially,
(cid:1)(cid:3) and a lower
we derive the semantic capacity of Gaussian channel Cs = 1
(cid:1) where P is the signal power and σ2 is the variance of Gaussian
bound C s = 1
noise. In addition, we obtain the channel capacity formula of time-limited, band-limited

2 log (cid:2)S4 (cid:0)1 + P

2 log (cid:0)1 + S4 P

σ2

σ2

and power-limited Gaussian channel, that is, Cs = B log

S4 (cid:16)
(cid:104)

1 + P
N0B

(cid:17)(cid:105)

where B is the

bandwidth and N0 is the single-sided power spectral density of white Gaussian noise. In

addition, we also obtain the rate distortion function of Gaussian source, that is, Rs(D) =
log P

S4D where P is the power of signal sample and D is the constraint of semantic distortion.
(8) Finally, we inspect the source and channel coding problem in the semantic sense. Both for

lossless and lossy cases, we prove the semantic source channel coding theorem. We find that
the code rate of semantic communication system satisfies Rs(D)(Hs( ˜U )) ≤ R ≤ Cs. Com-

pared with the classic communication system, the code rate range of sematic communication

can be further extended. This point reveals the significant potential of semantic coding.

The remainder of the paper is organized as follows. Section II presents the systematic model

of semantic communication and introduces the concept of synonymous mapping. In Section III,

we define the semantic entropy, the semantic joint and conditional entropy and discuss the basic

properties of these measures. In Section IV we define the semantic relative entropies and the

11

up/down semantic mutual information and discuss the corresponding properties. Then in Section

V, the semantic channel capacity and the semantic rate distortion function are introduced and in-

vestigated. Furthermore, we explore the semantically asymptotic equipartition property and prove

the semantic source coding theorem in Section VI. We extend the jointly asymptotic equipartition

property in the semantic sense. By using random coding and jointly typical decoding, we prove

the semantic channel coding theorem in Section VII. Correspondingly, By using jointly typical

encoding, we prove the semantic rate distortion coding theorem in Section VIII. In addition, we

extend the semantic information measures to the continuous case in Section IX. Especially, we

derive the semantic capacity of Gaussian channel and the rate distortion of Gaussian source. In

Section X, we prove the semantic source channel coding theorem. Finally, Section XI concludes

the paper.

II. SEMANTIC COMMUNICATION SYSTEM AND SYNONYMOUS MAPPING

In this section, we first introduce the system model of semantic communication and describe

the design criteria of semantic information transmission. Then we clarify the function of syn-

onymous mapping and emphasize its key role in semantic communication.

A. Notation Conventions

In this paper, calligraphy letters, such as X and Y, are mainly used to denote sets, and the

cardinality of X is defined as |X |. The Cartesian product of X and Y is written as X × Y.
Let X n denote the n-th Cartesian power of X and (cid:81)n
k=1 Xk denote the Cartesian product of n
sets X1, · · · , Xn. We write un to denote an n-dimensional vector (u1, u2, · · · , un). We use the

summation convention N[i1:im] to denote the integer summation of Ni1 + · · · + Nij + · · · + Nim,
where ∀Nij ∈ N.

We use f : X → Y to denote a mapping from X to Y. Furthermore. the extended mapping

f n : X n → Y n denotes an element-wise sequential mapping from X n to Y n.

We use dH(un, vn) to denote the Hamming distance between the binary vector un and vn.
Given ∀an, bn ∈ Rn, let ∥an − bn∥ denote the Euclidian distance between the vector an and bn.

Throughout this paper, log (·) means “logarithm to base 2” and ln (·) stands for the natural

logarithm. Let (x)+ = max(x, 0) be the non-negative part of x. Let (·)T denote the transpose

operation of the vector. Let E(Z) and Var(Z) denote the expectation and the variance of the

12

random variable Z respectively.

B. Semantic Communication System

The block diagram of semantic communication system is presented in Fig. 5. Compared

with the system model of classic communication in Fig. 1, the semantic communication system

extends the range of information processing and adds extra modules, such as semantic source,

semantic destination, synonymous mapping and demapping.

Fig. 5. The block diagram of semantic communication system.

At the transmitting side, the semantic source ˜U generates a semantic sequence and drives the

syntactical source U produces a message. With the help of synonymous mapping f , the encoder

transforms the syntactical message into a codeword and sends to the channel. On the other

hand, the receiver performs the reverse operations. The decoder recovers the codeword from the

received signal and feeds it into the syntactical destination V to reconstruct the message. Then
the semantic destination ˜V obtains the message and reasons the meaning.

According to the general understanding of semantic information, the system design of semantic

communication should obey the following criteria.

ChannelSyntactical Source EncoderDecoderSyntactical DestinationSemantic SourceSemanticDestinationSynonymous MappingfSynonymous Demapping g13

(i) Invisibility of semantic information Generally, semantic information is implied in syn-

tactic information and cannot be directly observed. We only observe various data, such

as text, speech, image or video so as to understand or infer the meaning of the message.

Therefore, we highlight that the semantic information is invisible but perceptible. So only

the syntactic information can be encoded and indirectly reveals the semantic information.

(ii) Synonym of semantic information The relationship between semantic information and

syntactic information is diverse and complex. However, whether it is single-modal or

multi-modal data, synonym, that is, one meaning has multiple manifestations, is a popular

phenomenon. The basic characteristic of synonyms is an one-to-many mapping, which

means that we can use a set of synonymous data to indicate the same semantic information.

Commonly, this mapping is predefined or deterministic.

By the first criterion, the semantic source/destination is a virtual module and implied behind

the syntactic source/destination. Although Shannon stated that semantic information is irrelevant

to the engineering problem, it is only suitable for the communication in Level A. If we design

the communication system of Level B, the semantic source/dstination will indirectly affect the

syntactical encoding and decoding process.

Correspondingly, by the second criterion, the synonymous mapping is deterministic rather than

statistic, which is constructed by the common knowledge of transmitter/receiver or the system

requirement. For the source coding, this mapping is mainly determined by the understanding of

message. Although people have different opinions, they all use the same background knowledge.

Therefore, knowledge base in the semantic communication system is a representative example.

On the other hand, for the channel coding, this mapping can be created based on the transmission

requirement. Unlike the traditional communication system, aided by the synonymous mapping,

the reliability requirement of semantic communication can be relaxed from one-bit-no-error to

tolerance of some error bits. This point will be further explained in Section VII.

C. Synonymous mapping

Now we formally introduce the definition of synonymous mapping as follows.

Definition 1. Given a syntactic information set U = {u1, · · · , ui, · · · , uN } and the corresponding
semantic information set ˜U = {˜u1, · · · , ˜uis, · · · , ˜u ˜N }, the synonymous mapping f : ˜U → U is

defined as the one-to-many mapping between ˜U and U.

14

Generally, the size of set ˜U is no more than U = {u1, u2, · · · , uN }, that is, ˜N ≤ N .
Furthermore, under the synonymous mapping f , U is partitioned into a group of synonymous
(cid:84) Ujs = ∅.

uN[1:(is−1)]+1, · · · , uN[1:(is−1)]+j, · · · , uN[1:(is−1)]+Nis

and ∀is ̸= js, Uis

(cid:110)

(cid:111)

sets Uis =
Therefore, we have |Uis| = Nis and U = (cid:83) ˜N

is=1 Uis.

Essentially, the synonymous mapping f generates an equivalence class partition of the syntactic

set. So we can construct the quotient set U/f = {Uis}.

For an arbitrary element ˜ui ∈ ˜U, is = 1, 2, · · · , ˜N , we have f : ˜uis → Uis. In the case of
non-confusion, for this mapping, we can drop out the subscript and present the synonymous set

as U˜u,

Fig. 6. An example of synonymous mapping between the semantic information set and the syntactic information set.

Figure 6 depicts an example of synonymous mapping. Each semantic element can be mapped

into an equivalent set of syntactic elements and every set has one or many elements. Further,

there is no overlap between any two sets.

Remark 2. In fact, the semantic variable ˜U is also a random variable. However, we emphasize

that it is implied behind the random variable U and can deduce the syntactic message u. In many

applications of deep learning, after the nonlinear processing of neural networks, the semantic

features of input source data are extracted and mapped into a latent space. Here, the nonlinear

mapping can be regarded as an instance of synonymous mapping and the latent space can be

Semantic information setSyntactical information set Synonymous mappingftreated as sample space of semantic variable.

III. SEMANTIC ENTROPY

In this section, we first give the definition of semantic entropy, that is, the measure of semantic

information. Then we discuss the property of semantic joint entropy and semantic conditional

15

entropy.

A. Semantic Information Measures

Let ˜U = {˜uis}

˜N
is=1 and U = {ui}N

i=1 be a semantic alphabet and a syntactic alphabet re-
spectively. Let U be a discrete random variable with alphabet U and probability mass function
p(u) = Pr {U = u} , u ∈ U. Given a synonymous mapping f : ˜U → U, the semantic information
of a semantic symbol ˜uis ∈ ˜U can be measured as follows,

Is (˜uis) = − log (p (Uis)) = − log

N[1:(is−1)]+Nis
(cid:88)







p (ui)

 .

(1)

Definition 2. Given a discrete random variable U , the corresponding semantic variable ˜U , and
the synonymous mapping f : ˜U → U, the semantic entropy of semantic variable ˜U is defined by

i=N[1:(is−1)]+1

Hs( ˜U ) = −

˜N
(cid:88)

is=1

p (Uis) log p (Uis)

˜N
(cid:88)

N[1:(is−1)]+Nis
(cid:88)

= −

p (ui) log

N[1:(is−1)]+Nis
(cid:88)







p (ui)



(2)

is=1

i=N[1:(is−1)]+1

i=N[1:(is−1)]+1

= −

˜N
(cid:88)

(cid:88)

is=1

i∈Nis

p (ui) log



p (ui)

 ,





(cid:88)

i∈Nis

where Nis = (cid:8)N[1:(is−1)] + 1, · · · , N[1:(is−1)] + Nis

(cid:9) is the index set associated with Uis.
Essentially, semantic entropy is a functional of the distribution of U and the synonymous mapping

f . Similar to information entropy, semantic entropy also indicates the integrity attribute of the
variable ˜U rather than single sample. Furthermore, it depends on the synonymous set partition

determined by the mapping f . In Eq. (2), the log is taken to the base 2 and we name it as the

semantic binary digit, that is, sebit. So the unit of semantic information is expressed as sebit.

The semantic entropy of ˜U can also be interpreted as the expectation of log

1

p(U˜u) , that is,

Hs( ˜U ) = Ep

(cid:20)

log

(cid:21)

.

1
p (U˜u)

For the semantic entropy, we have the following consequences by the definition.

Lemma 1.

Hs( ˜U ) ≥ 0.

16

(3)

(4)

Proof: Due to 0 ≤ p(ui) ≤ 1 and (cid:80)N

i=1 p(ui) = 1, we have 0 ≤ p(Uis) = (cid:80)

i∈Nis

So it follows that log

1

p(Uis ) ≥ 0.

p (ui) ≤ 1.
□

Lemma 2. The semantic entropy is no more than the associated information entropy, that is,

Hs( ˜U ) ≤ H(U ).

Proof: According to the definition of Eq. (2), we have

Hs( ˜U ) − H(U ) = −

˜N
(cid:88)

(cid:88)

is=1

i∈Nis

p (ui) log



p (ui)







(cid:88)

i∈Nis

N
(cid:88)

+

p (ui) log p (ui)

i=1
˜N
(cid:88)

(cid:88)

is=1

i∈Nis

p (ui) log

p (ui)

(cid:80)

p (ui)

i∈Nis

N
(cid:88)

i=1

p (ui) log 1 = 0.

=

≤

(5)

(6)

□

Lemma 3. The semantic entropy H( ˜U ) is a concave function of p(u).

The proof is referred to Appendix A.

Theorem 1. Hs( ˜U ) ≤ log | ˜U|, where | ˜U| = ˜N stands for the number of semantic symbols. The
equality holds if and only if the synonymous set Uis is uniformly distributed over U and U has

arbitrary distribution over Uis.

TABLE I
PROBABILITY DISTRIBUTION OF SYNTACTIC SOURCE U AND SYNONYMOUS MAPPING OF SEMANTIC SOURCE ˜U .

17

U

p(u)
˜U

p(˜u)

u1

0.3

u2

u3

0.15

0.15

u4

0.2

u5

0.1

u6

0.1

˜u1 → {u1}

˜u2 → {u2, u3}

˜u3 → {u4}

˜u4 → {u5, u6}

0.3

0.3

0.2

0.2

Proof: By the maximal entropy theorem of discrete source, we can easily obtain the

conclusion and further derive that the equality holds when p (˜uis) = 1
˜N
the elements in Uis can take an arbitrary distribution.

= (cid:80)

i∈Nis

p (ui). So
□

Note that the maximal semantic entropy log | ˜U| is no more than the maximum of information

entropy log |U|. That means the uncertainty of semantic information can be further reduced with

the help of synonymous mapping.

Example 1. Table I gives a probability distribution of source U and the associated semantic
variable ˜U under a synonymous mapping. The entropy of source U is calculated as H(U ) =
− (cid:80)6
i=1 p(ui) log p(ui) = 2.471 bits. Correspondingly, the semantic entropy of ˜U is calculated as
Hs( ˜U ) = − (cid:80)4
is=1 p(˜uis) log p(˜uis) = 1.971 sebits. Evidently, we observe that Hs( ˜U ) < H(U ).

B. Semantic joint entropy and semantic conditional entropy

The definition of semantic entropy can be further extended to a pair of semantic variables.

First, we introduce the jointly/conditionally synonymous mapping as following.

Definition 3. Given a pair of discrete semantic variables ( ˜U , ˜V ) and the corresponding random
variable pairs (U, V ), fu : ˜U → U denotes the synonymous mapping from ˜U to U and we
have fu : ˜uis → Uis where 1 ≤ is ≤ | ˜U| = ˜Nu and 1 ≤ i ≤ |U| = Nu. Similarly, we can
define the synonymous mapping fv : ˜V → V. Furthermore, the jointly synonymous mapping
fuv : ˜U × ˜V → U × V is defined as

Correspondingly, give a symbol ui, the conditionally synonymous mapping fv|u : ˜V|u → V|u

fuv : (˜uis, ˜vjs) → Uis × Vjs.

(7)

is defined as

fv|u : ˜vjs|ui → Vjs|ui.

(8)

Definition 4. Given a pair of discrete semantic variables ( ˜U , ˜V ) and the corresponding random
variable pairs (U, V ) with a joint distribution p(u, v), under the joint mapping fuv : ˜U × ˜V →
U × V, the semantic joint entropy Hs( ˜U , ˜V ) is defined as

18

Hs( ˜U , ˜V ) = −

˜Nu(cid:88)

˜Nv(cid:88)

p (Uis × Vjs) log p (Uis × Vjs)

= −

is=1

js=1

˜Nu(cid:88)

˜Nv(cid:88)

is=1

js=1

· log

(cid:88)

p (ui, vj)

(ui,vj )∈Uis ×Vjs
(cid:88)

(ui,vj )∈Uis ×Vjs

p (ui, vj) .

Lemma 4. The semantic joint entropy is no more than the joint entropy, that is,

Hs( ˜U , ˜V ) ≤ H(U, V )

Proof: By the definition of Eq. (9), we have

(cid:16) ˜U , ˜V

(cid:17)

Hs

− H(U, V )

˜Nu(cid:88)

˜Nv(cid:88)

(cid:88)

= −

is=1

js=1

(ui,vj )

p (ui, vj) log

(cid:88)

(ui,vj )

p (ui, vj)

+

=

≤

(cid:88)

(ui,vj )

(cid:88)

(ui,vj )
(cid:88)

(ui,vj )

p (ui, vj) log p (ui, vj)

p (ui, vj) log

p (ui, vj)
(ui,vj ) p (ui, vj)

(cid:80)

p (ui, vj) log 1 = 0

(9)

(10)

(11)

□

We also define the semantic conditional entropy of a semantic variable given another random

variable as follows.

Definition 5. Given a pair of discrete semantic variables ( ˜U , ˜V ) and the corresponding random
variable pairs (U, V ) ∼ p(u, v), under the conditional mapping fv|u : ˜V|u → V|u, the semantic

conditional entropy Hs( ˜V |U ) is defined as

Hs( ˜V |U ) = −

= −

Nu(cid:88)

˜Nv(cid:88)

i=1

js=1

Nu(cid:88)

˜Nv(cid:88)

p (ui) p (Vjs |ui ) log p (Vjs |ui )

(cid:88)

p (ui, vj) log

(cid:88)

p (vj |ui ) .

i=1

js=1

(ui,vj )

vj ∈Vjs |ui

Similarly, we can also define the semantic conditional entropy Hs( ˜U |V ).

19

(12)

Lemma 5. The semantic conditional entropy is no more than the conditional entropy, that is,

(cid:16) ˜V |U

(cid:17)

Hs

≤ H (V |U )

Proof: The proof follows along the same lines as Lemma 4.

Similarly, we also attain the following lemma.

(13)

□

Lemma 6. If the semantic variables ˜U and ˜V are regarded as a new random variable respec-

tively, the conditional entropy obeys the following relation, that is,

(cid:16) ˜V | ˜U

(cid:17)

H

(cid:16)

V | ˜U

(cid:17)

.

≤ H

(14)

The chain rule of entropies with two variables can be indicated by the following theorem.

Theorem 2. (Chain Rule of Entropies with Two Variables):

Hs( ˜U ) + Hs( ˜V |U ) ≤ Hs( ˜U , ˜V ) ≤ H(V ) + Hs( ˜U |V ) ≤ H(U, V )

(15)

Proof: For the left inequality, due to the property of joint probability, we can write

(cid:88)

p(ui, vj) ≤

(ui,vj )∈Uis ×Vjs





(cid:88)

ui∈Uis





p(ui)





(cid:88)

vj ∈Vis |ui



p(vj |ui )

 .

(16)

So we take the negative logarithm and expectation of inequality to prove this inequality. The

equality holds if and only if U and V are mutually independent. By using a similar method, we
can prove Hs( ˜V ) + Hs( ˜U |V ) ≤ Hs( ˜U , ˜V ).

Similarly, we can write

(cid:88)

p (vj)

p (ui |vj ) ≤

(cid:88)

p (ui, vj) .

ui∈Uis |vj

(ui,vj )∈Uis ×Vjs

(17)

Take the negative logarithm and expectation of inequality, we prove the medium inequality.

Correspondingly, we also write

(cid:88)

p (ui)

p (vj |ui ) ≤

(cid:88)

p (ui, vj)

vj ∈Vjs |ui

(ui,vj )∈Uis ×Vjs

and conclude that Hs( ˜U , ˜V ) ≤ H(U ) + Hs( ˜V |U ).

By using similar methods, we write

p(vj |ui )

(cid:88)

ui∈Uis

p (ui) ≤

(cid:88)

p (ui, vj)

(ui,vj )∈Uis ×Vjs

and conclude that Hs( ˜U , ˜V ) ≤ Hs( ˜U ) + H(V |U ).

In addition, we also write

p (ui |vj )

(cid:88)

vj ∈Vjs

p (vj) ≤

(cid:88)

p (ui, vj)

(ui,vj )∈Uis ×Vjs

20

(18)

(19)

(20)

and conclude that Hs( ˜U , ˜V ) ≤ Hs( ˜V ) + H(U |V ).

By Lemma 5, due to H(V )+Hs( ˜U |V )−H(U, V ) = H(V )+Hs( ˜U |V )−H(V )−H(U |V ) ≤ 0,
we prove the right inequality. Similarly, we can attain that H(U ) + Hs( ˜V |U ) ≤ H(U, V ). □

Definition 6. Given a semantic sequence ( ˜U1, ˜U2, · · · , ˜Un) and the associated syntactic sequence
(U1, U2, · · · , Un), f n : ˜U n → U n denotes the sequential synonymous mapping from ˜U to U and
we have

f n : (˜un) →

n
(cid:89)

U˜uk.

(21)

Correspondingly, give a subvector uk−1
1 → U|uk−1

is defined as

˜U|uk−1

1

1

k=1

, the conditionally synonymous mapping fuk|uk−1

1

:

fuk|uk−1

1

: ˜uk|uk−1

i → U˜uk|uk−1

1

.

(22)

Theorem 3. (Chain Rule of Sequential Entropy):

Given a semantic sequence ( ˜U1, ˜U2, · · · , ˜Un) and the associated syntactic sequence (U1, U2, · · · , Un),

under the sequential synonymous mapping f n and conditional synonymous mappings fuk|uk−1
we have

1

,

n
(cid:88)

Hs( ˜Uk

(cid:12)
(cid:12)U k−1
1

) ≤ Hs( ˜U n)

k=1
≤ ˜H( ˜U n−1

1

, Un) ≤ · · · ≤ ˜H( ˜U m

1 , U n

m+1)

≤ · · · ≤ ˜H( ˜U1, U n
2 )

≤ H(U n) =

n
(cid:88)

k=1

H(Uk

(cid:12)
(cid:12)U k−1
1

)

where ˜H( ˜U m

m+1) = (cid:80)m
1 , U n
denote sequential entropies.

k=1 H( ˜Uk

(cid:12)
(cid:12)
(cid:12)

˜U k−1
1

)+(cid:80)n

k=m+1 H(Uk

(cid:12)
(cid:12)
(cid:12)

˜U m

1 , U k−1

m+1 ) with m = n−1, · · · , 2, 1

21

(23)

Proof: In order to prove inequality (cid:80)n

(cid:12)
(cid:12)U k−1
1
of joint probability and synonymous mappings, we can write

k=1 Hs( ˜Uk

) ≤ Hs( ˜U n), by using the property

(cid:88)

1 ∈(cid:81)n
un

k=1 U˜uk

p(u1, u2, · · · , un) ≤

n
(cid:89)

(cid:88)

k=1

uk∈U˜uk

|uk−1
1

p(uk

(cid:12)
(cid:12)uk−1
1

).

(24)

So we take the negative logarithm and expectation to prove this inequality. The equality holds

if and only if all elements Uk are mutually independent.

˜H( ˜U m

By using the chain rule of sequential entropy, after a permutation of sequence, we have
m+1, ˜U m−1

)+H( ˜Um|U n

m) = ˜H(U n

m+1, ˜U m−1

m+1) = ˜H(U n
m+1, ˜U m−1

) and ˜H( ˜U m−1
). According to Lemma 6, it follows that H( ˜Um|U n

m+1, ˜U m−1
) ≤ H(Um|U n

, U n
m+1, ˜U m−1

1 , U n
H(Um|U n

)+
m+1, ˜U m−1

1

1

1

1

1

1

1

).

So we can attain that

˜H( ˜U m

1 , U n

m+1) ≤ ˜H( ˜U m−1

1

, U n

m), for m = n − 1, · · · , 2,

and

˜H( ˜U1, U n

2 ) ≤ H(U n).

(25)

(26)

□

Remark 3. In classic information theory, the joint entropy of a pair of random variables is the

entropy of one variable plus the conditional entropy of the other variable, that is, H (U, V ) =

H(U ) + H(V |U ). On the other hand, the semantic joint entropy does not satisfy the addition of

entropy and degrades to an inequality of information entropy plus semantic conditional entropy,
that is, Hs( ˜U , ˜V ) ≤ H(U ) + Hs( ˜V |U ) or Hs( ˜U , ˜V ) ≤ H(V ) + Hs( ˜U |V ).

Example 2. Table II gives a joint probability distribution of random variable pair (U, V ). Table
III illustrates the distribution of the associated semantic variable pair ( ˜U , ˜V ) under a joint

synonymous mapping fuv. Table IV and Table V give the conditional distribution of the semantic
variables ˜U |V and ˜V |U . The marginal distributions of the semantic variable ˜U and ˜V are

depicted in Table VI.

22

JOINT PROBABILITY DISTRIBUTION OF RANDOM VARIABLE PAIR (U, V ).

TABLE II

(U, V )

(u1, v1)

(u1, v2)

(u1, v3)

(u1, v4)

(u1, v5)

p(u, v)

0.05

0.1

0.15

0

0

(U, V )

(u2, v1)

(u2, v2)

(u2, v3)

(u2, v4)

(u2, v5)

p(u, v)

0.1

0.05

0.05

0.1

0

(U, V )

(u3, v1)

(u3, v2)

(u3, v3)

(u3, v4)

(u3, v5)

p(u, v)

0.1

0.05

0

0

0.05

(U, V )

(u4, v1)

(u4, v2)

(u4, v3)

(u4, v4)

(u4, v5)

p(u, v)

0.05

0

0

0.1

0.05

TABLE III
JOINT SYNONYMOUS MAPPING OF SEMANTIC VARIABLE PAIR ( ˜U , ˜V ).

fuv

(˜u1, ˜v1) → {(u1, v1), (u2, v1)}

(˜u1, ˜v2) → {(u1, v2), (u2, v2)}

p(˜u, ˜v)

0.15

0.15

fuv

(˜u1, ˜v3) → {(u1, v3), (u2, v3)}

(˜u1, ˜v4) → {(u1, v4), (u1, v5), (u2, v4), (u2, v5)}

p(˜u, ˜v)

0.2

0.1

fuv

(˜u2, ˜v1) → {(u3, v1), (u4, v1)}

(˜u2, ˜v2) → {(u3, v2), (u4, v2)}

p(˜u, ˜v)

0.15

0.05

fuv

(˜u2, ˜v3) → {(u3, v3), (u4, v3)}

(˜u2, ˜v4) → {(u3, v4), (u4, v4), (u3, v5), (u4, v5)}

p(˜u, ˜v)

0

0.2

The joint entropy of (U, V ) is calculated as H(U, V ) = − (cid:80)4

i=1

(cid:80)5

j=1 p(ui, vj) log p(ui, vj)

= 3.5842 bits.

The semantic joint entropy of ( ˜U , ˜V ) is Hs( ˜U , ˜V ) = − (cid:80)2

is=1

(cid:80)4

js=1 p(˜uis, ˜vjs) log p(˜uis, ˜vjs)

= 2.7087 sebits.

The conditional entropies are H(U |V ) = − (cid:80)4
(cid:80)4

and H(V |U ) = − (cid:80)5

j=1

i=1 p(ui, vj) log p(vj|ui) = 1.6132 bits.

(cid:80)5

j=1 p(ui, vj) log p(ui|vj) = 1.3377 bits

i=1

23

Correspondingly, the semantic conditional entropies are Hs( ˜U |V ) = − (cid:80)2

· log p(˜uis|vj) = 0.6623 sebits and Hs( ˜V |U ) = − (cid:80)4
Thus we have Hs( ˜U |V ) = 0.6623 sebits < H(U |V ) = 1.3377 bits and Hs( ˜V |U ) = 1.4755 sebits <
H(V |U ) = 1.6132 bits.

is=1
i=1 p(˜vjs, ui) log p(˜vjs|ui) = 1.4755 sebits.

j=1 p(˜uis, vj)

(cid:80)5

(cid:80)4

js=1

The entropies of random variables U and V are calculated as H(U ) = H(0.3, 0.3, 0.2, 0.2) =

1.971 bits and H(V ) = H(0.3, 0.2, 0.2, 0.2, 0.1) = 2.2464 bits respectively. Then, the semantic
entropies of ˜U and ˜V are Hs( ˜U ) = Hs(0.6, 0.4) = 0.971 sebits and Hs( ˜V ) = Hs(0.3, 0.2, 0.2, 0.3) =
1.971 sebits respectively. So it follows that Hs( ˜U , ˜V ) = 2.7087 sebits < H(U, V ) = 3.5842 bits,
Hs( ˜V ) + Hs( ˜U |V ) = 2.6633 sebits < Hs( ˜U , ˜V ) = 2.7087 sebits < H(V ) + Hs( ˜U |V ) =
2.9087 sebits, and Hs( ˜U ) + Hs( ˜V |U ) = 2.4465 sebits < Hs( ˜U , ˜V ) = 2.7087 sebits < H(U ) +
Hs( ˜V |U ) = 3.4465 sebits.

TABLE IV
CONDITIONAL SYNONYMOUS MAPPING OF SEMANTIC VARIABLE ˜U |V .

˜U |V

p(˜u|v)
˜U |V

p(˜u|v)

˜u1|v1 → {u1, u2}|v1

˜u1|v2 → {u1, u2}|v2

˜u1|v3 → {u1, u2}|v3

˜u1|v4 → {u1, u2}|v4

˜u1|v5 → {u1, u2}|v5

0.5

0.75

1

0.5

0

˜u2|v1 → {u3, u4}|v1

˜u2|v2 → {u3, u4}|v2

˜u2|v3 → {u3, u4}|v3

˜u2|v4 → {u3, u4}|v4

˜u2|v5 → {u3, u4}|v5

0.5

0.25

0

0.5

1

TABLE V
CONDITIONAL SYNONYMOUS MAPPING OF SEMANTIC VARIABLE ˜V |U .

˜V |U

p(˜v|u)
˜V |U

p(˜v|u)
˜V |U

p(˜v|u)
˜V |U

p(˜v|u)

˜v1|u1 → {v1}|u1

˜v1|u2 → {v1}|u2

˜v1|u3 → {v1}|u3

˜v1|u4 → {v1}|u4

1/6

1/3

0.5

0.25

˜v2|u1 → {v2}|u1

˜v2|u2 → {v2}|u2

˜v2|u3 → {v2}|u3

˜v2|u4 → {v2}|u4

1/3

1/6

0.25

0

˜v3|u1 → {v3}|u1

˜v3|u2 → {v3}|u2

˜v3|u3 → {v3}|u3

˜v3|u4 → {v3}|u4

0.5

1/6

0

0

˜v4|u1 → {v4, v5}|u1

˜v4|u2 → {v4, v5}|u2

˜v4|u3 → {v4, v5}|u3

˜v4|u4 → {v4, v5}|u4

0

1/3

0.25

0.75

TABLE VI
SYNONYMOUS MAPPINGS OF SEMANTIC VARIABLES ˜U AND ˜V .

24

fu

˜u1 → {u1, u2}

˜u2 → {u3, u4}

fv

˜v1 → {v1}

˜v2 → {v2}

˜v3 → {v3}

˜v4 → {v4, v5}

p(˜u)

0.6

0.4

p(˜v)

0.3

0.2

0.2

0.3

IV. SEMANTIC RELATIVE ENTROPY AND MUTUAL INFORMATION

In this section, we apply the synonymous mapping to define semantic relative entropy and

semantic mutual information. Three semantic relative entropies are measures of the distance

between two semantic/syntactic variables. We introduce two measures, such as up/down semantic

mutual information to evaluate the reduction in the semantic information of one variable due to

the knowledge of the other variable.

A. Semantic Relative Entropy

In classic information theory, the relative entropy or Kullback Leibler distance D (p∥q) =
u∈U p(u) log p(u)

is used to measure the difference between two probability mass function

q(u)

(cid:80)

p(u) and q(u). Similarly, by the synonymous mapping, we can define the semantic relative

entropy as following.

Definition 7. Given the semantic variable ˜U and the random variable U with two probability
mass function p(u) and q(u), under the synonymous mapping f : ˜U → U, the full semantic

relative entropy is defined as

Ds (ps∥qs) =

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

Two partial semantic relative entropies are defined as

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

p(ui)
q(ui)

.

and

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

Ds (ps∥q) =

Ds (p∥qs) =

p(ui) log

(cid:80)

ui∈Uis

p(ui)

q(ui)

p(ui) log

(cid:80)

p(ui)

q(ui)

,

.

ui∈Uis
Hereafter, in order to simplify the presentation, we use ps = (cid:80)

ui∈Uis

is=1

to stand for two different probability distributions of semantic variable ˜U .

(27)

(28)

(29)

p(ui) and qs = (cid:80)

q(ui)

ui∈Uis

ui∈Uis

Similar to the relative entropy, we have the following basic inequality for the semantic relative

entropy.

Theorem 4. (Semantic information inequality): Let p(u), q(u), u ∈ U, be two probability mass

function. Given the synonymous mapping f , we have

25

with equality if and only if






Ds(ps∥qs) ≥ 0

Ds(ps∥q) ≥ Ds(ps∥p)

Ds(p∥qs) ≥ Ds(p∥ps)






(cid:88)

ui∈Uis

(cid:88)

ui∈Uis

p(ui) =

(cid:88)

ui∈Uis

q(ui),

for all Uis

p(ui) = q(ui),

for all Uis

p(ui) =

(cid:88)

ui∈Uis

q(ui),

for all Uis

Proof: For the first inequality, by Jensen’s inequality, we can write

−Ds (ps∥qs) = −

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

p(ui)
q(ui)

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

q(ui)
p(ui)

q(ui)
p(ui)

˜N
(cid:88)

(cid:88)

=

is=1

ui∈Uis

p(ui) log

≤ log

= log

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui)

q(ui)

= log 1 = 0.

(30)

(31)

(32)

For the second inequality, we can write

Ds (ps∥q) =

=

+

≥

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

p(ui) log

(cid:80)

p(ui)

ui∈Uis

q(ui)

(cid:80)

p(ui)

ui∈Uis

p(ui)

p(ui) log

p(ui)
q(ui)

p(ui) log

(cid:80)

p(ui)

ui∈Uis

p(ui)

For the third inequality, we further derive as follows

= Ds(ps∥p) ≥ 0.

Ds (p∥qs) =

=

+

≥

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

p(ui) log

p(ui) log

p(ui) log

= Ds(p∥ps).

p(ui)

(cid:80)

q(ui)

ui∈Uis

p(ui)

(cid:80)

ui∈Uis

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

p(ui)

p(ui)
q(ui)

p(ui)

(cid:80)

p(ui)

ui∈Uis

26

(33)

(34)

Note that Ds (p∥qs) may be negative. In the practical application, we can take the non-negative
□

value, that is, (Ds (p∥qs))+.

Corollary 1.

Ds(p∥qs) ≤ Ds(ps∥qs) ≤ Ds(ps∥q).

(35)

Proof: For the left inequality, we can write

Ds(p∥qs) − Ds(ps∥qs)

=

−

=

≤

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

(cid:80)

p(ui)

q(ui)

ui∈Uis

p(ui) log

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

p(ui)
q(ui)

p(ui) log

(cid:80)

p(ui)

p(ui)

ui∈Uis

p(ui) log 1 = 0.

For the right inequality, we can write

Ds(ps∥qs) − Ds(ps∥q)

=

−

=

≤

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

p(ui) log

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

p(ui)
q(ui)

(cid:80)

ui∈Uis

p(ui)

q(ui)

q(ui)

q(ui)

ui∈Uis

p(ui) log

(cid:80)

p(ui) log 1 = 0.

Corollary 2.

Ds(p∥qs) ≤ D(p∥q) ≤ Ds(ps∥q).

27

(36)

(37)

□

(38)

Proof: For the left inequality, we can write

Ds(p∥qs) − D(p∥q)

=

−

=

≤

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

(cid:80)

p(ui)

q(ui)

ui∈Uis

p(ui) log

p(ui)
q(ui)

p(ui) log

(cid:80)

q(ui)

q(ui)

ui∈Uis

p(ui) log 1 = 0.

For the right inequality, we can write

Ds(p∥q) − Ds(ps∥q)

=

−

=

≤

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

˜N
(cid:88)

(cid:88)

is=1

ui∈Uis

p(ui) log

p(ui)
q(ui)

p(ui) log

p(ui) log

(cid:80)

(cid:80)

ui∈Uis

p(ui)

q(ui)

p(ui)

p(ui)

ui∈Uis

p(ui) log 1 = 0.

28

(39)

(40)

□

Theorem 5. Ds(ps∥qs), Ds(ps∥q), and Ds(p∥qs) are convex in the pair (p, q). Equivalently,

given (p1, q1) and (p2, q2) are two pairs of probability mass functions, for all 0 ≤ θ ≤ 1, we

have






Ds(θps,1 + (1 − θ)ps,2∥θqs,1 + (1 − θ)qs,2)

≤ θDs(ps,1∥qs,1) + (1 − θ)Ds(ps,2∥qs,2)

Ds(θps,1 + (1 − θ)ps,2∥θq1 + (1 − θ)q2)

≤ θDs(ps,1∥q1) + (1 − θ)Ds(ps,2∥q2)

Ds(θp1 + (1 − θ)p2∥θqs,1 + (1 − θ)qs,2)

≤ θDs(p1∥qs,1) + (1 − θ)Ds(p2∥qs,2)

29

(41)

The proof is referred to Appendix B.

Remark 4. In neural network model, relative entropy or cross entropy is an important cost

function used to training. For some deep learning application involved semantic information,

such as clustering, classification, recognition, we can use semantic relative entropy to alternate

the classic counterpart so as to further improve the system performance.

B. Semantic Mutual Information

We now introduce up semantic mutual information, which is a partial relative entropy to

indicate the large reduction in the semantic information of one variable due to the knowledge

of the other.

Definition 8. Consider two semantic variables ˜U and ˜V and two associated random variables

U and V with a joint probability mass function p(u, v) and marginal probability mass function
p(u) and p(v). Given the jointly synonymous mapping fuv : ˜U × ˜V → U × V, the up semantic
mutual information I s( ˜U ; ˜V ) is the partial entropy between the joint distribution ps (u, v) and
the product distribution p(u)p(v), i.e.,

I s( ˜U ; ˜V ) = −

˜Nu(cid:88)

˜Nv(cid:88)

is=1

js=1

(cid:88)

p (ui, vj)

(ui,vj )∈Uis ×Vjs
p (ui) p (vj)

· log

(cid:80)

(ui,vj )∈Uis ×Vjs

p (ui, vj)

=Ds (ps (u, v) ∥p(u)p(v))

=H(U ) + H(V ) − Hs( ˜U , ˜V ).

(42)

Similarly, the down semantic mutual information Is( ˜U ; ˜V ) is the partial entropy between the

joint distribution p (u, v) and the product distribution ps(u)ps(v), i.e.,

30

Is( ˜U ; ˜V ) = −

˜Nu(cid:88)

˜Nv(cid:88)

is=1

js=1

(cid:88)

p (ui, vj)

(cid:80)

ui∈Uis

(ui,vj )∈Uis ×Vjs
p (ui) (cid:80)
p (ui, vj)

· log

p (vj)

vj ∈Vjs

=Ds (p (u, v) ∥ps(u)ps(v))

=Hs( ˜U ) + Hs( ˜V ) − H(U, V ).

Correspondingly, the full semantic mutual information ˜Is( ˜U ; ˜V ) is defined as,
˜Nu(cid:88)

˜Nv(cid:88)

(cid:88)

˜Is( ˜U ; ˜V ) = −

p (ui, vj)

is=1

js=1

(cid:80)

(ui,vj )∈Uis ×Vjs
p (ui) (cid:80)

· log

ui∈Uis
(cid:80)

(ui,vj )∈Uis ×Vjs

p (vj)

vj ∈Vjs
p (ui, vj)

=Ds (ps (u, v) ∥ps(u)ps(v))

=Hs( ˜U ) + Hs( ˜V ) − Hs( ˜U , ˜V ).

(43)

(44)

Theorem 6. (Non-negativity of semantic mutual information): For any two semantic variables,
˜U , ˜V , given the jointly synonymous mapping fuv, we have


˜Is( ˜U ; ˜V ) ≥ 0




I s( ˜U ; ˜V ) ≥ H(U, V ) − Hs( ˜U , ˜V ) ≥ 0

Is( ˜U ; ˜V ) ≥ Hs( ˜U , ˜V ) − H(U, V )






ps (ui) ps (vj) = ps (ui, vj)

p (ui) p (vj) = p (ui, vj)

ps (ui) ps (vj) = ps (ui, vj)

(45)

(46)

with equality if and only if

Proof: By using Theorem 4, we obtain ˜Is( ˜U ; ˜V ) = Ds (ps (u, v) ∥ps(u)ps(v)) ≥ 0. So the
equality holds if and only if ps (ui) ps (vj) = ps (ui, vj), which means ˜U and ˜V are independent.
□

Similarly, we can prove the other two inequalities.

31

Note that the down semantic mutual information Is( ˜U ; ˜V ) may be negative. Considering the

practical case, we can set (Is( ˜U ; ˜V ))+.

Theorem 7. The up/down semantic mutual information is a concave function of p(u) for fixed

p(v|u) and a convex function of p(v|u) for fixed p(x).

This theorem is proved in Appendix C. We now show the relationship among these mutual

information measures.

Corollary 3.

Is( ˜U ; ˜V ) ≤ I(U ; V ) ≤ H(V ) − Hs( ˜V |U ) ≤ I s( ˜U ; ˜V )

Proof: To prove the left inequality, by Lemma 2, we can write

Is( ˜U ; ˜V ) − I(U ; V ) = Hs( ˜U ) + Hs( ˜V ) − H(U ) − H(V ) ≤ 0.

For the medium inequality, due to Lemma 5, we have

I(U ; V ) − (H(V ) − Hs( ˜V |U )) = −H(V |U ) + Hs( ˜V |U ) ≤ 0.

For the right inequality, due to Theorem 2, we have

H(V ) − Hs( ˜V |U ) − I s( ˜U ; ˜V )

= −Hs( ˜V |U ) − H(U ) + Hs( ˜U , ˜V ) ≤ 0.

Corollary 4.

Is( ˜U ; ˜V ) ≤ ˜Is( ˜U ; ˜V ) ≤ H(V ) − Hs( ˜V |U ) ≤ I s( ˜U ; ˜V )

Proof: By Definition 8 and Lemma 4, we can write the left inequality as

Is( ˜U ; ˜V ) − ˜Is( ˜U ; ˜V ) = −H(U, V ) + Hs( ˜U , ˜V ) ≤ 0.

(47)

(48)

(49)

(50)

□

(51)

(52)

Correspondingly, by Definition 8, Lemma 2, and Theorem 2, we can also write the medium

inequality as

˜Is( ˜U ; ˜V ) − (H(V ) − Hs( ˜V |U ))

= Hs( ˜U ) + Hs( ˜V ) − Hs( ˜U , ˜V ) − H(V ) + Hs( ˜V |U )

(53)

≤ Hs( ˜U ) + Hs( ˜V |U ) − Hs( ˜U , ˜V ) ≤ 0.

32

□

Theorem 8. (Chain Rule of Sequential Mutual Information):

Given a pair of semantic sequences ( ˜U n, ˜V n) and the associated pair of syntactic sequences

(U n, V n), under the sequential synonymous mapping f n

uv and conditional synonymous mappings

fuk,vn|uk−1

1

, for the sequential version of down semantic mutual information, we have

Is( ˜U n; ˜V n) ≤ Is( ˜U n−1

1

, Un; ˜V n)

≤ · · · ≤ Is( ˜U m

1 , U n

m+1; ˜V n)

≤ · · · ≤ Is( ˜U1, U n

2 ; ˜V n)

≤ I(U n; V n) =

n
(cid:88)

k=1

I(Uk; V n (cid:12)

(cid:12)U k−1
1

),

(54)

where Is( ˜U m

1 , U n

m+1; ˜V n) = ˜H( ˜U m

1 , U n

m+1) + Hs( ˜V n) − H(U n, V n) with m = n − 1, · · · , 2, 1.

Similarly, for the sequential version of up semantic mutual information, we have

I(U n; V n) =

n
(cid:88)

k=1

I(Uk; V n (cid:12)

(cid:12)U k−1
1

)

≤ I s( ˜U1, U n

2 ; ˜V n)

≤ · · · ≤ I s( ˜U m

1 , U n

m+1; ˜V n)

≤ · · · ≤ I s( ˜U n−1

1

, Un; ˜V n) ≤ I s( ˜U n; ˜V n),

where I s( ˜U m

1 , U n

m+1; ˜V n) = H(U n) + H(V n) − ˜H( ˜U m

1 , U n

m+1, ˜V n) with m = 1, 2, · · · , n.

Proof: By Theorem 3, we derive that

Is( ˜U n; ˜V n) = Hs( ˜U n) + Hs( ˜V n) − H(U n, V n)

≤ Hs( ˜U n−1

1

, Un) + Hs( ˜V n) − H(U n, V n)

≤ · · · ≤ Hs( ˜U m

1 , U n

m+1) + Hs( ˜V n) − H(U n, V n)

≤ Hs( ˜U m−1

1

, U n

m) + Hs( ˜V n) − H(U n, V n)

≤ · · · ≤ Hs( ˜U1, U n

2 ) + Hs( ˜V n) − H(U n, V n)

≤ H(U n) + H(V n) − H(U n, V n) = I(U n; V n).

So we prove the first chain of inequalities.

(55)

(56)

Similarly, by Theorem 3, we can attain that

I(U n; V n) = H(U n) + H(V n) − H(U n, V n)

≤ H(U n) + H(V n) − ˜H( ˜U1, U n

2 , ˜V n)

≤ · · · ≤ H(U n) + H(V n) − ˜H( ˜U m−1

1

, U n

m, ˜V n)

≤ H(U n) + H(V n) − ˜H( ˜U m

1 , U n

m+1, ˜V n)

≤ · · · ≤ H(U n) + H(V n) − ˜H( ˜U n−1

1

, Un, ˜V n)

≤ H(U n) + H(V n) − Hs( ˜U n, ˜V n) = I s( ˜U n; ˜V n).

So we prove the second chain of inequalities.

33

(57)

□

Remark 5. In classic information theory, the mutual information satisfies the following equalities

I(U ; V ) = H(U ) − H(U |V )

= H(V ) − H(V |U )

(58)

= H(U ) + H(V ) − H(U ; V ).

On the contrary, in semantic information theory, the semantic mutual information only obeys

the degraded inequalities




Is( ˜U ; ˜V ) ≤ ˜Is( ˜U ; ˜V ) ≤ H(V ) − Hs( ˜V |U ) ≤ I s( ˜U ; ˜V )



Is( ˜U ; ˜V ) ≤ ˜Is( ˜U ; ˜V ) ≤ H(U ) − Hs( ˜U |V ) ≤ I s( ˜U ; ˜V ).

(59)

Since the size relationship between mutual information and semantic mutual information is

uncertain, we only use the up and down semantic mutual information to indicate the upper bound

and the lower bound respectively. In fact, by Theorem 2, the up semantic mutual information
I s( ˜U ; ˜V ) can be further upper bounded by H(U ) + H(V ) − Hs( ˜U ) − Hs( ˜V |U ). However, since
this bound has not a clear physical meaning, we still use I s( ˜U ; ˜V ) as the upper bound.

Theorem 9. (Semantic Processing Inequality):

Given a Markov chain U → V → W , then we have

Is( ˜U ; ˜W ) ≤ Is( ˜U ; W ) ≤ I(U ; W ) ≤ I(U ; V ) ≤ I s( ˜U ; V ) ≤ I s( ˜U ; ˜V ).

(60)

Proof: By the chain rule of semantic mutual information (Theorem 8), we can expand

(down) mutual information in two different ways,

34

I(U ; V W ) = I(U ; V ) + I(U ; W |V )

= I(U ; W ) + I(U ; V |W )

≥ Is( ˜U ; W ) + I(U ; V |W )

≥ Is( ˜U ; ˜W ).

(61)

Since U and W are conditionally independent give V , we have I(U ; W |V ) = 0. Due to

I(U ; V |W ) ≥ 0, we have

I(U ; V ) ≥ I(U ; W ) ≥ Is( ˜U ; W ) ≥ Is( ˜U ; ˜W ).

(62)

Similarly, by the same chain rule, we can expand (up) mutual information in two different

ways,

I(U ; V W ) = I(U ; V ) + I(U ; W |V )

= I(U ; W ) + I(U ; V |W )

≤ I s( ˜U ; V ) + I(U ; W |V ) ≤ I s( ˜U ; ˜V ).

I(U ; W ) ≤ I(U ; V ) ≤ I s( ˜U ; V ) ≤ I s( ˜U ; ˜V )

Thus, we have

and complete the proof.

(63)

(64)

□

Example 3. According to the joint distribution in Table II, we calculate the mutual information

between U and V as I(U ; V ) = H(U ) + H(V ) − H(U, V ) = 0.6332 bits. Correspondingly, by

using the joint distribution in Table III, we can compute the up semantic mutual information as
I s( ˜U ; ˜V ) = H(U ) + H(V ) − Hs( ˜U ; ˜V ) = 1.5087 sebits.

Similarly, by using the distribution in Table VI, we compute the down semantic mutual
information as Is( ˜U ; ˜V ) = Hs(U ) + Hs(V ) − H(U ; V ) = −0.6422 sebits. Consider the non-
negative requirement, we set Is( ˜U ; ˜V ) = max{−0.6422, 0} = 0 sebits. By using the distri-
bution in Table IV and Table V, we calculate that H(V ) − Hs( ˜V |U ) = 0.7709 sebits and
H(U ) − Hs( ˜U |V ) = 1.3087 sebits. So we conclude that Is( ˜U ; ˜V ) = 0 sebits < I(U ; V ) =
0.6332 bits < H(V ) − Hs( ˜V |U ) = 0.7709 sebits < H(U ) − Hs( ˜U |V ) = 1.3087 sebits <
I s( ˜U ; ˜V ) = 1.5087 sebits.

35

V. SEMANTIC CHANNEL CAPACITY AND SEMANTIC RATE DISTORTION

In this section, we introduce the semantic channel capacity and semantic rate distortion. We

use the maximum up semantic mutual information to indicate the former. Similarly, the minimum

down semantic mutual information is used to present the latter.

A. Semantic Channel Capacity

Generally, the discrete channel of semantic communication can be modeled as the form of a

. Here, X and Y are the input and output syntactical
five-tuple, that is,
alphabet. And ˜X and ˜Y are the corresponding input and output semantic alphabet. Furthermore,

(cid:110) ˜X , X , Y, ˜Y, p(Y |X)

(cid:111)

p(Y |X) is the channel transition probability.

Definition 9. Given a discrete memoryless channel

(cid:111)
(cid:110) ˜X , X , Y, ˜Y, p(Y |X)

, the semantic channel

capacity is defined as

Cs = max
fxy

max
p(x)

I s( ˜X; ˜Y )

= max
fxy

max
p(x)

(cid:80)

· log

= max
fxy

max
p(x)

(cid:88)

(cid:88)

(cid:88)

p (xi) p (yj |xi )

is

js

(xi,yj )∈Xis ×Yjs
p (xi, yj)

(xi,yj )∈Xis ×Yjs

p (xi) p (yj)
(cid:105)
(cid:104)
H(X) + H(Y ) − Hs( ˜X, ˜Y )

(65)

where the maximum is taken over all possible input distribution p(x) and the jointly synonymous
mapping fxy : ˜X × ˜Y → X × Y.

By Corollary 3, the channel mutual information is no more than the up semantic mutual
information, that is, I(X; Y ) ≤ I s( ˜X; ˜Y ). Thus, we conclude that the channel capacity is no

more than the semantic capacity, i.e., C ≤ Cs.

B. Semantic Rate Distortion

Similar to the classic rate distortion theory, given a semantic/syntactic source distribution and a

distortion measure, we also need to investigate the minimum rate description required to achieve

a specific distortion.

36

Definition 10. Given a discrete source X ∼ p(x), x ∈ X and the associated semantic source
˜X, the decoder outputs an estimate ˆX with the associated semantic variable ˆ˜X. Under the
synonymous mapping fx and fˆx, the semantic distortion measure is a mapping

ds : ˜X × ˆ˜X → R+

(66)

from the Cartesian product of semantic source alphabet and reconstruction alphabet into the set
of non-negative real numbers. The semantic distortion ds(˜xis, ˆ˜xjs) = ds(Xis, ˆXjs) is a measure of
the cost of representing the semantic symbol ˜xis by the symbol ˆ˜xjs, equivalently, which is a cost
of representing the syntactical symbol set Xis by the reproduction set ˆXjs. For the traditional
lossy source coding, the semantic distortion can be measured using Hamming distortion or mean

squared error (MSE) distortion. On the other hand, for the lossy coding with neural network

model, the semantic distortion can also be evaluated by using word error rate (WER), structure

similarity index measure (SSIM), learned perceptual image patch similarity (LPIPS) etc.

So the average semantic distortion is defined as

¯ds = E

(cid:104)
ds(˜x, ˆ˜x)

(cid:105)

(cid:88)

(cid:88)

(cid:88)

=

p (xi, ˆxj) ds(Xis, ˆXjs).

is

js

(xi,ˆxj )∈Xis × ˆXjs

Furthermore, the test channel set PD is defined as

(cid:110)

PD =

p(ˆx |x) : ¯ds = E

(cid:16)

(cid:104)

ds

˜x, ˆ˜x

(cid:17)(cid:105)

(cid:111)

.

≤ D

Next, we give the formal definition of semantic rate distortion.

(67)

(68)

Definition 11. Given an i.i.d. source X with distribution p(x), the associated semantic source
˜X, and the semantic distortion function ds(˜xis, ˆ˜xjs), the semantic rate distortion is defined as,
Is( ˜X; ˆ˜X)

Rs(D) = min
fx,fˆx

min
p(ˆx|x )∈PD

= min
fx,fˆx

min
p(ˆx|x )∈PD

(cid:88)

(cid:88)

(cid:88)

p (xi, ˆxj)

is

js

(xi,ˆxj )∈Xis × ˆXjs

· log

(cid:80)

xi∈Xis

= min
fx,fˆx

min
p(ˆx|x )∈PD

p (xi, ˆxj)
p (xi) (cid:80)
(cid:104)
(cid:105)
Hs( ˜X) + Hs( ˆ˜X) − H(X, ˆX)

p (ˆxj)

ˆxj ∈ ˆXjs

(69)

.

37

Lemma 7. (Convexity of Rs(D)): The semantic rate distortion function Rs(D) is a non-increasing

convex function of D.

Proof: Similar to syntactic rate distortion R(D), Rs(D) is also a non-increasing function

in D.

Consider two rate distortion pairs (R1, D1) and (R2, D2), the corresponding joint distri-

bution achieving these pair are p1(x, ˆx) = p(x)p1(ˆx|x) and p2(x, ˆx) = p(x)p2(ˆx|x) respec-

tively. Consider the weighted distribution pθ = θp1 + (1 − θ)p2 and the weighted distortion

Dθ = θD1 + (1 − θ)D2. Due to the convexity of semantic mutual information, we have

Rs(Dθ) ≤ Is,pθ( ˜X; ˆ˜X)

≤ θIs,p1( ˜X; ˆ˜X) + (1 − θ)Is,p2( ˜X; ˆ˜X)

= θRs(D1) + (1 − θ)Rs(D2).

So we complete the proof.

(70)

□

According to Corollary 3, the down semantic mutual information is no more than the mutual
information, that is, Is( ˜X; ˜Y ) ≤ I(X; Y ). Thus, we conclude that the semantic rate distortion is
no more than the classic rate distortion, i.e., Rs(D) ≤ R(D).

VI. SEMANTIC LOSSLESS SOURCE CODING

In this section, we discuss the semantic lossless source coding. First, we investigate the

asymptotic equipartition property (AEP) of semantic coding and introduce the synonymous

typical set. Then we prove the semantic source coding theorem and give the optimal length of

semantic coding. Finally, we design the semantic Huffman coding to demonstrate the advantage

of semantic data compression.

A. Asymptotic Equipartition Property and Synonymous Typical Set

Similar to classic information theory, asymptotic equipartition property (AEP) is also an

important tool to prove the coding theorem in semantic information theory.

Definition 12. Given a discrete random variable U with the distribution p(u) and the corre-
sponding semantic variable ˜U , when we consider an i.i.d. semantic sequence ( ˜U1, ˜U2, · · · , ˜Un)

38

and the associated syntactic sequences (U1, U2, · · · , Un), the sequential synonymous mapping is
defined as f n : ˜U n → U n. That is to say, given a sequence ˜un, we have f n (˜un) = (cid:81)n

k=1 U˜uk.

Theorem 10. (Semantic and Syntactic AEP): If ( ˜U1, ˜U2, · · · , ˜Un, · · · ) is an i.i.d. semantic se-
quence and the associated syntactic sequence is (U1, U2, · · · , Un, · · · ), given the sequential
synonymous mapping f n : ˜U n → U n, then






lim
n→∞

−

lim
n→∞

−

1
n
1
n

log p( ˜U1, ˜U2, · · · , ˜Un) = Hs( ˜U )

log p (U1, U2, · · · , Un) = H (U )

(71)

Proof: For the first equality, since the ˜Uk are i.i.d., so are log p( ˜Uk). By the weak law of

large numbers, we have

−

1
n

log p( ˜U1, ˜U2, · · · , ˜Un) = −

1
n

(cid:104)

→ E
(cid:34)

n
(cid:88)

log p( ˜Uk)

k=1
(cid:105)
− log p( ˜U )

(cid:35)

p(u)

(cid:88)

u∈U˜u

= E

− log

= Hs( ˜U ).

The second equality holds from the classic information theory [4, Theorem 3.1.1].

(72)

□

Base on semantic and syntactic AEP, under the sequential synonymous mapping f n, we have

the following synonymous AEP.

Theorem 11. (Synonymous AEP): If ( ˜U1, ˜U2, · · · , ˜Un, · · · ) is an i.i.d. semantic sequence and
the associated syntactic sequence is (U1, U2, · · · , Un, · · · ), under the sequential synonymous
mapping f n : ˜U n → U n, we have

lim
n→∞

−

1
n

(cid:105)
(cid:104)
log p (U1, U2, · · · , Un) − log p( ˜U1, ˜U2, · · · , ˜Un)

(73)

= H (U ) − Hs( ˜U ).

Proof: By Theorem 10, we have

−

1
n

= −

(cid:104)

(cid:105)
log p (U1, U2, · · · , Un) − log p( ˜U1, ˜U2, · · · , ˜Un)

1
n

n
(cid:88)

k=1

(cid:105)
(cid:104)
log p(Uk) − log p( ˜Uk)

→ E [− log p(U )] − E
(cid:34)

(cid:104)

= E [− log p(u)] − E

− log

= H(U ) − Hs( ˜U ).

(cid:105)
− log p( ˜U )

(cid:35)

p(u)

(cid:88)

u∈U˜u

39

(74)

□

Similar to the definition of syntactically typical set A(n)

ϵ

in [4], we now introduce the seman-

tically typical set ˜A(n)

ϵ

and the synonymous typical set B(n)

ϵ

(˜un) respectively.

Definition 13. Given the typical sequences {˜un} with respect to the distribution p(u), the
semantically typical set ˜A(n)

is defined as the set of n-sequences with empirical semantic entropy

ϵ

ϵ-close to the true semantic entropy, that is,

˜A(n)

ϵ =

(cid:26)

˜un ∈ ˜U n :

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
log p (˜un) − Hs( ˜U )
(cid:12)
(cid:12)

1
n

(cid:27)

< ϵ

,

where

p (˜un) =

n
(cid:89)

k=1

p(˜uk) =

n
(cid:89)

(cid:88)

k=1

uk∈U˜uk

p(uk).

(75)

(76)

Definition 14. Given a specific typical sequence {˜un}, the synonymous typical set B(n)

ϵ

(˜un) with

the syntactically typical sequences {un} is defined as the set of n-sequences with the difference

of empirical entropies ϵ-close to the difference of true entropies, that is,

< ϵ,

B(n)
ϵ

(˜un) =

(cid:26)

un ∈ U n :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

−

−

−

log p (un) − H(U )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
log p (˜un) − Hs( ˜U )
(cid:12)
(cid:12)

1
n
1
n
1
n
(cid:16)
H(U ) − Hs( ˜U )

log p(˜un → un)

(cid:17)(cid:12)
(cid:111)
(cid:12)
(cid:12) < ϵ

,

< ϵ,

(77)

40

where p(un)=(cid:81)n

k=1 p(uk) and p(˜un)=(cid:81)n

k=1

(cid:80)

uk∈U˜uk

p(uk). Here the probability p(˜un → un) is

defined as

p(˜un → un) =




p (un)
p (˜un)

,



0,

if un = f n(˜un),

otherwise.

(78)

Under the sequential synonymous mapping f n : ˜U n → U n, the syntactically typical set A(n)

ϵ

can be further partitioned into multiple synonymous typical sets B(n)

ϵ

(˜un), that is,

A(n)

ϵ =

(cid:91)

B(n)
ϵ

(˜un) .

˜un∈ ˜A(n)

ϵ

, ˜un ̸= ˜vn, we have B(n)

ϵ

(˜un) (cid:84) B(n)

ϵ

(˜vn) = ∅.

(79)

And for ∀˜un, ˜vn ∈ ˜A(n)
Essentially, B(n)

ϵ

(˜un) is an equivalence class of the synonymous typical sequences. Thus for

ϵ
the syntactically typical set A(n)
construct an one-to-one mapping between ˜A(n)

, we can obtain a quotient set A(n)
and A(n)

ϵ /f n.

ϵ

ϵ

ϵ /f n =

(cid:110)

B(n)
ϵ

(cid:111)

(˜un)

and

We now discuss the properties of syntactically and semantically typical sets. In order to the

convenient reading, we first rewrite the properties of syntactically typical set as follows.

Theorem 12. ([4, Theorem 3.1.2]):
(1) If (u1, u2, · · · , un) ∈ A(n)

ϵ

, then H(U ) − ϵ ≤ − 1

n log p (u1, u2, · · · , un) ≤ H(U ) + ϵ or

equivalently

2−n(H(U )+ϵ) ≤ p (u1, u2, · · · , un) ≤ 2−n(H(U )−ϵ).

(80)

(2) Pr

(cid:110)

A(n)
ϵ

(cid:111)

(3) (1 − ϵ) 2n(H(U )−ϵ) ≤

> 1 − ϵ for n sufficiently large.
(cid:12)
(cid:12)
(cid:12) ≤ 2n(H(U )+ϵ) for n sufficiently large.

(cid:12)
(cid:12)A(n)
(cid:12)

ϵ

Then as the consequence of the semantic AEP, we give the properties of semantically typical

set as below.

Theorem 13.
(1) If (˜u1, ˜u2, · · · , ˜un) ∈ ˜A(n)

ϵ

equivalently,

, then Hs( ˜U ) − ϵ ≤ − 1

n log p (˜u1, ˜u2, · · · , ˜un) ≤ Hs( ˜U ) + ϵ,

2−n(Hs( ˜U )+ϵ) ≤ p (˜u1, ˜u2, · · · , ˜un) ≤ 2−n(Hs( ˜U )−ϵ).

(81)

(2) Pr

(cid:110) ˜A(n)

ϵ

(cid:111)

> 1 − ϵ for sufficiently large n.

(3) (1 − ϵ) 2n(Hs( ˜U )−ϵ) ≤

(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12) ≤ 2n(Hs( ˜U )+ϵ) for sufficiently large n.
(cid:12)

Proof: The proof of property (1) is directly from the definition of ˜A(n)

ϵ

. By Theorem 10,

the probability of the event ˜U n ∈ ˜A(n)

ϵ

tends to 1 as n → ∞. So for ∀ϵ > 0, ∃n0, for ∀n ≥ n0,

41

we have

Pr

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
log p( ˜U n) − Hs( ˜U )
(cid:12)
(cid:12)

1
n

(cid:27)

< ϵ

> 1 − ϵ.

Then we complete the proof of property (2).

After summing over the set ˜A(n)

, by using property (2), Eq. (81) can be rewritten as

ϵ
(cid:12)
(cid:12) 2−n(Hs( ˜U )+ϵ) ≤
(cid:12)

(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:88)

p (˜un) ≤ 1

ϵ

˜un∈ ˜A(n)
(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

p (˜un) ≤

(cid:12)
(cid:12) 2−n(Hs( ˜U )−ϵ)
(cid:12)

1 − ϵ ≤

(cid:88)

˜un∈ ˜A(n)

ϵ






Hence, we can write

and

(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12) ≥ (1 − ϵ) 2n(Hs( ˜U )−ϵ)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12) ≤ 2n(Hs( ˜U )+ϵ)
(cid:12)

respectively and complete the third property.

Furthermore, we give the properties of synonymous typical set as below.

(82)

(83)

(84)

(85)

□

Theorem 14.
(1) Given a semantic sequence (˜u1, ˜u2, · · · , ˜un) ∈ ˜A(n)

ϵ

, if (u1, u2, · · · , un) ∈ B(n)

ϵ

(˜un), then

H(U ) − Hs( ˜U ) − ϵ ≤ − 1

n log p(un)

p(˜un) ≤ H(U ) − Hs( ˜U ) + ϵ, equivalently,

2−n(H(U )−Hs( ˜U )+ϵ) ≤

p (un)
p (˜un)

≤ 2−n(H(U )−Hs( ˜U )−ϵ).

(86)

(2) 2n(H(U )−Hs( ˜U )−ϵ) ≤

(cid:12)
(cid:12)B(n)
(cid:12)

ϵ

(˜un)

(cid:12)
(cid:12) ≤ 2n(H(U )−Hs( ˜U )+ϵ) for sufficiently large n.
(cid:12)

Proof: The proof of property (1) is directly from the definition of B(n)

ϵ

(˜un).

To prove the left inequality of property (2), we write

p (˜un) =

(cid:88)

p (un)

un∈B(n)

ϵ

(˜un)

≤ p (˜un)

(cid:88)

2−n(H(U )−Hs( ˜U )−ϵ)

So we have

= p (˜un) (cid:12)

(cid:12)B(n)
ϵ

un∈B(n)

ϵ

(˜un)
(˜un)(cid:12)

(cid:12) 2−n(H(U )−Hs( ˜U )−ϵ).

(cid:12)
(cid:12)B(n)
ϵ

(˜un)(cid:12)

(cid:12) ≥ 2n(H(U )−Hs( ˜U )−ϵ).

On the other hand, for the right inequality, we can write

p (˜un) =

(cid:88)

p (un)

un∈B(n)

ϵ

(˜un)

≥ p (˜un)

(cid:88)

2−n(H(U )−Hs( ˜U )+ϵ)

un∈B(n)

ϵ

(˜un)
(˜un)(cid:12)

= p (˜un) (cid:12)

(cid:12)B(n)
ϵ

(cid:12) 2−n(H(U )−Hs( ˜U )+ϵ).

Similarly, we have

and complete the proof.

(cid:12)
(cid:12)B(n)
ϵ

(˜un)(cid:12)

(cid:12) ≤ 2n(H(U )−Hs( ˜U )+ϵ)

42

(87)

(88)

(89)

(90)

□

Remark 6. Both the probability of syntactically typical set A(n)
cally typical set ˜A(n)

ϵ

ϵ

trends to 1 with sufficiently large n. Furthermore, all the syntactically and

and the probability of semanti-

typical sequences is about

semantically typical sequences are almost equiprobability. Therefore, the number of syntactically
≈ 2n(H(U )±ϵ) and that of semantically typical sequences is
≈ 2n(Hs( ˜U )±ϵ). Simultaneously, the number of synonymous typical sequences is
≈ 2n(H(U )−Hs( ˜U )±ϵ). In fact, all the synonymous typical sets have almost the

(cid:12)
(cid:12)A(n)
(cid:12)

about

about

(cid:12)
(cid:12)
(cid:12)

ϵ

(cid:12)
˜A(n)
(cid:12)
ϵ
(cid:12)
(cid:12)
(cid:12)B(n)
(cid:12)

ϵ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(˜un)
(cid:12)

same number of typical sequences. Hence, hereafter, in the case of non-confusion, we abbreviate
B(n)
ϵ

(˜un) to B(n)

.

ϵ

B. Semantic Source Coding Theorem

We now discuss the semantic source coding. As shown in Fig. 7, in the side of transmitter,

with the help of synonymous mapping f n, the semantic index is is mapped into a syntactic

sequence U n and encoded into a source codeword X n. In the other side, the decoder decides
the codeword X n to an estimated syntactic sequence ˆU n. After de-synonymous mapping gn, we
obtain an index estimation of semantic sequence ˆis.

43

Fig. 7. Semantic lossless source encoder and decoder.

Definition 15. An (M, n) code for semantic source coding consists of the following parts:

(1) A semantic index set Is = {1, · · · , is, · · · , Ms} and a syntactic index set I = {1, · · · , i, · · · , M }.
(2) By the synonymous mapping f n : ˜U n → U n, one semantic sequence in the set ˜U n is mapped

into a syntactic sequence.

(3) An encoding function ϕ : U n → X n generates the set of codewords, namely, codebook,

C = {X n(1), X n(2), · · · , X n(M )}. Due to synonymous mapping, this codebook can be par-

titioned into synonymous codeword subsets Cs(is) = {X n(is, j), is ∈ Is, j = 1, 2, · · · , M
Ms
where X n(is, j) denotes the j-th codeword of the is-th subset.

},

(4) A decoding function ψ : X n → U n outputs the decision syntactic sequence ˆU n.
(5) After de-mapping, gn( ˆU n) = ˆis, the estimated semantic index is obtained. Note that both ψ

and gn are deterministic.

Under the synonymous mapping f n, Cs is an equivalence class of the synonymous codewords.
So we can construct the quotient set C/f n = {Cs}. Without loss of generality, we can assume
all the synonymous sets have the same number of codewords, that is, |Cs| = M
= 2nRs, where
Ms
Rs is called the rate of synonymous codeword set. So we have |C/f n| = Ms. Let R = 1
denote the semantic code rate. Furthermore, let R′ = R + Rs = 1

n log2 Ms
n log2 M be the syntactic code

rate.

Definition 16. The decoding error probability is defined as

e = Pr(gn(Cs) ̸= is).
P (n)

(91)

We now give the formal description of semantic source coding theorem.

EncoderDecoderˆnUngnfnU(),1,1,,2,2ssnRsnRnXijjisiˆsi44

Theorem 15. Given the semantic source ˜U and the syntactic source U with the synonymous
mapping f : ˜U → U, for each code rate R > Hs( ˜U ), there exists a series of (cid:0)2n(R+Rs), n(cid:1)
codes, when code length n tends to sufficiently large, the error probability is close to zero, i.e.
e → 0. On the contrary, if R < Hs( ˜U ), then for any (cid:0)2n(R+Rs), n(cid:1) code, the error probability
P (n)
tends to 1 with n sufficiently large.

Proof: First, we prove the direct part of the theorem. We select ϵ > 0 and construct a

one-to-one mapping from ˜A(n)

ϵ

(1 − ϵ) 2n(Hs( ˜U )−ϵ) ≤ |C/f n| = 2nR =

to C/f n. So for sufficiently large n, by Theorem 13, we have
(cid:12)
(cid:12) ≤ 2n(Hs( ˜U )+ϵ).
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12)
(cid:12)

(92)

Therefore, the semantic code rate satisfies

1
n

log (1 − ϵ) + Hs( ˜U ) − ϵ ≤ R ≤ Hs( ˜U ) + ϵ.

Also by Theorem 13, it follows that

e = Pr( ˜U n /∈ ˜A(n)
P (n)

ϵ ) < ϵ.

By Theorem 14, the size of synonymous set satisfies

1 ≤ |Cs| = 2nRs ≤ (cid:12)

(cid:12)B(n)
ϵ

(˜un)(cid:12)

(cid:12) ≤ 2n(H(U )−Hs( ˜U )+ϵ).

Therefore the syntactic code rate satisfies

1
n

log (1 − ϵ) + Rs + Hs( ˜U ) − ϵ ≤ R′ ≤ Hs( ˜U ) + Rs + ϵ.

(93)

(94)

(95)

(96)

When the synonymous set Cs only has one codeword to represent the semantically typical
sequence, that is, Rs = 0, letting ϵ → 0, both the code rate R and R′ tend to Hs( ˜U ), while P (n)
tends to 0. On the other hand, substituting Rs = H(U ) − Hs( ˜U ) into (96) and letting ϵ → 0,
R → Hs( ˜U ) and R′ → H(U ). So we prove the direct part of theorem.

e

Next we prove the converse part. Consider any code with code length n and the number of
synonymous sets satisfies 2nR ≤ 2n(Hs( ˜U )−ζ). So some sets are used to represent the semantically
typical sequences gn (Cs) ∈ ˜A(n)
gn (Cs) /∈ ˜A(n)

. By Theorem 13, for sufficiently large n, the probability of semantic sequences

and the rest sets to represent the semantic non-typical sequences

ϵ

ϵ

covered by the code is upper bounded by

1 − P (n)

e = 2n(Hs( ˜U )−ζ)2−n(Hs( ˜U )−ϵ) + Pr(gn (Cs) /∈ ˜A(n)
ϵ )

(97)

< 2−n(ζ−ϵ) + ϵ.

Therefore, we can write the error probability as

e > 1 − 2−n(ζ−ϵ) − ϵ.
P (n)

45

(98)

This inequality holds when n → ∞ for ϵ > 0 and ζ > ϵ. So we have P (n)
when n → ∞ and ϵ → 0, P (n)

e > 1−2ϵ. Additionally,

e → 1. So we complete the proof of the semantic source coding
□

theorem.

Remark 7. The semantic source coding theorem is an extended version of the counterpart in the
classic information theory. The limitation of semantic compression rate R is Hs( ˜U ) while the
corresponding syntactic rate R′ is Hs( ˜U ) + Rs. If the synonymous set is represented by only one
codeword, i.e., Rs = 0, then compression rate R′ tends to the semantic entropy Hs( ˜U ). On the
other hand, if the synonymous mapping is ignored and all the synonymous typical sequences are
distinct, thus Rs = H(U )−Hs( ˜U ) and the compression rate R′ achieves the information entropy
H(U ). Since the distinction between codewords in the synonymous set is no longer regarded as

an error, by using semantic coding, the source data can be further compressed and the efficiency

is improved.

C. Semantic Source Coding Method

Similar to the classic source coding, the variable length coding is desired for semantic source

coding. Thus we also obtain the semantic version of Kraft inequality as following.

Theorem 16. (Semantic Kraft Inequality): Given a discrete random variable U ∈ U = {ui}N
i=1,
˜N
is=1, and the synonymous mapping f : ˜U →
the corresponding semantic variable ˜U ∈ ˜U = {˜uis}
U. For any prefix code over an alphabet of size F exists if and only if the codeword length

l1, l2, · · · , l ˜N satisfies

˜N
(cid:88)

is=1

F −lis ≤ 1.

(99)

The proof is similar to that of classic Kraft inequality and omitted. It should be noted that the

semantic Kraft inequality has the same form as that of classic counterpart. However, since the

semantic prefix code is performed over a synonymous set rather than a single syntactic symbol,
the number of codewords is less than classic prefix code, that is, ˜N ≤ N .

PROBABILITY DISTRIBUTION AND HUFFMAN CODES OF SYNTACTIC SOURCE U .

TABLE VII

46

Syntactic symbol

u1

Probability

Syn. HC

1
2

0

u2

1
4

10

u3

1
8

u4

1
8

110

111

Furthermore, we can obtain the average code length of optimal semantic source code as

follows.

1, l∗

2, · · · , l∗
˜N

Theorem 17. Given the syntactic source distribution p(u) and the synonymous mapping f :
˜U → U, let l∗
expected length ¯L∗ of the optimal semantic code satisfies
Hs( ˜U )
log F

denote the optimal code lengths with an F -ary alphabet, then the

Hs( ˜U )
log F

≤ ¯L∗ <

(100)

+ 1.

Proof: Assign the code length as lis =

by semantic Kraft inequality, we have

(cid:108)
− logF

(cid:80)

i∈Uis

(cid:109)
p(ui)

. Similar to the classic version,

Hs( ˜U )
log F

≤ ¯L∗ ≤

˜N
(cid:88)

is=1

p(˜uis)lis <

Hs( ˜U )
log F

+ 1.

(101)

□

So we prove the theorem.

Example 4. (Semantic Huffman Coding): Now we describe an example of semantic Huffman

coding. For a syntactic source U with four symbols u1, u2, u3, u4, the probability distribution is

listed in Table VII. The information entropy is H(U ) = 1.75 bits. By using Huffman coding, the
codewords are shown in Table VII and the average code length is ¯L = 1.75 bits = H(U ).

If we give a synonymous mapping f , that is, ˜u1 → {u1}, ˜u2 → {u2}, ˜u3 → {u3, u4}, the
probability distribution of semantic source ˜U is listed in Table VIII. So the semantic entropy is
calculated as Hs( ˜U ) = 1.5 sebits. Correspondingly, by the semantic Huffman codewords listed
in Table VIII, the average code length is ¯Ls = 1.5 sebits = Hs( ˜U ). Distinctly, due to synonymous
mapping, the average code length of semantic Huffman code is smaller than that of traditional

Huffman code.

Given a syntactic sequence u = (u1u1u3u4u2u3u2), by Table VII, the syntactic Huffman coding

is x = (001101111011010). On the other hand, by Table VIII, the semantic Huffman coding is

47

TABLE VIII
PROBABILITY DISTRIBUTION AND HUFFMAN CODES OF SEMANTIC SOURCE ˜U .

Semantic symbol

˜u1 → {u1}

˜u2 → {u2}

˜u3 → {u3, u4}

Probability

Sem. HC

1
2

0

1
4

10

1
4

11

xs = (001111101110). Hence, the length of syntactic coding is L(x) = 15 bits and the length

of semantic coding is L(xs) = 12 sebits so that the latter is smaller than the former, i.e.,

L(xs) < L(x). Certainly, since the decoder can select arbitrary symbol from the set {u3, u4}

when decoding ˜u3, the result may be ˆu = (u1u1u3u3u2u4u2). Although such decoding sequence

is different from the original one u = (u1u1u3u4u2u3u2) in the syntactic sense, the semantic

information of the decoding results still keeps the same since u3 and u4 have the same meaning.

Remark 8. For the method of semantic source coding, we have two kinds of design thought.

The first kind thought is to modify the traditional source coding, such as Huffman, arithmetic

or universal coding. By using an elaborate synonymous mapping, these coding methods can be

devised to further improve the compression efficiency. For the second thought, based on the deep

learning method, we can construct a neural network model to perform semantic source coding.

In this model, the synonymous mapping and semantic coding can be integrated and optimized

to approach the theoretic limitation.

VII. SEMANTIC CHANNEL CODING

In this section, we investigate the semantic channel coding. First, we introduce the jointly

asymptotic equipartition property (JAEP) in the semantic sense and define the jointly synonymous

typical set. Then we prove the semantic channel coding theorem by using JAEP and jointly
synonymous typical set, which states that the semantic capacity, maxfxy maxp(x) I s( ˜X; ˜Y ), the
maximum up semantic mutual information, is the largest achievable rate of semantic communi-

cation. Finally, we consider the semantic channel decoding problem and propose the maximum

likelihood group (MLG) decoding algorithm. A simple example of semantic Hamming code is

analyzed based on the MLG decoding rule.

48

A. Jointly Asymptotic Equipartition Property and Jointly Synonymous Typical Set

Given the semantic channel model

(cid:110) ˜X , X , Y, ˜Y, p(Y |X)
(cid:111)
, X and Y are the input and output
syntactical alphabet and ˜X and ˜Y are the corresponding input and output semantic alphabet.
Furthermore, p(Y |X) is the channel transition probability and let fxy : ˜X × ˜Y → X × Y be the

jointly synonymous mapping.

Based on this channel model, we extend the jointly asymptotic equipartition property (JAEP)

to the semantic sense and use it to prove the channel coding theorem in semantic information

theory.

Definition 17. Given the semantic channel model, the jointly synonymous mapping over the
xy : ˜X n × ˜Y n → X n × Y n. Equivalently, given a sequence pair

sequence pairs is defined as f n

(˜xn, ˜yn), we have f n

xy (˜xn, ˜yn) = (cid:81)n

k=1 X˜xk × Y˜yk.

To facilitate the reader’s understanding, we first cite the definition of syntactically joint typical

set A(n)

ϵ

in [4].

Definition 18. Given the jointly typical sequences {xn, yn} with respect to the distribution
p(xn, yn), the syntactically jointly typical set A(n)

is defined as the set of n-sequence pairs

ϵ

with empirical entropies ϵ-close to the true entropies, that is,

(cid:26)

A(n)

ϵ =

(xn, yn) ∈ X n × Y n :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

−

−

(cid:12)
(cid:12)
log p (xn) − H(X)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

log p (yn) − H(Y )

< ϵ,

< ϵ,

log p (xn, yn) − H(X, Y )

1
n
1
n
1
n

(cid:27)

< ϵ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(102)

where p (xn, yn) = (cid:81)n

k=1 p(xk, yk).

Next, we introduce the semantically jointly typical set ˜A(n)

ϵ

.

Definition 19. Given the jointly typical sequences {˜xn, ˜yn} with respect to the distribution
p(xn, yn), the semantically jointly typical set ˜A(n)

is defined as the set of n-sequence pairs

ϵ

with empirical entropies ϵ-close to the true entropies, that is,

(cid:26)

˜A(n)

ϵ =

(˜xn, ˜yn) ∈ ˜X n × ˜Y n :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

−

−

(cid:12)
(cid:12)
log p (˜xn) − Hs( ˜X)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
log p (˜yn) − Hs( ˜Y )
(cid:12)
(cid:12)

< ϵ,

< ϵ,

(cid:12)
(cid:12)
log p (˜xn, ˜yn) − Hs( ˜X, ˜Y )
(cid:12)
(cid:12)

(cid:27)

< ϵ

1
n
1
n
1
n

49

(103)

where

p (˜xn, ˜yn) =

n
(cid:89)

k=1

p(˜xk, ˜yk) =

n
(cid:89)

(cid:88)

k=1

(xk,yk)∈X˜xk

×Y˜yk

p(xk, yk).

(104)

The semantic and syntactic sequence mapping is depicted in Fig. 8. As described in Section VI,
y , semantic source set ˜X and semantic destination set
xy, the typical
(˜xn, ˜yn).

under the synonymous mapping f n
˜Y are mapped into synonymous typical sets. Similarly, under the joint mapping f n
sequences in these sets can further compose the jointly synonymous typical set B(n)

x and f n

ϵ

Fig. 8. Semantic and syntactic sequence mapping over the channel.

Definition 20. Given a specifically typical sequence pair {˜xn, ˜yn}, the jointly synonymous typical
set B(n)

(˜xn, ˜yn) with the syntactically jointly typical sequences {xn, yn} is defined as the set

ϵ

of n-sequence pairs with the difference of empirical entropies ϵ-close to the difference of true

nnSyntactic sequence setSemantic sequence setReceived sequence setSynonymous mappingnxyfJointly synonymous mappingSynonymous typical setJointly synonymous typical setnXnxfSemantic sequence setnYSynonymous typical setSynonymous mappingnyfentropies, that is,

(˜xn, ˜yn)

50

B(n)
ϵ
(cid:26)

=

(xn, yn) ∈ X n × Y n :

< ϵ,

(105)

(cid:12)
(cid:12)
log p (˜xn, ˜yn) − Hs( ˜X, ˜Y )
(cid:12)
(cid:12)

< ϵ,

< ϵ,

< ϵ,

< ϵ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

−

−

−

−

−

−

−

(cid:12)
(cid:12)
log p (˜xn) − Hs( ˜X)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
log p (˜yn) − Hs( ˜Y )
(cid:12)
(cid:12)

1
n
1
n
1
n
1
n
1
n
1
n
1
n
(cid:16)
H(X, Y ) − Hs( ˜X, ˜Y )

(cid:12)
(cid:12)
log p (xn) − H(X)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
log p (yn) − H(Y )
(cid:12)
(cid:12)

log p((˜xn, ˜yn) → (xn, yn))

log p (xn, yn) − H(X, Y )

(cid:12)
(cid:12)
(cid:12)
(cid:12)

< ϵ,

(cid:17)(cid:12)
(cid:111)
(cid:12)
(cid:12) < ϵ

,

where the probability p((˜xn, ˜yn) → (xn, yn)) is defined as

p((˜xn, ˜yn) → (xn, yn))




p (xn, yn)
p (˜xn, ˜yn)

,

=



0,

if f n

xy(˜xn, ˜yn) = (xn, yn),

(106)

otherwise.

Under the jointly synonymous mapping f n

xy : ˜X n × ˜Y n → X n × Y n, the syntactically jointly

typical set A(n)

ϵ

can be partitioned into jointly synonymous typical sets B(n)

ϵ

(˜xn, ˜yn), that is,

A(n)

ϵ =

(cid:91)

B(n)
ϵ

(˜xn, ˜yn) .

(˜xn,˜yn)∈ ˜A(n)

ϵ

(107)

For ∀(˜un, ˜vn), (˜xn, ˜yn) ∈ ˜A(n)

ϵ

, (˜un, ˜vn) ̸= (˜xn, ˜yn), we have B(n)

ϵ

(˜xn, ˜yn) (cid:84) B(n)

ϵ

(˜un, ˜vn) = ∅.

Similarly, B(n)

ϵ

(˜xn, ˜yn) also is an equivalence class of the jointly synonymous typical se-

quences. Thus we can construct a quotient set A(n)
jointly typical set A(n)

ϵ

, and establish an one-to-one mapping between ˜A(n)

ϵ

ϵ /f n

xy =

and A(n)

ϵ /f n
xy.

(cid:110)

B(n)
ϵ

(˜xn, ˜yn)

(cid:111)

from the syntactically

We now discuss the properties of syntactically and semantically jointly typical sets. We first

rewrite the properties of syntactically jointly typical set in [4] as follows.

Theorem 18. (Syntactically joint AEP [4, Theorem 7.6.1]): Let (X n, Y n) be a sequence pair

51

with length n obeying the i.i.d. distribution p(xn, yn), then
(1) Pr((X n, Y n) ∈ A(n)

ϵ ) > 1 − ϵ for n sufficiently large. Or equivalently, if (xn, yn) ∈ A(n)

ϵ

,

then

2−n(H(X,Y )+ϵ) ≤ p (xn, yn) ≤ 2−n(H(X,Y )−ϵ).

(108)

(2) (1 − ϵ) 2n(H(X,Y )−ϵ) ≤

(cid:12)
(cid:12)
(cid:12) ≤ 2n(H(X,Y )+ϵ) for n sufficiently large.
˙X n and ˙Y n are independent sequences with the same marginals as xn and yn, i.e.,

(cid:12)
(cid:12)A(n)
(cid:12)

(3) If

ϵ

( ˙X n, ˙Y n) ∼ p(xn)p(yn), for n sufficiently large, we have

(1 − ϵ)2−n(I(X;Y )+3ϵ) ≤ Pr

(cid:16)

( ˙X n, ˙Y n) ∈ A(n)

ϵ

(cid:17)

≤ 2−n(I(X;Y )−3ϵ).

(109)

Then as the consequence of the semantic JAEP, we present the properties of semantically

jointly typical set as following.

Theorem 19. (Semantically joint AEP): Let ( ˜X n, ˜Y n) be a semantic sequence pair with length
n drawn i.i.d. according to p(˜xn, ˜yn), by using jointly synonymous mapping f n

xy, the associated

syntactic sequence pair is (X n, Y n) ∼ p(xn, yn), then
(1) Pr(( ˜X n, ˜Y n) ∈ ˜A(n)

ϵ ) > 1 − ϵ for n sufficiently large. Or equivalently if (˜xn, ˜yn) ∈ ˜A(n)

ϵ

,

then

(2) (1 − ϵ) 2n(Hs( ˜X, ˜Y )−ϵ) ≤

2−n(Hs( ˜X, ˜Y )+ϵ) ≤ p (˜xn, ˜yn) ≤ 2−n(Hs( ˜X, ˜Y )−ϵ).
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ 2n(Hs( ˜X, ˜Y )+ϵ) for n sufficiently large.
(cid:12)

˜A(n)
ϵ

(110)

(3) If

˙X n and ˙Y n are independent sequences with the same marginals as X n and Y n, i.e.,
( ˙X n, ˙Y n) ∼ p(xn)p(yn), and the corresponding semantic sequence is ( ˙˜X n, ˙˜Y n), for n
sufficiently large, we have

(1 − ϵ)2−n(I s( ˜X; ˜Y )+3ϵ) ≤ Pr(( ˙˜X n, ˙˜Y n) ∈ ˜A(n)
ϵ )

≤ 2−n(I s( ˜X; ˜Y )−3ϵ).

(111)

Proof: Similar to Theorem 18, the proof of property (1) is directly from the weak law of

large numbers and Definition 19.

According to property (1), Eq. (110) can be rewritten as






(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12) 2−n(Hs( ˜X, ˜Y )+ϵ) ≤
(cid:12)

(cid:88)

p (˜xn, ˜yn) ≤ 1

(˜xn,˜yn)∈ ˜A(n)

ϵ

1 − ϵ ≤

(cid:88)

p (˜xn, ˜yn) ≤

(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12) 2−n(Hs( ˜X, ˜Y )−ϵ)
(cid:12)

(˜xn,˜yn)∈ ˜A(n)

ϵ

So the cardinality of semantically jointly typical set can be written as

and

(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12) ≥ (1 − ϵ) 2n(Hs( ˜X, ˜Y )−ϵ)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

˜A(n)
ϵ

(cid:12)
(cid:12) ≤ 2n(Hs( ˜X, ˜Y )+ϵ)
(cid:12)

52

(112)

(113)

(114)

respectively. We complete the proof of the second property.

Now assume ˙X n and ˙Y n are independent but have the same marginals as X n and Y n, and after
de-mapping, ( ˙X n, ˙Y n) is mapped into a semantic sequence pair ( ˙˜X n, ˙˜Y n), so we can establish
an one-to-one mapping (xn, yn) ↔ ( ˙xn, ˙yn) ↔ ( ˙˜xn, ˙˜yn), then we have

(cid:16)

Pr

(cid:17)

( ˙˜X n, ˙˜Y n) ∈ ˜A(n)
(cid:16)

ϵ

( ˙X n, ˙Y n) ∈ B(n)

ϵ

= Pr

,

˙X n ∈ A(n)

ϵ (X n), ˙Y n ∈ A(n)

ϵ (Y n)

(cid:17)

=

(cid:88)

p(xn)p(yn)

(xn,yn)↔( ˙˜xn, ˙˜yn)∈ ˜A(n)

ϵ

≤ 2n(Hs( ˜X, ˜Y )+ϵ)2−n(H(X)−ϵ)2−n(H(Y )−ϵ)

= 2−n(I s( ˜X, ˜Y )−3ϵ).

Using a similar thought, we can also derive that

(cid:16)

Pr

( ˙˜X n, ˙˜Y n) ∈ ˜A(n)

ϵ

(cid:17)

p(xn)p(yn)

=

(cid:88)

˜A(n)
ϵ

≥ (1 − ϵ)2n(Hs( ˜X; ˜Y )−ϵ)2−n(H(X)+ϵ)2−n(H(Y )+ϵ)

= (1 − ϵ)2−n(I s( ˜X, ˜Y )+3ϵ).

Thus we complete the proof of the theorem.

Furthermore, we give the properties of jointly synonymous typical set as below.

(115)

(116)

□

Theorem 20.
(1) Given a semantic sequence pair (˜xn, ˜yn) ∈ ˜A(n)

ϵ

2−n(H(X,Y )−Hs( ˜X, ˜Y )+ϵ) ≤

, if (xn, yn) ∈ B(n)
p (xn, yn)
p (˜xn, ˜yn)

ϵ

(˜xn, ˜yn), then

53

(117)

≤ 2−n(H(X,Y )−Hs( ˜X, ˜Y )−ϵ).

(2) 2n(H(X,Y )−Hs( ˜X, ˜Y )−ϵ) ≤

(cid:12)
(cid:12)B(n)
(cid:12)

ϵ

(cid:12)
(cid:12) ≤ 2n(H(X,Y )−Hs( ˜X, ˜Y )+ϵ) for sufficiently large n.
(cid:12)
(˜xn, ˜yn)

Proof: The proof of property (1) is directly from the definition of B(n)

ϵ

(˜xn, ˜yn).

To prove the left inequality of property (2), we write

p (˜xn, ˜yn)

=

(cid:88)

(xn,yn)∈B(n)

ϵ

(˜xn,˜yn)

p (xn, yn)

≤ p (˜xn, ˜yn)

(cid:88)

2−n(H(X,Y )−Hs( ˜X, ˜Y )−ϵ)

(xn,yn)∈B(n)

ϵ

(˜xn,˜yn)

= p (˜xn, ˜yn) (cid:12)

(cid:12)B(n)
ϵ

(˜xn, ˜yn)(cid:12)

(cid:12) 2−n(H(X,Y )−Hs( ˜X, ˜Y )−ϵ).

So it follows that

(cid:12)
(cid:12)B(n)
ϵ

(˜xn, ˜yn)(cid:12)

(cid:12) ≥ 2n(H(X,Y )−Hs( ˜X, ˜Y )−ϵ)

On the other hand, we can write

p (˜xn, ˜yn)

=

(cid:88)

(xn,yn)∈B(n)

ϵ

(˜xn,˜yn)

p (xn, yn)

≥ p (˜xn, ˜yn)

(cid:88)

2−n(H(X,Y )−Hs( ˜X, ˜Y )+ϵ)

(xn,yn)∈B(n)

ϵ

(˜xn,˜yn)

= p (˜xn, ˜yn) (cid:12)

(cid:12)B(n)
ϵ

(˜xn, ˜yn)(cid:12)

(cid:12) 2−n(H(X,Y )−Hs( ˜X, ˜Y )+ϵ).

Similarly, we have

and complete the proof.

(cid:12)
(cid:12)B(n)
ϵ

(˜xn, ˜yn)(cid:12)

(cid:12) ≤ 2n(H(X,Y )−Hs( ˜X, ˜Y )+ϵ)

(118)

(119)

(120)

(121)

□

The relationships of typical sets, such as typical set, jointly typical set and synonymous typical

set, are shown in Fig. 9. By using synonymous mapping f n
A(n)

ϵ (Y n) can be partitioned into many synonymous typical sets B(n)

ϵ (X n) or A(n)

x or f n

ϵ

y , syntactically typical set

(X n) or B(n)

ϵ

(Y n).

54

Fig. 9. The relationship of typical sets.

1 The syntactically jointly typical set A(n)
quences simultaneously belonging to the typical set A(n)
pairs of syntactically typical X n and Y n are jointly typical.

ϵ (X n, Y n) consists of some syntactically typical se-

ϵ (X n) and A(n)

ϵ (Y n). Note that not all

The probability of jointly typical set A(n)

ϵ (X n, Y n) with sufficiently large n is close to 1. In

tactically jointly typical sequences is about
holds for the jointly typical set ˜A(n)

addition, since all the jointly typical sequences are almost equiprobability, the number of syn-
(cid:12)
(cid:12)A(n)
≈ 2n(H(X,Y )±ϵ). Similar conclusion
(cid:12)
ϵ ( ˜X n, ˜Y n) and the corresponding number of jointly typical
(cid:12)
≈ 2n(Hs( ˜X, ˜Y )±ϵ).
ϵ ( ˜X n, ˜Y n)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ϵ (X n, Y n)
(cid:12)

sequences is about

˜A(n)

(cid:12)
(cid:12)
(cid:12)

Furthermore, under the joint mapping f n

xy, as circled by dashed lines, some jointly typical
sequences constitute the jointly synonymous typical sets B(n)
(X n, Y n). Since the number of
jointly synonymous typical sequences is about 2n(H(X,Y )−Hs( ˜X, ˜Y )±ϵ), all the synonymous typical

ϵ

sets have almost the same number of typical sequences. In each set, the black circle marked by

1Note that B(n)

ϵ

(X n) denotes the set consisting the syntactically typical sequences X n whereas B(n)

ϵ

induced by the semantic typical sequence ˜xn. Correspondingly, B(n)

ϵ

(Y n) and B(n)

ϵ

(˜yn), B(n)

ϵ

have similar distinction.

(˜xn) denotes the set
(˜xn, ˜yn)

(X n, Y n) and B(n)

ϵ

nxnySyntactically typical seqSyntactically nontypical seqSemantically representative typical seq()()()()  2nnHXnAX=()()()()  2nnnHYAY=()()()()()()  2snnnHYHYBY−=()()()(),,  2nnnnHXYAXY=()()()()()(),,  2,snnnnHXYHXYBXY−=()()()()()()  2snHXHXnnBX−=55

a red box denotes the representative typical sequence. If we randomly choose a typical sequence

pair, the probability that this pair falls in a jointly synonymous typical set (equivalently represents
a semantically jointly typical sequence) is about 2−nI s( ˜X; ˜Y ). This means that there are about
2nI s( ˜X; ˜Y ) distinguishable sequences ˜X n in the semantic sense.

B. Semantic Channel Coding Theorem

We now discuss the problem of semantic channel coding. As shown in Fig. 10, with the

help of synonymous mapping f n, a semantic index is is mapped and encoded into the channel
codeword X n. Here, the semantic message ˜X n(is) is a broad concept, which can be a real

semantic sequence or a syntactic sequence with some semantic constraints. After going through

the channel, we obtain the received sequence Y n. Then the decoder outputs the decoded codeword
ˆX n. After de-synonymous mapping gn, we obtain an estimation of semantic index ˆis.

Fig. 10. Block diagram of semantic channel coding.

For a discrete memoryless channel, let p(Y |X) be the channel transition probabilities and X

and Y denote the input and output syntactical alphabet of channel respectively. Then the channel

transition probabilities for the n-th extension of the channel can be written as

p(yn |xn ) =

n
(cid:89)

k=1

p(yk |xk ).

(122)

Definition 21. An (M, n) code for the semantic channel

(cid:111)
(cid:110) ˜X , X , Y, ˜Y, p(Y |X)

consists of the

following parts:

(1) A semantic index set Is = {1, · · · , is, · · · , Ms} and a syntactic index set I = {1, · · · , i, · · · , M }.
(2) An encoding function ϕ : ˜X n → X n generates the set of codewords, namely, codebook,

C = {X n(1), X n(2), · · · , X n(M )}. Due to the synonymous mapping f n, this codebook can

be partitioned into synonymous codeword subsets Cs.

(3) A decoding function ψ : Y n → X n outputs the decision syntactic codeword ˆX n.

EncoderDecodernfngChannel()pyxnYˆnXnXsiˆsi(4) After de-mapping, gn( ˆX n) = ˆis, the estimated semantic index is obtained. Note that both ψ

and gn are deterministic.

56

According to the synonymous mapping f n, Cs is an equivalence class consisting of the
synonymous codewords. So we can construct a quotient set C/f n = {Cs} with |C/f n| = Ms. Let
R = 1

n log2 Ms denote the semantic code rate of channel coding. We configure each synonymous
, where Rs is named as the

set with the same number of codewords, that is, |Cs| = 2nRs = M
Ms
rate of synonymous set. Furthermore, let R′ = R + Rs = 1

n log2 M being the syntactic code rate.

Definition 22. Assume a semantic index is is mapped into a syntactic codeword X n(i) ∈ Cs(is),

the conditional decoding error probability given the index is is defined as
(cid:12)
(cid:12)X n = X n(i) ↔ ˜X n = ˜X n(is)
gn (ψ (Y n)) ̸= is
(cid:12)
(cid:12)
(cid:16) ˆX n(i) /∈ Cs(is)
(cid:17)
(cid:12)X n = X n(i) ↔ ˜X n = ˜X n(is)
(cid:12)

λis = Pr

= Pr

(cid:16)

(cid:17)

(123)

(cid:88)

=

yn

p (yn(i) |xn(i) ) I (ψ(yn) /∈ Cs(is))

where I(·) is the indicator function. Assume the index is is chosen uniformly on the set Is, the
average error probability P (n)

for an (M, n) code is defined as

e

P (n)

e =

1
Ms

Ms(cid:88)

is=1

λis.

(124)

We now give the formal description of semantic channel coding theorem.

Theorem 21. (Semantic Channel Coding Theorem)
(cid:110) ˜X , X , Y, ˜Y, p(Y |X)

Given the semantic channel

(cid:111)

, for each code rate R < Cs, there exists
a sequence of (cid:0)2n(R+Rs), n(cid:1) codes consisting of synonymous codeword set with the rate 0 ≤
Rs ≤ H(X, Y ) − Hs( ˜X, ˜Y ), when code length n tends to sufficiently large, the error probability
tends to zero, i.e. P (n)

e → 0.

On the contrary, if R > Cs, then for any (cid:0)2n(R+Rs), n(cid:1) code, the error probability tends to 1

with sufficiently large n.

Proof: We first prove the achievability part of the theorem and the converse will be left in

the next part.

Given the source distribution p(x) and the synonymous mapping fx, a (cid:0)2n(R+Rs), n(cid:1) code can
be generated randomly according to the distribution p(x). Note that 2n(R+Rs) codewords can be

independently generated based on the distribution
n
(cid:89)

p(xn) =

p(xk).

57

(125)

k=1
Furthermore, these codewords can be uniformly divided into 2nR groups according to the syn-

onymous mapping

x (˜xn) =
f n

n
(cid:89)

k=1

X˜xk.

Thus all the 2n(R+Rs) codewords can be listed as a matrix













C =

x1(1)
...
x1(2nRs)
...

x2(1)
...
x2(2nRs)
...

x1(2n(R+Rs)) x2(2n(R+Rs))

xn(1)
...
xn(2nRs)
...

· · ·
...
· · ·
. . .
· · · xn(2n(R+Rs))


















Cs(1)

...
Cs(2nR)

The probability of generating the synonymous codeword set Cs(1) is

Pr (Cs(1)) =

2nRs
(cid:89)

n
(cid:89)

i=1

k=1

p(xk(i))

(126)

(127)

(128)

All the codeword sets Cs(is) have the same generating probability. Furthermore, the probability

of generating a particular code C is

Pr (C) =

2n(R+Rs)
(cid:89)

n
(cid:89)

i=1

k=1

p(xk(i)).

(129)

Similar to the idea in [4], we also use jointly typical decoding for the semantic channel code.

If a codeword ˆX n(i) is decided, it must satisfy the following conditions.
1) ( ˆX n(i), Y n) is syntactically jointly typical, due to gn( ˆX n(i)) = ˆis, ( ˜X n(ˆis), ˜Y n) is also
semantically jointly typical. Equivalently, ( ˆX n(i), Y n) is jointly synonymous typical. Hence,
the decision codeword ˆX n(i) may be not equal to the transmit codeword X n(i) but both
ˆX n(i) and X n(i) belong to Cs(ˆis).

2) There is no other index m, satisfying ( ˆX n(m), Y n) ∈ A(n)

ϵ

or gn( ˆX n(m)) = ˆis.

We now calculate the average error probability of jointly typical decoding. Generally, this error

probability should be averaged over all the codebooks and all the codewords. However, based

on the symmetry of the code construction, due to averaging over all codes, the error probability

is not dependent on the specific the semantic index is and the syntactic index i. We relabel the
codewords in a synonymous set as X n(is, j) ∈ Cs(is), j ∈ (cid:8)1, 2, · · · , 2nRs(cid:9).

Without loss of generality, we can assume is = 1 and i = 1, that is, the codeword X n(1, 1)

is sent. Therefore, the average error probability can be written as

58

Pr(E) =

=

1
2nR

(cid:88)

C

2nR
(cid:88)

(cid:88)

is=1

C

P (C)λis(C)

P (C)λ1(C)

= Pr (E |X n(1, 1)) .

(130)

Given the received sequence Y n when sending the first codeword X n(1, 1), we define the

following events:

Eis = (cid:8)(X n(is, j), Y n) ∈ B(n)

ϵ

, X n(is, j) ∈ Cs(is)(cid:9),
is ∈ Is, j ∈ (cid:8)1, 2, · · · , 2nRs(cid:9).

(131)

Here, the event Eis means that the codewords in the is-th synonymous set Cs(is) and Y n are

jointly synonymous typical.

When sending the first codeword X n(1, 1) and receiving the received sequence Y n, by using

jointly typical decoding, two kinds of error will occur. The first error event is Ec

1 which means
that all the codewords in Cs(1) and Y n are not jointly typical. By the syntactically joint AEP,

we have

P (Ec

1) ≤ ϵ, for sufficiently large n.

(132)

On the other hand, the second error event is Eis, is ∈ (cid:8)2, · · · , 2nR(cid:9) which means that a
codeword in a wrong synonymous set is jointly typical with the received sequence Y n. Due to

the code construction process, X n(1, 1), X n(is, j), (is ̸= 1), and Y n are mutually independent.
Hence, by using semantically joint AEP (Theorem 19), the probability that X n(is, j) and Y n

are jointly synonymous typical is written as

P (Eis) ≤ 2−n(I s( ˜X; ˜Y )−3ϵ), is ∈ (cid:8)2, · · · , 2nR(cid:9) .

(133)

Consequently, combining (132) and (133), the error probability can be derived as

Pr(E) = Pr (E |X n(1, 1))

≤ P (Ec

1) +

2nR
(cid:88)

is=2

P (Eis)

≤ ϵ +

2nR
(cid:88)

is=2

2−n(I s( ˜X; ˜Y )−3ϵ)

= ϵ + (cid:0)2nR − 1(cid:1) 2−n(I s( ˜X; ˜Y )−3ϵ)

≤ ϵ + 2−n(I s( ˜X; ˜Y )−R−3ϵ)

≤ 2ϵ.

59

(134)

This formula holds for sufficiently large n and I s( ˜X; ˜Y ) − R − 3ϵ > 0.

Therefore, if the semantic code rate satisfies R < I s( ˜X; ˜Y ), the error probability can tend to

zero with the suitable ϵ and n. In addition, by Theorem 20, the size of synonymous codeword

set satisfies

1 ≤ 2nRs ≤ 2n(H(X;Y )−Hs( ˜X; ˜Y ))

= 2n(I s( ˜X; ˜Y )−I(X;Y ))

for sufficiently large n. Thus we have 0 ≤ Rs ≤ (I s( ˜X; ˜Y ) − I(X; Y )) and derive that

I(X; Y ) + Rs ≤ I s( ˜X; ˜Y ).

Then the syntactic code rate R′ = R + Rs can be upper bounded by

R′ ≤ 2I s( ˜X; ˜Y ) − I(X; Y )

= I s( ˜X; ˜Y ) + H(X, Y ) − Hs( ˜X, ˜Y ).

(135)

(136)

(137)

In summary, for any code rate below the semantic capacity Cs = maxfxy maxp(x) I s( ˜X; ˜Y ), we
can construct a code with the error probability being close to zero for sufficiently large n. This

proves the achievability of theorem.

□

Remark 9. In the classic channel coding theorem, in order to satisfying the requirement of

reliable communication, the code rate must be lower than the channel capacity C. On the

contrary, in the semantic channel coding, the code rate can be further increased to the semantic

channel capacity Cs under the condition of keeping the semantic reliability. Using synonymous

60

codeword set to present the semantic sequence is the key technique to achieve this goal. By

(136), due to C + Rs ≤ Cs, with the increasing of the number of codewords in a synonymous

codeword set, the semantic code rate gradually grows and approaches the semantic capacity.

Similar to the classic counterpart, although the proof of the semantic channel coding theorem is

also an existence method due to using the random coding, it may provide some hints to construct

the channel codes approaching the semantic capacity.

In order to prove the converse, we first illustrate the relationship between sequential syntactic

mutual information and sequential semantic mutual information.

Lemma 8. Assume ˜X n is the transmitted semantic sequence over a discrete memoryless channel
and the received sequence is Y n, we have

I( ˜X n; Y n) ≤ I s( ˜X n; ˜Y n), for all p(xn).

(138)

Proof: Due to the definition of discrete memoryless channel, we can write the sequential

mutual information as

I( ˜X n; Y n) − I s( ˜X n; ˜Y n)

= H( ˜X n) + H(Y n) − H( ˜X n, Y n)

(cid:104)
(cid:105)
H(X n) + H(Y n) − Hs( ˜X n, ˜Y n)

−

(139)

=

n
(cid:88)

k=1

(cid:105)
(cid:104)
Hs( ˜Xk) − H(Xk) + Hs( ˜Xk, ˜Yk) − H( ˜Xk, Yk)

≤ 0.

Due to H( ˜Xk) = Hs( ˜Xk) ≤ H(Xk) and Hs( ˜Xk, ˜Yk) ≤ H( ˜Xk, Yk) (Theorem 2), we prove the
□

lemma.

Then we investigate the sequential semantic mutual information many times using of discrete

memoryless channel.

Lemma 9. Assume X n is the transmitted codeword over a discrete memoryless channel and the

received sequence is Y n, under a jointly synonymous mapping f n

xy, we have

I s( ˜X n; ˜Y n) ≤ nCs, for all p(xn).

(140)

Proof: Due to the definitions of discrete memoryless channel and up-semantic mutual

information, we can write

I s( ˜X n; ˜Y n) = H(X n) + H(Y n) − Hs( ˜X n; ˜Y n)

[H(Xk) + H(Yk)] −

n
(cid:88)

k=1

Hs( ˜Xk, ˜Yk)

(cid:104)

(cid:105)
H(Xk) + H(Yk) − Hs( ˜Xk, ˜Yk)

≤

=

n
(cid:88)

k=1
n
(cid:88)

k=1

≤ nCs.

61

(141)

The first inequality is from the property of sequential entropy and the second is from the definition

of semantic channel capacity.

□

We now prove the converse to the semantic channel coding theorem.

Proof: (Converse to Theorem 21, (Semantic Channel Coding Theorem)):

Let Ws denote a semantic index uniformly drawn from (cid:8)1, 2, · · · , 2nR(cid:9). The error probability

can be written as P (n)

e = Pr( ˆWs ̸= Ws). So we have

nR = H(Ws) = H(Ws|Y n) + I(Ws; Y n)

(a)

≤ H(Ws|Y n) + I( ˜X n(Ws); Y n)

(b)
≤ 1 + P (n)

e nR + I( ˜X n(Ws); Y n)

(c)
≤ 1 + P (n)

e nR + I s( ˜X n(Ws); ˜Y n)

(d)
≤ 1 + P (n)

e nR + nCs.

(142)

Inequality (a) holds since ˜X n(Ws) is the function of Ws and (b) is from Fano’s inequality. In
equality (c) is from Lemma 8 and (d) from Lemma 9.

So the semantic code rate satisfies

R ≤ Cs + P (n)

e R +

1
n

,

which means R ≤ Cs for n → ∞. On the other hand, we can rewrite (143) as

P (n)

e ≥ 1 −

1
nR

−

Cs
R

.

(143)

(144)

This formula indicates that if R > Cs, the error probability is larger than zero for sufficiently

large n and the reliable transmission of semantic information can not be fulfilled. So we complete

the proof.

□

62

In classic communication systems, the channel capacity is a fundamental limitation of data

reliable transmission. In the past seventy years, people invented many powerful channel codes

to approach capacity, such as turbo, LDPC and polar codes. Similarly, the semantic capacity is

also a key parameter for the semantic transmission. In the future, the construction of channel

codes approaching semantic capacity will become one core issue of semantic communication.

C. Semantic Channel Coding Method

We now investigate the semantic channel coding method. Given a (cid:0)2n(R+Rs), n(cid:1) channel code
with length n and semantic rate R, the codebook C can be divided into 2nR synonymous codeword

groups Cs(is), is ∈ {1, 2, · · · , 2nR} and each group has 2nRs synonymous codewords. Consider

the synonymous mapping, we propose a new method, named as maximum likelihood group

(MLG) decoding algorithm to decode this semantic code. The basic idea is to calculate the

likelihood probability of the received signal on a synonymous group and compare all the group

likelihood probabilities so as to select a group with the maximum probability as the final decoding

result.

Definition 23. Assume one codeword xn ∈ Cs(is) is transmitted on the discrete memoryless
channel with the transition probability p(yn|xn), the group likelihood probability is defined as

P (yn|Cs(is)) ≜

2nRs
(cid:89)

l=1

p(yn|xn(is, l)).

So the maximum likelihood group decoding rule is written as

ˆis = arg max

is

P (yn|Cs(is))

= arg max

is

2nRs
(cid:89)

l=1

p(yn|xn(is, l)).

Equivalently, this rule can also presented as a logarithmic version,

ˆis = arg max

is

2nRs
(cid:88)

l=1

ln p(yn|xn(is, l)).

(145)

(146)

(147)

Hence, we can calculate all the group likelihood probabilities and select one group with the
maximum probability as the final decision. The index ˆis indicates the estimation of semantic
information ˆ˜xn.

Next, we discuss the MLG decoding in the additive white Gaussian noise (AWGN) channel.

When a signal is transmitted over the AWGN channel, the received signal can be represented

by an equivalent low-pass signal sampled at time k:

63

(148)
(cid:9) is the binary phase shifted key (BPSK) signal, zk is a sample of a zero-
where sk = (cid:8)±
mean complex Gaussian noise process with variance σ2 = N0/2. Let Es be the symbol energy

yk = sk + zk

Es

√

and N0 denote the single-sided power spectral density of the additive white noise. So the symbol
signal-to-noise ratio (SNR) is defined as Es
N0

.
Assume one codeword xn(is, l) is mapped into a transmitted signal vector sn(is, l) =
Es (1n − 2xn(is, l)) where 1n is the all-one vector with the length of n, by using MLG rule,

√

we can write

ˆis = arg max

is

2nRs
(cid:88)

l=1

2nRs
(cid:88)

l=1

2nRs
(cid:88)

l=1

= arg max

is

= arg min

is

ln p(yn|xn(is, l))

(cid:20)

ln

√

1
2πσ2

∥yn−sn(is,l)∥2
2σ2

e−

(cid:21)

∥yn − sn(is, l)∥2

(149)

= arg min

is

d2
E (yn, Cs(is)) ,

where d2

E (yn, Cs(is)) = (cid:80)2nRs

l=1 ∥yn − sn(is, l)∥2 is the squared Euclidian distance between the
receive vector yn and the code group Cs(is). Thus the MLG rule in AWGN channel is transformed

into the minimum distance grouping decoding rule.

Now we investigate the group-wise error probability (GEP) of semantic channel code.

Theorem 22. Given a semantic channel code C with equipartition code groups Cs, the GEP

between Cs(is) and Cs(js) is upper bounded by

P (Cs(is) → Cs(js)) ≤ exp

−dGH(Cs(is), Cs(js))

(cid:26)

(cid:27)

,

Es
N0

where dGH denotes the group Hamming distance which is defined as following

dGH(Cs(is), Cs(js)) =

(cid:104)(cid:80)2nRs

min
m

l=1 dH(xn(is, m), xn(js, l)) − (cid:80)2nRs
∥ (cid:80)2nRs

l=1 (xn(js, l) − xn(is, l)) ∥2

l=1,l̸=m dH(xn(is, m), xn(is, l))

(150)

(151)

.

(cid:105)2

64

Proof: Assume one codeword xn(1, 1) in the code group Cs(1) is transmitted, the received

signal vector can be represented as follows

yn = sn(1, 1) + zn,

(152)

where zn ∼ N (0, σ2I) is the Gaussian noise vector.

Suppose a codeword xn(js, l) ∈ Cs(js), js ̸= 1 is mapped into the signal vector sn(js, l). By

using the MLG rule, if a group-wise error occurs, the Euclidian distance between the received

vector and the transmitted signal vector group satisfy the inequality

E (yn, Cs(is)) > d2
d2

E (yn, Cs(js)) .

(153)

Substituting (149) and (152) into this inequality, we have

2nRs
(cid:88)

l=1

∥yn − sn(1, l)∥2

>

2nRs
(cid:88)

l=1

∥yn − sn(js, l)∥2 ⇒

∥zn∥2 +

2nRs
(cid:88)

l=2

∥sn(1, 1) − sn(1, l) + zn∥2

>

2nRs
(cid:88)

l=1

∥sn(1, 1) − sn(js, l) + zn∥2 .

(154)

After some manipulations, the error decision region can be written as


H =

zn :



(sn(js, l) − sn(1, l))

2nRs
(cid:88)


 (zn)T >






l=1

1
2





2nRs
(cid:88)

l=1

∥sn(1, 1) − sn(js, l)∥2

(155)

−

2nRs
(cid:88)

l=2

∥sn(1, 1) − sn(1, l)∥2










.

Let d2

E (sn(1, 1), Cs(js)) = (cid:80)2nRs

vector sn(1, 1) and the code group Cs(js) and d2

denote the inner distance of code group Cs(1).

l=1 ∥sn(1, 1) − sn(js, l)∥2 denote the distance between the transmit
l=2 ∥sn(1, 1) − sn(1, l)∥2

E (sn(1, 1), Cs(1)) = (cid:80)2nRs

So the codeword-to-group error probability can be derived as

P (xn(1, 1) → Cs(js))






(cid:118)
(cid:117)
(cid:117)
(cid:116)

= Q

(d2

E (sn(1, 1), Cs(js)) − d2

E (sn(1, 1), Cs(1)))2
(cid:13)
2
(cid:13)
l=1 (sn(js, l) − sn(1, l))
(cid:13)

(cid:80)2nRs

2N0

(cid:13)
(cid:13)
(cid:13)

65

(156)




 ,

Es (xn(js, l) − xn(1, 1)), then we have ∥sn(1, 1) − sn(js, l)∥2
(cid:13)
2
(cid:13)
l=1 (sn(js, l) − sn(1, l))
(cid:13)

(cid:80)2nRs

(cid:13)
(cid:13)
(cid:13)

=

where Q(x) = 1√
2π

(cid:82) ∞
x e−t2/2dt is the tail distribution function of the standard normal distribution.

Furthermore, due to (sn(1, 1) − sn(js, l)) = 2

√

= 4EsdH(xn(1, 1), xn(js, l)). Additionally, we can derive that

4Es

(cid:13)
(cid:13)
(cid:13)

(cid:80)2nRs

(cid:13)
2
(cid:13)
l=1 (xn(js, l) − xn(1, l))
(cid:13)

= 4Es∥∆(Cs(js), Cs(1))∥2. Thus the error probability can be

further written as

P (xn(1, 1) → Cs(js)) = Q

(cid:34)(cid:114)

dGH(xn(1, 1), Cs(js))

(cid:35)

,

2Es
N0

(157)

where dGH(xn(1, 1), Cs(js)) =[(cid:80)2nRs
− (cid:80)2nRs

l=1 dH(xn(1, 1), xn(js, l))

l=2 dH(xn(1, 1), xn(1, l))]2/∥∆(Cs(js), Cs(1))∥2 denotes the codeword-to-group Hamming

distance.

Furthermore, using the inequality Q(x) ≤ e− x2

2 , the codeword-to-group error probability can

be upper bounded by

P (xn(1, 1) → Cs(js)) ≤ e−dGH(xn(1,1),Cs(js)) Es
N0 .

(158)

Averaging over all the codewords of the group Cs(1), we obtain the upper bound of GEP as

follows

P (Cs(1) → Cs(js)) ≤

2nRs
(cid:88)

l=1

1
2nRs
(cid:26)

e−dGH(xn(1,l),Cs(js)) Es

N0

So we complete the proof.

≤ exp

−dGH(Cs(1), Cs(js))

(cid:27)

.

Es
N0

(159)

□

In the ML decoding, the minimum Hamming distance dH,min determines the error performance

of one linear channel code. Similarly, in the MLG decoding, the minimum group Hamming

distance dGH,min = min dGH(Cs(is), Cs(js)) dominates the performance of semantic channel code.

Example 5. We now give an example of semantic code constructed based on (7,4) Hamming

code with synonymous mapping and MLG decoding. The codebook is shown in Table IX. All

66

the sixteen codewords are divided into eight code groups and each group has two synonymous

codewords. For an instance, Cs(1) has two codewords (0000000) and (1101000) and its semantic

sequence is (000). So this code can be regarded as a (7,3) semantic Hamming code with code

rate R = 3

7 and Rs = 1
7.

By using ML decoding, the union bound of the error probability is

Pe ≤

n
(cid:88)

d=dH,min

(cid:32)(cid:114)

AdQ

2d

(cid:33)

≤

Es
N0

n
(cid:88)

d=dH,min

Ade−d Es
N0 .

(160)

Since the minimum Hamming distance of this code is dH,min = 3 and distance spectrum is

{A3 = 8, A4 = 6, A7 = 1}, the error probability of ML decoding is upper bounded by

e ≤ 8e−3 Es
P ML

N0 + 6e−4 Es

N0 + e−7 Es
N0 .

(161)

Let {Ad1,d2} denote the group distance spectrum and d1 and d2 mean the codeword-to-group

Hamming distance. By using MLG decoding, the union bound of the error probability is

n
(cid:88)

Pe ≤

d1,d2=dGH,min
n
(cid:88)

d1,d2=dGH,min

≤

Ad1,d2
2

(cid:34)

(cid:32)(cid:114)

Q

2d1

Es
N0

(cid:33)

(cid:32)(cid:114)

+ Q

2d2

(cid:33)(cid:35)

Es
N0

(cid:16)

Ad1,d2
2

e−d1

Es

N0 + e−d2

Es
N0

(cid:17)

.

(162)

So the minimum group Hamming distance of this code is dGH,min = 2 and group distance

spectrum is {A2,2 = 6, A4,4 = 1}. The corresponding upper bound of the MLG decoding is

e ≤ 6e−2 Es
P MLG

N0 + e−4 Es
N0 .

(163)

Compare with (161) and (163), we find that the minimum distance of semantic Hamming code

is decreased. However, for a long code length and well-designed synonymous mapping, the error

performance of MLG decoding will be better than that using ML decoding.

Remark 10. From the viewpoint of practical application, semantic channel codes are a new

kind of channel codes. Synonymous mapping provides a valuable idea for the construction and

decoding of semantic channel codes. Unlike the traditional channel codes, semantic channel

codes should optimize the minimum group Hamming distance. How to design an optimal syn-

onymous mapping to cleverly partition the code group is significant for the design of semantic

codes. Non-equipartition mapping may be more flexible than the equipartition mapping. On the

other hand, the optimal decoding of semantic codes is the MLG rule rather ML rule. However,

THE CODEBOOK OF (7,3) SEMANTIC HAMMING CODE WITH SYNONYMOUS MAPPING.

TABLE IX

67

Index is

Semantic sequence Hamming code group Cs(is)

1

2

3

4

5

6

7

8

000

001

010

011

100

101

110

111

{0000000, 1101000}

{0110100, 1011100}

{1110010, 0011010}

{1000110, 0101110}

{1010001, 0111001}

{1100101, 0001101}

{0100011, 1001011}

{0010111, 1111111}

due to the exponent complexity of MLG decoding algorithm, it is not practical for application.

So we should pursuit lower complexity decoding algorithms for the semantic channel codes in

the future.

VIII. SEMANTIC LOSSY SOURCE CODING

In this section, we mainly discuss the semantic lossy source coding. Firstly, we investigate

the semantic distortion measure and extend the concept of jointly typical sequence in the

semantic sense. Then we prove the semantic rate-distortion coding theorem by using JAEP

and synonymous typical set, which means that the semantic rate distortion function, Rs(D) =
Is( ˜X; ˆ˜X), namely the minimum down semantic mutual informa-

minfx,fˆx min

p(Y |X):Eds( ˜X, ˆ˜X)≤D

tion, is the lowest compression rate achieving by semantic lossy source coding.

A. Semantic Distortion and Jointly Typical Set

Assume a discrete source X ∼ p(x), x ∈ X with the associated semantic source ˜X produces
a sequence X n ∼ p(xn), after the quantization and reproduction, equivalently, through a test
channel with the transition probabilities p(ˆxn|xn), we obtain the representative sequence ˆX n ∼
p(ˆxn). Here, p(ˆxn) = (cid:80)
p(xn) p(xn)p(ˆxn|xn) denotes the distribution of reproduction sequences.

Definition 24. For a semantic sequence ˜xn, under the synonymous mapping f n

x , the associated
source sequence is xn. After going through a test channel with the transition probabilities

p(ˆxn|xn), we obtain the reconstruction sequence ˆxn. Under the de-synonymous mapping gn
ˆx ,

68

(164)

(165)

the associated semantic sequence is ˆ˜xn. Thus the semantic distortion between sequences ˜xn and
ˆ˜xn is defined by

ds(˜xn, ˆ˜xn) =

=

1
n

1
n

n
(cid:88)

k=1
n
(cid:88)

k=1

ds(˜xk, ˆ˜xk)

ds(X˜xk, ˆXˆ˜xk

).

Then the average semantic distortion over the semantic sequences is defined as

E

(cid:104)
ds( ˜X n, ˆ˜X n)

(cid:105)

=

(cid:88)

p (xn) p(ˆxn|xn)ds(˜xn, ˆ˜xn).

(xn,ˆxn)

Given the synonymous mapping f n

x , let A(n)

ϵ (X n) and ˜A(n)

ϵ ( ˜X n) denote the syntactically

and semantically typical set of source sequence respectively. Correspondingly, the synonymous
typical set is denoted as B(n)

(˜xn → X n). According to Theorem 14, we have

ϵ

2n(H(X)−Hs( ˜X)−ϵ) ≤ (cid:12)

(cid:12)B(n)
ϵ

(˜xn → X n)(cid:12)

(cid:12) ≤ 2n(H(X)−Hs( ˜X)+ϵ)

(166)

for sufficiently large n.

of reproduction sequence is written as p(ˆxn) = (cid:80)
ˆx , let A(n)
synonymous mapping f n

Furthermore, given a test channel with the transition probabilities p(ˆxn|xn), the distribution
p(xn) p(xn)p(ˆxn|xn). Similarly, given the
ϵ ( ˆ˜X n) denote the syntactically and semantically
typical set of reproduction sequence respectively. Moreover, the associated synonymous typical
set is addressed as B(n)

(ˆ˜xn → ˆX n). By Theorem 14, we also have

ϵ ( ˆX n) and ˜A(n)

ϵ

H( ˆX)−Hs( ˆ˜X)−ϵ

(cid:17)

(cid:16)
2n

≤

(cid:12)
(cid:12)B(n)
(cid:12)

ϵ

(cid:12)
(cid:12) ≤ 2n
(ˆ˜xn → ˆX n)
(cid:12)

(cid:16)
H( ˆX)−Hs( ˆ˜X)+ϵ

(cid:17)

(167)

for sufficiently large n. Since all the synonymous typical sets have almost the same size, we can
abbreviate them as B(n)

( ˆX n) and B(n)
(X n) respectively.
Given the joint distribution p(x, ˆx) on X × ˆX , let A(n)

ϵ

ϵ

ϵ (X n, ˆX n) denote the the syntactically

jointly typical set of source sequence and reproduction sequence and ˜A(n)

ϵ

be the semantically

jointly typical set.

The typical sequence mapping for lossy source coding is depicted in Fig. 11. Under the
ˆx , the semantic source sequence ˜xn and semantic reconstruction
synonymous mappings f n
( ˆX n) .
sequence ˆ˜xn are separately mapped into synonymous typical sets B(n)
Furthermore, by the conditional probability p(ˆxn|xn), the typical sequences in these sets can

(X n) and B(n)

x and f n

ϵ

ϵ

compose the jointly typical sequence.

69

Fig. 11. Typical sequence mapping for lossy source coding.

Then as the consequence of the JAEP, we present the properties of semantically jointly typical

set as following.

Theorem 23. Let (X n, ˆX n) be a sequence pair with length n drawn i.i.d. according to p(xn, ˆxn).
ˆx ), the semantic sequence ˜X n ( ˆ˜X n) is mapped into a
By using synonymous mapping f n
syntactic sequence X n ( ˆX n).

x (f n

(cid:16)

(1) Pr

(cid:17)

( ˜X n, ˆ˜X n) ∈ ˜A(n)
ϵ
Hs( ˜X, ˆ˜X)−ϵ

> 1 − ϵ for n sufficiently large.
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ 2n
(cid:12)

(cid:17)
Hs( ˜X, ˆ˜X)+ϵ

˜A(n)
ϵ

(cid:16)

(cid:17)

(cid:16)
(2) (1 − ϵ) 2n
(3) Given ˜Z n and ˆ˜Z n are two independent semantic sequences, if Z n and ˆZ n are two associated
syntactic sequences with the same distributions as X n and ˆX n, i.e., Z n ∼ p(xn) and ˆZ n ∼

for n sufficiently large.

≤

p(ˆxn), for n sufficiently large, we have

(1 − ϵ)2−n(Is( ˜X; ˆ˜X)+3ϵ) ≤ Pr

(cid:16)

( ˜Z n, ˆ˜Z n) ∈ ˜A(n)

ϵ

(cid:17)

≤ 2−n(Is( ˜X; ˆ˜X)−3ϵ).

(168)

Proof: Property (1) and (2) are restatements of semantically jointly typical set.

Assume ˜Z n and

ˆ˜Z n are two independent semantic sequences, the associated syntactic se-
quences Z n and ˆZ n are independent but have the same distributions as X n and ˆX n. Thus we

nSyntactic sequence setSemantic sequence setReconstruction sequence setSynonymous mappingSynonymous typical setJointly typical sequencenxfSemantic sequence setSynonymous typical setSynonymous mappingˆn()ˆ|nnpxxˆnxfnXˆnXcan establish two one-to-one mappings ˜zn ↔ zn ↔ xn and ˆ˜zn ↔ ˆzn ↔ ˆxn, then,
(cid:17)

(cid:16)

Pr

( ˜Z n, ˆ˜Z n) ∈ ˜A(n)
(cid:16)

Z n ∈ B(n)

ϵ

ϵ

= Pr

(X n), ˆZ n ∈ B(n)

ϵ

( ˆX n), (Z n, ˆZ n) ∈ A(n)

ϵ

(cid:17)

=

(cid:88)

p(˜xn)p(ˆ˜xn)

(˜xn,ˆ˜xn)↔(xn,ˆxn)∈A(n)

ϵ

≤ 2n(H(X, ˆX)+ϵ)2−n(Hs( ˜X)−ϵ)2−n

(cid:16)

Hs( ˆ˜X)−ϵ

(cid:17)

= 2−n(Is( ˜X; ˜Y )−3ϵ).

Using a similar method, we can also derive that
(cid:16)

(cid:17)

Pr

( ˜Z n, ˆ˜Z n) ∈ ˜A(n)

ϵ

p(˜xn)p(ˆ˜xn)

=

(cid:88)

A(n)
ϵ

≥ (1 − ϵ)2n(H(X; ˆX)−ϵ)2−n(Hs( ˜X)+ϵ)2−n

(cid:16)

Hs( ˆ˜X)+ϵ

(cid:17)

(cid:16)
= (1 − ϵ)2−n

(cid:17)
Is( ˜X; ˆ˜X)+3ϵ

.

Thus we complete the proof of the theorem.

70

(169)

(170)

□

Remark 11. It should be noted that the probability of Eq. (168) in Theorem 23 is different from

that of Eq. (111) in Theorem 19. For the former, it indicates the probability of selecting two

synonymous typical sequences (equivalently, representing two semantically typical sequences)

constituting a syntactically jointly typical pair which is used to evaluate the error probability of

jointly typical encoding. On the other hand, for the latter, it represents the probability of selecting

two syntactically typical sequences consisting a jointly synonymous typical pair (equivalently,

a semantically jointly typical pair) so that it reveals the error probability of jointly typical

decoding.

B. Semantic Rate Distortion Coding Theorem

We now investigate the problem of semantic lossy source coding. As depicted in Fig. 12, with

the help of synonymous mapping f n

x , a semantic index is is mapped into the syntactic source
sequence X n(i). Considering the distortion requirement, the encoder selects a suitable codeword
ˆX n(j) to represent the source sequence X n(i) then send the index i to the receiver. Then in the

side of receiver, the decoder outputs the reproduction sequence ˆX n(j). After de-mapping gn, we
obtain an estimation of semantic index ˆis.

71

Fig. 12. Block diagram of semantic lossy source coding.

For a lossy source coding system, let p( ˆX|X) be the conditional probability and X and
ˆX denote the source and reproduction syntactical alphabet respectively. Then the conditional

probabilities for the n-th extension can be written as

p(ˆxn |xn ) =

n
(cid:89)

k=1

p(ˆxk |xk ).

(171)

Definition 25. An (M, n) code for the semantic lossy source coding consists of the following

parts:

(1) Two semantic index sets Is = {1, · · · , Ms} and Ir = {1, · · · , Mr}. Two syntactic index sets

I = {1, · · · , M } and I ′ = {1, · · · , M ′}.

(2) A synonymous mapping f n

x : ˜X n → X n generates the set of source sequences, namely,
semanticbook, S = {X n(1), X n(2), · · · , X n(M ′)}. This semanticbook can be partitioned

into synonymous sequences subsets Sr.

(3) An encoding function ϕ : X n → ˆX n generates the set of codewords, namely, codebook,

C = { ˆX n(1), ˆX n(2), · · · , ˆX n(M )}. Due to the synonymous mapping f n

ˆx , this codebook can

be partitioned into synonymous codeword subsets Cs.

(4) An decoding function ψ : ˆX n → ˆX n outputs the decision syntactic codeword ˆX n.

(5) After de-mapping, gn

ˆx ( ˆX n) = ˆis, the estimated semantic index is obtained. Note that both ψ

and gn

ˆx are deterministic.

According to the synonymous mappings f n

x and f n
ˆx , Sr is an equivalence class consisting of the
synonymous sequences and Cs is an equivalence class consisting of the synonymous codewords.

So we can construct the quotient sets S/f n

x = {Sr} with |S/f n

x | = Mr and C/f n

|C/f n

ˆx | = Ms. Let R = 1

n log2 Ms denote the semantic rate of source coding and R0 = 1

ˆx = {Cs} with
n log2 Mr

EncoderDecoderˆnxgsiˆsinxfj()()ˆ,21,snRRnjXj+()()0,21,rRRnnXii+72

denote the semantic rate of source sequences. We configure each synonymous set with the same
number of sequences or codewords, that is, |Sr| = M ′
Mr

= 2nRr and |Cs| = M
Ms

= 2nRs.

We now give the formal description of semantic rate-distortion coding theorem.

Theorem 24. (Semantic Rate-Distortion Coding Theorem):

Given an i.i.d. syntactic source X ∼ p(x) with the associated semantic source ˜X under the
synonymous mapping f and the bounded semantic distortion function ds(˜x, ˆ˜x), for each code
rate R > Rs(D), there exists a sequence of (cid:0)2n(R+Rs), n(cid:1) codes, when code length n tends to
sufficiently large, the semantic distortion satisfies Eds( ˜X, ˆ˜X) < D.

On the contrary, if R < Rs(D), then for any (cid:0)2n(R+Rs), n(cid:1) code, the semantic distortion meets

Eds( ˜X, ˆ˜X) > D with sufficiently large n.

Proof: We first prove the achievability part of the theorem and the converse will be left in

the next part.

Given the desired distortion D and the conditional probability distribution p(ˆx|x), set the
x p(x)p(ˆx|x). A (cid:0)2n(R0+Rr), n(cid:1) semantic-
rate-distortion function as Rs( D
book S = (cid:8)X n(1), X n(2), · · · , X n(2n(R0+Rr))(cid:9) can be generated randomly according to the

1+ϵ ) and let p(ˆx) = (cid:80)

distribution

p(xn) =

n
(cid:89)

k=1

p(xk).

(172)

This set consists of the syntactic sequences to represent the semantic source sequence. Further-

more, these sequences can be uniformly divided into 2nR0 groups according to the synonymous

mapping f n

x (˜xn) = (cid:81)n

k=1 X˜xk. Let Sr(ir) ⊂ B(n)

ϵ

(X n) denote the ir-th synonymous sequence

set.

Similarly, a (cid:0)2n(R+Rs), n(cid:1) code C =

(cid:111)
(cid:110) ˆX n(1), · · · , ˆX n(2n(R+Rs))

can be generated randomly

according to the distribution

p(ˆxn) =

n
(cid:89)

k=1

p(ˆxk) =

n
(cid:89)

(cid:88)

k=1

xk

p(xk)p(ˆxk|xk).

(173)

Correspondingly, these codewords can be uniformly divided into 2nR groups according to the
( ˆX n) denote the is-th synonymous
codeword set. Thus the semanticbook S and the codebook C are produced and shared in the

synonymous mapping f n

. Let Cs(is) ⊂ B(n)

ˆx (ˆ˜xn) = (cid:81)n

ˆXˆ˜xk

k=1

ϵ

encoder and the decoder.

Similar to the idea in [5], we also use jointly typical encoding for the semantic lossy source

73

reconstruction sequence

code. Given a semantic index wr, base on the synonymous mapping f n
x , we determine a syntactic
source sequence xn. Furthermore, find an index ws such that (˜xn, ˆ˜X n(ws)) ∈ ˜A(n)
. Equivalently,
we can select one codeword ˆX n(ws, l) in a synonymous set Cs(ws) to present the semantic
ˆ˜X n(ws) so as to satisfy (xn, ˆX n(ws, l)) ∈ A(n)
one semantic index, choose the smallest one. If no such semantic index exists, let ws = 1. After
sending (ws, l) to the decoder, the decoder produces the reconstruction sequence ˆxn = ˆX n(ws, l).
Then under the de-synonymous mapping gn

ˆx , we obtain the estimated semantic index ˆwr.

. If there is more than

ϵ

ϵ

We now analyze the expected distortion by using semantic coding. Let Ws denote the semantic

index chosen by the encoder. We can bound the semantic distortion averaged over the random

choice of the semanticbook S and the codebook C. The encoding error events can be expressed

as

E =

(cid:110)(cid:16)

(cid:17)
X n, ˆX n(Ws, l)

/∈ A(n)
ϵ

(cid:111)
, ˆX n(Ws, l) ∈ Cs(Ws)

.

This error event can be divided into two types of events, that is, E = E1

(cid:83) E2, where

E1 = (cid:8)X n /∈ A(n)

ϵ (X n)(cid:9)

and

E2 =

(cid:110)
X n ∈ Sr(wr), ˆX n(ws, l) ∈ Cs(ws),

(cid:16)

(cid:17)
X n, ˆX n(ws, l)

ϵ (X n, ˆX n),
for all ws ∈ (cid:8)1, 2, · · · , 2nR(cid:9) , l ∈ (cid:8)1, 2, · · · , 2nRs(cid:9)(cid:9) .

/∈ A(n)

(174)

(175)

(176)

So the error probability can be upper bounded by P (n)

e = Pr(E) ≤ P (E1) + P (E2). For the

first term, by the weak law of large numbers, the probability tends to zero as n → ∞. For the
( ˆX n), we derive the probability
second term, recall that Sr(wr) ⊂ B(n)

(X n) and Cs(ws) ⊂ B(n)

ϵ

ϵ

shown as follows
(cid:88)

P (E2) =

p(xn)

ϵ

xn∈A(n)
(cid:16)

· P

X n ∈ Sr(wr), ˆX n(ws, l) ∈ Cs(ws), (X n, ˆX n(ws, l)) /∈ A(n)

ϵ

74

, ∀ws ∈ Is|X n = xn(cid:17)

=

=

(cid:88)

p(xn)

xn∈A(n)

ϵ

2nR
(cid:89)

(cid:16)

P

ws=1

X n ∈ Sr(wr), ˆX n(ws, l) ∈ Cs(ws), (X n, ˆX n(ws, l)) /∈ A(n)

ϵ

(cid:17)

(cid:88)

p(xn)

(cid:16)

(cid:104)
P

xn∈A(n)

ϵ

X n ∈ Sr(wr), ˆX n(1, l) ∈ Cs(1), (X n, ˆX n(1, l)) /∈ A(n)

ϵ

(cid:17)(cid:105)2nR

.

(177)

k=1 p(ˆxk), by Theorem 23, for sufficiently large n, we have

Since xn ∈ A(n)

ϵ

and ˆX n(1, l) ∼ (cid:81)n
(cid:16)

P

X n ∈ Sr(wr), ˆX n(1, l) ∈ Cs(1), (X n, ˆX n(1, l)) ∈ A(n)

ϵ

≥ (1 − ϵ)2−n(Is( ˜X; ˆ˜X)+3ϵ).

Furthermore, since (1 − x)m ≤ e−mx for x ∈ [0, 1] and m ≥ 0, we have

P (E2) ≤

(cid:16)

1 − (1 − ϵ)2−n(Is( ˜X; ˆ˜X)+3ϵ)(cid:17)2nR

≤ exp

(cid:16)

= exp

(cid:16)

−2nR(1 − ϵ)2−n(Is( ˜X; ˆ˜X)+3ϵ)(cid:17)
−(1 − ϵ)2n(R−Is( ˜X; ˆ˜X)−3ϵ)(cid:17)

,

(cid:17)

(178)

(179)

which tends to zero as n → ∞ and ϵ → 0 if R > Is( ˜X; ˆ˜X). Consequently, the error probability
P (n)
e → 0.

Hence, we can drive the expectation of semantic distortion as follows
(cid:105)
(cid:104)
ds( ˜X n, ˆ˜X n(Ws))|E
(cid:104)
ds( ˜X n, ˆ˜X n(Ws))|E c(cid:105)

(cid:105)
ds( ˜X n, ˆ˜X n(Ws))

+ P (E c)E

= P (E)E

E

(cid:104)

(180)

≤ P (n)

e dmax

+ (1 − P (n)

e

)(1 + ϵ)E(ds( ˜X, ˆ˜X)),

where dmax is the maximum semantic distortion.
By the assumption that E(ds( ˜X, ˆ˜X)) ≤ D

1+ϵ , we have E

ciently large n if R > Is( ˜X; ˆ˜X) + 3ϵ.

(cid:104)

(cid:105)
ds( ˜X n, ˆ˜X n(Ws))

≤ D for suffi-

Furthermore, by Theorem 14, we have 1 ≤ 2nRr ≤ 2n(H(X)−Hs( ˜X)) and 1 ≤ 2nRs ≤ 2n(H( ˆX)−Hs( ˆ˜X)).

Hence, it follows that Is( ˜X; ˆ˜X) ≤ R′ = R + Rr + Rs ≤ Is( ˜X; ˆ˜X) + H(X) − Hs( ˜X) + H( ˆX) −

75

Hs( ˆ˜X) = I(X; ˆX). Thus we can conclude that if Rr = Rs = 0 with sufficiently large n, the
compression rate R′ → Rs(D). On the other hand, if Rr and Rs gradually increase, then the
□
compression rate R′ → R(D). This complete the proof of achievability.

Next, we prove the converse of the theorem. In order to prove the converse, we first illustrate

the relationship between the sequential syntactic mutual information and the sequential semantic

mutual information.

Lemma 10. Assume X n is the syntactic sequence of discrete memoryless source and the recon-
struction semantic sequence is ˆ˜X n, we have

I(X n; ˆ˜X n) ≥ Is( ˜X n; ˆ˜X n), for all p(xn).

(181)

Proof: Due to the definitions of discrete memoryless source, we can write the sequential

mutual information as

I(X n; ˆ˜X n) − Is( ˜X n; ˆ˜X n)
= H(X n) + H( ˆ˜X n) − H(X n, ˆ˜X n)

(cid:104)
Hs( ˜X n) + Hs( ˆ˜X n) − H(X n; ˆX n)

(cid:105)

−

(182)

=

n
(cid:88)

k=1

(cid:104)

(cid:105)
H(Xk) − Hs( ˜Xk) + H(Xk, ˆXk) − H(Xk, ˆ˜Xk)

≥ 0.

Due to H( ˆ˜Xn) = Hs( ˆ˜Xn), H(Xk) ≥ Hs( ˜Xk), and H(Xk, ˆXk) ≥ H(Xk, ˆ˜Xk) (Theorem 2), we
□
prove the lemma.

Proof: (Converse to Theorem 24, (Semantic Rate Distortion Coding Theorem)):
(cid:105)
ds( ˜X n, ˆ˜X n)

In order to prove limn→∞ E

≤ D for any sequence of (2n(R+Rs), n) codes, we

(cid:104)

must have R ≥ Rs(D). So we consider the following inequality,

nR ≥ H(Ws) ≥ I(Ws; X n)
≥ I( ˆ˜X n(Ws); X n)

(a)

≥ Is( ˜X n; ˆ˜X n)

(b)
=

(c)
≥

n
(cid:88)

k=1
n
(cid:88)

k=1

Is( ˜Xk; ˆ˜Xk)

(cid:104)

(cid:16)

E

Rs

ds( ˜Xk; ˆ˜Xk)

(cid:105)(cid:17)

(d)
≥ nRs

(cid:104)

(cid:16)

E

ds( ˜X n; ˆ˜X n)

(cid:105)(cid:17)

,

76

(183)

where (a) follows by Lemma 10, (b) follows from the property of discrete memoryless source,

(c) follows by the definition of Rs(D), and (d) follows by the convexity of Rs(D). So we
conclude that Rs(D) ≤ R for sufficiently large n and complete the proof of the converse. □

Remark 12. For the conventional lossy source coding, we can use quantization, linear prediction,

and transform coding to approach the rate-distortion function. By now, many efficient methods

based on deep learning are applied in lossy source compression, such as convolutional neural

network, transformer based network and so on. Heuristically, these new coding methods suffi-

ciently utilize the semantic information of source and demonstrate better performance than the

conventional ones. However, there is not a mature theoretic framework to design and optimize

these deep learning based source coding methods. Semantic rate distortion may reveal some

insights for future lossy source coding. By integrating the synonymous set into the traditional

lossy source coding or neural network model, we believe that the semantic lossy source coding

will provide a new solution for source compression in speech, image, and video.

IX. SEMANTIC INFORMATION MEASURE OF CONTINUOUS MESSAGE

In this section, we extend the semantic information measures, such as semantic entropy,

semantic mutual information to the continuous message. First we give the definitions of semantic

entropy and mutual information in the continuous case. Then we investigate the capacity of

Gaussian channel in the semantic sense and obtain the semantic channel capacity formula of

band-limited Gaussian channel. Finally, we derive the semantic rate-distortion function for the

Gaussian source.

77

A. Semantic Entropy and Semantic Mutual Information for Continuous Message

In order to indicate the semantic entropy in the continuous case, we first define the synonymous

mapping for the continuous variable as following.

Definition 26. Given a continuous random variable U with a probability density distribution
p(u), u ∈ Ω, the associated discrete semantic variable is ˜U , the synonymous mapping between
˜U and U is defined as

f : ˜U → Ω,

(184)

is=1 is the semantic alphabet and Ω = (cid:83) ˜N
˜N

where ˜U = {˜uis}
random variable with ∀is ̸= js, Ωis

is=1 Ωis is the support set of the
(cid:84) Ωjs = ∅. Specifically, for any ˜uis ∈ ˜U, it can be mapped
into a subset Ωis ⊂ Ω. Hence under the mapping f , the support set is partitioned into a series of

synonymous intervals and |Ωis| = Lis is named as the synonymous length of the is-th interval.

Definition 27. Given a continuous random variable U with a probability density function p(u),
u ∈ Ω, under a synonymous mapping f , for the associated semantic variable ˜U , the semantic

entropy is defined as

Hs( ˜U ) = −

(cid:90)

Ω

p(u) log p(u)du − E(log L),

(185)

where E(log L) = (cid:80)∞

is=−∞

(cid:82)
Ωis

interval length.

p(u)du log Lis is the expectation of logarithm of synonymous

We now illustrate the relationship between the continuous semantic entropy and the discrete

counterpart. Let L[−∞:is] = (cid:80)is
js=−∞ Ljs. Suppose the range of U is divided into synonymous in-
tervals Ωis = (cid:2)L[−∞:is−1], L[−∞:is]
(cid:3) of length Lis under the synonymous mapping f . Furthermore,
each interval is divided into bins of length ∆ so that Lis = Jis∆. Assume the density function
is continuous in bins, by the integration mean value theorem, there is a value uJ[−∞:is−1]+j such
that

p(uJ[−∞:is−1]+j)∆ =

p(u)du.

(186)

(cid:90) L[−∞:is]+(j+1)∆

L[−∞:is]+j∆

Considering the synonymous mapping f , we can write

Jis −1
(cid:88)

j=0

p(uJ[−∞:is−1]+j)∆ = p(˜uis)Jis∆.

(187)

78

So the semantic entropy of the quantized version is
∞
(cid:88)

Hs( ˜U ∆) = −

p(˜uis)Jis∆ log [p(˜uis)Jis∆]

is=−∞

∞
(cid:88)

Jis −1
(cid:88)

is=−∞

j=0

= −

p(uJ[−∞:is−1]+j)∆ log

(cid:34)Jis −1
(cid:88)

j=0

(cid:35)

p(uJ[−∞:is−1]+j)∆

(188)

⇒
∆→0

−

∞
(cid:88)

(cid:90) L[−∞:is]

(cid:34)(cid:90) L[−∞:is]

(cid:35)

p(v)dv

du,

p(u) log

is=−∞

L[−∞:is−1]

L[−∞:is−1]

where the last equality is from the Riemann integrability.

By using the integration mean value theorem, there is a value ζ in the interval (cid:2)L[−∞:is−1], L[−∞:is]

(cid:3)

such that (cid:82) L[−∞:is]
L[−∞:is−1]

p(v)dv = Lisp(ζ). So the quantized semantic entropy can be further approx-

imated as

∞
(cid:88)

(cid:90) L[−∞:is]

−

p(u) log

(cid:34)(cid:90) L[−∞:is]

(cid:35)

p(v)dv

du

is=−∞
∞
(cid:88)

= −

L[−∞:is−1]
(cid:90) L[−∞:is]

L[−∞:is−1]

p(u) log [Lisp(ζ)] du

is=−∞
∞
(cid:88)

is=−∞
(cid:90) ∞

−∞

≈ −

= −

L[−∞:is−1]

(cid:90)

Ωis

p(u) log [Lisp(u)] du

(189)

p(u) log p(u)du −

(cid:34)(cid:90)

∞
(cid:88)

is=−∞

Ωis

(cid:35)

p(u)du

log Lis = Hs( ˜U ).

Although Eq. (185) is an approximation form, it has a concise expression and clear physical

meaning. So we use this formula to present the semantic entropy in the continuous case.

Corollary 5. Given a continuous random variable U with a probability density function p(u)
and the associated semantic variable ˜U under a synonymous mapping f , the semantic entropy

is lower bounded by

Hs( ˜U ) ≥ −

(cid:90)

Ω

p(u) log p(u)du − log S,

(190)

where S = E(L) is the average length of synonymous interval. The equality holds when the

optimal mapping f is a proportional partition based on the probability of synonymous interval.

Proof: The semantic entropy can be written as
(cid:34)(cid:90)

(cid:90)

Hs( ˜U ) = −

p(u) log p(u)du −

˜N
(cid:88)

Ω

is=1

Ωis

(cid:35)

p(u)du

log

Lis
|Ω|

− log |Ω|.

(191)

(cid:104)(cid:82)

Let

Ωis

(cid:105)
p(u)du

= ps,is and qs,is = Lis

|Ω| . Since D(ps||qs) ≥ 0, we have

(cid:34)(cid:90)

˜N
(cid:88)

−

is=1

Ωis

(cid:35)

p(u)du

log

Lis
|Ω|

≥ −

(cid:34)(cid:90)

˜N
(cid:88)

is=1

Ωis

(cid:35)

p(u)du

log

(cid:34)(cid:90)

Ωis

(cid:35)

p(u)du

.

(192)

79

The equality holds when the following condition is satisfied
(cid:90)

p(u)du =

, is = 1, 2, · · · , ˜N .

L∗
is
|Ω|

Ωis

(193)

This condition means that the synonymous length L∗
probability of synonymous interval (cid:82)

p(u)du for the optimal mapping f .

Ωis

is of interval Ωis is proportional to the

Furthermore, by using Jensen’s inequality, we can derive that

Hs( ˜U ) ≥ −

≥ −

= −

(cid:90)

Ω

(cid:90)

Ω

(cid:90)

Ω

p(u) log p(u)du −

(cid:34)(cid:90)

˜N
(cid:88)

(cid:35)

p(u)du

log L∗
is

is=1

Ωis
(cid:34)(cid:90)

˜N
(cid:88)

p(u) log p(u)du − log

is=1

Ωis

p(u) log p(u)du − log S,

(cid:35)

p(u)du

L∗
is

where S is the average length of synonymous interval, which is defined as

S = E(L) =

(cid:34)(cid:90)

˜N
(cid:88)

is=1

Ωis

(cid:35)

p(u)du

L∗
is.

So we complete the proof.

(194)

(195)

□

In fact, under the synonymous mapping, if S → 0, Hs( ˜U ) = − (cid:82) ∞

−∞ p(u) log p(u)du−log S →
∞, the first term is the differential entropy of U , that is, h(U ) = − (cid:82) ∞
−∞ p(u) log p(u)du.
Specifically, if S = 1, then Hs( ˜U ) = h(U ), that is, the semantic entropy is equal to the
differential entropy. On the other hand, if S → ∞, then Hs( ˜U ) → −∞. That means if the

synonymous length goes sufficiently large we can obtain no extra semantic information.

Remark 13. The average synonymous length S indicates the identification ability of information.
If S = 1, the semantic variable ˜U obtains the same identification result as the random variable

U . On the other hand, if S > 1, the former loses some identification ability and attains a smaller

semantic entropy than the latter.

We now give the definition of joint/conditional synonymous mapping as following.

Definition 28. Given a continuous random variable pair (U, V ) with a probability density
distribution p(u, v), u ∈ Ωu, v ∈ Ωv, the associated discrete semantic variable pair is ( ˜U , ˜V ),
the joint synonymous mapping is defined as

fuv : ˜U × ˜V → Ωu × Ωv,

(196)

80

is,js=1 is the semantic alphabet and ∀is, js, (cid:12)
˜Nu, ˜Nv
where ( ˜U, ˜V) = {(˜uis, ˜vjs)}
Lis and Ljs are the synonymous lengths. Similarly, the conditional synonymous mapping is

(cid:12)
(cid:12) = LisLjs. Here

(cid:12)Ω(is,js)

defined as

where for all js, |Ωjs|U | = Ljs.

fv|u : ˜V|U → Ωv|U,

(197)

Thus we give the definitions of semantic conditional entropy and semantic joint entropy as

following.

Definition 29. Given a pair of semantic variables ( ˜U , ˜V ) and the associated continuous random

variable pairs (U, V ) with a joint density function p(u, v), under a joint mapping fuv, the
semantic joint entropy Hs( ˜U , ˜V ) is defined as

Hs( ˜U , ˜V ) = −

(cid:90)

(cid:90)

Ωv

Ωu

p(u, v) log p(u, v)dudv − E [log(LuLv)] ,

(198)

where Lu and Lv are the synonymous lengths of random variables U and V respectively.

Correspondingly, under an conditional mapping fv|u, the semantic conditional entropy Hs( ˜V |U )

is defined as

Hs( ˜V |U ) = −

(cid:90)

(cid:90)

Ωv

Ωu

p(u, v) log p(v|u)dudv − E(log Lv),

(199)

where Lv is the synonymous length of random variable V .

Example 6. Consider a random variable U with the uniform distribution p(u) = 1
b−a, u ∈ [a, b].
Assume the synonymous mapping f evenly partitions the interval [a, b] into ˜N parts, so the
synonymous length is S = b−a
˜N
(cid:90) b

. Hence the semantic entropy of the associated variable ˜U is

Hs( ˜U ) = −

1
b − a

log

1
b − a

a

b − a
˜N

du − log

= log ˜N sebits.

(200)

Example 7. Let U denote the Gaussian random variable with the density function p(u) =
2πσ2 e− u2

2σ2 . Under a proportional-partition mapping f with the synonymous length S, the

1√

semantic entropy is written as

Hs( ˜U ) = E[− log p(U )] − log S

√

(cid:18)

log

= E

2πσ2 + (log e)

(cid:19)

U 2
2σ2

− log S

=

=

1
2
1
2

log 2πσ2 + (log e)

log

2πeσ2
S2

sebits.

EU 2
2σ2 − log S

81

(201)

We now give the definition of up/down semantic mutual information as follows.

Definition 30. Given a pair of semantic variables ( ˜U , ˜V ) and the associated continuous random

variable pair (U, V ) with a joint density function p(u, v), under a joint mapping fuv, the up
semantic mutual information I s( ˜U ; ˜V ) is defined as

I s( ˜U ; ˜V ) = H(U ) + H(V ) − Hs( ˜U , ˜V )
(cid:90)

(cid:90)

= −

p(u, v) log

p(u)p(v)
p(u, v)

Ωv

Ωu

dudv + E [log(LuLv)] ,

(202)

where Lu and Lv is the synonymous length of random variable U and V . Similarly, the down
semantic mutual information Is( ˜U ; ˜V ) is defined as

Is( ˜U ; ˜V ) = Hs( ˜U ) + Hs( ˜V ) − H(U, V )
(cid:90)

(cid:90)

= −

p(u, v) log

p(u)p(v)
p(u, v)

Ωv

Ωu

dudv − E [log(LuLv)] .

(203)

Clearly, in (202) and (203), the first term is the classic mutual information. The main difference

between semantic and syntactic mutual information is the logarithmic production of synonymous
lengths. If E [log(LuLv)] ≥ 0, we have

Is( ˜U ; ˜V ) ≤ I(U ; V ) ≤ I s( ˜U ; ˜V ).

(204)

Similar to the discrete case, the down semantic mutual information may be negative. Consider
the practical condition, we can set (Is( ˜U ; ˜V ))+.

Analog to the typical set for the discrete case, asymptotic equipartition property also holds for

the continuous case and we can introduce the typical set for this case. Conceptually, the volume
of continuous typical set for the semantic variable can be approximated as 2nHs( ˜U ) = 2nH(U )

. So

Sn

the synonymous length S can be interpreted as the reduced proportion in each side length. Similar

interpretation can be applied for the semantic conditional/joint entropy and mutual information.

82

Due to the page length limitation, we will not discuss the details of the continuous typical set

in the semantic sense.

Remark 14. In the algorithm of signal detection and estimation, we often make a decision based

on observations in an interval. This process can be modeled as a synonymous mapping. Thus, we

can handle some problems in the radar signal detection, hypothesis testing, integrated sensing

and communication from the viewpoint of semantic information processing. Therefore, semantic

information theory may provide a theoretical explanation and establish the fundamental limits

for these signal processing problems.

B. Semantic Capacity of Gaussian Channel

Consider a Gaussian channel model, the received signal yk at time k can be written as

yk = xk + zk,

(205)

where zk ∼ N (0, σ2) is the noise sample drawn i.i.d. from a Gaussian distribution with variance
σ2. Under a joint synonymous mapping fxy, the associated semantic variables are ˜Y , ˜X and ˜Z
respectively. Given the average power constraint EX 2 ≤ P , by using Jensen’s inequality, the

semantic capacity of the Gaussian channel can be derived as

Cs = max
fxy

max
{p(x):EX 2≤P }

I s( ˜X; ˜Y )

= max
p(x);fxy

H(X) + H(Y ) − Hs( ˜X, ˜Y )

= max
p(x)

H(X) + H(Y ) − H(X, Y ) + max
fxy

E [log(LxLy)]

(206)

=

=

1
2
1
2

(cid:18)

log

1 +

(cid:18)

log

1 +

(cid:19)

(cid:19)

P
σ2
P
σ2

+ log [E(Lx)E(Ly)]

+ log(S2),

where we assume Sx = E(Lx) = Sy = E(Ly) = S and S ≥ 1.

Remark 15. Like the classic information theory, the semantic capacity of Gaussian channel is

achieved when X ∼ N (0, P ) and the synonymous mapping fxy is a proportional-partition map-

ping. Specifically speaking, if the transmitted variable X (the received signal Y ) is partitioned

based on an equiprobability mapping, the semantic capacity may be achieved.

Furthermore, we can also obtain a lower bound of the semantic capacity, that is

Cs =

(cid:33)

(cid:32)

1
2

log

P + σ2
σ2
S4

≥

=

1
2

1
2

(cid:32)

log

(cid:33)

P + σ2
S4
σ2
S4

log

(cid:19)

(cid:18)
1 + S4 P
σ2

= C s.

83

(207)

Theorem 25. Given a Gaussian channel with power constraint P , noise variance σ2 and average

synonymous length S ≥ 1, the achievable semantic rate is

1
2
Similarly, the lower bound of semantic capacity is

Cs =

P
σ2

1 +

log

(cid:18)

(cid:19)

+ log(S2) sebits per transmission.

C s =

(cid:18)

1
2

log

1 + S4 P
σ2

(cid:19)

sebits per transmission.

(208)

(209)

By using random coding, synonymous mapping, and joint typicality decoding in the continuous

case, we can prove this theorem. The details is omitted due to the limitation of page length.

Remark 16. We now give a geometric interpretation for this theorem as shown in Fig. 13.

Given signal power P and noise variance σ2, and a codeword of length n, for the classic

channel coding, the transmitted codeword is normally scattered in a sphere of radius

nP and

nσ2 (labeled by red
the decoding region of each codeword is confined to a sphere of radius
circle) with high probability. Since the energy of received vectors is no more than (cid:112)n(P + σ2),
2 with the radius (cid:112)n(P + σ2). Therefore,
the volume of received vector sphere is An(n(P +σ2)) n
in order to avoid the intersection of the decoding region, the maximum number of decoding

√

√

spheres in this volume is limited as

An(n(P + σ2)) n
An(nσ2) n

2

2

= 2

n

2 log(1+ P

σ2 ).

(210)

In the syntactic sense, such a maximum number of codewords with no error probability decoding

cannot be surpassed.

On the contrary, in the semantic sense, thanks to the synonymous mapping, the radius of a
decoding sphere is further reduced to (cid:112)nσ2/S4. Due to the radius decreasing, the semantic
decoding sphere (labeled by black circle) has a smaller volume An(nσ2/S4) n

2 . Hence, the

maximum number of semantic decoding spheres in received vector volume is limited as

An(n(P + σ2)) n
An(nσ2/S4) n

2

2

= 2

n

2 log(S4(1+ P

σ2 )).

(211)

84

Fig. 13. Semantic Sphere packing for the Gaussian channel.

Furthermore, if we consider a conservative estimate for the received vector volume, we can
limit the energy of the received vector as (cid:112)n(P + σ2/S4). So the number of semantic decoding

spheres is bounded by

An(n(P + σ2/S4)) n
An(nσ2/S4) n

2

2

= 2

n

2 log(1+S4 P

σ2 ).

(212)

Compare with the classic coding in Gaussian channel, when the synonymous length is equal

to one, the semantic capacity is the same as the classic capacity. However, with the growth

of synonymous length, we find that semantic coding based on the synonymous mapping can

further reduce the uncertainty range of decoding region so that the volume of decoding sphere

can be decreased. Therefore, semantic channel coding can pack larger number of codewords in

the same volume of received vector sphere than the traditional method. In this sense, the capacity

of Gaussian channel can be further improved by using semantic coding.

C. Semantic Capacity of Band-limited Gaussian Channel

We now investigate the semantic capacity of band-limited Gaussian channel. Given a Gaussian

noise channel with limited bandwidth B and two-sided power spectrum N0/2, we transmit signals

on this channel with a limited time interval [0, T ] and limited power P . So Shannon’s channel

capacity formula is written as

(cid:18)

C = B log

1 +

(cid:19)

P
N0B

bits per second.

(213)

nP()2nP+2n24/nS()24/nPS+Theorem 26. If a signal S(t) in a time interval [0, T ] with the power constraint P is transmitted

on the Gaussian noise channel with limited bandwidth B, under a synonymous mapping f with

the average synonymous length S ≥ 1, the semantic channel capacity can be written by

85

Cs = B log

(cid:20)
S4

(cid:18)

(cid:18)

1 +

(cid:19)(cid:21)

P
N0B
(cid:19)

P
N0B

= B log

1 +

+ 4B log S sebits per second.

Correspondingly, the lower bound of semantic capacity can be written by
(cid:18)
1 + S4 P
N0B

sebits per second.

C s = B log

(cid:19)

(214)

(215)

Proof: By using the Shannon-Nyquist sampling theorem, the signal S(t) is decomposed
(cid:17)(cid:105)

into a series of i.i.d. samples. Hence the semantic capacity per sample is 1

S4 (cid:16)
(cid:104)

.

2 log

1 + P
N0B

Since there are 2B samples each second, the semantic capacity of the channel can be rewritten

as Cs = B log

(cid:104)

S4 (cid:16)

1 + P
N0B

(cid:17)(cid:105)

. A similar method can also be applied to derive the lower bound

of semantic capacity. So we complete the proof.

□

Figure 14 depicts the comparison of semantic and classic capacity of band-limited Gaussian

channel at various bit signal-to-noise ratios (Eb/N0). For the semantic cases, we draw the

semantic capacity and the lower bound for different synonymous lengths, such as S = 8, S = 4,

and S = 2. We can see that the semantic capacity of Gaussian channel is significantly larger

than the classic capacity due to using the synonymous mapping and semantic coding. With

the increasing of average synonymous length, the improvement of capacity will become more

noticeable.

If the bandwidth tend to infinity, for the lower bound of semantic channel capacity, we obtain

the limitation as following,

lim
B→∞

C s =

S4
ln 2

·

P
N0

≈ 1.44S4 P
N0

.

(216)

On the other hand, let η = Cs

B be the spectrum efficiency. When η → 0. we obtain the limitation

of Eb/N0 as

ln 2
S4 ≈
Next, we explore the minimum energy per sebit. Let P = ER be the signal power and µ = R
B

0.693
S4

Eb
N0

lim
η→0

(217)

=

.

be the spectrum efficiency. Since R ≤ B log(1 + S4 ER

N0B ), we can derive the minimum energy

86

Fig. 14. Semantic and classic capacity of Gaussian channel.

needed for semantic communication at spectrum efficiency µ, that is,

E(µ) =

1
S4µ

(2µ − 1).

(218)

Here we assume the Gaussian noise power N0 = 1. Figure 15 shows the minimum energy

needed for semantic and classic communication under various spectrum efficiency with the

average synonymous lengths S = 2 and S = 4. Compared with the classic communication, we

can observe that the minimum energy of semantic communication is dramatically reduced. So

it follows that semantic communication may be an important method to implement the green

communication.

All these theoretic results show an extraordinary advantage of semantic coding over the classic

coding. They reveal the tremendous potential of semantic channel coding in the communication

Semantic CapacityLB of Semantic CapacityClassic Capacity8S=4S=2S=0/ (dB)bEN87

Fig. 15. Minimum energy versus spectrum efficiency of semantic and classic communication.

application.

D. Semantic Rate Distortion of Gaussian Source

As a dual problem, we now consider the semantic rate-distortion of Gaussian source and have

the following theorem.

Theorem 27. Given a Gaussian source X ∼ N (0, P ) and the reconstruction signal ˆX, under
the synonymous mapping fx and fˆx, the associated semantic variables are ˜X and ˆ˜X respectively,
with the mean squared error (MSE) distortion E[( ˜X − ˆ˜X)2] ≤ D, the signal model can be written
as

˜X = ˆ˜X + Z,

(219)

where Z is the noise sample drawn i.i.d. from a Gaussian distribution with variance D. So the

0246810Spectrum efficiency (bit/s/Hz)10-310-210-1100101102103Energy (J)Classic com.Semantic com. S=2Semantic com. S=4semantic rate-distortion of the Gaussian source is
(cid:18) P

Rs(D) =

log

S4D




1
2


0,

88

(220)

(cid:19)

, 0 ≤ D ≤

D >

P
S4 ,
P
S4 .

where S is the average synonymous length.

Proof: For the Gaussian source X ∼ N (0, P ), by using Corollary 5, we can write the down

semantic mutual information as

Is( ˜X; ˆ˜X) = Hs( ˜X) + Hs( ˆ˜X) − H(X, ˆX)

= h(X) − E log(Lx) + h( ˆX) − E log(Lˆx) − H(X, ˆX)

≥ I(X; ˆX) − log E(Lx) − log E(Lˆx)

≥

=

=

1
2
1
2
1
2

log

log

log

(cid:19)

(cid:19)

(cid:18) P
D
(cid:18) P
D
(cid:18) P

S4D

− log(SxSy)

− log(S2)

(cid:19)

,

(221)

where we assume Sx = E(Lx) = Sˆx = E(Lˆx) = S and S ≥ 1. Like the classic information

theory, this semantic rate-distortion function is achieved when Z ∼ N (0, D) and fx (fˆx) is
an equiprobability partition synonymous mapping. If 0 ≤ D ≤ P/S4, Rs(D) ≥ 0, otherwise
□

Rs(D) = 0.

We now give a geometric interpretation for this theorem as shown in Fig. 16. For the classic

lossy source code, we should use a group of encoding spheres of radius

nD to cover the

√

source volume of radius
is 2nR(D) = (cid:0) P

(cid:1)n/2

D

nP . So the minimum number of the source codewords required

. On the contrary, due to the synonymous mappings for the source and

reconstruction sequence, the equivalent volume of source space can be reduced to a sphere of
radius (cid:112)nP/S4 so that the minimum number of codewords is 2nRs(D) =

(cid:17)n/2

.

(cid:16) P/S4
D

Figure 17 shows the semantic and syntactic rate distortion function of a Gaussian source. Here,

the source signal power is P = 1 and the synonymous lengths are set to S = 1.5 and S = 2

respectively. We observe that the semantic rate distortion functions dramatically decrease with

the growth of synonymous length and become significantly lower than the classic counterparts.

√

89

Fig. 16. Semantic Sphere covering for the Gaussian source.

These results manifest that semantic lossy source coding has enormous potential for the source

compression in the future.

X. SEMANTIC JOINT SOURCE CHANNEL CODING

In this section, we consider the semantic joint source channel coding. Similar to the classic

information theory, we can tie together two basic methods of semantic communication: semantic

source coding and semantic channel coding. Figure 18 gives the system model of semantic joint
source channel coding. Let ˜U denote the discrete memoryless semantic source with entropy
Hs( ˜U ) and ds(˜u, ˆ˜u) be a semantic measure with rate-distortion function Rs(D). Furthermore, let
(cid:111)
(cid:110) ˜X ( ˜U), X , Y, ˜Y( ˆ˜U), p(Y |X)
X denote the coded symbol after synonymous mapping f and

be a discrete memoryless channel with semantic capacity Cs.

Theorem 28. (Semantic Source Channel Coding Theorem in Lossy case):

Given a discrete memoryless source U ∼ p(u) and a discrete memoryless channel p(y|x),
the source is associated with a semantic variable ˜U under the synonymous mapping f and the

codeword X n is transmitted on the channel. In the case of lossy transmission, if the code rate
satisfies Rs(D) < R < Cs, there exists a sequence of (cid:0)2nR, n(cid:1) semantic source channel codes,
when code length n tends to sufficiently large, then semantic distortion satisfies Eds( ˜X, ˆ˜X) < D.
On the contrary, if Rs(D) > Cs, then for any (cid:0)2nR, n(cid:1) code, with sufficiently large n, the

semantic distortion meets Eds( ˜X, ˆ˜X) > D.

nPnD4/nPS90

Fig. 17. Rate distortion function of Gaussian source.

Fig. 18. Semantic joint source channel coding.

Proof: We use separate semantic lossy source coding and semantic channel coding to prove

the achievability.

(1) Source coding: Give any ϵ > 0, there exists a sequence of semantic lossy source codes
with rate R ≥ Rs(D) + ϵ that satisfies Eds(˜u, ˆ˜u) < D. The index of each code can be regarded

as a semantic message to be sent to the channel.

(2) Channel coding: The source indices can be encoded and reliably transmitted over the

channel if R ≤ Cs − ϵ.

In the channel decoder, when n → ∞, the error probability P (n)

e → 0 so that the source decoder

D()RD(),1.5sRDS=(),2sRDS=()/() ((se)bits per sample)sRDRDEncoderDecoderngChannelnYnX()pyxnfnUˆnUˆnUnU91

can reconstruct sequence and de-mapping the semantic index ˆis and the average distortion meets
Eds( ˜X, ˆ˜X) ≤ D.

Next we prove the converse. By the converse proof of semantic rate distortion coding the-
orem (Theorem 24), we have Rs(D) ≤ Is( ˜U n, ˆ˜U n). Furthermore, by Corollary 3, we have
Is( ˜U n, ˆ˜U n) ≤ I s( ˜U n, ˆ˜U n). Then we can derive I s( ˜U n, ˆ˜U n) ≤ Cs. So we complete the proof of
□
the theorem.

Theorem 29. (Semantic Source Channel Coding Theorem in Lossless case):

Given a discrete memoryless source U ∼ p(u) and a discrete memoryless channel p(y|x),
the source is associated with a semantic variable ˜U under the synonymous mapping f and the

codeword X n is transmitted on the channel. In the case of lossless transmission, for each code
rate Hs( ˜U ) < R < Cs, there exists a sequence of (cid:0)2nR, n(cid:1) semantic source channel codes, when
code length n tends to sufficiently large, the error probability tends to zero.

On the contrary, if Hs( ˜U ) > Cs, then for any (cid:0)2nR, n(cid:1) code, with sufficiently large n, the

error probability cannot achieve arbitrarily low.

The proof of this theorem is similar to Theorem 28 by setting D = 0. In principle, the separate

semantic source and channel coding is asymptotically optimal for lossless or lossy transmission.

Therefore, the fundamental criteria for the semantic communication are summarized as following






Hs( ˜U ) ≤ R ≤ Cs,

for lossless transmission,

Rs(D) ≤ R ≤ Cs,

for lossy transmission.

(222)

On the contrary, for classic communication, the code rate should be confined in H(U ) ≤ R ≤ C

or R(D) ≤ R ≤ C. In these common intervals, both classic communication and semantic

communication can work well. On the other hand, if H(U ) > C or R(D) > C, the classic
communication cannot work yet the semantic communication still works well as long as Hs( ˜U ) ≤
Cs or Rs(D) ≤ Cs. Thus, we conclude that semantic source-channel coding can extend the range

of code rates and provide new insight to improve the performance of the communication system.

XI. CONCLUSIONS

In this paper, we develop an information-theoretic framework of semantic communication.

We start from the synonym, a fundamental property of semantic information, to build the se-

mantic information measures including semantic entropy, up/down semantic mutual information,

92

semantic channel capacity, and semantic rate distortion function. Then we extend the asymptotic

equipartition property to the semantic sense and introduce the synonymous typical set to prove

three significant coding theorems, that is, semantic source coding theorem, semantic channel

coding theorem, and semantic rate distortion coding theorem. Additionally, we investigate the

semantic information measures in the continuous case and derive the semantic capacity of

Gaussian channel and semantic rate distortion of Gaussian source. All these works uncover

the critical features of semantic communication and constitute the theoretic basis of semantic

information theory.

For the theoretic analysis, the semantic information theory needs further development. In

this paper, we only consider the semantic information measure and the fundamental limitation

in the discrete or continuous memoryless case. In the future, we can further investigate the

measure and limitation of semantic information in various memory source or channel cases,

such as stationary and ergodic process (e.g. Markov process) or non-stationary non-ergodic

process. Strong asymptotic equipartition property and strong typicality in the semantic sense

should be further explored. On the other hand, the analysis of semantic capacity or semantic

rate distortion with finite block length may also be an interesting research topic. In addition,

in various multiuser communication scenarios, such as multiple access, broadcasting, relay etc.,

we can further analyze and derive the corresponding measure and performance limit of semantic

information.

Guided by the classic information theory, in the past seventy years, the source coding and

channel coding techniques have approached the theoretic limitation. On the contrary, the se-

mantic information theory paves a new way for the coding techniques. From the viewpoint

of semantic processing, with the help of synonymous mapping, the lossless source coding has

much space to improve and the existing coding methods can be further modified and polished.

The construction of semantic channel codes may be centered on the group Hamming distance

and the optimization of decoding algorithms will be concentrated on the group decoding so

that the information transmission techniques will usher in a new era that surpasses the classic

limitation and approaches the semantic capacity. By the optimization of synonymous mapping,

the classic lossy source coding techniques, such as vector quantization, prediction coding, and

transform coding, will demonstrate new advantages to further improve the compression efficiency.

Briefly, the performance bottleneck of classic communication will be broken and the traditional

93

communication will naturally evolve to the semantic communication.

For the new coding techniques based on deep learning (DL), the semantic information theory

will lift its mystery veil and provide a systematic design and optimization tool. The synonymous

mapping will provide a reasonable explanation for the semantic information extracted by the

deep neural network. The basic structures of mainstream DL models, such as convolutional

neural networks, transformer model, variational auto-encoder and so on, may be analyzed and

optimized based on the semantic information measures. Furthermore, the system architecture of

semantic communication based on deep learning can be simplified or optimized guided by the

semantic information theory.

In summary, the theoretic framework proposed in this paper may help understanding the

essential features of semantic information and shed light on some ambiguity problems in semantic

communication. We believe that the semantic information theory will uncover a new chapter

of information theory and have a profound impact on many fields such as communication,

signal detection and estimation, deep learning and machine learning, and integrated sensing and

communication etc.

A. Proof of Lemma 3

APPENDIX

Proof: Suppose two probability distributions p1(u) and p2(u), for all 0 ≤ θ ≤ 1, we have

pθ(u) = θp1(u) + (1 − θ)p2(u). Using Jensen’s inequality, we can write

θHs(p1(u)) + (1 − θ)Hs(p2(u)) − Hs(pθ(u))

(cid:88)

(cid:88)

= θ

p1(ui) log

is

i∈Nis

(cid:80)

(cid:80)

i∈Nis

i∈Nis

pθ(ui)
p1(ui)

+ (1 − θ)

(cid:88)

(cid:88)

p2(ui) log

(cid:80)

(cid:80)

i∈Nis

i∈Nis

pθ(ui)
p2(ui)

≤ θ log

i∈Nis

is
(cid:88)
(cid:88)

is

i∈Nis

pθ(ui) + (1 − θ) log

(cid:88)

(cid:88)

is

i∈Nis

pθ(ui)

= θ log 1 + (1 − θ) log 1 = 0.

So we prove the concavity of semantic entropy.

(223)

□

B. Proof of Theorem 5

Proof: To prove the first inequality, by using Jensen’s inequality, we have

Ds(θps,1 + (1 − θ)ps,2∥θqs,1 + (1 − θ)qs,2)

− θDs(ps,1∥qs,1) − (1 − θ)Ds(ps,2∥qs,2)

(cid:88)

(cid:88)

= θ

p1(ui)

· log

is
(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

ui∈Uis
(cid:88)

+ (1 − θ)

(cid:88)

p2(ui)

θp1(ui) + (1 − θ)p2(ui)
θq1(ui) + (1 − θ)q2(ui)

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

is

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

ui∈Uis
θp1(ui) + (1 − θ)p2(ui)
θq1(ui) + (1 − θ)q2(ui)

(cid:80)

(cid:80)

ui∈Uis

ui∈Uis

· log

q1(ui)
p1(ui)

q2(ui)
p2(ui)

94

(224)

(cid:88)

(cid:88)

≤ log

is

ui∈Uis

θp1(ui) + (1 − θ)p2(ui) = log 1 = 0.

The other two inequalities can also be proved by using similar methods. So we prove the theorem.

□

C. Proof of Theorem 7

Proof: First, we prove I s( ˜U ; ˜V ) is a concave function of p(u) for fixed p(v|u). Since
I s( ˜U ; ˜V ) = H(U ) + H(V ) − Hs( ˜U , ˜V ), the entropies of H(U ) and H(V ) are concave functions
of p(u) for fixed p(v|u). Furthermore, the semantic joint entropy Hs( ˜U , ˜V ) is also a concave
function of p(u). So we conclude that I s( ˜U ; ˜V ) is also a concave function of p(u).

Second, we prove Is( ˜U ; ˜V ) is a convex function of p(v|u) for fixed p(u). Since Is( ˜U ; ˜V ) =
Ds (p (u, v) ∥ps(u)ps(v)), due to the convexity of semantic relative entropy, we conclude that
Is( ˜U ; ˜V ) is a convex function of p(v|u).

By using the similar methods, we can prove I s( ˜U ; ˜V ) is a convex function of p(v|u) for fixed
□

p(u) and Is( ˜U ; ˜V ) is a concave function of p(u) for fixed p(v|u).

REFERENCES

[1] C. E. Shannon, “A Mathematical Theory of Communication,” The Bell System Technical

Journal, vol. 27, pp. 379-423, 623-656, July, Oct., 1948.

95

[2] C. E. Shannon and W. Weaver, The Mathematical Theory of Communication, The University

of Illinois Press, 1949.

[3] W. Weaver, “Recent Contributions to the Mathematical Theory of Communication,” ETC:

A Review of General Semantics, pp. 261-81, 1953.

[4] T. Cover and J. Thomas, Elements of Information Theory, New York: Wiley, 1991.

[5] A. El Gamal and Y. H. Kim, Network Information Theory, Cambridge: Cambridge

University Press, 2011.

[6] R. Carnap, Y. Bar-Hillel, “An Outline of A Theory of Semantic Information,” RLE Technical

Reports 247, Research Laboratory of Electronics, Massachusetts Institute of Technology,

Cambridge MA,1952.

[7] L. Floridi, “Outline of A Theory of Strongly Semantic Information,” Minds and machines,

vol. 14, no. 2, pp. 197-221, 2004.

[8] N. J. Nilsson, “Probabilistic Logic,” Artificial Intelligence, vol. 28, no. 1, pp. 71-87, 1986.

[9] J. Bao, P. Basu, M. Dean, et al., “Towards A Theory of Semantic Communication,” IEEE

Network Science Workshop, West Point, NY, USA, Jun. 2011.

[10] A. D. Luca, S. Termini, “A Definition of A Non-probabilistic Entropy In The Setting of

Fuzzy Sets,” Information and Control, vol. 20, pp. 301-312, 1972.

[11] A. D. Luca, S. Termini, “Entropy of L-Fuzzy Sets,” Information and Control, vol. 24, pp.

55-73, 1974.

[12] W. Wu, “General Source and General Entropy,” Journal of Beijing University of Posts and

Telecommunications, vol. 5, no. 1, pp. 29-41, 1982.

[13] J. Liu, W. Zhang, and H. V. Poor, “A Rate-Distortion Framework for Characterizing

Semantic Information,” 2021 IEEE International Symposium on Information Theory (ISIT),

Melbourne, Australia,2021.

[14] T. Guo, Y. Wang, et al., “Semantic Compression with Side Information: A Rate-Distortion

Perspective,” arXiv preprint arXiv:2208.06094, 2022.

[15] Y. Shao, Q. Cao, and D. Gunduz, “A Theory of Semantic Communication,” arXiv preprint

arXiv:2212.01485, 2022.

[16] J. Tang, Q. Yang, and Z. Zhang, “Information-Theoretic Limits on Compression of Semantic

Information,” arXiv preprint arXiv:2306.02305, 2023.

[17] P. Zhang, W. Xu, H. Gao, et al., “Toward Wisdom-Evolutionary and Primitive-Concise 6G:

96

A New Paradigm of Semantic Communication Networks,” Engineering, vol. 8, no. 1, pp.

60-73, 2022.

[18] G. Shi, Y. Xiao, Y. Li, et al., “From Semantic Communication to Semantic-aware

Networking: Model, Architecture, and Open Problems,” IEEE Communications Magazine,

vol. 59, no. 8, pp. 44-50, 2021.

[19] Z. Qin, X. Tao, J. Lu, et al., “Semantic Communications: Principles and Challenges,” arXiv

preprint arXiv:2201.01389, 2021.

[20] H. Xie, Z. Qin, X. Tao, et al., “Task-oriented Multi-user Semantic Communications,” IEEE

Journal on Selected Areas in Communications, vol. 40, no. 9, pp. 2584-2597, 2022.

[21] D. G¨ud¨uz, Z. Qin, et al., “Beyond Transmitting Bits: Context, Semantics, and Task-oriented

Communications,” IEEE Journal on Selected Areas in Communications, vol. 41, no. 1, pp.

5-41, Jan. 2023.

[22] K. Niu, J. Dai, S. Yao, et al., “A Paradigm Shift Towards Semantic Communications,” IEEE

Communications Magazine, vol. 60, no. 11, pp. 113-119, 2022.

[23] E. Arıkan, “Channel polarization: a method for constructing capacity achieving codes for

symmetric binary-input memoryless channels,” IEEE Trans. Inf. Theory, vol. 55, no. 7, pp.

3051-3073, July 2009.

