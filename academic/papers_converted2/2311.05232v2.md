# A Survey On Hallucination In Large Language Models: Principles, Taxonomy, Challenges, And Open Questions

LEI HUANG, Harbin Institute of Technology, China WEIJIANG YU, Huawei Inc., China WEITAO MA and WEIHONG ZHONG, Harbin Institute of Technology, China ZHANGYIN FENG and HAOTIAN WANG, Harbin Institute of Technology, China QIANGLONG CHEN and WEIHUA PENG, Huawei Inc., China XIAOCHENG FENG∗, BING QIN, and TING LIU, Harbin Institute of Technology, China The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR
systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.

CCS Concepts: - Computing methodologies →**Natural language generation**; - General and reference
→**Surveys and overviews**.

Additional Key Words and Phrases: Large Language Models, Hallucination, Factuality, Faithfulness ACM Reference Format:
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2024. A Survey on Hallucination in Large Language Models:
Principles, Taxonomy, Challenges, and Open Questions. *ACM Transactions on Information Systems* 1, 1, Article 1
(January 2024), 58 pages. https://doi.org/10.1145/3703155

∗corresponding author
Authors' addresses: Lei Huang, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China,
150001, lhuang@ir.hit.edu.cn; Weijiang Yu, Huawei Inc., Bantian Subdistrict, Shenzhen, Guangdong, China, 518129, weijiangyu8@gmail.com; Weitao Ma, wtma@ir.hit.edu.cn; Weihong Zhong, whzhong@ir.hit.edu.cn, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001; Zhangyin Feng, zyfeng@ir.hit.edu.cn; Haotian Wang, wanght1998@gmail.com, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001;
Qianglong Chen, chenqianglong.ai@gmail.com; Weihua Peng, pengwh.hit@gmail.com, Huawei Inc., Bantian Subdistrict, Shenzhen, Guangdong, China, 518129; Xiaocheng Feng, xcfeng@ir.hit.edu.cn; Bing Qin, qinb@ir.hit.edu.cn; Ting Liu, tliu@ir.hit.edu.cn, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

## 1 Introduction

Recently, the emergence of large language models (LLMs) [383], exemplified by LLaMA [299, 300], Claude [9], Gemini [7, 259] and GPT-4 [232], has ushered in a significant paradigm shift in natural language processing (NLP), achieving unprecedented progress in language understanding [116, 124], generation [373, 393] and reasoning [57, 151, 250, 326, 354]. Furthermore, the extensive factual knowledge encoded within LLMs has demonstrated considerable advancements in leveraging LLMs for information seeking [6, 246], potentially reshaping the landscape of information retrieval systems [394]. Nevertheless, in tandem with these remarkable advancements, concerns have arisen about the tendency of LLMs to generate hallucinations [15, 105], resulting in seemingly plausible yet factually unsupported content. Further compounding this issue is the capability of LLMs to generate highly convincing and human-like responses [265], which makes detecting these hallucinations particularly challenging, thereby complicating the practical deployment of LLMs, especially realworld information retrieval (IR) systems that have integrated into our daily lives like chatbots
[8, 231], search engines [4, 214], and recommender systems [97, 171]. Given that the information provided by these systems can directly influence decision-making, any misleading information has the potential to spread false beliefs, or even cause harm.

Notably, hallucinations in conventional natural language generation (NLG) tasks have been extensively studied [125, 136], with hallucinations defined as generated content that is either nonsensical or unfaithful to the provided source content. These hallucinations are categorized into two types: *intrinsic hallucination*, where the generated output contradicts the source content, and extrinsic hallucination, where the generated output cannot be verified from the source. However, given their remarkable versatility across tasks [15, 30], understanding hallucinations in LLMs presents a unique challenge compared to models tailored for specific tasks. Besides, as LLMs typically function as open-ended systems, the scope of hallucination encompasses a broader concept, predominantly manifesting factual errors. This shift necessitates a reevaluation and adjustment of the existing taxonomy of hallucinations, aiming to enhance its adaptability in the evolving landscape of LLMs.

In this survey, we propose a redefined taxonomy of hallucination tailored specifically for applications involving LLMs. We categorize hallucination into two primary types: factuality hallucination and faithfulness hallucination. *Factuality hallucination* emphasizes the discrepancy between generated content and verifiable real-world facts, typically manifesting as factual inconsistencies.

Conversely, *faithfulness hallucination* captures the divergence of generated content from user input or the lack of self-consistency within the generated content. This category is further subdivided into instruction inconsistency, where the content deviates from the user's original instruction; context inconsistency, highlighting discrepancies from the provided context; and logical inconsistency, pointing out internal contradictions within the content. Such categorization refines our understanding of hallucinations in LLMs, aligning it closely with their contemporary usage.

Delving into the underlying causes of hallucinations in LLMs is essential not merely for enhancing the comprehension of these phenomena but also for informing strategies aimed at alleviating them. Recognizing the multifaceted sources of LLM hallucinations, our survey identifies potential contributors into three main aspects: data, training, and inference stages. This categorization allows us to span a broad spectrum of factors, providing a holistic view of the origins and mechanisms by which hallucinations may arise within LLM systems. Furthermore, we comprehensively outline a variety of effective detection methods specifically devised for detecting hallucinations in LLMs, as well as an exhaustive overview of benchmarks related to LLM hallucinations, serving as appropriate testbeds to assess the extent of hallucinations generated by LLMs and the efficacy of detection methods. Beyond evaluation, significant efforts have been undertaken to mitigate hallucinations of LLMs. These initiatives are comprehensively surveyed in our study, in accordance with the corresponding causes, spanning from data-related, training-related, and inference-related approaches. In addition, the effectiveness of retrieval-augmented generation (RAG) in mitigating hallucinations has garnered tremendous attention within the field. Despite the considerable potential of RAG, current systems inherently face limitations and even suffer from hallucinations. Accordingly, our survey undertakes an in-depth analysis of these challenges, aiming to provide valuable insights aimed at developing more robust RAG systems. We also highlight several promising avenues for future research, such as hallucinations in large vision-language models and understanding of knowledge boundaries in LLM hallucinations, paving the way for forthcoming research in the field.

Comparing with Existing Surveys. As hallucination stands out as a major challenge in generative AI, numerous research [136, 192, 258, 298, 312, 376] has been directed towards hallucinations.

While these contributions have explored LLM hallucination from various perspectives and provided valuable insights, our survey seeks to delineate their distinct contributions and the comprehensive scope they encompass. Ji et al. [136] primarily shed light on hallucinations in pre-trained models for NLG tasks, leaving LLMs outside their discussion purview. Tonmoy et al. [298] mainly focused on discussing the mitigation strategies combating LLM hallucinations. Besides, Liu et al. [192] took a broader view of LLM trustworthiness without delving into specific hallucination phenomena, whereas Wang et al. [312] provided an in-depth look at factuality in LLMs. However, our work narrows down to a critical subset of trustworthiness challenges, specifically addressing factuality and extending the discussion to include faithfulness hallucinations. To the best of our knowledge, Zhang et al. [376] presented research closely aligned with ours, detailing LLM hallucination taxonomies, evaluation benchmarks, and mitigation strategies. However, our survey sets itself apart through a unique taxonomy and organizational structure. We present a detailed, layered classification of hallucinations and conduct a more comprehensive analysis of the causes of hallucinations. Crucially, our proposed mitigation strategies are directly tied to these causes, offering a targeted and coherent framework for addressing LLM hallucinations.

Organization of this Survey. In this survey, we present a comprehensive overview of the latest developments in LLM hallucinations, as shown in Fig 1. We commence by constructing a taxonomy of hallucinations in the realm of LLM (§2). Subsequently, we analyze factors contributing to LLM
hallucinations in depth (§3), followed by a review of various strategies and benchmarks employed for the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches designed to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by current RAG systems (§6) and delineate potential pathways for forthcoming research (§7).

2
DEFINITIONS
For the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a succinct introduction to LLMs (§2.1), delineating the scope of this survey. Subsequently, we delve into the training stages of LLMs (§2.2), as a thorough understanding of the training mechanisms contributes significantly to elucidating the origins of hallucinations. Lastly, we expound upon the concept of hallucinations in LLMs (§2.3), further categorizing it into two distinct types. 2.1
Large Language Models Before delving into the causes of hallucination, we first introduce the concept of LLMs. Typically, LLMs refer to a series of general-purpose models that leverage the transformer-based language model architecture and undergo extensive training on massive textual corpora with notable examples including GPT-3 [29], PaLM [54], LLaMA [300], GPT-4 [232] and Gemini [259]. By scaling the amount of data and model capacity, LLMs raise amazing emergent abilities, typically including in-context learning (ICL) [29], chain-of-thought prompting [326] and instruction following [244].

Misinformation and
e.g. Bender et al. [20], Lee et al. [159], Lin et al. [182]
Knowledge Boundary
e.g. Katz et al. [149], Onoe et al. [230], Singhal et al. [279]
Inferior Alignment Data
e.g. Gekhman et al. [98], Li et al. [168]
Hallucination from Pre-training
e.g. Li et al. [180], Liu et al. [183], Wang and Sennrich [313]
Hallucination from SFT
e.g. Schulman [269], Yang et al. [341], Zhang et al. [362]
Hallucination from RLHF
e.g. Cotra [64], Perez et al. [245], Sharma et al. [274], Wei et al. [327]
Imperfect Decoding Strategies
e.g. Holtzman et al. [118], Stahlberg and Byrne [283]
Over-confidence
e.g. Chen et al. [45, 46], Liu et al. [193], Miao et al. [212]
Softmax Bottleneck
e.g. Chang and McCallum [38], Miao et al. [212]
Reasoning Failure
e.g. Berglund et al. [22], Zheng et al. [386] e.g. Dhuliawala et al. [74], Manakul et al. [205], Min et al. [216] e.g. Fabbri et al. [80], Maynez et al. [208], Scialom et al. [271] e.g. TruthfulQA [182], HalluQA [49], HaluEval-2.0 [168] e.g. SelfCheckGPT-Wikibio [213], HaluEval [169], FELM [42]
Data Filtering
e.g. Abbas et al. [1], Gunasekar et al. [107], Touvron et al. [300] e.g. Dai et al. [67], Huang et al. [127], Mitchell et al. [219] e.g. Gao et al. [94], Ram et al. [255], Yu et al. [358] e.g. Li et al. [180], Liu et al. [183, 189], Shi et al. [276] e.g. Rimsky [264], Sharma et al. [274], Wei et al. [327] e.g. Chuang et al. [59], Lee et al. [160], Li et al. [172] e.g. Chang et al. [36], Shi et al. [275], Wan et al. [309]

2.2
Training Stages of Large Language Models The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities.

2.2.1
Pre-training. Pre-training is widely acknowledged as a foundational stage for LLM to acquire knowledge and capabilities [388]. During this phase, LLMs engage in autoregressive prediction of subsequent tokens within sequences. Through self-supervised training on extensive textual corpora, LLMs acquire knowledge of language syntax, world knowledge, and reasoning abilities, thereby laying a solid groundwork for further fine-tuning. Besides, recent research [72, 291] suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of LLMs lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world.

2.2.2
Supervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pre-training primarily optimizes for completion.

Consequently, pre-trained LLMs fundamentally serve as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses. To bridge this gap, SFT [370] has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies [60, 129]
have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities.

2.2.3
Reinforcement Learning from Human Feedback. While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences.

Among various methods that utilize human feedback, RLHF stands out as an representative solution for aligning with human preferences through reinforcement learning [55, 233, 285]. Typically, RLHF employs a preference model [26] trained to predict preference rankings given a prompt alongside a pair of human-labeled responses. To align with human preferences, RLHF optimizes the LLM to generate outputs that maximize the reward provided by the trained preference model, typically employing a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) [270].

Such integration of human feedback into the training loop has proven effective in enhancing the alignment of LLMs, guiding them toward producing high-quality and harmless responses.

2.3
Hallucinations in Large Language Models The concept of hallucination traces its roots to the fields of pathology and psychology and is defined as *the perception of an entity or event that is absent in reality* [202]. Within the realm of NLP, hallucination is typically referred to as a phenomenon in which the generated content appears nonsensical or unfaithful to the provided source content [89, 208]. This concept bears a loose resemblance to the phenomenon of hallucination observed in human psychology. Generally, hallucinations in natural language generation tasks can be categorized into two primary types:
intrinsic hallucination and *extrinsic hallucination* [126, 136, 174].

Specifically, *intrinsic hallucinations* pertain to the model outputs that directly conflict with the provided source context. On the other hand, *extrinsic hallucinations* involve outputs that cannot be verified using the provided source context or external knowledge bases. This means the generated text is neither supported by nor directly contradicts the available information, rendering the output unverifiable and potentially misleading.

However, in the era of LLMs, the versatile capabilities of these models have facilitated their widespread use across diverse fields, highlighting limitations in existing task-specific categorization paradigms. Considering that LLMs place a significant emphasis on user-centric interactions and prioritize alignment with user directives, coupled with the fact that their hallucinations predominantly surface at factual levels, we introduce a more granular taxonomy building upon the foundational work by Ji et al. [136]. This refined taxonomy seeks to encapsulate the distinct intricacies associated with LLM hallucinations. To provide a more intuitive illustration of our definition of LLM
hallucination, we present examples for each type of hallucination in Table 1, namely factuality hallucination and *faithfulness hallucination*.

2.3.1
Factuality Hallucination. The emergence of LLMs marks a significant shift from traditional task-specific toolkits to AI assistants that have a heightened focus on open-domain interactions.

output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM hallucination.

.
Category
Type
Example Dialog
Explanation
Instruction: What are Thomas Edison's main contributions
to science and technology?
Factual Contradiction
The response is factually incorrect. In reality, Edison
improved the light bulb, building on earlier designs by others, and **Alexander Graham Bell inven**-
ted the telephone.
Response: Thomas Edison developed the first practical telephone and invented the light bulb.
Instruction: What are the major environmental impacts of
the construction of the Eiffel Tower?
Factuality Hallucination
Factual Fabrication
Response: The construction of the Eiffel Tower in 1889 led
to the extinction of the Parisian tiger, a species that played a crucial role in the region's ecosystem. Additionally, it is widely recognized as the event that sparked the global green architecture movement.
The response erroneously states that the construction of the Eiffel Tower led to the extinction of the 'Parisian tiger' and sparked the global green architecture movement. In fact, there is no evidence of a 'Parisian tiger' ever existing, making this a fabricated claim.
Moreover, attributing the origins of green architecture to the Eiffel Tower is an exaggeration, as this
movement has diverse roots and cannot be traced back to a single event.
Instruction: Translate the English question into Spanish:
"What is the capital of France?"
Instruction
Inconsistency
Response: The capital of France is Paris.
The user explicitly provided an instruction to translate
the English question into Spanish. Instead of performing the translation as instructed, the LLM chose to answer the question.
Faithfulness Hallucination
Retrieved context: The Nile is a major north-flowing river
in northeastern Africa. It's traditionally considered the longest river in the world. Originating in the Great Lakes region of central Africa, it flows through several countries before emptying into the Mediterranean Sea.
Context Inconsistency
The summary incorrectly states that the Nile originates from the "mountain ranges of central Africa", while the paragraph mentions that the Nile originates in the
Great Lakes region of central Africa.
Instruction: Summarize the retrieval document above.
Response: The Nile originates from the mountain ranges
of central Africa and flows into the Mediterranean.
Instruction: Solve the equation 2x + 3 = 11 step by step.
Logical Inconsistency

The first step correctly isolates the term with x to get
2x = 8. However, the next step inaccurately divides 8 by 2 to yield a result of x = 3, which is inconsistent with the earlier reasoning.

of the equation is x=3.

This shift is primarily attributed to their vast parametric factual knowledge. However, existing LLMs occasionally exhibit tendencies to produce outputs that are either inconsistent with real-world facts or unverifiable [168], posing challenges to the trustworthiness of artificial intelligence. In this context, we categorize these factuality hallucinations into two primary types:
Factual Contradiction refers to situations where the LLM's output contains facts that can be grounded in real-world information, but present contradictions. This type of hallucination occurs most frequently and arises from diverse sources, encompassing the LLM's capture, storage, and expression of factual knowledge. Depending on the error type of contradictions, it can be further divided into two subcategories: *entity-error hallucination* and *relation-error hallucination*.

- **Entity-error hallucination** refers to the situations where the generated text of LLMs
contains erroneous entities. As shown in Table 1, when asked about "the inventor of the
telephone", the model erroneously states *"Thomas Edison"*, conflicting with the real fact that it
was *"Alexander Graham Bell"*.
- **Relation-error hallucination** refers to instances where the generated text of LLMs contains
wrong relations between entities. As shown in Table 1, when inquired about "the inventor of
the light bulb", the model incorrectly claims *"Thomas Edison"*, despite the fact that he improved
upon existing designs and did not invent it.
Factual Fabrication refers to instances where the LLM's output contains facts that are unverifiable against established real-world knowledge. This can be further divided into unverifiability hallucination and *overclaim hallucination*.

- **Unverifiability hallucination** pertains to statements that are entirely non-existent or
cannot be verified using available sources. As shown in Table 1, when asked about "the major environmental impacts of the construction of the Eiffel Tower", the model incorrectly states that
"the construction led to the extinction of the Parisian tiger", a species that does not exist and
thus, this claim cannot be substantiated by any historical or biological record.
- **Overclaim hallucination** involves claims that lack universal validity due to subjective biases.
As shown in Table 1, the model claims that "the Eiffel Tower's construction is widely recognized
as the event that sparked the global green architecture movement." This is an overclaim, as
there is no broad consensus or substantial evidence to support the statement.
2.3.2
Faithfulness Hallucination. LLMs are inherently trained to align with user instructions. As the use of LLMs shifts towards more user-centric applications, ensuring their consistency with user-provided instructions and contextual information becomes increasingly vital. Furthermore, LLM's faithfulness is also reflected in the logical consistency of its generated content. From this perspective, we categorize three subtypes of faithfulness hallucinations:
Instruction inconsistency refers to the LLM's outputs that deviate from a user's directive.

While some deviations might serve safety guidelines, the inconsistencies here signify unintentional misalignment with non-malicious user instructions. As described in Table 1, the user's actual intention is translation, However, the LLM erroneously deviated from the user's instruction and performed a question-answering task instead.

Context inconsistency points to instances where the LLM's output is unfaithful with the user's provided contextual information. For example, as shown in Table 1, the user mentioned the Nile's source being in the Great Lakes region of central Africa, yet the LLM's response contradicted the context.

Logical inconsistency underscores when LLM outputs exhibit internal logical contradictions, often observed in reasoning tasks. This manifests as inconsistency both among the reasoning steps themselves and between the steps and the final answer. For example, as shown in Table 1, while the reasoning step of dividing both sides of the equation by 2 is correct, the final answer of x=4 is inconsistent with the reasoning chain, leading to an incorrect result.

3
HALLUCINATION CAUSES
LLM hallucinations have multifaceted origins, spanning the entire spectrum of LLMs' capability acquisition process. In this section, we delve into the root causes of hallucinations in LLMs, primarily categorized into three key aspects: (1) *Data* (§3.1), (2) *Training* (§3.2), and (3) *Inference* (§3.3).

## 3.1 Hallucination From Data

Data for training LLMs are comprised of two primary components: (1) pre-training data, through which LLMs acquire their general capabilities and factual knowledge [388], and (2) alignment data, which teach LLMs to follow user instructions and align with human preferences [322].

Although these data constantly expand the capability boundaries of LLMs, they inadvertently become the principal contributors to LLM hallucinations. This primarily manifests in three aspects:
the presence of misinformation and biases in the flawed pre-training data sources (§3.1.1), the knowledge boundary inherently bounded by the scope of the pre-training data (§3.1.2), and the hallucinations induced by inferior alignment data (§3.1.3).

3.1.1
Misinformation and biases. Neural networks possess an intrinsic tendency to memorize training data [35], and this memorization tendency grows with model size [34, 54]. In general, the inherent memorization capability is a double-edged sword in the fight against hallucinations. On the one hand, the capacities of LLMs to memorize suggests their potential to capture profound

| Type                         | Premise            | Input   |
|------------------------------|--------------------|---------|
| Who is credited with the     |                    |         |
| invention of the light bulb? |                    |         |
| Imitative                    |                    |         |
| Falsehood                    |                    |         |
| Thomas Edison                | is cred-           |         |
| ited with the invention      |                    |         |
| of the light bulb.           |                    |         |
| Within                       | the                | LLM's   |
| training data, a fre-        |                    |         |
| quently                      | referenced         |         |
| but                          | incorrect          | fact    |
| appears                      | multiple           |         |
| times:                       |                    |         |
| Thomas Edison                |                    |         |
| invented the light bulb.     |                    |         |
| While                        | Edison             | im-     |
| proved                       | the                | light   |
| bulb's design, he was        |                    |         |
| not its sole inventor.       |                    |         |
| The                          | model's            | answer  |
| reflects a misconception     |                    |         |
| prevalent in its training    |                    |         |
| data, exemplifying an        |                    |         |
| imitative falsehood.         |                    |         |
| Societal                     |                    |         |
| Biases                       |                    |         |
| In the model's train-        |                    |         |
| ing                          | data,              | certain |
| names may frequently         |                    |         |
| be                           | associated         | with    |
| specific nationalities.      |                    |         |
| Dr. Kim                      | from South Ko-     |         |
| rea                          | recently published |         |
| a paper on quantum           |                    |         |
| computing,                   | detailing          |         |
| new algorithms and           |                    |         |
| their applications.          |                    |         |
| The                          | model              | erro-   |
| neously added "South         |                    |         |
| Korea" based on the          |                    |         |
| name "Kim"                   |                    |         |
| revealing                    |                    |         |
| its bias in linking cer-     |                    |         |
| tain names to specific       |                    |         |
| nationalities, which can     |                    |         |
| cause hallucinations in      |                    |         |
| the summary.                 |                    |         |
| Provide a summary of the     |                    |         |
| following news: Dr. Kim re-  |                    |         |
| cently published a break-    |                    |         |
| through paper on quantum     |                    |         |
| computing. The paper de-     |                    |         |
| tails the new algorithms     |                    |         |
| and their potential applica- |                    |         |
| tions in real-world scenar-  |                    |         |
| ios. It has been hailed as   |                    |         |
| revolutionary by experts.    |                    |         |

world knowledge. On the other hand, it becomes problematic in the context of misinformation and biases present within pre-training data and may inadvertently be amplified, manifesting as imitative falsehood [182] and the reinforcement of societal biases. For a more comprehensive understanding, detailed examples are presented in Table 2.

Imitative Falsehood. Misinformation such as fake news and unfounded rumors has been widely spread among social media platforms and gradually serves as a significant contributor to LLM hallucinations. The increasing demand for large-scale corpora for pre-training necessitates the employment of heuristic data collection methods. While facilitating the acquisition of extensive data, challenges arise in maintaining consistent data quality, which inevitably introduces such misinformation in pre-training data [20, 328]. In this situation, LLMs will increase the likelihood of generating such false statements due to their remarkable memorization capabilities, leading to imitative falsehoods. The issue is further exacerbated by the fact that LLMs have drastically lowered the barriers to content creation, posing considerable risks to the trustworthiness of public discourse and internet ecosystems.

Societal Biases. In addition to misinformation, biases are also deeply rooted in social media platforms, showing up variously, like biased hiring, prejudiced news, and hate-spewing. As a purely negative phenomenon, biases and hallucinations have obvious differences, mainly manifested as stereotypes spreading and social inequalities. However, certain biases are intrinsically tied to hallucinations, especially those related to gender [242] and nationality [155, 226]. For instance, LLMs might associate the profession of nursing with females, even when gender isn't explicitly mentioned in the user-provided context, exemplifying context inconsistency as discussed in Section
(§2.3). Such biases can be inadvertently acquired from internet-based texts, which are rife with diverse and biased viewpoints, and subsequently be propagated into the generated content [155].

| Type                               | Input                 | Model Ouput   | Explanation   |
|------------------------------------|-----------------------|---------------|---------------|
| Long-tail Knowledge                |                       |               |               |
| What causes phenylke-              |                       |               |               |
| tonuria?                           |                       |               |               |
| The correct cause of phenylke-     |                       |               |               |
| tonuria is mutations in the PAH    |                       |               |               |
| gene.                              |                       |               |               |
| Such specialized medical           |                       |               |               |
| information is less frequently     |                       |               |               |
| encountered in the training        |                       |               |               |
| corpus, making LLMs struggle       |                       |               |               |
| to memorize.                       |                       |               |               |
| Phenylketonuria is caused by       |                       |               |               |
| a mutation in the hemoglobin       |                       |               |               |
| gene, which results in an          |                       |               |               |
| unusual                            | interaction           | with          |               |
| hemoglobin                         | ,                     | affecting     | its           |
| ability to transport oxygen in     |                       |               |               |
| the bloodstream.                   |                       |               |               |
| Up-to-date Knowledge               |                       |               |               |
| Who will be the Demo-              |                       |               |               |
| cratic Party candidate             |                       |               |               |
| for the 2024 US presi-             |                       |               |               |
| dential election?                  |                       |               |               |
| After President Joe Biden an-      |                       |               |               |
| nounced his withdrawal from the    |                       |               |               |
| race on July 21, 2024, he endorsed |                       |               |               |
| Kamala Harris as the Democratic    |                       |               |               |
| Party candidate.                   |                       |               |               |
| Such hallucina-                    |                       |               |               |
| tions arise because LLMs' para-    |                       |               |               |
| metric knowledge cannot be         |                       |               |               |
| updated in real-time, leading      |                       |               |               |
| to potential fabrication regard-   |                       |               |               |
| ing current events.                |                       |               |               |
| Michelle                           | Obama                 | has           | an-           |
| nounced her candidacy and          |                       |               |               |
| is leading in the polls, with      |                       |               |               |
| significant endorsements from      |                       |               |               |
| key Democratic figures such        |                       |               |               |
| as Barack Obama and Bernie         |                       |               |               |
| Sanders.                           | She has also outlined |               |               |
| a policy platform focusing on      |                       |               |               |
| healthcare reform and climate      |                       |               |               |
| change initiatives.                |                       |               |               |
| Copyright-sensitive                |                       |               |               |
| Knowledge                          |                       |               |               |
| Describe the ending of             |                       |               |               |
| the novel "The Secrets             |                       |               |               |
| of the Alchemist" that             |                       |               |               |
| was published last year.           |                       |               |               |
| In the dramatic conclusion of      |                       |               |               |
| "The Secrets of the Alchemist",    |                       |               |               |
| the protagonist discovers the      |                       |               |               |
| true secret behind the philoso-    |                       |               |               |
| pher's stone, leading to a peace-  |                       |               |               |
| ful resolution with all charac-    |                       |               |               |
| ters sharing in the wisdom.        |                       |               |               |
| The novel "The Secrets of the      |                       |               |               |
| Alchemist" is under copyright      |                       |               |               |
| protection, and LLMs have not      |                       |               |               |
| been trained directly on such      |                       |               |               |
| copyrighted materials.             |                       |               |               |
| Thus,                              |                       |               |               |
| the model's output fabricates de-  |                       |               |               |
| tails about the book's ending.     |                       |               |               |

3.1.2
Knowledge Boundary. While the vast pre-training corpora have empowered LLMs with extensive factual knowledge, they inherently possess knowledge boundaries. These boundaries arise primarily from two sources: (1) the inability of LLMs to memorize all factual knowledge encountered during pre-training, especially the less frequent long-tail knowledge; and (2) the intrinsic boundary of the pre-training data itself, which does not include rapidly evolving world knowledge or content restricted by copyright laws. Consequently, when LLMs encounter information that falls outside their limited knowledge boundaries, they are more susceptible to generating hallucinations. We present detailed examples for clear illustration in Table 3.

Long-tail Knowledge. The distribution of knowledge within the pre-training corpora is inherently non-uniform, which results in LLMs demonstrating varying levels of proficiency across different types of knowledge. Recent studies have highlighted a strong correlation between the model's accuracy on general domain questions and the volume of relevant documents [145] or entity popularity [204] within the pre-training corpora. Furthermore, given that LLMs are predominantly trained on extensive general domain corpora [93, 243, 254], they may exhibit deficits in domain-specific knowledge. This limitation becomes particularly evident when LLMs are confronted with tasks that require domain-specific expertise, such as medical [179, 279] and legal [149, 353] questions, these models may exhibit pronounced hallucinations, often manifesting as factual fabrication.

Up-to-date Knowledge. Beyond the shortfall in long-tail knowledge, another intrinsic limitation concerning the knowledge boundaries within LLMs is their constrained capacity for up-to-date knowledge. The factual knowledge embedded within LLMs exhibits clear temporal boundaries and can become outdated over time [148, 166, 230]. Once these models are trained, their internal knowledge is never updated. This poses a challenge given the dynamic and ever-evolving nature of our world. When confronted with queries that transcend their temporal scope, LLMs often resort to fabricating facts or providing answers that might have been correct in the past but are now outdated.

Copyright-sensitive Knowledge. Due to licensing restrictions [262], existing LLMs are legally constrained to training on corpora that are publicly licensed [63, 93] or otherwise available for use without infringing copyright laws [10, 115]. This limitation significantly impacts the breadth and diversity of knowledge that LLMs can legally acquire. A significant portion of valuable knowledge, encapsulated in copyrighted materials such as recent scientific research, proprietary data, and copyrighted literary works, remains inaccessible to LLMs. This exclusion creates a knowledge gap, leading to potential hallucinations when LLMs attempt to generate information in domains where their training data is inaccessible [215].

3.1.3
Inferior Alignment Data. After the pre-training stage, LLMs have embedded substantial factual knowledge within their parameters, thereby establishing obvious knowledge boundaries.

During the supervised fine-tuning (SFT) stage, LLMs are typically trained on instruction pairs labeled by human annotators, potentially introducing new factual knowledge that extends beyond the knowledge boundary established during pre-training. Gekhman et al. [98] analyzed the training dynamics of incorporating new factual knowledge during the SFT process and found that LLMs struggle to acquire such new knowledge effectively. Most importantly, they discovered a correlation between the acquisition of new knowledge through SFT and increased hallucinations, suggesting that introducing new factual knowledge encourages LLMs to hallucinate. Additionally, Li et al. [168]
conducted extensive analysis on the effect of instructions in producing hallucinations. Findings indicated that task-specific instructions which primarily focus on task format learning, tend to yield a higher proportion of hallucinatory responses. Moreover, overly complex and diverse instructions also lead to increased hallucinations.

3.2
Hallucination from Training As detailed in Section 2.2, the distinct stages of training impart various capabilities to LLMs, with pre-training focusing on acquiring general-purpose representations and world knowledge, and alignment enables LLMs to better align with user instructions and preferences. While these stages are critical for equipping LLMs with remarkable capabilities, shortfalls in either stage can inadvertently pave the way for hallucinations.

3.2.1
Hallucination from Pre-training. Pre-training constitutes the foundational stage for LLMs, predominantly utilizing a transformer-based architecture following the paradigm established by GPT [29, 251, 252], and further developed by OPT[372], Falcon [243], and Llama-2 [300]. This stage employs a causal language modeling objective, where models learn to predict subsequent tokens solely based on preceding ones in a unidirectional, left-to-right manner. While facilitating efficient training, it inherently limits the ability to capture intricate contextual dependencies, potentially increasing risks for the emergence of hallucination [180]. Moreover, recent research has exposed that LLMs can occasionally exhibit unpredictable reasoning hallucinations spanning both long-range and short-range dependencies, which potentially arise from the limitations of soft attention [52, 111], where attention becomes diluted across positions as sequence length increases. Notably, the phenomenon of exposure bias [21, 256] has been a longstanding and serious contribution to hallucinations, resulting from the disparity between training and inference in the auto-regressive generative model. Such inconsistency can result in hallucinations [313], especially when an erroneous token generated by the model cascades errors throughout the subsequent sequence, akin to a snowball effect [368].

3.2.2
Hallucination from Supervised Fine-tuning. LLMs have inherent capability boundaries established during pre-training. SFT seeks to utilize instruction data and corresponding responses to unlock these pre-acquired abilities. However, challenges arise when the demands of annotated instructions exceed the model's pre-defined capability boundaries. In such cases, LLMs are trained to fit responses beyond their actual knowledge boundaries. As discussed in §3.1.3, over-fitting on new factual knowledge encourages LLMs prone to fabricating content, amplifying the risk of hallucinations [98, 269]. Moreover, another significant reason lies in the models' inability to reject.

Traditional SFT methods typically force models to complete each response, without allowing them to accurately express uncertainty [341, 362]. Consequently, when faced with queries that exceed their knowledge boundaries, these models are more likely to fabricate content rather than reject it.

This misalignment of knowledge boundaries, coupled with the inability to express uncertainty, are critical factors that contribute to the occurrence of hallucinations during the SFT stage.

3.2.3
Hallucination from RLHF. Several studies [13, 31] have demonstrated that LLM's activations encapsulate an internal belief related to the truthfulness of its generated statements. Nevertheless, misalignment can occasionally arise between these internal beliefs and the generated outputs. Even when LLMs are refined with human feedback [233], they can sometimes produce outputs that diverge from their internal beliefs. Such behaviors, termed as sycophancy [64], underscore the model's inclination to appease human evaluators, often at the cost of truthfulness. Recent studies indicate that models trained via RLHF exhibit pronounced behaviors of pandering to user opinions.

Such sycophantic behaviors are not restricted to ambiguous questions without definitive answers
[245], like political stances, but can also arise when the model chooses a clearly incorrect answer, despite being aware of its inaccuracy [327]. Delving into this phenomenon, Sharma et al. [274]
suggested that the root of sycophancy may lie in the training process of RLHF models. By further exploring the role of human preferences in this behavior, the research indicates that the tendency for sycophancy is likely driven by both humans and preference models showing a bias towards sycophantic responses over truthful ones.

## 3.3 Hallucination From Inference Decoding Plays An Important Role In Manifesting The Capabilities Of Llms After Pretraining And Alignment. However, Certain Shortcomings In Decoding Strategies Can Lead To Llm Hallucinations.

3.3.1
Imperfect Decoding Strategies. LLMs have demonstrated a remarkable aptitude for generating highly creative and diverse content, a proficiency that is critically dependent on the pivotal role of randomness in their decoding strategies. Stochastic sampling [84, 118] is currently the prevailing decoding strategy employed by these LLMs. The rationale for incorporating randomness into decoding strategies stems from the realization that high likelihood sequences often result in surprisingly low-quality text, which is called *likelihood trap* [118, 209, 283, 363]. The diversity introduced by the randomness in decoding strategies comes at a cost, as it is positively correlated with an increased risk of hallucinations [59, 78]. An elevation in the sampling temperature results in a more uniform token probability distribution, increasing the likelihood of sampling tokens with lower frequencies from the tail of the distribution. Consequently, this heightened tendency to sample infrequently occurring tokens exacerbates the risk of hallucinations [5].

3.3.2
Over-confidence. Prior studies in conditional text generation [45, 212] have highlighted the issue of *over-confidence* which stems from an excessive focus on the partially generated content, often prioritizing fluency at the expense of faithfully adhering to the source context. While LLMs, primarily adopting the causal language model architecture, have gained widespread usage, the over-confidence phenomenon continues to persist. During the generation process, the prediction of the next word is conditioned on both the language model context and the partially generated text. However, as demonstrated in prior studies [19, 189, 307], language models often exhibit a localized focus within their attention mechanisms, giving priority to nearby words and resulting in a notable deficit in context attention [275]. Furthermore, this concern is further amplified in LLMs that exhibit a proclivity for generating lengthy and comprehensive responses. In such cases, there is even a heightened susceptibility to the risk of instruction forgetting [46, 193]. This insufficient attention can directly contribute to faithfulness hallucinations, wherein the model outputs content that deviates from the original context.

3.3.3
Softmax Bottleneck. The majority of language models utilize a softmax layer that operates on the final layer's representation within the language model, in conjunction with a word embedding, to compute the ultimate probability associated with word prediction. Nevertheless, the efficacy of Softmax-based language models is impeded by a recognized limitation known as the Softmax bottleneck [342], wherein the employment of softmax in tandem with distributed word embeddings constrains the expressivity of the output probability distributions given the context which prevents LMs from outputting the desired distribution. Additionally, Chang and McCallum [38] discovered that when the desired distribution within the output word embedding space exhibits multiple modes, language models face challenges in accurately prioritizing words from all the modes as the top next words, which also introduces the risk of hallucination.

3.3.4
Reasoning Failure. Beyond the challenges with long-tail knowledge, effective utilization of knowledge is inextricably linked with reasoning capabilities. For instance, in multi-hop questionanswering scenarios, even if the LLM possesses the necessary knowledge, it may struggle to produce accurate results if multiple associations exist between questions, due to its limitations in reasoning
[386]. Furthermore, Berglund et al. [22] unveiled a specific reasoning failure in LLMs termed the Reversal Curse. Specifically, while the model can correctly answer when the question is formulated as "A is B", it exhibits a failed logical deduction when asked the converse "B is A". This discrepancy in reasoning extends beyond simple deductions.

4
HALLUCINATION DETECTION AND BENCHMARKS
The issue of hallucinations within LLMs has garnered considerable attention, raising concerns about the reliability of LLMs and their deployment in practical applications. As LLMs become increasingly adept at generating human-like text, accurately distinguishing between hallucinated versus factual content becomes increasingly vital. Moreover, effectively measuring the level of hallucination in LLM is crucial for improving their reliability. Thus, in this section, we delve into hallucination detection approaches (§4.1) and benchmarks for assessing LLM hallucinations (§4.2).

4.1
Hallucination Detection Existing strategies for detecting hallucinations in LLMs can be categorized based on the type of hallucination: (1) factuality hallucination detection, which aims to identify factual inaccuracies in the model's outputs, and (2) faithfulness hallucination detection, which focuses on evaluating the faithfulness of model's outputs to the contextual information provided.

4.1.1
Factuality Hallucination Detection. Factuality hallucination detection involves assessing whether the output of LLMs aligns with real-world facts. Typical methods generally fall into two categories: *fact-checking*, which involves verifying the factuality of the generated response against trusted knowledge sources, and *uncertainty estimation*, which focuses on detecting factual inconsistency via internal uncertainty signals.

Fact-checking. Given that the output of LLMs is typically comprehensive and consists of multiple factual statements, the fact-checking approach is generally divided into two primary steps:
(1) fact extraction, which involves extracting independent factual statements within the model's outputs (2) fact verification, which aims at verifying the correctness of these factual statements against trusted knowledge sources. Depending on the type of knowledge sources employed for verification, fact-checking methodologies can be broadly categorized into two distinct parts: external retrieval and *internal checking*.

- *External retrieval*: The most intuitive strategy for fact verification is external retrieval. Min et al.
[216] developed FACTSCORE, a fine-grained factual metric tailored for evaluating long-form
text generation. It first decomposes the generation content into atomic facts and subsequently computes the percentage supported by reliable knowledge sources. Expanding on this concept,
Chern et al. [50] proposed a unified framework that equips LLMs with the capability to identify
factual inaccuracies by utilizing a collection of external tools dedicated to evidence gathering.
In addition to retrieving supporting evidence solely based on decomposited claims, Huo et al.
[128] improved the retrieval process through query expansion. By combining the original
question with the LLM-generated answer, they effectively addressed the issue of topic drift,
ensuring that the retrieved evidence aligns with both the question and the LLM's response.
- *Internal checking*: Given the extensive factual knowledge encoded in their parameters, LLMs
have been explored as factual knowledge sources for fact-checking. Dhuliawala et al. [74]
introduced the Chain-of-Verification (CoVe), where an LLM first generates verification questions for a draft response and subsequently leverages its parametric knowledge to assess the consistency of the answer against the original response, thereby detecting potential
inconsistencies.Kadavath et al. [143] and Zhang et al. [375] calculates the probability 𝑝(*𝑇𝑟𝑢𝑒*)
to assess the factuality of the response to a boolean question, relying exclusively on the
model'sinternal knowledge. Additionally, Li et al. [168] observed that most atomic statements
are interrelated, some may serve as contextual backgrounds for others, which potentially
leads to incorrect judgments. Thus, they instruct the LLM to directly predict hallucination
judgments considering all factual statements. However, as LLMs are not inherently reliable
factual databases [385], solely relying on LLMs' parametric knowledge for fact-checking may result in inaccurate assessments.
Uncertainty Estimation. While many approaches to hallucination detection rely on external knowledge sources for fact-checking, several methods have been devised to address this issue in zero-resource settings, thus eliminating the need for retrieval. The foundational premise behind these strategies is that the origin of LLM hallucinations is inherently tied to the model's uncertainty.

Therefore, by estimating the uncertainty of the factual content generated by the model, it becomes feasible to detect hallucinations. The methodologies in uncertainty estimation can broadly be categorized into two approaches: based on *LLM internal states* and *LLM behavior*, as shown in Fig. 2.

- *LLM internal states*: The internal states of LLMs can serve as informative indicators of their
uncertainty, often manifested through metrics like token probability or entropy. Varshney et al.
[306] determined the model's uncertainty towards key concepts quantified by considering
the minimal token probability within those concepts. The underlying rationale is that a
low probability serves as a strong indicator of the model's uncertainty, with less influence
from higher probability tokens present in the concept. Similarly, Luo et al. [198] employed
a self-evaluation-based approach for uncertainty estimation by grounding in the rationale
that a language model's ability to adeptly reconstruct an original concept from its generated
explanation is indicative of its proficiency with that concept. By initially prompting the model
to generate an explanation for a given concept and then employing constrained decoding to have the model recreate the original concept based on its generated explanation, the
probability score from the response sequence can serve as a familiarity score for the concept.
Furthermore, Yao et al. [345] interpreted hallucination through the lens of adversarial attacks.
Utilizing gradient-based token replacement, they devised prompts to induce hallucinations.
Notably, they observed that the first token generated from a raw prompt typically exhibits
low entropy, compared to those from adversarial attacks. Based on this observation, they
proposed setting an entropy threshold to define such hallucination attacks.
- *LLM behavior*: However, when systems are only accessible via API calls [100, 214, 231],
access to the output's token-level probability distribution might be unavailable. Given this
constraint, several studies have shifted their focus to probing a model's uncertainty, either
through natural language prompts [143, 335] or by examining its behavioral manifestations. For instance, by sampling multiple responses from an LLM for the same prompt, Manakul et al.
[205] detected hallucinations via evaluating the consistency among the factual statements.
However, these methods predominantly rely on direct queries that explicitly solicit information or verification from the model. Agrawal et al. [3], inspired by investigative interviews,
advocated for the use of indirect queries. Unlike direct ones, these indirect counterparts often pose open-ended questions to elicit specific information. By employing these indirect queries,
consistency across multiple model generations can be better evaluated. Beyond assessing
uncertainty from the self-consistency of a single LLM's multiple generations, one can embrace
a multi-agent perspective by incorporating additional LLMs. Drawing inspiration from legal
cross-examination practices, Cohen et al. [62] introduced the LMvLM approach. This strategy
leverages an examiner LM to question an examinee LM, aiming to unveil inconsistencies of
claims during multi-turn interaction.
4.1.2
Faithfulness Hallucination Detection. Ensuring the faithfulness of LLMs to provide context or user instructions is pivotal for their practical utility in IR applications, from conversational search to interactive dialogue systems. We categorize existing hallucination detection metrics tailored to faithfulness into the following groups, with an overview shown in Fig. 3: (1) Fact-based (2)
Classifier-based (3) QA-based (4) Uncertainty-based (5) LLM-based.

Fact-based Metrics. In the realm of assessing faithfulness, one of the most intuitive methods involves measuring the overlap of pivotal facts between the generated content and the source content. Given the diverse manifestations of facts, faithfulness can be measured based on *n-gram*, entities, and *relation triples*. Traditional *n-gram-based* metrics, such as BLEU [239], ROUGE [181]
and PARENT-T [324], typically fall short in differentiating the nuanced discrepancies between the generated content and the source content [208]. *Entity-based* metrics [225] make a step further by calculating the overlap of entities, as any omission or inaccurate generation of these key entities could lead to an unfaithful response. Notably, even if entities match, the relations between them might be erroneous. Thus, *relation-based* metrics [99] focus on the overlap of relation tuples and introduce a metric that computes the overlap of relation tuples extracted using trained end-to-end fact extraction models.

Classifier-based Metrics. Beyond computing fact overlap, another straightforward approach to assessing the faithfulness of the model generation involves utilizing classifiers trained on data from related tasks such as natural language inference (NLI) and fact-checking, or data comprised of synthetically task-specific hallucinated and faithful content. A foundational principle for assessing the faithfulness of generated text is anchored on the idea that genuinely faithful content should inherently be entailed by its source content. In line with this, numerous studies [82, 208] have trained classifiers on NLI datasets to identify factual inaccuracies, especially in the context of abstract summarization. However, Mishra et al. [217] highlighted that the mismatch in input granularity between conventional NLI datasets and inconsistency detection datasets limits their applicability for effectively detecting inconsistencies. Building on this, more advanced studies have proposed methods such as fine-tuning on adversarial datasets [17], decomposing the entailment decisions at the dependency arc level [101], and segmenting documents into sentence units then aggregating scores between sentence pairs [154]. While using data from related tasks to finetune the classifier has shown promise in evaluating faithfulness, it's essential to recognize the inherent gap between related tasks and the downstream task. The scarcity of annotated data further constrains their applicability. In response to this challenge, a surge of research explores leveraging data-augmentation methods to construct synthetical data for fine-tuning the classifier, either by rule-based perturbation [79, 152, 266] or generation [389].

QA-based Metrics. In contrast to classifier-based metrics, QA-based metrics [77, 119, 271, 310]
have recently garnered attention for their enhanced ability to capture information overlap between the model's generation and its source. These metrics operate by initially selecting target answers from the information units within the LLM's output, and then questions are generated by the question-generation module. The questions are subsequently used to generate source answers based on the user context. Finally, the faithfulness of the LLM's responses is calculated by comparing the matching scores between the source and target answers. Although these methodologies share a common thematic approach, they exhibit variability in aspects like answer selection, question generation, and answer overlap, leading to diverse performance outcomes. Building on this foundational work, Fabbri et al. [80] conducted an in-depth evaluation of the components within QA-based metrics, yielding further enhancements in faithfulness evaluation.

Uncertainty-based Metrics. Drawing parallels with the uncertainty-based approaches employed for detecting factuality hallucinations (§4.1.1), the application of uncertainty estimation in assessing faithfulness has been widely explored, typically characterized by entropy and logprobability. For entropy-based uncertainty, Xiao and Wang [333] has revealed a positive correlation between hallucination likelihood in data-to-text generation and predictive uncertainty, which is estimated by deep ensembles [156]. In a related vein, Guerreiro et al. [106] leveraged the variance in hypotheses yielded by Monte Carlo Dropout [92] as an uncertainty measure within neural machine translation. More recently, van der Poel et al. [305] employed conditional entropy [337]
to assess model uncertainty in abstractive summarization. Regarding log-probability, it can be applied at different levels of granularity, such as word or sentence level. Notably, several studies
[91, 106, 359] have adopted length-normalized sequence log-probability to measure model confidence. Furthermore, considering the hallucinated token can be assigned high probability when the preceding context contains the same hallucinated information, Zhang et al. [374] focused on the most informative and important keywords and introduced a penalty mechanism to counteract the propagation of hallucinated content.

LLM-based Judgement. Recently, the remarkable instruction-following ability of LLMs has underscored their potential for automatic evaluation [51, 190, 314]. Exploiting this capability, researchers have ventured into novel paradigms for assessing the faithfulness of model-generated content [2, 95, 133, 153, 199]. By providing LLMs with concrete evaluation guidelines and feeding them both the model-generated and source content, they can effectively assess faithfulness. The final evaluation output can either be a binary judgment on faithfulness [199] or a k-point Likert scale indicating the degree of faithfulness [95]. For prompt selection, evaluation prompt can either be direct prompting, chain-of-thought prompting [2], using in-context-learning [133] or allowing the model to generate evaluation results accompanying with explanations [153].

## 4.2 Hallucination Benchmarks

In this section, we present a comprehensive overview of existing hallucination benchmarks, which can be categorized into two primary domains: Hallucination Evaluation Benchmarks (§4.2.1), which assess the extent of hallucinations generated by existing cutting-edge LLMs, and Hallucination Detection Benchmarks (§4.2.2), designed specifically to evaluate the performance of existing hallucination detection methods. Collectively, these benchmarks establish a unified framework, enabling a nuanced and thorough exploration of hallucinatory patterns in LLMs.

|                      |              | Attribute    | Task         |
|----------------------|--------------|--------------|--------------|
| Benchmark            | Datasets     | Data Size    | Language     |
| Factuality           | Faithfulness | Manual       | Task Type    |
| Generative QA        |              |              |              |
| TruthfulQA           |              |              |              |
| [182]                |              |              |              |
| -                    | 817          | English      |              |
| ✔                    | ✗            | ✔            |              |
| Multi-Choice QA      |              |              |              |
| Question             | Answer       |              |              |
| LLM-Judge &          |              |              |              |
| Human                |              |              |              |
| Multi-Choice QA      | Acc          |              |              |
| REALTIMEQA           |              |              |              |
| [148]                |              |              |              |
| -                    | Dynamic      | English      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        |              |              |              |
| Question             | Answer       |              |              |
| EM & F1              |              |              |              |
| SelfCheckGPT-Wikibio |              |              |              |
| [213]                |              |              |              |
| -                    | 1,908        | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            |              |              |              |
| Paragraph &          |              |              |              |
| Concept              |              |              |              |
| Passage              | AUROC        |              |              |
| Task-specific        | 30,000       | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Query        | Response     | Acc          |
| HaluEval             |              |              |              |
| [169]                |              |              |              |
| General              | 5,000        | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Task Input   | Response     | Acc          |
| Med-HALT             |              |              |              |
| [303]                |              |              |              |
| -                    | 4,916        | Multilingual |              |
| ✔                    | ✗            | ✗            |              |
| Multi-Choice QA      | Question     | Choice       |              |
| Pointwise Score      |              |              |              |
| & Acc                |              |              |              |
| Wiki-FACTOR          | 2,994        | English      |              |
| ✔                    | ✗            | ✗            |              |
| Multi-Choice QA      | Question     | Answer       | likelihood   |
| FACTOR               |              |              |              |
| [223]                |              |              |              |
| News-FACTOR          | 1,036        | English      |              |
| ✔                    | ✗            | ✗            |              |
| Multi-Choice QA      | Question     | Answer       | likelihood   |
| SenHallu             | 200          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Paper        | Summary      |              |
| P & R & F1           |              |              |              |
| BAMBOO               |              |              |              |
| [76]                 |              |              |              |
| AbsHallu             | 200          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Paper        | Summary      |              |
| P & R & F1           |              |              |              |
| ChineseFactEval      |              |              |              |
| [311]                |              |              |              |
| -                    | 125          | Chinese      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | -            | Score        |
| Misleading           | 175          | Chinese      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | Answer       | LLM-Judge    |
| Misleading-hard      | 69           | Chinese      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | Answer       | LLM-Judge    |
| HaluQA               |              |              |              |
| [49]                 |              |              |              |
| Knowledge            | 206          | Chinese      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | Answer       | LLM-Judge    |
| Never-changing       | 150          | English      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | Answer       | Human        |
| Slow-changing        | 150          | English      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | Answer       | Human        |
| Fast-changing        | 150          | English      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | Answer       | Human        |
| FreshQA              |              |              |              |
| [308]                |              |              |              |
| False-premise        | 150          | English      |              |
| ✔                    | ✗            | ✔            |              |
| Generative QA        | Question     | Answer       | Human        |
| FELM                 |              |              |              |
| [42]                 |              |              |              |
| -                    | 3,948        | English      |              |
| ✔                    | ✔            | ✗            |              |
| Detection            | Question     |              |              |
| Response             |              |              |              |
| Balanced             |              |              |              |
| Acc & F1             |              |              |              |
| PHD-LOW              | 100          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Entity       |              |              |
| Response             | P & R & F1   |              |              |
| PHD-Meidum           | 100          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Entity       |              |              |
| Response             | P & R & F1   |              |              |
| PHD                  |              |              |              |
| [340]                |              |              |              |
| PHD-High             | 100          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Entity       |              |              |
| Response             | P & R & F1   |              |              |
| ScreenEval           |              |              |              |
| [158]                |              |              |              |
| -                    | 52           | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Document     | Summary      | AUROC        |
| COVID-QA             | N/A          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            |              |              |              |
| Question             |              |              |              |
| Answer               | AUROC        |              |              |
| DROP                 | N/A          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            |              |              |              |
| Question             |              |              |              |
| Answer               | AUROC        |              |              |
| Open Assistant       | N/A          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Question     | Answer       | AUROC        |
| RealHall             |              |              |              |
| [90]                 |              |              |              |
| TriviaQA             | N/A          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Question     | Answer       | AUROC        |
| LSum                 |              |              |              |
| [85]                 |              |              |              |
| -                    | 6,166        | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Document     | Summary      | Balanced Acc |
| HotpotQA             | 250          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Question     | Answer       | AUROC        |
| SAC                  |              |              |              |
| 3                    |              |              |              |
| [364]                |              |              |              |
| NQ-Open              | 250          | English      |              |
| ✗                    | ✔            | ✗            |              |
| Detection            | Question     | Answer       | AUROC        |
| HaluEval 2.0         |              |              |              |
| [168]                |              |              |              |
| Biomedicine          | 1,535        | English      |              |
| ✔                    | ✗            | ✗            |              |
| Generative QA        | Question     | Answer       | MiHR & MaHR  |
| Finance              | 1,125        | English      |              |
| ✔                    | ✗            | ✗            |              |
| Generative QA        | Question     | Answer       | MiHR & MaHR  |
| Science              | 1,409        | English      |              |
| ✔                    | ✗            | ✗            |              |
| Generative QA        | Question     | Answer       | MiHR & MaHR  |
| Education            | 1,701        | English      |              |
| ✔                    | ✗            | ✗            |              |
| Generative QA        | Question     | Answer       | MiHR & MaHR  |
| Open domain          | 3,000        | English      |              |
| ✔                    | ✗            | ✗            |              |
| Generative QA        | Question     | Answer       | MiHR & MaHR  |

4.2.1
Hallucination Evaluation Benchmarks. Hallucination evaluation benchmarks are devised to quantify the tendency of LLMs to generate hallucinations, particularly emphasizing factual inaccuracies and inconsistency from the given contexts. Given the adeptness of LLMs at memorizing high-frequency count knowledge, the primary focus of current hallucination evaluation benchmarks targets long-tailed knowledge and challenging questions that can easily elicit imitative falsehood.

As for evaluating, these benchmarks typically utilize multiple choice QA, where performance is measured through accuracy metrics, or generative QA, evaluated either through human judgment or scores given by proxy models.

Long-tail Factual Knowledge. The selection criteria for gathering long-tail factual questionanswering samples typically include the frequency of appearance, recency, and specific domains.

Regarding the frequency of appearance, benchmarks such as PopQA [204] and Head-to-Tail [290]
are constructed based on entity popularity derived directly from Wikipedia. Considering that world knowledge is constantly evolving, it becomes crucial to validate the LLM's factuality concerning the current world. Among benchmarks characterized by ever-changing, REALTIMEQA [148] and FreshQA [308] stands out. REALTIMEQA offers real-time, open-domain multiple-choice questions that are regularly updated to reflect the latest developments. These questions are derived from newly published news articles, encompassing a broad spectrum of topics, including politics, business, sports, and entertainment. Similarly, FreshQA challenges LLMs with questions designed to represent varying degrees of temporal change—categorized into never-changing, slow-changing, and fastchanging world knowledge. This benchmark is further enriched by including questions based on false premises, requiring debunking, thus comprising a total of 600 meticulously hand-crafted questions. Moreover, long-tail knowledge often pertains to specific domains. For instance, Med-
HALT [303] is distinguished by its focus on the medical domain, challenging LLMs with multiplechoice questions derived from a variety of countries. Additionally, Malaviya et al. [203] collected expert-curated questions across 32 fields of study, resulting in a high-quality long-form QA dataset with 2,177 questions.

Imitative Falsehood Knowledge. Imitative falsehood knowledge is specifically designed to challenge LLMs through adversarial prompting. This approach crafts questions in such a way that they are prone to misleading LLMs due to false beliefs or misconceptions. The two most representative benchmarks are TruthfulQA [182] and HalluQA [49]. TruthfulQA comprises 817
questions that span 38 diverse categories, such as health, law, finance, and politics. Crafted using an adversarial methodology, it aims to elicit "imitative falsehoods"—misleading responses that models might generate due to their frequent presence in training data. The benchmark is divided into two parts, one of which contains manually curated questions that were further refined by filtering out those correctly answered by GPT-3, resulting in 437 filtered questions. The other part includes
380 unfiltered non-adversarial questions. Drawing from the construction approach of TruthfulQA, HalluQA is crafted to specifically assess hallucinations in Chinese LLMs, focusing on imitative falsehoods and factual errors. The benchmark comprises 450 handcrafted adversarial questions across 30 domains and is categorized into two parts. The misleading section captures questions that successfully deceive GLM-130B, while the knowledge section retains questions that both ChatGPT
and Puyu consistently answer incorrectly. To comprehensively evaluate LLM hallucinations across various domains, Li et al. [168] constructed an upgraded hallucination evaluation benchmark, HaluEval 2.0, based on [169]. This benchmark includes 8,770 questions that LLMs are prone to hallucination, across five domains: biomedicine, finance, science, education, and open domain.

4.2.2
Hallucination Detection Benchmarks. For hallucination detection benchmarks, most prior studies have primarily concentrated on task-specific hallucinations, such as abstractive summarization [81, 102, 152, 208, 236, 310], data-to-text[240, 296], and machine translation [389]. However, the content generated in these studies often originates from models with lesser capabilities, such as BART [164] and PEGASUS [366]. As a result, they may not accurately reflect the effectiveness of hallucination detection strategies, underlining the necessity for a significant shift toward developing benchmarks that encapsulate more complex scenarios reflective of the era of LLMs.

For example, SelfCheckGPT-Wikibio [213] offers a sentence-level dataset created by generating synthetic Wikipedia articles with GPT-3, manually annotated for factuality, highlighting the challenge of detecting hallucinations in the biography domain. Complementing this, HaluEval [169]
combines automated generation with human annotation to evaluate LLMs' ability to recognize hallucinations across 5,000 general user queries and 30,000 task-specific samples, leveraging a "sampling-then-filtering" approach. Building upon existing research predominantly focused on short documents, BAMBOO [76] and ScreenEval [158] extend the scope in long-form hallucination detection. Further, FELM [42], distinguishes itself by assessing factuality across diverse domains including world knowledge, science, and mathematics, producing 817 samples annotated for various facets of factual accuracy, thereby addressing the need for cross-domain evaluation of factuality in LLM-generated content. On a different note, PHD [340], shifts the focus towards passage-level detection of non-factual content by analyzing entities from Wikipedia, thus offering a nuanced view on the knowledge depth of LLMs. RealHall [90] and SAC3 [364] align closely with real-world applications focusing on open-domain question-answering, whereas LSum [85] concentrating on summarization tasks.

## 5 Hallucination Mitigation

In this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in LLMs. Drawing from insights discussed in *Hallucination Causes* (§3), we systematically categorize these methods based on the underlying causes of hallucinations. Specifically, we focus on approaches addressing *Data-related Hallucinations* (§5.1), Training-related Hallucinations
(§5.2) and *Inference-related Hallucinations* (§5.3), each offering tailored solutions to tackle specific challenges inherent to their respective cause.

## 5.1 Mitigating Data-Related Hallucinations

As analyzed in §3.1, data-related hallucinations generally emerge as a byproduct of misinformation, biases, and knowledge gaps, which are fundamentally rooted in the pre-training data. Several methods are proposed to mitigate such hallucinations, primarily categorized into three distinct parts: (1) *data filtering* aiming at selecting high-quality data to avoid introducing misinformation and biases, (2) *model editing* focusing on injecting up-to-date knowledge by editing model's parameters, and (3) *retrieval-augmented generation* leveraging external non-parametric database for knowledge supplying.

5.1.1
Data Filtering. To reduce the presence of misinformation and biases, an intuitive approach involves the careful selection of high-quality pre-training data from reliable sources. In this way, we can ensure the factual correctness of data while also minimizing the introduction of social biases. As early as the advent of GPT-2, Radford et al. [252] underscored the significance of exclusively scraping web pages that had undergone rigorous curation and filtration by human experts. However, as pre-training datasets continue to scale, manual curation becomes a challenge. Given that academic or specialized domain data is typically factually accurate, gathering high-quality data emerges as a primary strategy. Notable examples include *the Pile* [93] and "textbook-like" data sources [107, 177].

Additionally, up-sampling factual data during the pre-training phase has been proven effective in enhancing the factual correctness of LLMs [300], thus alleviating hallucination.

In addition to strictly controlling the source of data, deduplication serves as a crucial procedure.

Existing practices typically fall into two categories: exact duplicates and near-duplicates. For exact duplicates, the most straightforward method involves exact substring matching to identify identical strings. However, given the vastness of pre-training data, this process can be computationally intensive, a more efficient method utilizes the construction of a suffix array [206], enabling effective computation of numerous substring queries in linear time. Regarding near-duplicates, the identification often involves approximate full-text matching, typically utilizing hash-based techniques to identify document pairs with significant n-gram overlap. Furthermore, MinHash [28] stands out as a prevalent algorithm for large-scale deduplication tasks [110]. Additionally, SemDeDup [1] makes use of embeddings from pre-trained models to identify semantic duplicates, which refers to data pairs with semantic similarities but not identical.

Discussion. Since data filtering works directly at the source of hallucinations, it effectively mitigates hallucinations by ensuring the use of high-quality, factually accurate sources. Despite its effectiveness, the efficiency and scalability of current data filtering methods pose significant challenges as data volumes expand. Additionally, these methods often overlook the influence of LLM-generated content, which can introduce new risks and inaccuracies. To advance, future research must focus on developing more efficient, automated data filtering algorithms that can keep pace with the rapid expansion of datasets and the complexities of LLM-generated content.

5.1.2
Model Editing. Model editing [280, 320, 369] has garnered rising attention from researchers, which aims to rectify model behavior by incorporating additional knowledge. Current model editing techniques can be categorized into two classes: *locate-then-edit* and *meta-learning*.

Locate-then-edit. Locate-then-edit methods [66, 210] consist of two stages, which first locate the
"buggy" part of the model parameters and then apply an update to them to alter the model's behavior.

For example, ROME [210] located the edits-related layer by destroying and subsequently restoring the activations and then updates the parameters of FFN in a direct manner to edit knowledge. MEMIT
[211] employed the same knowledge locating methods as ROME, enabling the concurrent updating of multiple layers to facilitate the simultaneous integration of thousands of editing knowledge. However, Yao et al. [347] found that these methods lack non-trivial generalization capabilities and varying performance and applicability to different model architectures. The best-performing methods ROME and MEMIT empirically only work well on decoder-only LLMs.

Meta-learning. Meta-learning methods [70, 218] train an external hyper-network to predict the weight update of the original model. Nevertheless, meta-learning methods often require additional training and memory cost, where MEND [218] utilized a low-rank decomposition with a specialized design to reduce the size of hyper-networks. Notably, MEND would exhibit a cancellation effect, where parameter shifts corresponding to different keys significantly counteract each other. MAL-
MEN [292] further addressed this issue by framing the parameter shift aggregation as a least squares problem rather than a simple summation, thereby greatly enhancing its capacity for extensive editing. While these methods can fine-grainedly adjust the behavior of the model, modifications to the parameters could have a potentially harmful impact on the inherent knowledge of the model.

Discussion. Model editing provides a precise way to mitigate hallucinations induced by specific misinformation without extensive retraining. However, these methods struggle with large-scale updates and can adversely affect the model's overall performance, particularly when continuous edits are applied. Consequently, future research should focus on improving model editing to handle large-scale knowledge updates more efficiently and address hallucinations caused by social biases.

5.1.3
Retrieval-Augmented Generation. Typically, retrieval-augmented generation (RAG) [109, 165,
278] follows a retrieve-then-read pipeline, where relevant knowledge is firstly retrieved by a retriever
[146] from external sources, and then the final response is generated by a *generator* conditioning on both user query and retrieved documents. By decoupling external knowledge from LLM, RAG
can effectively alleviate the hallucination caused by the knowledge gap without affecting the performance of LLM. Common practices can be divided into three parts, as shown in Fig 4: one-time retrieval, *iterative retrieval*, and *post-hoc retrieval*, depending on the timing of retrieval.

One-time Retrieval. One-time retrieval aims to directly prepend the external knowledge obtained from a single retrieval to the LLMs' prompt. Ram et al. [255] introduced In-context RALM, which entails a straightforward yet effective strategy of prepending chosen documents to the input text of LLMs. Beyond conventional knowledge repositories such as Wikipedia, ongoing research endeavors have explored alternative avenues, specifically the utilization of knowledge graphs (KGs). These KGs serve as a pivotal tool for prompting LLMs, facilitating their interaction with the most recent knowledge, and eliciting robust reasoning pathways [14, 249, 329]. Varshney et al. [306] introduce the Parametric Knowledge Guiding (PKG) framework, enhancing LLMs with domain-specific knowledge. PKG employs a trainable background knowledge module, aligning it with task knowledge and generating relevant contextual information.

Iterative Retrieval. When confronted with intricate challenges like multi-step reasoning [344]
and long-form question answering [83, 284], traditional one-time retrieval may fall short. Addressing these demanding information needs, recent studies have proposed iterative retrieval, which allows for continuously gathering knowledge throughout the generation process. Recognizing the substantial advancements chain-of-thought prompting [326] has brought to LLMs in multi-step reasoning, numerous studies [113, 301, 346] try to incorporate external knowledge at each reasoning step and further guide retrieval process based on ongoing reasoning, reducing factual errors in reasoning chains. Building upon chain-of-thought prompting, Press et al. [247] introduced *self-ask*.

Diverging from the conventional continuous, undelineated chain-of-thought prompting, self-ask delineates the question it intends to address at each step, subsequently incorporating a search action based on the follow-up question. Instead of solely depending on chain-of-thought prompting for retrieval guidance, both Feng et al. [87] and Shao et al. [273] employed an iterative retrievalgeneration collaborative framework, where a model's response serves as an insightful context to procure more relevant knowledge, subsequently refining the response in the succeeding iteration.

Beyond multi-step reasoning tasks, Jiang et al. [140] shifted their emphasis to long-form generation.

They proposed an active retrieval augmented generation framework, which iteratively treats the upcoming prediction as a query to retrieve relevant documents. If the prediction contains tokens of low confidence, the sentence undergoes regeneration. In addition to using iterative retrieval to improve intermediate generations, Zhang et al. [371] presented MixAlign, which iteratively refines user questions using model-based guidance and seeking clarifications from users, ultimately enhancing the alignment between questions and knowledge.

Post-hoc Retrieval. Beyond the traditional *retrieve-then-read* paradigm, a line of work has delved into post-hoc retrieval, refining LLM outputs through subsequent retrieval-based revisions.

To enhance the trustworthiness and attribution of LLMs, Gao et al. [94] adopted the researchthen-revise workflow, which initially research relevant evidence and subsequently revise the initial generation based on detected discrepancies with the evidence. Similarly, Zhao et al. [381] introduced the *verify-and-edit* framework to enhance the factual accuracy of reasoning chains by incorporating external knowledge. For reasoning chains that show lower-than-average consistency, the framework generates verifying questions and then refines the rationales based on retrieved knowledge, ensuring a more factual response. Yu et al. [358] enhanced the post-hoc retrieval method through diverse answer generation. Instead of generating just a single answer, they sample various potential answers, allowing for a more comprehensive retrieval feedback. Additionally, by employing an ensembling technique that considers the likelihood of the answer before and after retrieval, they further mitigate the risk of misleading retrieval feedback.

Discussion. One crucial advantage of retrieval-augmented generation methodology is its effectiveness in mitigating hallucinations caused by knowledge gaps, and their generality, which allows for application across any domain. This flexibility is further enhanced by the modularity of the approach, treating external knowledge bases like plug-ins that can be swapped or modified as needed. In terms of the drawbacks, it can be easily impacted by irrelevant retrievals, which may decrease the overall performance by introducing noise or incorrect information into the response generation process. Furthermore, the current paradigm exhibits shallow interactions between the retriever and generator components, leading to suboptimal knowledge utilization. Hence, future research should focus on developing a robust RAG system that minimizes the impact of irrelevant retrieval, as well as integrating adaptive learning components that can dynamically adjust retrieval strategies based on the context of the query and the performance of previous interactions.

5.2
Mitigating Training-related Hallucination Training-related hallucinations typically arise from the intrinsic limitations of the architecture and training strategies adopted by LLMs. In this context, we discuss various optimization methods ranging from training stages (§5.2.1) and alignment stages (SFT & RLHF) (§5.2.2), aiming to mitigate hallucinations within the training process.

5.2.1
Mitigating Pretraining-related Hallucination. One significant avenue of research in mitigating pretraining-related hallucination centers on the limitations inherent in model architectures, especially *unidirectional representation* and *attention glitches*. In light of this, numerous studies have delved into designing novel model architectures specifically tailored to address these flaws. To address the limitations inherent in unidirectional representation, Li et al. [180] introduced BATGPT
which employs a bidirectional autoregressive approach. This design allows the model to predict the next token based on all previously seen tokens, considering both past and future contexts, thus capturing dependencies in both directions. Building on this idea, Liu et al. [189] highlighted the potential of encoder-decoder models to make better use of their context windows, suggesting a promising direction for future LLMs architecture design. Besides, recognizing the limitations of soft attention within self-attention-based architecture, Liu et al. [183] proposed attention-sharpening regularizers. This plug-and-play approach specifies self-attention architectures using differentiable loss terms [365] to promote sparsity, leading to a significant reduction in reasoning hallucinations.

In the pre-training phase of LLMs, the choice of objective plays a pivotal role in determining the model's performance. However, conventional objectives can lead to fragmented representations and inconsistencies in model outputs. Recent advancements have sought to address these challenges by refining pre-training strategies, ensuring richer context comprehension, and circumventing biases.

Addressing the inherent limitations in training LLMs, where unstructured factual knowledge at a document level often gets chunked due to GPU memory constraints and computational efficiency, leading to fragmented information and incorrect entity associations, Lee et al. [160] introduced a factuality-enhanced training method. By appending a TOPICPREFIX to each sentence in factual documents, the approach transforms them into standalone facts, significantly reducing factual errors and enhancing the model's comprehension of factual associations. Similarly, considering that randomly concatenating shorter documents during pre-training might introduce inconsistencies in model outputs, Shi et al. [276] proposed In-Context Pretraining, an innovative approach in which LLMs are trained on sequences of related documents. By altering the document order, this method aims to maximize similarity within the context windows. It explicitly encourages LLMs to reason across document boundaries, potentially bolstering the logical consistency between generations.

Discussion. Strategies designed to mitigate pretraining-related hallucinations typically are fundamental, potentially yielding significant improvements. However, they typically involve modifications to pre-training architectures and objectives, which are computationally intensive. Moreover, these integrations may lack broad applicability. Moving forward, the focus should be on developing adaptable and efficient strategies that can be universally applied without extensive system overhaul.

5.2.2
Mitigating Misalignment Hallucination. Hallucinations induced during alignment often stem from capability misalignment and belief misalignment. However, defining the knowledge boundary of LLMs proves challenging, making it difficult to bridge the gap between LLMs' inherent capabilities and the knowledge presented in human-annotated data. While limited research addresses capability misalignment, the focus mainly shifts toward belief misalignment.

Hallucinations stemming from belief misalignment often manifest as sycophancy, a tendency of LLMs to seek human approval in undesirable ways. This sycophantic behavior can be attributed to the fact that human preference judgments often favor sycophantic responses over more truthful ones [274], paving the way for reward hacking [268]. To address this, a straightforward strategy is to improve human preference judgments and, by extension, the preference model. Recent research
[25, 268] has investigated the use of LLMs to assist human labelers in identifying overlooked flaws.

Additionally, Sharma et al. [274] discovered that aggregating multiple human preferences enhances feedback quality, thereby reducing sycophancy.

Besides, modifications to LLMs' internal activations have also shown the potential to alter model behavior. This can be achieved through methods like fine-tuning [327] or activation steering during inference [69, 117, 289]. Specifically, Wei et al. [327] proposed a synthetic-data intervention, finetuning language models using synthetic data where the claim's ground truth is independent of a user's opinion, aiming to reduce sycophantic tendencies.

Another avenue of research [263, 264] has been to mitigate sycophancy through activation steering. This approach involves using pairs of sycophantic/non-sycophantic prompts to generate the sycophancy steering vector, derived from averaging the differences in intermediate activations.

During inference, subtracting this vector can produce less sycophantic LLM outputs.

Discussion. Mitigating hallucinations through post-training methods represents a direct and effective approach, bypassing the complexities associated with data sourcing and pre-training. However, a notable gap in current research is the limited attention given to capability misalignment within LLMs. Future research should prioritize understanding the knowledge boundaries in capability alignment to address hallucinations effectively. 5.3
Mitigating Inference-related Hallucination Decoding strategies in LLMs play a pivotal role in determining the factuality and faithfulness of the generated content. However, as analyzed in Section §3.3, imperfect decoding often results in outputs that might lack factuality or stray from the original context. In this subsection, we explore two advanced strategies aimed at refining the decoding strategy to enhance both the factuality and faithfulness of the LLMs' outputs.

5.3.1
Factuality Enhanced Decoding. Factuality Enhanced Decoding aims to improve the reliability of outputs from LLMs by prioritizing the factuality of the information they generate. This line of methods focuses on aligning model outputs closely with established real-world facts, thereby minimizing the risk of disseminating false or misleading information.

Factuality Decoding. Considering the randomness in the sampling process can introduce non-factual content into open-ended text generation, Lee et al. [160] introduced the factual-nucleus sampling algorithm that dynamically adjusts the nucleus probability 𝑝throughout sentence generation. By dynamically adjusting the nucleus probability based on decay factors and lower boundaries and resetting the nucleus probability at the beginning of every new sentence, the decoding strategy strikes a balance between generating factual content and preserving output diversity. Moreover, some studies [31, 220] posit that the activation space of LLMs contains interpretable structures related to factuality. Building on this idea, Li et al. [172] introduced Inference-Time Intervention
(ITI). This method first identifies a direction in the activation space associated with factually correct statements and then adjusts activations along the truth-correlated direction during inference.

By repeatedly applying such intervention, LLMs can be steered towards producing more factual responses. Similarly, Chuang et al. [59] delved into enhancing the factuality of LLM's decoding process from a perspective of factual knowledge storage. They exploit the hierarchical encoding of factual knowledge within transformer LLMs, noting that lower-level information is captured in earlier layers and semantic information in the later ones. Drawing inspiration from [175], they introduce DoLa, a strategy that dynamically selects and contrasts logits from different layers to refine decoding factuality. By placing emphasis on knowledge from higher layers and downplaying that from the lower layers, DoLa showcases its potential to make LLMs more factual, thus reducing hallucinations.

Post-editing Decoding. Unlike methods that directly modify the probability distribution to prevent hallucinations during the initial decoding, post-editing decoding seeks to harness the self-correction capabilities of LLMs [237] to refine the originally generated content without relying on an external knowledge base. Dhuliawala et al. [74] introduced the Chain-of-Verification (COVE), which operates under the assumption that, when appropriately prompted, LLMs can self-correct their mistakes and provide more accurate facts. Starting with an initial draft, it first formulates verification questions and then systematically answers those questions in order to finally produce an improved revised response. Similarly, Ji et al. [137] focused on the medical domain and introduced an iterative self-reflection process. This process leverages the inherent ability of LLMs to first generate factual knowledge and then refine the response until it aligns consistently with the provided background knowledge.

Discussion. Factuality decoding methods, which typically assess the factuality at each decoding step, can offer substantial improvements. Furthermore, due to their plug-and-play nature, they allow for application without the need for computation-intensive training. Nevertheless, one of the primary limitations of these methods lies in balancing factual accuracy with maintaining the diversity and informativeness of the generated content, which can sometimes lead to compromises in either aspect. On the other hand, post-editing decoding strategies, despite their effectiveness, heavily rely on the self-correction capabilities of LLMs, which may be unreliable. Furthermore, applying self-reflection can be time-consuming, limiting their practicality for real-time applications.

Hence, it is crucial to achieve an optimal balance between factuality and computational efficiency.

5.3.2
Faithfulness Enhanced Decoding. On the other hand, Faithfulness Enhanced Decoding prioritizes alignment with the provided context and also emphasizes enhancing the consistency within the generated content. Thus, in this section, we summarize existing work into two categories, including *Context Consistency* and *Logical Consistency*.

Context Consistency. In the era of LLMs, the issue of faithfulness hallucination typically lies in insufficient attention to the given context, which inspired numerous research to design inferencetime strategies to enhance context consistency. Shi et al. [275] proposed context-aware decoding
(CAD), which modifies the model's original output distribution in a contrastive formulation [175].

By amplifying the difference between output probabilities with and without context, CAD encourages the LLM to focus more on contextual information rather than over-rely on prior knowledge.

However, due to the inherent trade-off between diversity and context attribution [103, 363], overemphasizing contextual information can reduce diversity. To address this, Chang et al. [36] introduced a dynamic decoding algorithm to bolster faithfulness while preserving diversity. Specifically, the algorithm involves two parallel decoding steps, one with the context and one without. During the decoding, the KL divergence between two token distributions serves as a guiding signal, indicating the relevance of the source context. This signal is utilized to dynamically adjust the sampling temperature to improve source attribution when the source is relevant. In a parallel line of work, Choi et al. [53] introduced knowledge-constrained decoding (KCD), which employed a token-level hallucination detection discriminator to identify contextual hallucinations and then guides the faithful generation process by reweighing the token distribution. In addition to modifying output distribution in place to enhance contextual attention, another line of work has explored a generic post-edit approach to enhance faithfulness. Gao et al. [94] adopted a *research-and-revise* workflow, where the research stage raises questions about various aspects of the model's initial response and gathers evidence for each query, while the revision stage detects and revises any disagreements between the model's response and the evidence. Similarly, Lei et al. [161] first detected contextual hallucinations at both the sentence and entity levels and then incorporated the judgments to refine the generated response. Moreover, several studies have explored methods to overcome the softmax bottleneck, which constrains the expression of diversity and faithful representations.

These approaches include employing a mixture of Softmax, which uses multiple hidden states to compute softmax multiple times and merge the resulting distributions [343] and incorporating pointer networks, which enables LLMs to copy the context words [37], thereby reducing context hallucinations.

Logical Consistency. Inspired by the human thinking process, chain-of-thought [326] has been introduced to encourage LLMs to decompose complex problems into explicitly intermediate steps, thereby enhancing the reliability of the reasoning process [58]. Despite effective, recent research
[157, 302] demonstrated that the intermediate rationales generated by LLMs do not faithfully capture their underlying behavior. A branch of research has been inspired to improve the consistency of intermediate rationales generated by LLMs, particularly in multi-step reasoning [61] and logical reasoning [18]. To enhance the self-consistency in chain-of-thought, Wang et al. [318] employed a knowledge distillation framework. They first generate a consistent rationale using contrastive decoding [175] and then fine-tune the student model with a counterfactual reasoning objective, which effectively eliminates reasoning shortcuts [27] that derive answers without considering the rationale. Furthermore, by employing contrastive decoding directly, LLMs can reduce surface-level copying and prevent missed reasoning steps [229]. In addition, Li et al. [167] conducted a deep analysis of the causal relevance among the context, CoT, and answer during unfaithful reasoning.

Analysis revealed that the unfaithfulness issue lies in the inconsistencies in the context information obtained by the CoT and the answer. To address this, they proposed inferential bridging, which takes the attribution method to recall contextual information as hints to enhance CoT reasoning and filter out noisy CoTs that have low semantic consistency and attribution scores to the context. Paul et al.

[241] decomposed the reasoning process into two modules: an inference module, which employs Direct Preference Optimization [253] to align the LLM towards preferring correct reasoning chains over counterfactual chains, and a reasoning module, which encourages the LLM to reason faithfully over the reasoning steps using a counterfactual and causal preference objective. Compared to natural language reasoning, logical reasoning demands rigorous logical calculation, whereas plain text often lacks precise logical structure, leading to unfaithful reasoning. To address this, Xu et al.

[338] introduced Symbolic CoT (SymbCoT), which incorporates symbolic expressions within CoT
to describe intermediate reasoning steps. Specifically, SymbCoT translates the natural language context into a symbolic representation and then formulates a step-by-step plan to address the logical reasoning problem, followed by a verifier to check the translation and reasoning chain, thereby ensuring faithful logical reasoning.

Discussion. Faithfulness Enhanced Decoding significantly advances the alignment of LLM
outputs with provided contexts and enhances the internal consistency of the generated content.

However, strategies such as context-aware decoding often lack adaptive mechanisms, limiting their effectiveness in scenarios that demand dynamic attention to context. Furthermore, many decoding strategies require the integration of additional models that do not focus on context, introducing significant computational overhead and reducing efficiency.

## 6 Hallucinations In Retrieval Augmented Generation

Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucinations and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale external knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus reducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs
[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still produce hallucinations [16]. Hallucinations in RAG present considerable complexities, manifesting as outputs that are either factually inaccurate or misleading. These hallucinations occur when the content generated by the LLM does not align with real-world facts, fails to accurately reflect the user's query, or is not supported by the retrieved information. Such hallucinations can stem from two primary factors: **retrieval failure** (§6.1) and **generation bottleneck** (§6.2). Through a comprehensive analysis of the limitations present in current RAG systems, we aim to shed light on potential improvements for retrieval-augmented LLMs, paving the way for more reliable information retrieval systems.

## 6.1 Retrieval Failure

The retrieval process is a crucial initial step in the RAG framework, tasked with retrieving the most relevant information for information-seeking queries. Consequently, failures in the retrieval stage can have serious downstream effects on the RAG pipeline, leading to hallucinations. These failures typically stem from three primary parts: the formulation of user queries, the reliability and scope of retrieval sources, and the effectiveness of the retriever.

6.1.1
User Queries. User queries play a fundamental role in guiding the retrieval process with RAG
systems. The specificity and clarity of these queries critically influence the effectiveness of retrieval outcomes. In this section, we discuss factors that may contribute to hallucinations from three perspectives: blind retrieval, misinterpretation of ambiguous queries, and the challenges in accurate retrieval of complex queries. Some examples are presented in Table 5 for a better understanding.

Retrieval Intent Decisions. Not all queries necessitate retrieval. Blind retrieval for queries that do not require external knowledge can counterproductively lead to misleading responses. As shown in Table 5, the query about *"the boiling point of water at sea level"* pertains to a basic scientific fact that the model could address without external retrieval. However, the retrieval system was

| Type                                                              | Example Dialog   | Explanation   |
|-------------------------------------------------------------------|------------------|---------------|
| Retrieved context:                                                |                  |               |
| At high altitudes, water boils at lower                           |                  |               |
| temperatures than at sea level, such as 95°C at elevations        |                  |               |
| above 2,000 meters. This altitude-induced reduction can affect    |                  |               |
| cooking times and methods.                                        |                  |               |
| Retrieval                                                         |                  |               |
| Intent                                                            |                  |               |
| Query:                                                            |                  |               |
| What is the boiling point of water at sea level?                  |                  |               |
| This query involves a basic scientific fact that should be        |                  |               |
| available within the model's parametric knowledge.                |                  |               |
| However, the system inappropriately retrieves                     |                  |               |
| and uses                                                          |                  |               |
| altitude-specific information about boiling temperatures,         |                  |               |
| leading to an incorrect response for sea-level conditions.        |                  |               |
| Response:                                                         |                  |               |
| The boiling point of water at sea level is                        | 95°C             | .             |
| Retrieved context:                                                |                  |               |
| The Hennessey Venom GT famously                                   |                  |               |
| set a speed record by completing a mile on a racetrack in         |                  |               |
| 32 seconds, highlighting its performance in motorsport events.    |                  |               |
| Ambiguous                                                         |                  |               |
| Queries                                                           |                  |               |
| Query:                                                            |                  |               |
| What's the record for the fastest mile run on track?              |                  |               |
| The query is ambiguous because it does not specify whe-           |                  |               |
| ther it refers to a human runner or a vehicle.                    |                  |               |
| The retrieval                                                     |                  |               |
| system erroneously pulls information related to car racing        |                  |               |
| records instead of athletics.                                     |                  |               |
| Response:                                                         |                  |               |
| The fastest mile on a track was completed in                      |                  |               |
| 32 seconds by the Hennessey Venom GT.                             |                  |               |
| Retrieved context:                                                |                  |               |
| Alexander Fleming, known for his ground-                          |                  |               |
| breaking work in discovering penicillin, significantly influenced |                  |               |
| modern antibiotics. Fleming's discovery saved countless lives.    |                  |               |
| Complex                                                           |                  |               |
| Queries                                                           |                  |               |
| Query:                                                            |                  |               |
| Which university did the scientist who discovered                 |                  |               |
| penicillin graduate from?                                         |                  |               |
| The retrieval system only manages to fetch information            |                  |               |
| about Fleming's professional achievements in the discovery        |                  |               |
| of penicillin                                                     |                  |               |
| . However, the document does not provide informa-                 |                  |               |
| tion about his educational background, thus the model generates a |                  |               |
| hallucinatory answer.                                             |                  |               |
| Response:                                                         |                  |               |
| Alexander Fleming graduated from Oxford University.               |                  |               |

inappropriately activated, blindly retrieving inaccurate information and consequently leading to an undesirable response. Consequently, several studies [75, 204, 228, 378] have proposed to make a shift from passive retrieval to adaptive retrieval. In general, these strategies can be divided into two categories: *heuristic-based* and self-aware judgment. *Heuristic-based* methods employ heuristic rules to determine the necessity of retrieval. For instance, Mallen et al. [204] observed a positive correlation between LLMs' memorization capabilities and entity popularity and suggested triggering retrieval only when the entity popularity in the user query falls below a certain threshold. Similarly, Jeong et al. [135] determined the timing of retrieval based on the query complexity, whereas Asai et al. [11] considered whether the query is factual relevant. *Self-aware judgment* leverages the models' intrinsic judgment to decide the necessity for information retrieval. Feng et al. [86], Ren et al. [261] and Wang et al. [321] directly prompted LLMs for retrieval decisions, recognizing that LLMs possess a certain level of awareness regarding their knowledge boundaries [143, 350].

Moreover, Jiang et al. [140] introduced an active retrieval strategy that triggers retrieval only when the LLM generates low-probability tokens. Similarly, Su et al. [288] not only considered the uncertainty of each token but also its semantic contribution and impact on the subsequent context.

More recently, Cheng et al. [48] proposed four orthogonal criteria for determining the retrieval timing, which include intent-aware, knowledge-aware, time-sensitive-aware, and self-aware.

Ambiguous Queries. Ambiguous user queries, containing omission, coreference, and ambiguity, significantly complicate the retrieval system's ability to fetch precisely relevant information, thereby increasing the likelihood of generating undesirable responses. As shown in Table 5, due to the ambiguity of the query about *"the record for the fastest mile run on track"*, the retrieval system erroneously retrieved information from automobile racing events, which led the model to generate a response suited for vehicles instead of athletes. A prevalent mitigation strategy is query rewriting, where queries are refined and decontextualized to better match relevant documents. Wang et al.

[317] and Jagerman et al. [132] have explored prompting approaches where the LLM is prompted to generate a pseudo-document or rationale based on the original query, which is then used for further retrieval. Additionally, Ma et al. [200] introduced a trainable rewriter which is trained using the feedback from the LLM via reinforcement learning. Mao et al. [207] employed the feedback signals from the reranker to train the rewrite model, thus eliminating the reliance on annotated data.

However, the challenges deepen in conversational search, which encounters a more complex issue of context-dependent query understanding with the lengthy conversational history. Addressing this, Yoon et al. [351] proposed a similar framework for optimizing the LLM to generate retrieverpreferred query rewrites. This operated by generating a variety of queries and then using the preference of the rank of retrieved passage to optimize the query rewriting model.

Complex Queries. Complex user queries, characterized by requiring intensive reasoning [286]
or encompassing multiple aspects [272, 319], pose significant challenges to the retrieval system.

Such queries require advanced understanding and decomposition capabilities, which may exceed the current capabilities of the current retrieval methods based on keyword or semantic matching, often leading to partial or incorrect retrievals. For example, as shown in Table 5, due to the multi-step nature of the query about "Which university did the scientist who discovered penicillin graduate from?", direct retrieval often leads to incomplete results, thereby resulting in hallucinatory responses. A common approach involves query decomposition, where the complex query is decomposed into sub-queries to facilitate more accurate information retrieval. For instance, Wang et al.

[319] implemented a sub-aspect explorer that utilizes the extensive world knowledge embedded LLMs to identify potential sub-aspects of user queries, thereby providing explicit insights into the user's underlying intents. Similarly, Shao et al. [272] concentrated on the demanding task of expository writing, aiming at retrieving comprehensive information to compose Wikipedialike articles from scratch on a specific topic. This approach involves decomposing the topic into various perspectives and simulating multi-turn conversations with LLMs, each personified with different perspectives for question asking. Additionally, Cao et al. [32] and Chu et al. [56] explored knowledge-intensive complex reasoning and employed a divide-and-conquer strategy. This strategy begins with decomposing complex questions into question trees, where at each node, the LLM
retrieves and aggregates answers from diverse knowledge sources.

6.1.2
Retrieval Sources. The reliability and scope of retrieval sources are crucial determinants of the efficacy of RAG systems. Effective retrieval depends not only on the clarity of the user queries but also on the quality and comprehensiveness of the sources from which information is retrieved.

When these sources contain factually incorrect or outdated information, the risk of retrieval failures increases significantly, potentially leading to the generation of incorrect or misleading information.

As the landscape of content creation evolves with the rapid advancement of Artificial Intelligence Generated Content (AIGC) [33], an increasing volume of LLM-generated content is permeating the internet, subsequently becoming integrated into retrieval sources [39]. This integration is reshaping the dynamics of information retrieval, as evidenced by recent empirical studies [68, 339] suggesting that modern retrieval models tend to favor LLM-generated content over human-authored content.

Recent research [44] has explored the implications of progressively integrating LLM-generated content into RAG systems. The findings indicate that, without appropriate intervention, humangenerated content may progressively lose its influence within RAG systems. Additionally, Tan et al. [293] investigated the performance of RAG systems when incorporating LLM-generated into retrieved contexts, revealing a significant bias favoring generated contexts. This bias stems from the high similarity between generated context and questions, as well as the semantic incompleteness of retrieved contexts. More seriously, the propensity of LLMs to produce factually inaccurate hallucinations exacerbates the reliability issues of retrieval sources. As LLM-generated content often contains factual errors, its integration into retrieval sources can mislead retrieval systems, further diminishing the accuracy and reliability of the information retrieved.

To combat these biases, several approaches have been explored. Inspired by common practice in pre-training data processing [23], Asai et al. [12] proposed a scenario that incorporates a quality filter designed to ensure the high quality of the retrieval datastore. Additionally, Pan et al. [238]
proposed Credibility-aware Generation (CAG), which equips LLMs with the ability to discern and handle information based on its credibility. This approach assigns different credibility levels to information, considering its relevance, temporal context, and the trustworthiness of its source, thus effectively reducing the impact of flawed information in RAG systems.

6.1.3
Retriever. When the user query is explicit and the retrieval source is reliable, the effectiveness of the retrieval process depends crucially on the performance of the retriever. In such scenarios, the retriever's effectiveness is significantly compromised by improper chunking and embedding practices.

Chunking. Given the extensive nature of retrieval sources, which often encompass lengthy documents like web pages, it poses significant challenges for LLMs with limited context length. Thus, chunking emerges as an indispensable step in RAG, which involves segmenting these voluminous documents into smaller, more manageable chunks to provide precise and relevant evidence for LLMs.

According to actual needs, the chunking granularity ranges from documents to paragraphs, even sentences. However, inappropriate retrieval granularity can compromise the semantic integrity and affect the relevance of retrieved information [224], thereby affecting the performance of LLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified length such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking, which is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in capture structure and dependency of lengthy documents, Sarthi et al. [267] proposed RAPTOR, an indexing and retrieval system. By recursively embedding, clustering, and summarizing chunks of text, RAPTOR constructs a tree to capture both high-level and low-level details. When retrieval, RAPTOR enables LLMs to integrate information from different levels of abstraction, providing a more comprehensive context for user queries. Instead of chunking text with a fixed chunk size, semantic chunking adaptively identifies breakpoints between sentences through embedding similarity, thereby preserving semantic continuity [144]. Furthermore, Chen et al. [43] pointed out the limitations of the existing retrieval granularity. On the one hand, while a coarser retrieval with a longer context can theoretically provide a more comprehensive context, it often includes extraneous details that could potentially distract LLMs. On the other hand, a fine-grain level can provide more precise and relevant information, it has limitations such as not being self-contained and lacking necessary contextual information. To address these shortcomings, Chen et al. [43]
introduced a novel retrieval granularity, proposition, which is defined as atomic expressions within the text, each encapsulating a distinct factoid and presented in a concise self-contained natural language format.

Embedding. Once the retrieval text is chunked, text chunks are subsequently transformed into vector representation via an embedding model. Such a representation scheme is supported by the well-known data structure of *vector database* [142], which systematically organizes data as keyvalue pairs for efficient text retrieval. In this manner, the relevance score can be computed according to the similarity function between the text representation and query representation. However, a sub-optimal embedding model may compromise performance, which affects the similarity and matching of chunks to user queries, potentially misleading LLMs. Typically, a standard embedding model [96, 130, 147, 382] learns the query and text representations with encoder-based architecture
(*e.g.* BERT [73], RoBERTa [191]) via contrastive learning [304], where the loss is constructed by contrasting a positive pair of query-document against a set of random negative pairs. However, these embeddings showcase their limitations when applied to new domains, such as medical and financial applications [222, 295]. In these cases, recent studies [71, 234, 277, 332] propose to fine-tune the embedding models on domain-specific data to enhance retrieval relevance. For example, REPLUG
[277] utilizes language modeling scores of the answers as a proxy signal to train the dense retriever.

More recently, Muennighoff et al. [221] have introduced generative representational instruction tuning where a single LLM is trained to handle both generative and embedding tasks, which largely reduces inference latency in RAG by caching representations. Despite these advancements, the field faces challenges, particularly with the fine-tuning of high-performing yet inaccessible embedding models, such as OpenAI's text-embedding-ada-002. Addressing this gap, Zhang et al.

[367] introduced a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model which significantly enhances the performance of the black-box embeddings.

## 6.2 Generation Bottleneck

After the retrieval process, the generation stage emerges as a pivotal point, responsible for generating content that faithfully reflects the retrieved information. However, this stage can encounter significant bottlenecks that may lead to hallucinations. We summarize two key capabilities of LLMs that are closely related to these bottlenecks: contextual awareness and contextual alignment. Each plays an important role in ensuring the reliability and credibility of the RAG system.

6.2.1
Contextual Awareness. Contextual awareness involves understanding and effectively utilizing contextual information retrieved. This section discusses the key factors that impact the LLM's ability to maintain contextual awareness, which can be categorized into three main parts: (1) the presence of noisy retrieval in context, (2) context conflicts, and (3) insufficient utilization of context information.

Noisy Context. As emphasized in §6.1, the failure in the retrieval process may inevitably introduce irrelevant information, which will propagate into the generation stage. When the generator is not robust enough to these irrelevant retrievals, it will mislead the generator and even introduce hallucinations [65].

Yoran et al. [352] conducted a comprehensive analysis on the robustness of current retrievalaugmented LLMs, revealing a significant decrease in performance with random retrieval. While using an NLI model to filter out irrelevant passages is effective, this method comes with the trade-off of inadvertently discarding some relevant passages. A more effective solution is to train LLMs to ignore irrelevant contexts by incorporating irrelevant contexts in training data. Similarly, Yu et al.

[357] introduced Chain-of-Note, which enables LLMs to first generate reading notes for retrieved contexts and subsequently formulate the final answer. In this way, LLMs can not only filter irrelevant retrieval to improve noise robustness but also respond with unknown when retrieval is insufficient to answer user queries. In addition to improving LLM robustness by learning to ignore irrelevant content in the context, several studies [139, 176, 323, 336] propose to compress the context to filter out irrelevant information. Specifically, Li [176] and Jiang et al. [139] made use of small language models to compute self-information and perplexity for prompt compression, finding the most informative content. Similarly, Wang et al. [323] proposed to filter out irrelevant content and leave precisely supporting content based on lexical and information-theoretic approaches. Besides, efforts have been also made to employ summarization models as compressors. Xu et al. [336] presented both extractive and abstractive compressors, which are trained to improve LLMs' performance while keeping the prompt concise. Liu et al. [188] involved summarization compression and semantic compression, where the former achieves compression by summarizing while the latter removes tokens with a lower impact on the semantic.

Context Conflict. Retrieval-augmented LLMs generate answers through the combined effect of parametric knowledge and contextual knowledge. As discussed in §3.3.2, LLMs may sometimes exhibit over-confidence, which can bring new challenges to the faithfulness of RAG systems when facing knowledge conflicts. Knowledge conflicts in RAG are situations where contextual knowledge contradicts LLMs' parametric knowledge. Longpre et al. [194] first investigated knowledge conflicts in open-domain question answering, where conflicts are automatically created by replacing all spans of the gold answer in the retrieval context with a substituted entity. Findings demonstrate that generative QA reader models (*e.g.* T5) tend to trust parametric memory over contextual information.

By further training the retriever to learn to trust the contextual evidence with augmented training examples by entity substitution, the issue of over-reliance on parametric knowledge is mitigated.

Similar findings are also reported by Li et al. [166] who demonstrated that fine-tuning LLMs on counterfactual contexts can effectively improve the controllability of LLMs when dealing with contradicts contexts. Also building upon counterfactual data augmentation, Neeman et al.

[227] trained models to predict two disentangled answers, one based on contextual knowledge and the other leveraging parametric knowledge to address knowledge conflicts. Besides, Zhou et al. [390] introduced two effective prompting-based strategies, namely opinion-based prompts and counterfactual demonstrations. Opinion-based prompts transform the context to narrators' statements, soliciting the narrators' opinions, whereas counterfactual demonstrations employ counterfactual instances to improve faithfulness in situations of knowledge conflict. While Longpre et al. [194] and Li et al. [166] concentrated their research on the context of a limited single evidence setting, Chen et al. [40] further expanded this study to consider a more realistic scenario in which models consider multiple evidence passages and find models rely almost exclusively on contextual evidence.

Considering previous studies [166, 194] mostly focused on smaller models, Xie et al. [334]
raised doubts about the applicability of their conclusions in the era of LLMs. Such heuristic entitylevel substitution may lead to incoherent counter-memory, thereby making it trivial for LLMs to overlook the construct knowledge conflicts. By directly eliciting LLMs to generate a coherent counter-memory that factually conflicts with the parametric memory, LLMs exhibit their high receptivity to external evidence.

Context Utilization. Despite successfully retrieving evidence relevant to factoid queries, LLMs can encounter a significant performance degradation due to insufficient utilization of the context, especially for information located in the middle of the long context window, a notable issue known as the *lost-in-the-middle* phenomenon [189]. Beyond factoid QA, recent studies have further demonstrated such a *middle-curse* also holds in abstractive summarization [257], long-form QA [41] and passage ranking [294]. One potential explanation lies in the use of rotary positional embedding
(RoPE) [287], which is widely used in open-source LLMs, due to its excellent performance in length extrapolation [380]. As a representative relative position embedding, RoPE features a long-term decay property, which inherently biases the LLM to give precedence to current or proximate tokens, thereby diminishing its attention on those that are more distant. Another contributing factor is that the most salient information often resides at the beginning or the end of pre-training data, a characteristic commonly observed in news reports [257]. Such an issue brings forth challenges in retrieval-augmented LLMs, as retrieval-augmented LLMs are typically designed with extensive lengths to accommodate more retrieval documents.

To mitigate this crucial issue, He et al. [114] introduced several tasks specially designed for information seeking to enhance the capability of information utilization by explicitly repeating the question and extracting the index of supporting documents before generating answers. Furthermore, Zhang et al. [377] introduced Multi-scale Positional Encoding (Ms-PoE), which mitigates the longterm decay effect characteristic of RoPE by rescaling position indices. Ms-PoE provides a plugand-play solution to enhance the ability of LLMs to effectively capture information in the middle of the context without the need for additional fine-tuning. Besides, Ravaut et al. [257] proposed hierarchical and incremental summarization, which effectively preserves the salient information and compresses the length of context to avoid the *middle-curse*.

6.2.2
Contextual Alignment. Contextual alignment ensures that LLM outputs faithfully align with relevant context. This section outlines the primary components of contextual alignment, which include: (1) source attribution and (2) faithful decoding.

Source Attribution. Source attribution [121] in retrieval-augmented LLMs refers to the process by which the model identifies and utilizes the origins of information within its generation process.

This component is crucial for ensuring that the outputs of RAG systems are not only relevant but also verifiable and grounded in credible sources.

To achieve source attribution in RAG systems, recent studies have been explored, which can be categorized into three lines based on the type of attribution. (1) *Plan-then-Generate*: Fierro et al.

[88] introduced the blueprint model for attribution, which conceptualizes text plans as a series of questions that serve as blueprints for generation process, dictating both the content and the sequence of the output. Compared with abstractive questions, Huang et al. [120] enabled the model to first ground to extractive evidence spans, which guides the subsequent generation process.

Leveraging either abstract questions or extractive spans as planning facilitates a built-in attribution mechanism, as they provide a natural link between retrieved information and the subsequent generation. Similarly, Slobodkin et al. [282] broke down the conventional end-to-end generation process into three intuitive stages: content selection, sentence planning, and sentence fusion. By initially identifying relevant source segments and subsequently conditioning the generation process on them, the selected segments naturally serve as attributions. (2) *Generate-then-Reflect*: Asai et al.

[11] proposed training the LLM to generate text with reflection tokens. These reflection tokens empower the LLM to decide whether to retrieve, assess the relevance of the retrieved document, and critique its own generation to ensure attributability. By critiquing its generation. Furthermore, Ye et al. [348] introduced AGREE, designed to facilitate self-grounding in LLMs. AGREE trains LLMs to generate well-grounded claims with citations and identify claims that lack verification.

An iterative retrieval process is then employed to actively seek additional information for these unsupported statements. (3) *Self-Attribution*: In addition to leveraging external supervised signals for attribution, Qi et al. [248] proposed a self-attribution mechanism that utilizes model-internal signals. It operates by first identifying context-sensitive answer tokens, which are then paired with retrieved documents that contributed to the model generation via saliency methods.

Faithful Decoding. Despite significant optimizations in the RAG pipeline that facilitate the incorporation of highly relevant content into the model's context, current LLMs still cannot guarantee faithful generation. The unfaithful utilization of relevant context by LLMs undermines the reliability of their outputs, even when the sources of information are verifiably accurate. Wu et al. [331] analyzed the model's knowledge preference when internal knowledge conflicts with contextual information and observed the tug-of-war between the LLM's internal prior and external evidence. To tackle this issue, recent research [275, 330] has focused on faithful decoding within RAG systems, aiming to improve the models' ability to generate content that faithfully aligns with contextual information. Shi et al. [275] presented context-aware decoding, which modifies the model's original output probability distribution into the pointwise mutual information (PMI)
formulation. The strategy operates by amplifying the difference between the output probabilities when a model is used with and without context, thereby enhancing the faithfulness of LLMs to the provided context. Li et al. [173] adopted a semi-parametric language modeling approach [150]
which facilitates the integration of contextual spans of arbitrary length into LM generations. The generation is then verified via speculative decoding, further ensuring model faithfulness. More recently, Wu et al. [330] proposed faithfulness-oriented decoding, which leverages a lightweight faithfulness detector to monitor the beam-search process. The detector leverages fine-grained decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and semantic alignment to synchronously detect unfaithful sentences. When an unfaithful generation is detected, it triggers the backtrack operation and selects the beam with the more faithful score, thus ensuring greater faithfulness to the retrieval sources.

7
FUTURE DISCUSSION
As the field of research on hallucinations in LLMs continues to evolve, our focus shifts towards the next horizon of inquiry. We explore prospective areas of study, notably the phenomenon of hallucinations in vision-language models (§7.1) and the challenge of delineating and understanding knowledge boundaries within LLMs (§7.2). 7.1
Hallucination in Large Vision-Language Models Enabling the visual perception ability, along with exceptional language understanding and generation capabilities, Large Vision-Language Models (LVLMs) have exhibited remarkable visionlanguage capabilities [47, 123, 186, 201, 355, 356, 360, 392]. Unlike previous pre-trained multi-modal models that gain limited vision-language abilities from large-scale visual-language pre-training datasets [170, 197, 325, 387], LVLMs exploit advanced LLMs to unleash the power of interacting with humans and the environment. The consequent diverse applications of LVLMs also bring new challenges to maintaining the reliability of such systems. Recent studies have revealed that current LVLMs are suffering from multi-modal hallucinations, where models provide responses misaligned with the corresponding visual information [104, 187, 297]. Such multi-modal hallucinations could cause unexpected behaviors when applying LVLMs to real-world scenarios, which therefore had to be further investigated and mitigated.

Li et al. [178] and Lovenia et al. [195] took the first step towards evaluating the object hallucinations in the LVLMs. Evaluations and experiments reveal that current LVLMs are prone to generate inconsistent responses with respect to the associated image, including non-existent objects, wrong object types, and attributes, incorrect semantic relationships, etc. [315, 361]. Furthermore, Liu et al.

[185], Zong et al. [395] and Liu et al. [184] show that LVLMs can be easily fooled and experience a severe performance drop due to their over-reliance on the strong language prior, as well as its inferior ability to defend against inappropriate user inputs [112, 134]. Jiang et al. [138], Wang et al.

[315] and Jing et al. [141] took a step forward to holistically evaluate multi-modal hallucination.

What's more, when presented with multiple images, LVLMs sometimes mix or miss parts of the visual context, as well as fail to understand temporal or logical connections between them, which might hinder their usage in many scenarios, yet properly identifying the reason for such disorders and tackling them still requires continued efforts. Despite the witnessed perception errors, LVLMs can generate flawed logical reasoning results even when correctly recognizing all visual elements, which remains further investigation.

Efforts have been made towards building a more robust large vision-language model. Gunjal et al.

[108], Lu et al. [196], Wang et al. [316], and Liu et al. [185] proposed to further finetune the model for producing more truthful and helpful responses. Another line of work chooses to post-hoc rectify the generated inconsistent content, such as [391], and [349], which introduced expert models. To free from the external tools, Leng et al. [162], Huang et al. [122], and Zhao et al. [379] tried to fully utilize the LVLM itself to alleviate hallucinations. Though proved to be effective, those methods usually require additional data annotations, visual experts, training phases, and computational costs, which prevent LVLMs from effectively scaling and generalizing to various fields. Thus, more universal approaches are expected to build a more reliable system, such as faithful and large-scale visual-text pre-training and alignment methods.

## 7.2 Understanding Knowledge Boundary In Llms

Despite the impressive capacity to capture factual knowledge from extensive data, LLMs still face challenges in recognizing their own knowledge boundaries. This shortfall leads to the occurrence of hallucinations, where LLMs confidently produce falsehoods without an awareness of their own knowledge limits [235, 261, 384]. Numerous studies delve into probing knowledge boundaries of LLMs, utilizing strategies such as evaluating the probability of a correct response in a multiplechoice setting [143], or quantifying the model's output uncertainty by evaluating the similarity among sets of sentences with uncertain meanings.

Furthermore, a line of work [13, 31, 172, 220] has revealed that LLMs contain latent structures within their activation space that relate to beliefs about truthfulness. Recent research [281] also found substantial evidence for LLMs' ability to encode the unanswerability of questions, despite the fact that these models exhibit overconfidence and produce hallucinations when presented with unanswerable questions. Nonetheless, Levinstein and Herrmann [163] have employed empirical and conceptual tools to probe whether or not LLMs have beliefs. Their empirical results suggest that current lie-detector methods for LLMs are not yet fully reliable, and the probing methods proposed by Burns et al. [31] and Azaria and Mitchell [13] do not adequately generalize. Consequently, whether we can effectively probe LLMs' internal beliefs is ongoing, requiring further research.

8
CONCLUSION
In this comprehensive survey, we have undertaken an in-depth examination of hallucinations within large language models, delving into the intricacies of their underlying causes, pioneering detection methodologies as well as related benchmarks, and effective mitigation strategies. Although significant strides have been taken, the conundrum of hallucination in LLMs remains a compelling and ongoing concern that demands continuous investigation. Moreover, we envision this survey as a guiding beacon for researchers dedicated to advancing robust information retrieval systems and trustworthy artificial intelligence. By navigating the complex landscape of hallucinations, we hope to empower these dedicated individuals with invaluable insights that drive the evolution of AI technologies toward greater reliability and safety.

## References

2303.09540
[2] Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2023. Evaluating
correctness and faithfulness of instruction-following models for question answering. *ArXiv preprint* abs/2307.16877
(2023). https://arxiv.org/abs/2307.16877
[3] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language Models Know When They're Hallucinating References? *ArXiv preprint* abs/2305.18248 (2023). https://arxiv.org/abs/2305.18248
[4] Perplexity AI. 2023. *Perplexity AI.* https://www.perplexity.ai/
[5] Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yun-Hsuan Sung. 2023. Characterizing
Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models. *ArXiv preprint* abs/2302.05578
(2023). https://arxiv.org/abs/2302.05578
[6] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad. 2022. A Review on
Language Models as Knowledge Bases. *CoRR* abs/2204.06031 (2022). https://doi.org/10.48550/ARXIV.2204.06031
arXiv:2204.06031
[7] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,
Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian
Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James
Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong
Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George
Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White,
Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub
Sygnowski, and et al. 2023. Gemini: A Family of Highly Capable Multimodal Models. *CoRR* abs/2312.11805 (2023).
https://doi.org/10.48550/ARXIV.2312.11805 arXiv:2312.11805
[8] Anthropic. 2023. *Claude.* https://claude.ai/
[9] Antropic. 2024. *Claude 3 haiku: our fastest model yet. 2024.* https://www.anthropic.com/news/claude-3-haiku
[10] ArXiv. 2023. *arxiv dataset*. https://www.kaggle.com/datasets/Cornell-University/arxiv/versions/134
[11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to Retrieve,
Generate, and Critique through Self-Reflection. *CoRR* abs/2310.11511 (2023). https://doi.org/10.48550/ARXIV.2310.
11511 arXiv:2310.11511
[12] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau
Yih. 2024. Reliable, Adaptable, and Attributable Language Models with Retrieval. *CoRR* abs/2403.03187 (2024).
https://doi.org/10.48550/ARXIV.2403.03187 arXiv:2403.03187
[13] Amos Azaria and Tom M. Mitchell. 2023. The Internal State of an LLM Knows When its Lying. ArXiv preprint
abs/2304.13734 (2023). https://arxiv.org/abs/2304.13734
[14] Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented Language Model Prompting for Zero-
Shot Knowledge Graph Question Answering. *ArXiv preprint* abs/2306.04136 (2023). https://arxiv.org/abs/2306.04136
[15] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng
Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of
ChatGPT on Reasoning, Hallucination, and Interactivity. *ArXiv preprint* abs/2302.04023 (2023). https://arxiv.org/abs/
2302.04023
[16] Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven
Failure Points When Engineering a Retrieval Augmented Generation System. *CoRR* abs/2401.05856 (2024). https:
//doi.org/10.48550/ARXIV.2401.05856 arXiv:2401.05856
[17] Mario Barrantes, Benedikt Herudek, and Richard Wang. 2020. Adversarial nli for factual correctness in text summarisation models. *ArXiv preprint* abs/2005.11739 (2020). https://arxiv.org/abs/2005.11739
[18] Pierre Basso. 1993. Conditional Causal Logic: A Formal Theory of the Meaning Generating Processes in a Cognitive
System. In Proceedings of the 13th International Joint Conference on Artificial Intelligence. Chambéry, France, August 28 - September 3, 1993, Ruzena Bajcsy (Ed.). Morgan Kaufmann, 845–851. http://ijcai.org/Proceedings/93-2/Papers/002.pdf
[19] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. ArXiv
preprint abs/2004.05150 (2020). https://arxiv.org/abs/2004.05150
[20] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of
Stochastic Parrots: Can Language Models Be Too Big?. In FAccT '21: 2021 ACM Conference on Fairness, Accountability,
and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, Madeleine Clare Elish, William Isaac, and
Richard S. Zemel (Eds.). ACM, 610–623. https://doi.org/10.1145/3442188.3445922
[21] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled Sampling for Sequence Prediction
with Recurrent Neural Networks. In Advances in Neural Information Processing Systems 28: Annual Conference on
Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, Corinna Cortes, Neil D.
Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (Eds.). 1171–1179. https://proceedings.neurips.cc/
paper/2015/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html
[22] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans.
2023. The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A". *ArXiv preprint* abs/2309.12288 (2023).
https://arxiv.org/abs/2309.12288
[23] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,
Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,
Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR
abs/2204.06745 (2022). https://doi.org/10.48550/ARXIV.2204.06745 arXiv:2204.06745
[24] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den
Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela
Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent
Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research,
Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR,
2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html
[25] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda
Askell, Andy Jones, Anna Chen, et al. 2022. Measuring progress on scalable oversight for large language models.
ArXiv preprint abs/2211.03540 (2022). https://arxiv.org/abs/2211.03540
[26] Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. The method of paired
comparisons. *Biometrika* 39, 3/4 (1952), 324–345. https://www.jstor.org/stable/2334029
[27] Ruben Branco, António Branco, João António Rodrigues, and João Ricardo Silva. 2021. Shortcutted Commonsense:
Data Spuriousness in Deep Learning of Commonsense Reasoning. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican
Republic, 1504–1521. https://doi.org/10.18653/v1/2021.emnlp-main.113
[28] Andrei Z Broder. 1997. On the resemblance and containment of documents. In Proceedings. Compression and Complexity
of SEQUENCES 1997 (Cat. No. 97TB100171). IEEE, 21–29.
[29] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html
[30] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023.
Sparks of Artificial General Intelligence: Early experiments with GPT-4. *ArXiv preprint* abs/2303.12712 (2023).
https://arxiv.org/abs/2303.12712
[31] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models
without supervision. *ArXiv preprint* abs/2212.03827 (2022). https://arxiv.org/abs/2212.03827
[32] Shulin Cao, Jiajie Zhang, Jiaxin Shi, Xin Lv, Zijun Yao, Qi Tian, Lei Hou, and Juanzi Li. 2023. Probabilistic Treeof-thought Reasoning for Answering Knowledge-intensive Complex Questions. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika
Bali (Eds.). Association for Computational Linguistics, 12541–12560. https://doi.org/10.18653/V1/2023.FINDINGS-
EMNLP.835
[33] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. 2023. A Comprehensive Survey
of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT. *CoRR* abs/2303.04226 (2023).
https://doi.org/10.48550/ARXIV.2303.04226 arXiv:2303.04226
[34] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022.
Quantifying memorization across neural language models. *ArXiv preprint* abs/2202.07646 (2022). https://arxiv.org/
abs/2202.07646
[35] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,
Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th
USENIX Security Symposium (USENIX Security 21). 2633–2650.
[36] Chung-Ching Chang, David Reitter, Renat Aksitov, and Yun-Hsuan Sung. 2023. KL-Divergence Guided Temperature
Sampling. *ArXiv preprint* abs/2306.01286 (2023). https://arxiv.org/abs/2306.01286
[37] Haw-Shiuan Chang, Zonghai Yao, Alolika Gon, Hong Yu, and Andrew McCallum. 2023. Revisiting the Architectures
like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond. In
Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers,
Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 12707–12730. https:
//doi.org/10.18653/V1/2023.FINDINGS-ACL.805
[38] Haw-Shiuan Chang and Andrew McCallum. 2022. Softmax Bottleneck Makes Language Models Unable to Represent
Multi-mode Word Distributions. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, 8048–8073. https:
//doi.org/10.18653/v1/2022.acl-long.554
[39] Canyu Chen and Kai Shu. 2023. Combating Misinformation in the Age of LLMs: Opportunities and Challenges. CoRR
abs/2311.05656 (2023). https://doi.org/10.48550/ARXIV.2311.05656 arXiv:2311.05656
[40] Hung-Ting Chen, Michael J. Q. Zhang, and Eunsol Choi. 2022. Rich Knowledge Sources Bring Complex Knowledge
Conflicts: Recalibrating Models to Reflect Conflicting Evidence. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav
Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 2292–2307. https:
//doi.org/10.18653/V1/2022.EMNLP-MAIN.146
[41] Hung-Ting Chen, Fangyuan Xu, Shane A Arora, and Eunsol Choi. 2023. Understanding Retrieval Augmentation for
Long-Form Question Answering. *ArXiv preprint* abs/2310.12150 (2023). https://arxiv.org/abs/2310.12150
[42] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. 2023. FELM:
Benchmarking Factuality Evaluation of Large Language Models. *ArXiv preprint* abs/2310.00741. https://arxiv.org/
abs/2310.00741
[43] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu.
2023. Dense X Retrieval: What Retrieval Granularity Should We Use? *CoRR* abs/2312.06648 (2023). https://doi.org/10.
48550/ARXIV.2312.06648 arXiv:2312.06648
[44] Xiaoyang Chen, Ben He, Hongyu Lin, Xianpei Han, Tianshu Wang, Boxi Cao, Le Sun, and Yingfei Sun. 2024. Spiral of
Silence: How is Large Language Model Killing Information Retrieval? - A Case Study on Open Domain Question
Answering. arXiv:2404.10496 [cs.IR] https://arxiv.org/abs/2404.10496
[45] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. 2022. Towards Improving Faithfulness in Abstractive
Summarization. In *NeurIPS*. http://papers.nips.cc/paper_files/paper/2022/hash/9b6d7202750e8e32cd5270eb7fc131f7-
Abstract-Conference.html
[46] Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. 2023. Improving Translation Faithfulness
of Large Language Models via Augmenting Instructions. *ArXiv preprint* abs/2308.12674 (2023). https://arxiv.org/abs/
2308.12674
[47] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2023. Measuring and Improving Chain-of-
Thought Reasoning in Vision-Language Models. *ArXiv preprint* abs/2309.04461 (2023). https://arxiv.org/abs/2309.04461
[48] Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan,
and Xipeng Qiu. 2024. Unified Active Retrieval for Retrieval Augmented Generation. *CoRR* abs/2406.12534 (2024).
https://doi.org/10.48550/ARXIV.2406.12534 arXiv:2406.12534
[49] Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu
Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu. 2023. Evaluating Hallucinations in Chinese Large Language Models.
arXiv:2310.03368 [cs.CL]
[50] I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei
Liu, et al. 2023. FacTool: Factuality Detection in Generative AI–A Tool Augmented Framework for Multi-Task and
Multi-Domain Scenarios. *ArXiv preprint* abs/2307.13528 (2023). https://arxiv.org/abs/2307.13528
[51] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations?
ArXiv preprint abs/2305.01937 (2023). https://arxiv.org/abs/2305.01937
[52] David Chiang and Peter Cholak. 2022. Overcoming a Theoretical Limitation of Self-Attention. In Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics, Dublin, Ireland, 7654–7664. https://doi.org/10.18653/v1/2022.acl-long.527
[53] Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. 2023. KCTS: Knowledge-Constrained Tree Search
Decoding with Token-Level Hallucination Detection. *ArXiv preprint* abs/2310.09044 (2023). https://arxiv.org/abs/
2310.09044
[54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua
Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,
Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,
Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan
Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai,
Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,
Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling Language Modeling with Pathways. J. Mach. Learn.
Res. 24 (2023), 240:1–240:113. http://jmlr.org/papers/v24/22-1144.html
[55] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement
Learning from Human Preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 4299–4307.
https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html
[56] Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, and
Bing Qin. 2024. BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question
Answering. arXiv:2406.19820 [cs.CL] https://arxiv.org/abs/2406.19820
[57] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing
Qin, and Ting Liu. 2023. A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. ArXiv preprint
abs/2309.15402 (2023). https://arxiv.org/abs/2309.15402
[58] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin,
and Ting Liu. 2023. A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. *CoRR* abs/2309.15402
(2023). https://doi.org/10.48550/ARXIV.2309.15402 arXiv:2309.15402
[59] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding
by contrasting layers improves factuality in large language models. *ArXiv preprint* abs/2309.03883 (2023). https:
//arxiv.org/abs/2309.03883
[60] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. *ArXiv preprint* abs/2210.11416
(2022). https://arxiv.org/abs/2210.11416
[61] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry
Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve
Math Word Problems. *CoRR* abs/2110.14168 (2021). arXiv:2110.14168 https://arxiv.org/abs/2110.14168
[62] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs LM: Detecting Factual Errors via Cross
Examination. *ArXiv preprint* abs/2305.13281 (2023). https://arxiv.org/abs/2305.13281
[63] Together Computer. 2023. *RedPajama: an Open Dataset for Training Large Language Models*. https://github.com/
togethercomputer/RedPajama-Data
[64] Ajeya Cotra. 2021. Why AI alignment could be hard with modern deep learning. https://www.cold-takes.com/whyai-alignment-could-be-hard-with-modern-deep-learning/ Cold Takes.
[65] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek,
Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. CoRR
abs/2401.14887 (2024). https://doi.org/10.48550/ARXIV.2401.14887 arXiv:2401.14887
[66] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge Neurons in Pretrained
Transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, Dublin, Ireland, 8493–8502. https://doi.org/10.18653/v1/2022.acllong.581
[67] Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She, and Zhifang Sui. 2022. Neural knowledge bank
for pretrained transformers. *ArXiv preprint* abs/2208.00399 (2022). https://arxiv.org/abs/2208.00399
[68] Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, and Jun Xu. 2023. LLMs may
Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. *CoRR* abs/2310.20501
(2023). https://doi.org/10.48550/ARXIV.2310.20501 arXiv:2310.20501
[69] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne
Liu. 2020. Plug and Play Language Models: A Simple Approach to Controlled Text Generation. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https:
//openreview.net/forum?id=H1edEyBKDS
[70] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
Online and Punta Cana, Dominican Republic, 6491–6506. https://doi.org/10.18653/v1/2021.emnlp-main.522
[71] Maria Angels de Luis Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estevão Filho, Todd
Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha,
Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra. 2024. RAG vs Fine-tuning: Pipelines,
Tradeoffs, and a Case Study on Agriculture. *CoRR* abs/2401.08406 (2024). https://doi.org/10.48550/ARXIV.2401.08406
arXiv:2401.08406
[72] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi
Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. 2023. Language Modeling Is Compression.
ArXiv preprint abs/2309.10668 (2023). https://arxiv.org/abs/2309.10668
[73] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association
for Computational Linguistics, 4171–4186. https://doi.org/10.18653/V1/N19-1423
[74] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston.
2023. Chain-of-Verification Reduces Hallucination in Large Language Models. *ArXiv preprint* abs/2309.11495 (2023).
https://arxiv.org/abs/2309.11495
[75] Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve Only When It Needs:
Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models. *CoRR* abs/2402.10612
(2024). https://doi.org/10.48550/ARXIV.2402.10612 arXiv:2402.10612
[76] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023. BAMBOO: A Comprehensive Benchmark
for Evaluating Long Text Modeling Capacities of Large Language Models. *ArXiv preprint* abs/2309.13345 (2023).
https://arxiv.org/abs/2309.13345
[77] Esin Durmus, He He, and Mona Diab. 2020. FEQA: A Question Answering Evaluation Framework for Faithfulness
Assessment in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 5055–5070. https://doi.org/10.18653/v1/2020.aclmain.454
[78] Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose. 2021. Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. In Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic,
2197–2214. https://doi.org/10.18653/v1/2021.emnlp-main.168
[79] Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems:
The begin benchmark. *ArXiv preprint* abs/2105.00071 (2021). https://arxiv.org/abs/2105.00071
[80] Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QA-Based Factual
Consistency Evaluation for Summarization. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics,
Seattle, United States, 2587–2601. https://doi.org/10.18653/v1/2022.naacl-main.187
[81] Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021.
SummEval: Re-evaluating Summarization Evaluation. *Transactions of the Association for Computational Linguistics* 9
(2021), 391–409. https://doi.org/10.1162/tacl_a_00373
[82] Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking Generated
Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,
Florence, Italy, 2214–2220. https://doi.org/10.18653/v1/P19-1213
[83] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long Form
Question Answering. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.
Association for Computational Linguistics, Florence, Italy, 3558–3567. https://doi.org/10.18653/v1/P19-1346
[84] Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical Neural Story Generation. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics, Melbourne, Australia, 889–898. https://doi.org/10.18653/v1/P18-1082
[85] Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, and Qianli
Ma. 2023. Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension
and Embellishment Abilities of LLMs. *CoRR* abs/2310.19347 (2023).
https://doi.org/10.48550/ARXIV.2310.19347
arXiv:2310.19347
[86] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Cook: Empowering general-purpose language models with modular and collaborative knowledge. arXiv preprint arXiv:2305.09955
(2023).
[87] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023. Retrieval-Generation Synergy
Augmented Large Language Models. *ArXiv preprint* abs/2310.05149 (2023). https://arxiv.org/abs/2310.05149
[88] Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, and Mirella
Lapata. 2024. Learning to Plan and Generate Text with Citations. *CoRR* abs/2404.03381 (2024). https://doi.org/10.
48550/ARXIV.2404.03381 arXiv:2404.03381
[89] Katja Filippova. 2020. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In Findings of the
Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 864–870.
https://doi.org/10.18653/v1/2020.findings-emnlp.76
[90] Robert Friel and Atindriyo Sanyal. 2023.
Chainpoll: A high efficacy method for LLM hallucination detection.
arXiv:2310.18344 [cs.CL]
[91] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.
GPTScore: Evaluate as You Desire.
CoRR
abs/2302.04166 (2023). https://doi.org/10.48550/ARXIV.2302.04166 arXiv:2302.04166
[92] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a Bayesian Approximation: Representing Model Uncertainty in
Deep Learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City,
NY, USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48), Maria-Florina Balcan and Kilian Q.
Weinberger (Eds.). JMLR.org, 1050–1059. http://proceedings.mlr.press/v48/gal16.html
[93] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish
Thite, Noa Nabeshima, et al. 2021. The pile: An 800gb dataset of diverse text for language modeling. ArXiv preprint
abs/2101.00027 (2021). https://arxiv.org/abs/2101.00027
[94] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni
Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. RARR: Researching and Revising What Language Models
Say, Using Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki
Okazaki (Eds.). Association for Computational Linguistics, 16477–16508. https://aclanthology.org/2023.acl-long.910
[95] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization
evaluation with chatgpt. *ArXiv preprint* abs/2304.02554 (2023). https://arxiv.org/abs/2304.02554
[96] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings.
CoRR abs/2104.08821 (2021). arXiv:2104.08821 https://arxiv.org/abs/2104.08821
[97] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-REC: Towards
Interactive and Explainable LLMs-Augmented Recommender System. *CoRR* abs/2303.14524 (2023). https://doi.org/10.
48550/ARXIV.2303.14524 arXiv:2303.14524
[98] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does
Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? *CoRR* abs/2405.05904 (2024). https://doi.org/10.
48550/ARXIV.2405.05904 arXiv:2405.05904
[99] Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing The Factual Accuracy of Generated
Text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD
2019, Anchorage, AK, USA, August 4-8, 2019, Ankur Teredesai, Vipin Kumar, Ying Li, Rómer Rosales, Evimaria Terzi,
and George Karypis (Eds.). ACM, 166–175. https://doi.org/10.1145/3292500.3330955
[100] Google. 2023. *Bard.* https://bard.google.com/
[101] Tanya Goyal and Greg Durrett. 2020. Evaluating Factuality in Generation with Dependency-level Entailment. In
Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics,
Online, 3592–3603. https://doi.org/10.18653/v1/2020.findings-emnlp.322
[102] Tanya Goyal and Greg Durrett. 2021. Annotating and Modeling Fine-grained Factuality in Summarization. In
Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational Linguistics, Online, 1449–1462. https://doi.org/10.
18653/v1/2021.naacl-main.114
[103] Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Jiaming Wu, Heng Gong, and Bing Qin. 2022. Improving Controllable Text
Generation with Position-Aware Weighted Decoding. In Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, Dublin, Ireland, 3449–3467. https://doi.org/10.18653/v1/2022.findingsacl.272
[104] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang,
Yaser Yacoob, et al. 2023. Hallusionbench: An advanced diagnostic suite for entangled language hallucination &
visual illusion in large vision-language models. *arXiv preprint arXiv:2310.14566* (2023).
[105] Nuno Miguel Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and
André F. T. Martins. 2023. Hallucinations in Large Multilingual Translation Models. *ArXiv preprint* abs/2303.16104
(2023). https://arxiv.org/abs/2303.16104
[106] Nuno M. Guerreiro, Elena Voita, and André Martins. 2023. Looking for a Needle in a Haystack: A Comprehensive
Study of Hallucinations in Neural Machine Translation. In Proceedings of the 17th Conference of the European Chapter
of the Association for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia,
1059–1075. https://aclanthology.org/2023.eacl-main.75
[107] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan
Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. ArXiv preprint
abs/2306.11644 (2023). https://arxiv.org/abs/2306.11644
[108] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2023. Detecting and preventing hallucinations in large vision language
models. *ArXiv preprint* abs/2308.06394 (2023). https://arxiv.org/abs/2308.06394
[109] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval Augmented Language
Model Pre-Training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 3929–3938. http://proceedings.mlr.
press/v119/guu20a.html
[110] Bikash Gyawali, Lucas Anastasiou, and Petr Knoth. 2020. Deduplication of Scholarly Documents using Locality
Sensitive Hashing and Word Embeddings. In *Proceedings of the Twelfth Language Resources and Evaluation Conference*.
European Language Resources Association, Marseille, France, 901–910. https://aclanthology.org/2020.lrec-1.113
[111] Michael Hahn. 2020. Theoretical Limitations of Self-Attention in Neural Sequence Models. Transactions of the
Association for Computational Linguistics 8 (2020), 156–171. https://doi.org/10.1162/tacl_a_00306
[112] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang. 2024. The
Instinctive Bias: Spurious Images lead to Hallucination in MLLMs. *arXiv preprint arXiv:2402.03757* (2024).
[113] Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model
inference. *ArXiv preprint* abs/2301.00303 (2023). https://arxiv.org/abs/2301.00303
[114] Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin
Zhang, Zejian Xie, and Jiaxing Zhang. 2023. Never Lost in the Middle: Improving Large Language Models via Attention
Strengthening Question Answering. *CoRR* abs/2311.09198 (2023).
https://doi.org/10.48550/ARXIV.2311.09198
arXiv:2311.09198
[115] Peter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, and Daniel E. Ho.
2022. Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed,
A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.).
http://papers.nips.cc/paper_files/paper/2022/hash/
bc218a0c656e49d4b086975a9c785f47-Abstract-Datasets_and_Benchmarks.html
[116] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
Measuring Massive Multitask Language Understanding. In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=d7KBjmI3GmQ
[117] Evan Hernandez, Belinda Z Li, and Jacob Andreas. 2023. Inspecting and editing knowledge representations in language
models. *ArXiv preprint* abs/2304.00740 (2023). https://arxiv.org/abs/2304.00740
[118] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case of Neural Text Degeneration.
In *8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
OpenReview.net. https://openreview.net/forum?id=rygGQyrFvH
[119] Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. 𝑄2: Evaluating
Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics, Online and Punta Cana, Dominican Republic, 7856–7870.
https://doi.org/10.18653/v1/2021.emnlpmain.619
[120] Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng,
Duyu Tang, Dandan Tu, and Bing Qin. 2024. Learning Fine-Grained Grounded Citations for Attributed Large Language
Models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,
August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,
14095–14113. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.838
[121] Lei Huang, Xiaocheng Feng, Weitao Ma, Liang Zhao, Yuchun Fan, Weihong Zhong, Dongliang Xu, Qing Yang,
Hongtao Liu, and Bing Qin. 2024.
Advancing Large Language Model Attribution through Self-Improving.
arXiv:2410.13298 [cs.CL] https://arxiv.org/abs/2410.13298
[122] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and
Nenghai Yu. 2023. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and
retrospection-allocation. *arXiv preprint arXiv:2311.17911* (2023).
[123] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan
Mohammed, Qiang Liu, et al. 2023. Language is not all you need: Aligning perception with language models. ArXiv
preprint abs/2302.14045 (2023). https://arxiv.org/abs/2302.14045
[124] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,
Yikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation
models. *ArXiv preprint* abs/2305.08322 (2023). https://arxiv.org/abs/2305.08322
[125] Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng, and Bing Qin. 2021. The Factual Inconsistency Problem in
Abstractive Text Summarization: A Survey. *CoRR* abs/2104.14839 (2021). arXiv:2104.14839 https://arxiv.org/abs/2104.
14839
[126] Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng, and Bing Qin. 2021. The Factual Inconsistency Problem in
Abstractive Text Summarization: A Survey. *ArXiv preprint* abs/2104.14839 (2021). https://arxiv.org/abs/2104.14839
[127] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. 2023. Transformer-Patcher:
One Mistake Worth One Neuron. In The Eleventh International Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/pdf?id=4oYUGeGBPm
[128] Siqing Huo, Negar Arabzadeh, and Charles L. A. Clarke. 2023. Retrieving Supporting Evidence for LLMs Generated
Answers. *ArXiv preprint* abs/2306.13781 (2023). https://arxiv.org/abs/2306.13781
[129] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu
Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through
the lens of generalization. *ArXiv preprint* abs/2212.12017 (2022). https://arxiv.org/abs/2212.12017
[130] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard
Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. *Trans. Mach. Learn. Res.* 2022
(2022). https://openreview.net/forum?id=jKN1pXi7b0
[131] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,
Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented
Language Models. *J. Mach. Learn. Res.* 24 (2023), 251:1–251:43. http://jmlr.org/papers/v24/23-0037.html
[132] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query Expansion by Prompting Large Language Models. *CoRR* abs/2305.03653 (2023). https://doi.org/10.48550/ARXIV.2305.03653 arXiv:2305.03653
[133] Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig,
and Chunting Zhou. 2023. Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. ArXiv
preprint abs/2306.01200 (2023). https://arxiv.org/abs/2306.01200
[134] Joonhyun Jeong. 2023. Hijacking Context in Large Multi-modal Models. *arXiv preprint arXiv:2312.07553* (2023).
[135] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. 2024. Adaptive-RAG: Learning to
Adapt Retrieval-Augmented Large Language Models through Question Complexity. *CoRR* abs/2403.14403 (2024).
https://doi.org/10.48550/ARXIV.2403.14403 arXiv:2403.14403
[136] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and
Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. *ACM Comput. Surv.* 55, 12 (2023),
248:1–248:38. https://doi.org/10.1145/3571730
[137] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards Mitigating Hallucination
in Large Language Models via Self-Reflection. *ArXiv preprint* abs/2310.06271 (2023). https://arxiv.org/abs/2310.06271
[138] Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. 2024.
Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models.
arXiv preprint arXiv:2402.15721 (2024).
[139] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LLMLingua: Compressing Prompts for
Accelerated Inference of Large Language Models. arXiv:2310.05736 [cs.CL]
[140] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and
Graham Neubig. 2023. Active Retrieval Augmented Generation. *ArXiv preprint* abs/2305.06983 (2023).
https:
//arxiv.org/abs/2305.06983
[141] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. 2023. Faithscore: Evaluating hallucinations in
large vision-language models. *arXiv preprint arXiv:2311.01477* (2023).
[142] Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, and Min Zhang. 2024. When Large
Language Models Meet Vector Databases: A Survey. *CoRR* abs/2402.01763 (2024). https://doi.org/10.48550/ARXIV.
2402.01763 arXiv:2402.01763
[143] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac
Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know.
ArXiv preprint abs/2207.05221 (2022). https://arxiv.org/abs/2207.05221
[144] Greg Kamradt. 2024. *The 5 Levels Of Text Splitting For Retrieval*. youtube. https://www.youtube.com/watch?v=
8OJC21T2SL4
[145] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large Language Models
Struggle to Learn Long-Tail Knowledge. In International Conference on Machine Learning, ICML 2023, 23-29 July
2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 15696–15707.
https:
//proceedings.mlr.press/v202/kandpal23a.html
[146] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau
Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online,
6769–6781. https://doi.org/10.18653/v1/2020.emnlp-main.550
[147] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie
Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 6769–6781. https:
//doi.org/10.18653/V1/2020.EMNLP-MAIN.550
[148] Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A
Smith, Yejin Choi, and Kentaro Inui. 2022. RealTime QA: What's the Answer Right Now? *ArXiv preprint* abs/2207.13332
(2022). https://arxiv.org/abs/2207.13332
[149] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. Gpt-4 passes the bar exam.
Available at SSRN 4389233 (2023).
https://www.datascienceassn.org/sites/default/files/GPT-4%20Passes%20the%
20Bar%20Exam.pdf
[150] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through
Memorization: Nearest Neighbor Language Models. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=HklBjCEKvH
[151] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models
are zero-shot reasoners. *Advances in neural information processing systems* 35 (2022), 22199–22213.
[152] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency
of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computational Linguistics, Online, 9332–9346. https://doi.org/10.18653/v1/2020.
emnlp-main.750
[153] Philippe Laban, Wojciech Kryściński, Divyansh Agarwal, Alexander R Fabbri, Caiming Xiong, Shafiq Joty, and
Chien-Sheng Wu. 2023. LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. ArXiv preprint
abs/2305.14540 (2023). https://arxiv.org/abs/2305.14540
[154] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-Visiting NLI-based Models
for Inconsistency Detection in Summarization. *Transactions of the Association for Computational Linguistics* 10 (2022),
163–177. https://doi.org/10.1162/tacl_a_00453
[155] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto.
2023. When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In
Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. Association
for Computational Linguistics, Dubrovnik, Croatia, 3206–3219. https://aclanthology.org/2023.eacl-main.234
[156] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 6402–6413.
https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html
[157] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li,
Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph,
Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang,
Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan,
Jan Brauner, Samuel R. Bowman, and Ethan Perez. 2023. Measuring Faithfulness in Chain-of-Thought Reasoning.
CoRR abs/2307.13702 (2023). https://doi.org/10.48550/ARXIV.2307.13702 arXiv:2307.13702
[158] Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023. Fast and Accurate Factual Inconsistency
Detection Over Long Documents. arXiv:2310.13189 [cs.CL]
[159] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas
Carlini. 2022. Deduplicating Training Data Makes Language Models Better. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,
Dublin, Ireland, 8424–8445. https://doi.org/10.18653/v1/2022.acl-long.577
[160] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022.
Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing
Systems 35 (2022), 34586–34599.
[161] Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al. 2023. Chain of Natural Language
Inference for Reducing Large Language Model Ungrounded Hallucinations. *ArXiv preprint* abs/2310.03951 (2023).
https://arxiv.org/abs/2310.03951
[162] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint
arXiv:2311.16922 (2023).
[163] BA Levinstein and Daniel A Herrmann. 2023. Still No Lie Detector for Language Models: Probing Empirical and
Conceptual Roadblocks. *ArXiv preprint* abs/2307.00175 (2023). https://arxiv.org/abs/2307.00175
[164] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,
and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, Online, 7871–7880.
https://doi.org/10.18653/v1/2020.aclmain.703
[165] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle,
Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.
cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html
[166] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X. Yu, and Sanjiv Kumar.
2023. Large Language Models with Controllable Working Memory. In Findings of the Association for Computational
Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki
(Eds.). Association for Computational Linguistics, 1774–1793. https://doi.org/10.18653/v1/2023.findings-acl.112
[167] Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. 2024. Towards Faithful Chain-of-Thought: Large
Language Models are Bridging Reasoners. *CoRR* abs/2405.18915 (2024). https://doi.org/10.48550/ARXIV.2405.18915
arXiv:2405.18915
[168] Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. The Dawn
After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. *CoRR* abs/2401.03205
(2024). https://doi.org/10.48550/ARXIV.2401.03205 arXiv:2401.03205
[169] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination
Evaluation Benchmark for Large Language Models. *CoRR* abs/2305.11747 (2023). https://doi.org/10.48550/ARXIV.
2305.11747 arXiv:2305.11747
[170] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. *ArXiv preprint* abs/2301.12597 (2023). https://arxiv.org/abs/2301.
12597
[171] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: A Generative
Framework for Personalized Recommendation and User Interests Interpretation. In Proceedings of the 2023 SIGIR
Workshop on eCommerce co-located with the 46th International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR 2023), Taipei, Taiwan, July 27, 2023 (CEUR Workshop Proceedings, Vol. 3589), Surya
Kallumadi, Yubin Kim, Tracy Holloway King, Shervin Malmasi, Maarten de Rijke, and Jacopo Tagliabue (Eds.).
CEUR-WS.org. https://ceur-ws.org/Vol-3589/paper_2.pdf
[172] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inference-Time Intervention:
Eliciting Truthful Answers from a Language Model. *ArXiv preprint* abs/2306.03341 (2023). https://arxiv.org/abs/2306.
03341
[173] Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024. Nearest
Neighbor Speculative Decoding for LLM Generation and Attribution. *CoRR* abs/2405.19325 (2024). https://doi.org/10.
48550/ARXIV.2405.19325 arXiv:2405.19325
[174] Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. 2022. Faithfulness in Natural Language
Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods. *ArXiv preprint* abs/2203.05227
(2022). https://arxiv.org/abs/2203.05227
[175] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and
Mike Lewis. 2022. Contrastive decoding: Open-ended text generation as optimization. *ArXiv preprint* abs/2210.15097
(2022). https://arxiv.org/abs/2210.15097
[176] Yucheng Li. 2023. Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-
Information-Based Content Filtering. arXiv:2304.12102 [cs.CL]
[177] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks
Are All You Need II: phi-1.5 technical report. *ArXiv preprint* abs/2309.05463 (2023). https://arxiv.org/abs/2309.05463
[178] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating Object Hallucination
in Large Vision-Language Models. https://arxiv.org/abs/2305.10355
[179] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. 2023. ChatDoctor: A Medical Chat Model Fine-tuned
on LLaMA Model using Medical Domain Knowledge. *ArXiv preprint* abs/2303.14070 (2023). https://arxiv.org/abs/
2303.14070
[180] Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. 2023. BatGPT: A Bidirectional Autoregessive Talker
from Generative Pre-trained Transformer. *ArXiv preprint* abs/2307.00360 (2023). https://arxiv.org/abs/2307.00360
[181] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches
Out. Association for Computational Linguistics, Barcelona, Spain, 74–81. https://aclanthology.org/W04-1013
[182] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods.
In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*.
Association for Computational Linguistics, Dublin, Ireland, 3214–3252. https://doi.org/10.18653/v1/2022.acl-long.229
[183] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023. Exposing Attention Glitches
with Flip-Flop Language Modeling. *ArXiv preprint* abs/2306.00946 (2023). https://arxiv.org/abs/2306.00946
[184] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023.
HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark
Challenging for GPT-4V(Ision), LLaVA-1.5, and Other Multi-Modality Models. https://arxiv.org/abs/2310.14566
[185] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating Hallucination in
Large Multi-Modal Models via Robust Instruction Tuning. arXiv:2306.14565 [cs.CV]
[186] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. ArXiv preprint
abs/2304.08485 (2023). https://arxiv.org/abs/2304.08485
[187] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei
Peng. 2024. A survey on hallucination in large vision-language models. *arXiv preprint arXiv:2402.00253* (2024).
[188] Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023. TCRA-LLM: Token Compression Retrieval
Augmented Large Language Model for Inference Cost Reduction. In Findings of the Association for Computational
Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association
for Computational Linguistics, 9796–9810. https://aclanthology.org/2023.findings-emnlp.655
[189] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
2023. Lost in the Middle: How Language Models Use Long Contexts. *ArXiv preprint* abs/2307.03172 (2023). https:
//arxiv.org/abs/2307.03172
[190] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation
using gpt-4 with better human alignment. *ArXiv preprint* abs/2303.16634 (2023). https://arxiv.org/abs/2303.16634
[191] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,
and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. *CoRR* abs/1907.11692
(2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692
[192] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language
Models' Alignment. *ArXiv preprint* abs/2308.05374 (2023). https://arxiv.org/abs/2308.05374
[193] Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023. Instruction Position Matters in Sequence Generation
with Large Language Models. *ArXiv preprint* abs/2308.12097 (2023). https://arxiv.org/abs/2308.12097
[194] Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-
Based Knowledge Conflicts in Question Answering. In Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021,
Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational
Linguistics, 7052–7063. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.565
[195] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. 2023. Negative Object Presence
Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models. https://arxiv.org/abs/2310.05338
[196] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. 2023.
Evaluation and Mitigation of Agnosia in Multimodal Large Language Models. *ArXiv preprint* abs/2309.04041 (2023).
https://arxiv.org/abs/2309.04041
[197] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou.
2020. Univl: A unified video and language pre-training model for multimodal understanding and generation. ArXiv
preprint abs/2002.06353 (2020). https://arxiv.org/abs/2002.06353
[198] Junyu Luo, Cao Xiao, and Fenglong Ma. 2023. Zero-Resource Hallucination Prevention for Large Language Models.
ArXiv preprint abs/2309.02654 (2023). https://arxiv.org/abs/2309.02654
[199] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for text
summarization.
[200] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting for Retrieval-Augmented
Large Language Models. *CoRR* abs/2305.14283 (2023). https://doi.org/10.48550/ARXIV.2305.14283 arXiv:2305.14283
[201] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-ChatGPT: Towards
Detailed Video Understanding via Large Vision and Language Models. *ArXiv preprint* abs/2306.05424 (2023). https:
//arxiv.org/abs/2306.05424
[202] Fiona Macpherson and Dimitris Platchias. 2013.
Hallucination: Philosophy and psychology.
MIT
Press. https://books.google.com/books?hl=zh-CN&lr=&id=_bwtAAAAQBAJ&oi=fnd&pg=PR5&dq=Hallucination:
+Philosophy+and+psychology&ots=2E62kf7_yC&sig=rH9HGXYacNkxOJNMVbw514aChZo
[203] Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2023. ExpertQA: Expert-
Curated Questions and Attributed Answers. arXiv:2309.07852 [cs.CL]
[204] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to
Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational
Linguistics, 9802–9822. https://doi.org/10.18653/v1/2023.acl-long.546
[205] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination
Detection for Generative Large Language Models. *ArXiv preprint* abs/2303.08896 (2023). https://arxiv.org/abs/2303.
08896
[206] Udi Manber and Gene Myers. 1993. Suffix arrays: a new method for on-line string searches. siam Journal on Computing
22, 5 (1993), 935–948.
[207] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and
Ningyu Zhang. 2024. RaFe: Ranking Feedback Improves Query Rewriting for RAG. *CoRR* abs/2405.14431 (2024).
https://doi.org/10.48550/ARXIV.2405.14431 arXiv:2405.14431
[208] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factuality in
Abstractive Summarization. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*.
Association for Computational Linguistics, Online, 1906–1919. https://doi.org/10.18653/v1/2020.acl-main.173
[209] Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If beam search is the answer, what was the question?. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for
Computational Linguistics, Online, 2173–2185. https://doi.org/10.18653/v1/2020.emnlp-main.170
[210] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and Editing Factual Associations in
GPT. In *NeurIPS*. http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-
Conference.html
[211] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. 2023. Mass-Editing Memory in
a Transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May
1-5, 2023. OpenReview.net. https://openreview.net/pdf?id=MkbcAHIYgyS
[212] Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, and Jie Zhou. 2021. Prevent the Language Model from
being Overconfident in Neural Machine Translation. In Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers). Association for Computational Linguistics, Online, 3456–3468. https://doi.org/10.18653/v1/2021.acl-long.268
[213] Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step
reasoning. *ArXiv preprint* abs/2308.00436 (2023). https://arxiv.org/abs/2308.00436
[214] Microsoft. 2023. *New Bing.* https://www.bing.com/new
[215] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. 2023.
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. *CoRR* abs/2308.04430 (2023).
https:
//doi.org/10.48550/ARXIV.2308.04430 arXiv:2308.04430
[216] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer,
and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text
Generation. *ArXiv preprint* abs/2305.14251 (2023). https://arxiv.org/abs/2305.14251
[217] Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, and Kartik Talamadupula. 2021. Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text
Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 1322–1336.
https://doi.org/10.18653/v1/2021.naacl-main.104
[218] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. 2022. Fast Model Editing at
Scale. In *The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022*.
OpenReview.net. https://openreview.net/forum?id=0DcZxeWfOPt
[219] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. 2022. Memory-Based Model
Editing at Scale. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA
(Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári,
Gang Niu, and Sivan Sabato (Eds.). PMLR, 15817–15831. https://proceedings.mlr.press/v162/mitchell22a.html
[220] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola.
2022. Relative representations enable zero-shot latent space communication. *ArXiv preprint* abs/2209.15430 (2022).
https://arxiv.org/abs/2209.15430
[221] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela.
2024. Generative Representational Instruction Tuning. *CoRR* abs/2402.09906 (2024). https://doi.org/10.48550/ARXIV.
2402.09906 arXiv:2402.09906
[222] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding Benchmark.
In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023,
Dubrovnik, Croatia, May 2-6, 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational
Linguistics, 2006–2029. https://doi.org/10.18653/V1/2023.EACL-MAIN.148
[223] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown,
Amnon Shashua, and Yoav Shoham. 2023. Generating benchmarks for factuality evaluation of language models.
ArXiv preprint abs/2307.06908 (2023). https://arxiv.org/abs/2307.06908
[224] Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikrishna Karanam, and
Sumit Shekhar. 2023. A Neural CRF-based Hierarchical Approach for Linear Text Segmentation. In Findings of the
Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, Andreas Vlachos and Isabelle
Augenstein (Eds.). Association for Computational Linguistics, 853–863. https://doi.org/10.18653/V1/2023.FINDINGS-
EACL.65
[225] Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen
McKeown, and Bing Xiang. 2021. Entity-level Factual Consistency of Abstractive Text Summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Association
for Computational Linguistics, Online, 2727–2733. https://doi.org/10.18653/v1/2021.eacl-main.235
[226] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, and Shomir Wilson. 2023.
Nationality Bias in Text Generation. In Proceedings of the 17th Conference of the European Chapter of the Association
for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia, 116–122.
https:
//aclanthology.org/2023.eacl-main.9
[227] Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, and Omri Abend. 2023. DisentQA:
Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational
Linguistics, 10056–10070. https://doi.org/10.18653/V1/2023.ACL-LONG.559
[228] Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. When Do LLMs Need Retrieval Augmentation? Mitigating
LLMs' Overconfidence Helps Retrieval Augmentation. *CoRR* abs/2402.11457 (2024). https://doi.org/10.48550/ARXIV.
2402.11457 arXiv:2402.11457
[229] Sean O'Brien and Mike Lewis. 2023. Contrastive Decoding Improves Reasoning in Large Language Models. ArXiv
preprint abs/2309.09117 (2023). https://arxiv.org/abs/2309.09117
[230] Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity Cloze By Date: What LMs Know
About Unseen Entities. In *Findings of the Association for Computational Linguistics: NAACL 2022*. Association for
Computational Linguistics, Seattle, United States, 693–702. https://doi.org/10.18653/v1/2022.findings-naacl.52
[231] OpenAI. 2022. *Introducing chatgpt.* https://openai.com/blog/chatgpt [232] OpenAI. 2023. GPT-4 Technical Report. *ArXiv preprint* abs/2303.08774 (2023). https://arxiv.org/abs/2303.08774
[233] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In *NeurIPS*.
http://papers.nips.cc/paper_files/paper/2022/hash/
b1efde53be364a73914f58805a001731-Abstract-Conference.html
[234] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. *CoRR* abs/2312.05934 (2023). https://doi.org/10.48550/ARXIV.2312.05934 arXiv:2312.05934
[235] Lorenzo Pacchiardi, Alex J Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal, Owain Evans, and Jan
Brauner. 2023. How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. ArXiv
preprint abs/2309.15840 (2023). https://arxiv.org/abs/2309.15840
[236] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive
Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for
Computational Linguistics, Online, 4812–4829. https://doi.org/10.18653/v1/2021.naacl-main.383
[237] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically
correcting large language models: Surveying the landscape of diverse self-correction strategies. ArXiv preprint
abs/2308.03188 (2023). https://arxiv.org/abs/2308.03188
[238] Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, and Le Sun. 2024. Not All
Contexts Are Equal: Teaching LLMs Credibility-aware Generation. *CoRR* abs/2404.06809 (2024). https://doi.org/10.
48550/ARXIV.2404.06809 arXiv:2404.06809
[239] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation
of Machine Translation. In *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*.
Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311–318.
https://doi.org/10.3115/
1073083.1073135
[240] Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan
Das. 2020. ToTTo: A Controlled Table-To-Text Generation Dataset. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn,
Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 1173–1186. https://doi.org/10.18653/V1/
2020.EMNLP-MAIN.89
[241] Debjit Paul, Robert West, Antoine Bosselut, and Boi Faltings. 2024. Making Reasoning Matter: Measuring and
Improving Faithfulness of Chain-of-Thought Reasoning. *CoRR* abs/2402.13950 (2024). https://doi.org/10.48550/ARXIV.
2402.13950 arXiv:2402.13950
[242] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. 2021. Data and its
(dis)contents: A survey of dataset development and use in machine learning research. *Patterns* 2, 11 (2021), 100336.
https://doi.org/10.1016/J.PATTER.2021.100336
[243] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only.
ArXiv preprint abs/2306.01116 (2023).
https://arxiv.org/abs/2306.01116
[244] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4.
ArXiv preprint abs/2304.03277 (2023). https://arxiv.org/abs/2304.03277
[245] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson,
Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron
McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson,
Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal
Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson
Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott
Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan,
Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny
Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023. Discovering Language Model
Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for
Computational Linguistics, 13387–13434. https://doi.org/10.18653/V1/2023.FINDINGS-ACL.847
[246] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.
2019. Language Models as Knowledge Bases?. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Association for Computational Linguistics, Hong Kong, China, 2463–2473. https://doi.org/10.18653/v1/D19-1250
[247] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing
the compositionality gap in language models. *ArXiv preprint* abs/2210.03350 (2022). https://arxiv.org/abs/2210.03350
[248] Jirui Qi, Gabriele Sarti, Raquel Fernández, and Arianna Bisazza. 2024. Model Internals-based Answer Attribution for
Trustworthy Retrieval-Augmented Generation. *CoRR* abs/2406.13663 (2024). https://doi.org/10.48550/ARXIV.2406.
13663 arXiv:2406.13663
[249] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. FoodGPT: A Large Language Model in Food
Testing Domain with Incremental Pre-training and Knowledge Graph Prompt. *ArXiv preprint* abs/2308.10173 (2023).
https://arxiv.org/abs/2308.10173
[250] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and
Huajun Chen. 2022. Reasoning with language model prompting: A survey. *ArXiv preprint* abs/2212.09597 (2022).
https://arxiv.org/abs/2212.09597
[251] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by
generative pre-training. (2018).
[252] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are
unsupervised multitask learners. *OpenAI blog* 1, 8 (2019), 9.
[253] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct
Preference Optimization: Your Language Model is Secretly a Reward Model. *ArXiv preprint* abs/2305.18290 (2023).
https://arxiv.org/abs/2305.18290
[254] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.
Res. 21 (2020), 140:1–140:67. http://jmlr.org/papers/v21/20-074.html
[255] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
2023. In-Context Retrieval-Augmented Language Models. *ArXiv preprint* abs/2302.00083 (2023). https://arxiv.org/
abs/2302.00083
[256] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with
Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto
Rico, May 2-4, 2016, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.
06732
[257] Mathieu Ravaut, Aixin Sun, Nancy F. Chen, and Shafiq Joty. 2024. On Context Utilization in Summarization with Large Language Models. arXiv:2310.10570 [cs.CL]
[258] Vipula Rawte, Amit P. Sheth, and Amitava Das. 2023. A Survey of Hallucination in Large Foundation Models. ArXiv preprint abs/2309.05922 (2023). https://arxiv.org/abs/2309.05922
[259] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, Luke Vilnis, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman, Nithya Attaluri, Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma, Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala, Martin Chadwick, Juliette Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai, Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Yingjie Miao, Lukas Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago Ontañón, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta, Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau, Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May, Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, Anja Hauth, Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Ramasesh, Anton Briukhov, Da-Woon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra, Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara Sainath, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan, Mark Omernick, Colin Gaffney, Gabriela Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin Chiu, Rory Blevins, Zach Gleicher, Adrià Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek, Séb Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek, Kashyap Krishnakumar, Shuo yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir, Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Sébastien Cevey, Jonas Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Michael Chang, Samer Hassan, Diana Mincu, Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravi Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee, Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma, Mario Lučić, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjösund, Elena Buchatskaya, Elspeth White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Jane Labanowski, Nicola De Cao, David Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Zhufeng Pan, Zachary Nado, Stephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Chung-Cheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Fred Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen, Ruibo Liu, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika

Rogozińska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang,
Dave Lacey, Anastasija Ilić, Yao Zhao, Lora Aroyo, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan,
Sadegh Jazayeri, Raphaël Lopez Kaufman, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David
Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp,
Nejc Trdin, Anitha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel,
Somer Greene, Anaïs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana
Kulizhskaya, Sonam Goenka, Brennan Saeta, Kiran Vodrahalli, Christian Frank, Dario de Cesare, Brona Robenek,
Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy,
Yelin Kim, Dinghua Li, Bill Rosgen, Zoe Ashwood, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan,
Hongkun Yu, Çağlar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang,
Ming Zhang, Rui Zhu, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier Dousse, Willi Gierke, Soheil Hassas Yeganeh, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu,
Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic,
Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi
Lam, Josef Broder, Dan Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Daniel Toyama, Mrinal Shukla, Sujoy
Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi,
Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, Félix de Chaumont Quitry,
Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq
Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi
Raad, Hannah Forbes, Anna Bulanova, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie
Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui
Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia,
Clement Farabet, Pedro Valenzuela, Quan Yuan, Chris Welty, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse,
Nandita Dukkipati, Adam Paszke, Andrew Bolt, Elnaz Davoodi, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha
Vashisht, Rebeca Santamaria-Fernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin
Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, Mohamed Elhawaty, Andrey Khorlin,
Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Alejandro Lince, Norman Casagrande, Jay Hoover,
Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa, Anna Koop, Praveen Kumar, Thibault
Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun,
Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian
Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar, Sanjay Ghemawat,
Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev,
Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn,
Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A.
Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber
Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel Kaed, Jing Li, Jakub Sygnowski, Shreyas Rammohan
Belle, Zhe Chen, Jaclyn Konzelmann, Siim Põder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne
Liu, Azade Nova, Jun Xu, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2024.
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530 [cs.CL]
[260] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng
Wang. 2023. Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation.
ArXiv preprint abs/2307.11019 (2023). https://arxiv.org/abs/2307.11019
[261] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng
Wang. 2023. Investigating the factual knowledge boundary of large language models with retrieval augmentation.
ArXiv preprint abs/2307.11019 (2023). https://arxiv.org/abs/2307.11019
[262] Reuters. 2023. *U.S. Copyright Office says some AI-assisted works may be copyrighted.* https://www.reuters.com/world/
us/us-copyright-office-says-some-ai-assisted-works-may-be-copyrighted-2023-03-15/
[263] Nina Rimsky. 2023. *Modulating sycophancy in an RLHF model via activation steering*. https://www.alignmentforum.
org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation
[264] Nina Rimsky. 2023. *Reducing sycophancy and improving honesty via activation steering*. https://www.alignmentforum.
org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation
[265] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. Can
AI-Generated Text be Reliably Detected? *CoRR* abs/2303.11156 (2023). https://doi.org/10.48550/ARXIV.2303.11156
arXiv:2303.11156
[266] Sashank Santhanam, Behnam Hedayatnia, Spandana Gella, Aishwarya Padmakumar, Seokhwan Kim, Yang Liu, and
Dilek Hakkani-Tur. 2021. Rome was built in 1776: A case study on factual correctness in knowledge-grounded
response generation. *ArXiv preprint* abs/2110.05456 (2021). https://arxiv.org/abs/2110.05456
[267] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. RAPTOR:
Recursive Abstractive Processing for Tree-Organized Retrieval. *CoRR* abs/2401.18059 (2024). https://doi.org/10.48550/
ARXIV.2401.18059 arXiv:2401.18059
[268] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Selfcritiquing models for assisting human evaluators. *ArXiv preprint* abs/2206.05802 (2022). https://arxiv.org/abs/2206.
05802
[269] John Schulman. 2023. *Reinforcement Learning from Human Feedback: Progress and Challenges*. Berkeley EECS.
https://www.youtube.com/watch?v=hhiLw5Q_UFg
[270] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization
Algorithms. *ArXiv preprint* abs/1707.06347 (2017). https://arxiv.org/abs/1707.06347
[271] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick
Gallinari. 2021. QuestEval: Summarization Asks for Fact-based Evaluation. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana,
Dominican Republic, 6594–6604. https://doi.org/10.18653/v1/2021.emnlp-main.529
[272] Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. 2024. Assisting in
Writing Wikipedia-like Articles From Scratch with Large Language Models. *CoRR* abs/2402.14207 (2024). https:
//doi.org/10.48550/ARXIV.2402.14207 arXiv:2402.14207
[273] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing Retrieval-
Augmented Large Language Models with Iterative Retrieval-Generation Synergy. *ArXiv preprint* abs/2305.15294
(2023). https://arxiv.org/abs/2305.15294
[274] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng,
Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal
Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. 2023. Towards Understanding
Sycophancy in Language Models. *ArXiv preprint* abs/2310.13548 (2023). https://arxiv.org/abs/2310.13548
[275] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023. Trusting
Your Evidence: Hallucinate Less with Context-aware Decoding. *ArXiv preprint* abs/2305.14739 (2023).
https:
//arxiv.org/abs/2305.14739
[276] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer,
Scott Yih, and Mike Lewis. 2023. In-Context Pretraining: Language Modeling Beyond Document Boundaries. ArXiv
preprint abs/2310.10638 (2023). https://arxiv.org/abs/2310.10638
[277] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wentau Yih. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models. *CoRR* abs/2301.12652 (2023). https:
//doi.org/10.48550/ARXIV.2301.12652 arXiv:2301.12652
[278] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces
Hallucination in Conversation. In *Findings of the Association for Computational Linguistics: EMNLP 2021*. Association for
Computational Linguistics, Punta Cana, Dominican Republic, 3784–3803. https://doi.org/10.18653/v1/2021.findingsemnlp.320
[279] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather
Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mansfield,
Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Agüera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong,
Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh
Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level Medical Question Answering with
Large Language Models. *ArXiv preprint* abs/2305.09617 (2023). https://arxiv.org/abs/2305.09617
[280] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. 2020. Editable Neural
Networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net. https://openreview.net/forum?id=HJedXaEtvS
[281] Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, and Shauli Ravfogel. 2023. The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. ArXiv preprint
abs/2310.11877 (2023). https://arxiv.org/abs/2310.11877
[282] Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, and Ido Dagan. 2024. Attribute First, then Generate: Locallyattributable Grounded Text Generation. *CoRR* abs/2403.17104 (2024). https://doi.org/10.48550/ARXIV.2403.17104
arXiv:2403.17104
[283] Felix Stahlberg and Bill Byrne. 2019. On NMT Search Errors and Model Errors: Cat Got Your Tongue?. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China,
3356–3362. https://doi.org/10.18653/v1/D19-1331
[284] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form
Answers. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*. Association
for Computational Linguistics, Abu Dhabi, United Arab Emirates, 8273–8288. https://aclanthology.org/2022.emnlpmain.566
[285] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,
and Paul F. Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html
[286] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu Liu, Quan Shi, Zachary S.
Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, and Tao Yu. 2024. BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. arXiv:2407.12883 [cs.CL] https://arxiv.org/abs/2407.12883
[287] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. RoFormer: Enhanced Transformer with Rotary
Position Embedding. *CoRR* abs/2104.09864 (2021). arXiv:2104.09864 https://arxiv.org/abs/2104.09864
[288] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented
Generation based on the Real-time Information Needs of Large Language Models. *CoRR* abs/2403.10081 (2024).
https://doi.org/10.48550/ARXIV.2403.10081 arXiv:2403.10081
[289] Nishant Subramani, Nivedita Suresh, and Matthew Peters. 2022. Extracting Latent Steering Vectors from Pretrained
Language Models. In *Findings of the Association for Computational Linguistics: ACL 2022*. Association for Computational
Linguistics, Dublin, Ireland, 566–581. https://doi.org/10.18653/v1/2022.findings-acl.48
[290] Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Head-to-Tail: How Knowledgeable
are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs? *CoRR* abs/2308.10168 (2023).
https://doi.org/10.48550/ARXIV.2308.10168 arXiv:2308.10168
[291] Ilya Sutskever. 2023. *An observation on Generalization*. Youtube. https://www.youtube.com/watch?v=AKMuA_
TVz3A&t=5s
[292] Chenmien Tan, Ge Zhang, and Jie Fu. 2024. Massive Editing for Large Language Model via Meta Learning. In The
Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=L6L1CJQ2PE
[293] Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. 2024. Blinded by Generated Contexts:
How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA? *CoRR* abs/2401.11911 (2024).
https://doi.org/10.48550/ARXIV.2401.11911 arXiv:2401.11911
[294] Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2023. Found in the Middle: Permutation
Self-Consistency Improves Listwise Ranking in Large Language Models. *CoRR* abs/2310.07712 (2023).
https:
//doi.org/10.48550/ARXIV.2310.07712 arXiv:2310.07712
[295] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous
Benchmark for Zero-shot Evaluation of Information Retrieval Models. *CoRR* abs/2104.08663 (2021). arXiv:2104.08663
https://arxiv.org/abs/2104.08663
[296] Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P Parikh. 2019. Sticking to the facts: Confident decoding for
faithful data-to-text generation. *ArXiv preprint* abs/1910.08684 (2019). https://arxiv.org/abs/1910.08684
[297] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring
the visual shortcomings of multimodal llms. *arXiv preprint arXiv:2401.06209* (2024).
[298] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and
Amitava Das. 2024. A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models.
CoRR abs/2401.01313 (2024). https://doi.org/10.48550/ARXIV.2401.01313 arXiv:2401.01313
[299] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. *CoRR* abs/2302.13971 (2023).
https:
//doi.org/10.48550/ARXIV.2302.13971 arXiv:2302.13971
[300] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,
Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith,
Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. ArXiv preprint
abs/2307.09288 (2023). https://arxiv.org/abs/2307.09288
[301] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with
Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14,
2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics,
10014–10037. https://aclanthology.org/2023.acl-long.557
[302] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language Models Don't Always Say
What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. *ArXiv preprint* abs/2305.04388 (2023).
https://arxiv.org/abs/2305.04388
[303] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023. Med-halt: Medical domain hallucination
test for large language models. *ArXiv preprint* abs/2307.15343 (2023). https://arxiv.org/abs/2307.15343
[304] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding.
CoRR abs/1807.03748 (2018). arXiv:1807.03748 http://arxiv.org/abs/1807.03748
[305] Liam van der Poel, Ryan Cotterell, and Clara Meister. 2022. Mutual Information Alleviates Hallucinations in Abstractive
Summarization. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*. Association
for Computational Linguistics, Abu Dhabi, United Arab Emirates, 5956–5965. https://aclanthology.org/2022.emnlpmain.399
[306] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. *ArXiv preprint* abs/2307.03987
(2023). https://arxiv.org/abs/2307.03987
[307] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention:
Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 5797–5808.
https://doi.org/10.18653/v1/P19-1580
[308] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou,
Quoc Le, and Thang Luong. 2023. FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation.
arXiv:2310.03214 [cs.CL]
[309] David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, and Mohit Bansal. 2023. Faithfulness-Aware Decoding
Strategies for Abstractive Summarization. In Proceedings of the 17th Conference of the European Chapter of the
Association for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia, 2864–2880.
https://aclanthology.org/2023.eacl-main.210
[310] Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and Answering Questions to Evaluate the Factual
Consistency of Summaries. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*.
Association for Computational Linguistics, Online, 5008–5020. https://doi.org/10.18653/v1/2020.acl-main.450
[311] Binjie Wang, Ethan Chern, and Pengfei Liu. 2023. ChineseFactEval: A Factuality Benchmark for Chinese LLMs.
[312] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang
Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang.
2023. Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. ArXiv preprint
abs/2310.07521 (2023). https://arxiv.org/abs/2310.07521
[313] Chaojun Wang and Rico Sennrich. 2020. On Exposure Bias, Hallucination and Domain Shift in Neural Machine
Translation. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*. Association for
Computational Linguistics, Online, 3544–3552. https://doi.org/10.18653/v1/2020.acl-main.326
[314] Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is
chatgpt a good nlg evaluator? a preliminary study. *ArXiv preprint* abs/2303.04048 (2023). https://arxiv.org/abs/2303.
04048
[315] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023.
An llm-free multi-dimensional benchmark for mllms hallucination evaluation. *arXiv preprint arXiv:2311.07397* (2023).
[316] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2024. Mitigating fine-grained hallucination by
fine-tuning large vision-language models with caption rewrites. In *International Conference on Multimedia Modeling*.
Springer, 32–45.
[317] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. In Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,
2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 9414–9423.
https://doi.org/10.18653/V1/2023.EMNLP-MAIN.585
[318] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-consistent
chain-of-thought distillation. *ArXiv preprint* abs/2305.01879 (2023). https://arxiv.org/abs/2305.01879
[319] Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, and Zhicheng Dou. 2024. RichRAG: Crafting
Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation. *CoRR* abs/2406.12566 (2024). https:
//doi.org/10.48550/ARXIV.2406.12566 arXiv:2406.12566
[320] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. 2023. Knowledge Editing for
Large Language Models: A Survey. arXiv:2310.16218 [cs.CL]
[321] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided Retrieval Augmentation for Large
Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10,
2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 10303–10315.
https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.691
[322] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and
Qun Liu. 2023. Aligning Large Language Models with Human: A Survey. *CoRR* abs/2307.12966 (2023).
https:
//doi.org/10.48550/ARXIV.2307.12966 arXiv:2307.12966
[323] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham Neubig. 2023. Learning to Filter Context
for Retrieval-Augmented Generation. *CoRR* abs/2311.08377 (2023).
https://doi.org/10.48550/ARXIV.2311.08377
arXiv:2311.08377
[324] Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. 2020. Towards Faithful Neural Table-to-Text
Generation with Content-Matching Constraints. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics. Association for Computational Linguistics, Online, 1072–1086. https://doi.org/10.18653/
v1/2020.acl-main.101
[325] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2022. SimVLM: Simple Visual
Language Model Pretraining with Weak Supervision. In The Tenth International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/forum?id=GUrhfTuf_3
[326] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.
Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing
Systems 35 (2022), 24824–24837.
[327] Jerry W. Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le. 2023. Simple synthetic data reduces sycophancy in
large language models. *ArXiv preprint* abs/2308.03958 (2023). https://arxiv.org/abs/2308.03958
[328] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia
Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles,
Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and
Iason Gabriel. 2021. Ethical and social risks of harm from Language Models. *ArXiv preprint* abs/2112.04359 (2021).
https://arxiv.org/abs/2112.04359
[329] Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts
in Large Language Models. *ArXiv preprint* abs/2308.09729 (2023). https://arxiv.org/abs/2308.09729
[330] Di Wu, Jia-Chen Gu, Fan Yin, Nanyun Peng, and Kai-Wei Chang. 2024. Synchronous Faithfulness Monitoring for
Trustworthy Retrieval-Augmented Generation. *CoRR* abs/2406.13692 (2024). https://doi.org/10.48550/ARXIV.2406.
13692 arXiv:2406.13692
[331] Kevin Wu, Eric Wu, and James Zou. 2024. ClashEval: Quantifying the tug-of-war between an LLM's internal prior
and external evidence. arXiv:2404.10198 [cs.CL] https://arxiv.org/abs/2404.10198
[332] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-Pack: Packaged Resources To Advance General
Chinese Embedding. *CoRR* abs/2309.07597 (2023). https://doi.org/10.48550/ARXIV.2309.07597 arXiv:2309.07597
[333] Yijun Xiao and William Yang Wang. 2021. On Hallucination and Predictive Uncertainty in Conditional Language
Generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume. Association for Computational Linguistics, Online, 2734–2744. https://doi.org/10.18653/v1/
2021.eacl-main.236
[334] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive Chameleon or Stubborn Sloth: Unraveling
the Behavior of Large Language Models in Knowledge Clashes. *ArXiv preprint* abs/2305.13300 (2023).
https:
//arxiv.org/abs/2305.13300
[335] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can LLMs Express Their
Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. *ArXiv preprint* abs/2306.13063 (2023).
https://arxiv.org/abs/2306.13063
[336] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. RECOMP: Improving Retrieval-Augmented LMs with Compression
and Selective Augmentation. *ArXiv preprint* abs/2310.04408 (2023). https://arxiv.org/abs/2310.04408
[337] Jiacheng Xu, Shrey Desai, and Greg Durrett. 2020. Understanding Neural Abstractive Summarization Models via
Uncertainty. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
Association for Computational Linguistics, Online, 6275–6281. https://doi.org/10.18653/v1/2020.emnlp-main.508
[338] Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. 2024. Faithful Logical Reasoning via Symbolic Chain-of-Thought. *CoRR* abs/2405.18357 (2024).
https://doi.org/10.48550/ARXIV.2405.18357
arXiv:2405.18357
[339] Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, and Xueqi Cheng. 2023. AI-
Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. *CoRR* abs/2311.14084 (2023). https:
//doi.org/10.48550/ARXIV.2311.14084 arXiv:2311.14084
[340] Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023. A New Benchmark and Reverse Validation Method for Passagelevel Hallucination Detection. *ArXiv preprint* abs/2310.06498 (2023). https://arxiv.org/abs/2310.06498
[341] Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for Honesty. CoRR
abs/2312.07000 (2023). https://doi.org/10.48550/ARXIV.2312.07000 arXiv:2312.07000
[342] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018. Breaking the Softmax Bottleneck: A
High-Rank RNN Language Model. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=
HkwZSG-CZ
[343] Zhilin Yang, Thang Luong, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Mixtape: Breaking the Softmax Bottleneck
Efficiently. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15922–15930.
https://proceedings.
neurips.cc/paper/2019/hash/512fc3c5227f637e41437c999a2d3169-Abstract.html
[344] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.
Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
Brussels, Belgium, 2369–2380. https://doi.org/10.18653/v1/D18-1259
[345] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. 2023. LLM Lies: Hallucinations are not Bugs,
but Features as Adversarial Examples. *ArXiv preprint* abs/2310.01469 (2023). https://arxiv.org/abs/2310.01469
[346] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React:
Synergizing reasoning and acting in language models. *ArXiv preprint* abs/2210.03629 (2022). https://arxiv.org/abs/
2210.03629
[347] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.
2023. Editing Large Language Models: Problems, Methods, and Opportunities. *ArXiv preprint* abs/2305.13172 (2023).
https://arxiv.org/abs/2305.13172
[348] Xi Ye, Ruoxi Sun, Sercan Ö. Arik, and Tomas Pfister. 2023. Effective Large Language Model Adaptation for Improved
Grounding. *CoRR* abs/2311.09533 (2023). https://doi.org/10.48550/ARXIV.2311.09533 arXiv:2311.09533
[349] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and
Enhong Chen. 2023. Woodpecker: Hallucination Correction for Multimodal Large Language Models.
https:
//arxiv.org/abs/2310.16045
[350] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do Large Language
Models Know What They Don't Know?. In Findings of the Association for Computational Linguistics: ACL 2023,
Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for
Computational Linguistics, 8653–8665. https://doi.org/10.18653/V1/2023.FINDINGS-ACL.551
[351] Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2024. Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search. CoRR
abs/2402.11827 (2024). https://doi.org/10.48550/ARXIV.2402.11827 arXiv:2402.11827
[352] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making Retrieval-Augmented Language Models Robust
to Irrelevant Context. *CoRR* abs/2310.01558 (2023). https://doi.org/10.48550/ARXIV.2310.01558 arXiv:2310.01558
[353] Fangyi Yu, Lee Quartey, and Frank Schilder. 2022. Legal Prompting: Teaching a Language Model to Think Like a
Lawyer. *ArXiv preprint* abs/2212.01326 (2022). https://arxiv.org/abs/2212.01326
[354] Fei Yu, Hongbo Zhang, and Benyou Wang. 2023. Nature language reasoning, a survey. *ArXiv preprint* abs/2303.14725
(2023). https://arxiv.org/abs/2303.14725
[355] Weijiang Yu, Jian Liang, Lei Ji, Lu Li, Yuejian Fang, Nong Xiao, and Nan Duan. 2021. Hybrid reasoning network
for video-based commonsense captioning. In *Proceedings of the 29th ACM international conference on multimedia*.
5213–5221.
[356] Weijiang Yu, Haofan Wang, Guohao Li, Nong Xiao, and Bernard Ghanem. 2023. Knowledge-aware Global Reasoning
for Situation Recognition. *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2023).
[357] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-Note:
Enhancing Robustness in Retrieval-Augmented Language Models. *CoRR* abs/2311.09210 (2023). https://doi.org/10.
48550/ARXIV.2311.09210 arXiv:2311.09210
[358] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023. Improving Language Models
via Plug-and-Play Retrieval Feedback. *ArXiv preprint* abs/2305.14002 (2023). https://arxiv.org/abs/2305.14002
[359] Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating Generated Text as Text Generation.
In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,
Percy Liang, and Jennifer Wortman Vaughan (Eds.). 27263–27277. https://proceedings.neurips.cc/paper/2021/hash/
e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html
[360] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From Recognition to Cognition: Visual Commonsense
Reasoning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
16-20, 2019. Computer Vision Foundation / IEEE, 6720–6731. https://doi.org/10.1109/CVPR.2019.00688
[361] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li. 2023. Halle-switch: Controlling
object hallucination in large vision language models. *arXiv e-prints* (2023), arXiv–2310.
[362] Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong
Zhang. 2023. R-Tuning: Teaching Large Language Models to Refuse Unknown Questions. *CoRR* abs/2311.09677 (2023).
https://doi.org/10.48550/ARXIV.2311.09677 arXiv:2311.09677
[363] Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. 2021. Trading Off Diversity and Quality
in Natural Language Generation. In *Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)*.
Association for Computational Linguistics, Online, 25–33. https://aclanthology.org/2021.humeval-1.3
[364] Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, and Sricharan Kumar. 2023. SAC3: Reliable Hallucination
Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. arXiv:2311.01740 [cs.CL]
[365] Jiajun Zhang, Yang Zhao, Haoran Li, and Chengqing Zong. 2018. Attention with sparsity regularization for neural
machine translation and summarization. *IEEE/ACM Transactions on Audio, Speech, and Language Processing* 27, 3
(2018), 507–518.
[366] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. PEGASUS: Pre-training with Extracted Gapsentences for Abstractive Summarization. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 11328–11339.
http://proceedings.mlr.press/v119/zhang20ae.html
[367] Mingtian Zhang, Shawn Lan, Peter Hayes, and David Barber. 2024. Mafin: Enhancing Black-Box Embeddings with
Model Augmented Fine-Tuning. arXiv:2402.12177 [cs.LG]
[368] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. 2023. How language model hallucinations can
snowball. *ArXiv preprint* abs/2305.13534 (2023). https://arxiv.org/abs/2305.13534
[369] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian
Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang,
Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. 2024. A Comprehensive Study of Knowledge Editing for
Large Language Models. arXiv:2401.01286 [cs.CL]
[370] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,
Fei Wu, et al. 2023. Instruction Tuning for Large Language Models: A Survey. *ArXiv preprint* abs/2308.10792 (2023).
https://arxiv.org/abs/2308.10792
[371] Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023. Mitigating Language Model Hallucination
with Interactive Question-Knowledge Alignment. *ArXiv preprint* abs/2305.13669 (2023). https://arxiv.org/abs/2305.
13669
[372] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T.
Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,
Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models.
ArXiv preprint abs/2205.01068 (2022). https://arxiv.org/abs/2205.01068
[373] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023.
Benchmarking large language models for news summarization. *ArXiv preprint* abs/2301.13848 (2023).
https:
//arxiv.org/abs/2301.13848
[374] Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang,
and Luoyi Fu. 2023. Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus. In Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December
6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 915–932.
https://aclanthology.org/2023.emnlp-main.58
[375] Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024.
Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation. *CoRR* abs/2402.09267 (2024).
https://doi.org/10.48550/ARXIV.2402.09267 arXiv:2402.09267
[376] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong
Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren's Song in the AI Ocean: A Survey
on Hallucination in Large Language Models. *ArXiv preprint* abs/2309.01219 (2023). https://arxiv.org/abs/2309.01219
[377] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and Zhangyang
Wang. 2024. Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional
Encoding. arXiv:2403.04797 [cs.CL]
[378] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation
for Short-form Open-Domain Question Answering. arXiv:2402.16457 [cs.CL]
[379] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. 2024. Mitigating Object Hallucination in Large Vision-
Language Models via Classifier-Free Guidance. *arXiv preprint arXiv:2402.08680* (2024).
[380] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bing Qin, and Ting Liu. 2023. Length Extrapolation of Transformers: A
Survey from the Perspective of Position Encoding. *CoRR* abs/2312.17044 (2023). https://doi.org/10.48550/ARXIV.2312.
17044 arXiv:2312.17044
[381] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023. Verify-and-Edit: A Knowledge-
Enhanced Chain-of-Thought Framework. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and
Naoaki Okazaki (Eds.). Association for Computational Linguistics, 5823–5840. https://doi.org/10.18653/v1/2023.acllong.320
[382] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2024. Dense text retrieval based on pretrained language
models: A survey. *ACM Transactions on Information Systems* 42, 4 (2024), 1–60.
[383] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,
Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. *ArXiv preprint* abs/2303.18223 (2023).
https://arxiv.org/abs/2303.18223
[384] Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun
Ren, and Dawei Yin. 2023. Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method.
ArXiv preprint abs/2310.17918 (2023). https://arxiv.org/abs/2310.17918
[385] Danna Zheng, Mirella Lapata, and Jeff Z. Pan. 2024.
Large Language Models as Reliable Knowledge Bases?
arXiv:2407.13578 [cs.CL] https://arxiv.org/abs/2407.13578
[386] Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why Does ChatGPT Fall Short in Answering Questions
Faithfully? *ArXiv preprint* abs/2304.10513 (2023). https://arxiv.org/abs/2304.10513
[387] Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng Gong, Xiaocheng Feng, and Bing Qin. 2023. STOA-VLP:
Spatial-Temporal Modeling of Object and Action for Video-Language Pre-Training. In Proceedings of the Thirty-
Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial
Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, Vol. 37. 3715–3723. https:
//doi.org/10.1609/aaai.v37i3.25483
[388] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.
2023. Lima: Less is more for alignment. *ArXiv preprint* abs/2305.11206 (2023). https://arxiv.org/abs/2305.11206
[389] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting Hallucinated Content in Conditional Neural Sequence Generation. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 1393–1404.
https://doi.org/10.18653/v1/2021.findings-acl.120
[390] Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful Prompting for Large Language
Models. In *Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023*,
Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 14544–14556. https:
//aclanthology.org/2023.findings-emnlp.968
[391] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.
2023. Analyzing and Mitigating Object Hallucination in Large Vision-Language Models. https://arxiv.org/abs/2310.
00754
[392] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. *ArXiv preprint* abs/2304.10592 (2023).
https:
//arxiv.org/abs/2304.10592
[393] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang.
2023. Multilingual machine translation with large language models: Empirical results and analysis. ArXiv preprint
abs/2304.04675 (2023). https://arxiv.org/abs/2304.04675
[394] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong
Wen. 2023. Large Language Models for Information Retrieval: A Survey. *CoRR* abs/2308.07107 (2023).
https:
//doi.org/10.48550/ARXIV.2308.07107 arXiv:2308.07107
[395] Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, and Timothy Hospedales. 2023. Fool Your (Vision
and) Language Model With Embarrassingly Simple Permutations. *ArXiv preprint* abs/2310.01651 (2023).
https:
//arxiv.org/abs/2310.01651