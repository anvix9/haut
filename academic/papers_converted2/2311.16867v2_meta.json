{
    "language": "English",
    "filetype": "pdf",
    "toc": [
        [
            1,
            "Introduction",
            2
        ],
        [
            1,
            "State-of-the-art: from language modeling to frontier models",
            5
        ],
        [
            1,
            "Design philosophy",
            6
        ],
        [
            1,
            "Experiments and motivations for data, architecture, and hyperparameters",
            7
        ],
        [
            2,
            "Setup for small-scale experiments",
            7
        ],
        [
            2,
            "Data: web vs curated, code and multilinguality impact on English performance",
            8
        ],
        [
            3,
            "Web data alone can outperform curated corpora",
            8
        ],
        [
            3,
            "Against a strong web baseline, curated data can even be detrimental",
            10
        ],
        [
            3,
            "Limited code and multilingual data do not strongly degrade English performance",
            11
        ],
        [
            2,
            "Architecture and pretraining: validating popular recipes, and inference optimizations",
            12
        ],
        [
            3,
            "Extending multiquery into multigroup for tensor parallel training and inference",
            12
        ],
        [
            3,
            "Rotary positionnal embeddings may only offer a limited edge over ALiBi",
            14
        ],
        [
            3,
            "The extra memory cost of GLU may not be worth it for cost-efficient training",
            14
        ],
        [
            3,
            "Small tweaks help scalability: parallel layers and no biases in linear layers",
            15
        ],
        [
            3,
            "Validating best practices for hyperparameters: z-loss, weight decay, LR search",
            15
        ],
        [
            2,
            "Further experimentation required: ideas that did not make the cut",
            17
        ],
        [
            2,
            "Wrapping-it up: validating overall dataset and architecture recipes",
            18
        ],
        [
            1,
            "Implementation",
            19
        ],
        [
            2,
            "The Falcon dataset: predominantly web, with added curated and conversational data",
            19
        ],
        [
            3,
            "The Macrodata Refinement pipeline and the RefinedWeb dataset",
            20
        ],
        [
            3,
            "The Microdata curated corpora and conversational masking",
            21
        ],
        [
            2,
            "The Falcon architecture and recipe for efficient inference and (stable) training",
            22
        ],
        [
            3,
            "Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up",
            22
        ],
        [
            3,
            "Large language model alchemy: hyperparameters for pretraining",
            24
        ],
        [
            2,
            "Large-scale distributed training on cloud infrastructure with Gigatron",
            24
        ],
        [
            3,
            "Combining 3D parallelism for fine-grained control, and ZeRO for scalability",
            25
        ],
        [
            3,
            "State-of-the-art throughput with dedicated Triton kernels",
            28
        ],
        [
            3,
            "Efficient memory use via selective recomputation implemented as a monolayer",
            28
        ],
        [
            3,
            "Numerical precision: all you need is bfloat16?",
            29
        ],
        [
            3,
            "Quality-of-life features for improved flexibility and reliability",
            29
        ],
        [
            2,
            "Run management: keeping large-scale infrastructure running",
            29
        ],
        [
            1,
            "Results",
            30
        ],
        [
            2,
            "To prompt or not to prompt: comparing evaluations across codebases",
            31
        ],
        [
            2,
            "Comparisons with PaLM on a natural language tasks aggregate",
            33
        ],
        [
            2,
            "Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks",
            34
        ],
        [
            2,
            "State-of-the-art comparisons on common sense, question answering, and code tasks",
            34
        ],
        [
            2,
            "Comparison with other models using the EleutherAI Evaluation Harness",
            36
        ],
        [
            1,
            "Limitations",
            37
        ],
        [
            2,
            "Limitations of our findings and ablations",
            37
        ],
        [
            2,
            "Limitations of the Falcon models",
            38
        ],
        [
            1,
            "Conclusion",
            39
        ],
        [
            1,
            "Contributions",
            51
        ],
        [
            1,
            "Acknowledgements",
            51
        ],
        [
            1,
            "Model card",
            52
        ],
        [
            1,
            "Datasheet",
            53
        ],
        [
            1,
            "Comparisons with undocumented models",
            53
        ],
        [
            1,
            "Pseudocode samples",
            53
        ],
        [
            2,
            "Measurement plan to measure all to all bandwidths/latencies efficiently",
            53
        ],
        [
            2,
            "Converting tree token depth into an attention mask:",
            53
        ],
        [
            2,
            "Zero-1 pseudo-code",
            53
        ],
        [
            1,
            "Prompts",
            54
        ]
    ],
    "pages": 57,
    "ocr_stats": {
        "ocr_pages": 0,
        "ocr_failed": 0,
        "ocr_success": 0
    },
    "block_stats": {
        "header_footer": 0,
        "code": 7,
        "table": 21,
        "equations": {
            "successful_ocr": 2,
            "unsuccessful_ocr": 0,
            "equations": 2
        }
    },
    "postprocess_stats": {
        "edit": {}
    }
}