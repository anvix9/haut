# A Mathematical Theory Of Semantic Communication

Kai Niu, *Member, IEEE*, Ping Zhang, Fellow, IEEE

## Abstract

The year 1948 witnessed the historic moment of the birth of classic information theory (CIT).

Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function H(U), channel capacity C = maxp(x) I(X; Y ) and rate-distortion function R(D) =
minp(ˆx|x):Ed(x,ˆx)≤D I(X; ˆX). Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework of semantic information theory (SIT). We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information. Stemming from this core concept, synonymous mapping f, we introduce the measures of semantic information, such as semantic entropy Hs( ˜U), up/down semantic mutual information Is( ˜X; ˜Y ) (Is( ˜X; ˜Y )), semantic capacity Cs = maxfxy maxp(x) Is( ˜X; ˜Y ), and semantic rate-distortion function Rs(D) = min{fx,fˆx} minp(ˆx|x):Eds(˜x,ˆ˜x)≤D Is( ˜X; ˆ˜X). Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is, Hs( ˜U) ≤H(U), Cs ≥C and Rs(D) ≤R(D). All these works composite the basis of semantic information theory. In addition, we discuss the semantic information measures in the continuous case.

Especially, for the band-limited Gaussian channel, we obtain a new channel capacity formula, Cs =

B log
   h
   S4 
      1 +
         P
        N0B
           i
            , where the average synonymous length S indicates the identification ability of

information. In summary, the theoretic framework of SIT proposed in this paper is a natural extension of CIT and may reveal great performance potential for future communication.

K. Niu and P. Zhang are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China (e-mail: {niukai, pzhang}@bupt.edu.cn).

Synonymous mapping, Semantic entropy, Semantic relative entropy, Up/Down semantic mutual information, Semantic channel capacity, Semantic distortion, Semantic rate distortion function, Semantically typical set, Synonymous typical set, Semantically jointly typical set, Jointly typical decoding, Jointly typical encoding, Synonymous length, Maximum likelihood group decoding, Semantic source channel coding.

## I. Introduction

Classic information theory (CIT), established by C. E. Shannon [1] in 1948, was a great achievement in the modern information and communication field. As shown in Fig. 1, the classic communication system includes source, encoder, channel with noise, decoder and destination. This theory is concerned with the uncertainty of information and introduces four critical measures, such as entropy, mutual information, channel capacity, and rate-distortion function to evaluate the performance of information processing and transmission. Especially, three famous coding theorems, such as, lossless/lossy source coding theorem and channel coding theorem, reveal the fundamental limitation of data compression and information transmission. Over the past 70 years or so, people developed many advanced techniques to approach these theoretical limits. For the lossless source coding, Huffman coding and arithmetic coding are the representative optimal coding methods can achieve the source entropy. Similarly, for the channel coding, polar code, as a great breakthrough [23], is the first constructive capacity-achieving coding scheme. Correspondingly, for the lossy source coding, some modern coding schemes, such as BPG (Better Portable Graphics) standard and H. 265/266 standard, can approach the rate-distortion lower bounds of image and video sources. It follows that information and communication technologies guided by CIT have approached the theoretical limitation and the performance improvement of modern communication systems encounters a lot of bottlenecks.

Essentially, Weaver [3], just one year after Shannon published the seminal paper on information theory, pointed out that communication involves problems at three levels as follows:
"**LEVEL A**. How accurately can the symbols of communication be transmitted? (The technical problem.)
LEVEL B. How precisely do the transmitted symbols convey the desired meaning? (The semantic problem.)

| Channel   |
|-----------|
| Noise     |

## Level C. How Effectively Does The Received Meaning Affect Conduct In The Desired Way? (The Effectiveness Problem.)"

Shannon [1] wrote that "semantic aspects of communication are irrelevant to the engineering problem". Thus, classic information theory only handles LEVEL A (technical) problem of the information. On the contrary, date back to the time of classic information theory birth, many works also focused on LEVEL B problem and the semantic communication theory. Carnap and Bar-Hillel [6] and Floridi [7] considered using propositional logic sentences to express semantic information. They introduced the semantic information entropy, which is calculated based on logical probability [8] rather than statistical probability in CIT. Then Bao *et al.* [9] extended this theoretical framework and derived the semantic source coding and semantic channel coding theorem based on propositional logic probabilities. On the other hand, De Luca *et al.* [10]
[11] regarded semantic information as fuzzy variable and defined fuzzy entropy to measure the uncertainty of semantic information. Then Wu [12] extended this work and introduced general entropy, general conditional entropy, and general mutual information based on fuzzy variable.

However, the propositional logic or fuzzy variables are only suitable for the simple processing of text or speech source and can not sufficiently describe semantic information of the complex data, such as image or video source. Furthermore, although some recent works [13–16] investigated the theoretic property of semantic information, e.g. rate-distortion function, designing semantic communication system still lacks of systematic guiding theory.

Recently, semantic communication systems based on deep learning demonstrate excellent performance than the traditional counterparts. Many works [17–20] investigated the design principles and technical challenges of semantic communication. As surveyed in [21, 22], semantic communication techniques become a hot topic in the communication community and provide a promising methodology to break through the Shannon limits. However, semantic communication research faces a dilemma. We can neither precisely answer what is the semantic information or the meaning of information nor provide the fundamental limits to guide the design of the semantic communication system. Thus, there is an urgent necessity to establish a mathematical theory of semantic communication in order to solve these basic problems.

If we want to establish a semantic information theory, we should firstly consider the essence of semantic information, that is, what is the meaning of information. Let's investigate the semantic information from the source side and the destination side respectively. Figure 2 shows some examples of semantic information of text, speech, image and video source. In Fig. 2(a), the words "happy, joyful, content, joyous" have the same or similar meaning, that is, all these text data means "happy". From the viewpoint of linguistics, these words compose the synonym of "happy". From the understanding of human beings, we can regard these words as having the same semantic information. So we can use synonym alternation to generate the same meaning presentation, as shown in the following sentences
"She appeared **happy** and content after receiving the good news".

"She appeared **joyful** and content after receiving the good news".

Although these two sentences have different presentations, the processing of the language center of the brain, they have the same semantic information. Generally, such synonym phenomenon ubiquitously exists in various language texts. Many different presentations of phrases, words, and sentences have the same or similar meaning and compose the synonymous mapping to indicate the same semantic information. Therefore, we conclude that synonymous mapping is a critical feature of semantic information of text data.

Similarly, synonym phenomenon can also be observed in speech source. As shown in Fig.

2(b), a piece of speech has three presentations, that is, waveform, Mel spectrogram, and power spectrogram. However, all these presentations indicate the same meaning "Don't ask me to carry an oily rag like that". So we conclude that all these speech waveform or spectrograms compose the synonym presentation and have the same semantic information.

We can also find the synonym objects in image source, as depicted in Fig. 2(c). After the object-oriented segmentation, this figure is decomposed into four objects, that is, sky, lake, bears, and ground. Intuitively, single pixel in this figure have no meaning whereas a set of many pixels indicates some meaningful object. Furthermore, for the frame sequence shown in Fig. 2(d), by using the object extraction method, we can obtain the meaning of this video "A jockey guides his horse in a gallop". From the two examples, it follows that synonymous mapping for the semantic understanding is a popular phenomenon in image and video source.

On the other hand, in the destination side, we also observe the synonym phenomenon for various downstream tasks. Figure 3 depicts some representative examples. For the task of character recognition in Fig. 3(a), different shapes and fonts of the images in each row present the same letter. So we easily derive the meaning from these images, that is, "SEMANTIC". Similarly, for the task of image classification, shown in Fig. 3(b), various images stands for the same entity or object, such as forest, staircase, ocean etc. Furthermore, Fig. 3(c) depicts the task of obstacle detection and the marker boxes presents the pedestrians. Hence, we can conclude that many downstream tasks involve the synonymous mapping and semantic reasoning.

Remark 1. In a word, after inspecting various examples of source and destination, we can summarize these two rules for the semantic information processing.

(1) All the perceptible messages, such as text, speech, image, video, and so on, are syntactic information. However, based on these messages, we can derive or reason some semantic information. There are common and stable mappings between the syntactic information and the semantic information. These mappings can be built from the knowledge of human beings or the configuration conditions of the application scene.

(2) Generally, the relationships between syntactic information and semantic information are very complex. However, in most instances, the semantic information has a single meaning whereas the presentations of source data and downstream tasks are myriads. So synonymous mapping is a major relationship and popularly exists in various tasks of semantic reasoning. Certainly, there may exist the ambiguity of semantic information. Nonetheless, such ambiguity is secondary and can be removed by using multiple interactions. So in this paper, we mainly handle the synonym of semantic information and the ambiguity will be left to future works.

In this paper, we aim to establish a systematic framework of semantic information theory as the natural extension of classic information theory. The outline of the theoretic framework is shown in Fig. 4.

Therefore, the contributions of this paper can be summarized as follows.

(1) We develop a systematic model for semantic communication with specific design criteria.
In this model, semantic information remains invisible but perceptible, characterized by its prevalent synonymous features. Hence, we recognize synonymy as the fundamental aspect of semantic information. To illustrate the connection between semantic and syntactic
information, we introduce synonymous mapping which is an one-to-many mapping from
the semantic alphabet to syntactic alphabet. Essentially, synonymous mapping constructs an equivalent class relationship between the semantic space and the syntactic space.
(2) Stemming for a synonymous mapping fu, we introduce the semantic variable ˜U associated
with a random variable U. Essentially, the semantic variable is also a random variable. Hence,
we define the semantic entropy Hs( ˜U) to measure the uncertainty of semantic information
and name the unit as semantic bit (**sebit**). Analogous to the classic information theory, this
measure can also be extended to semantic conditional/joint entropy. In addition, we prove that all the semantic entropies are no more than their classic counterparts.
(3) We introduce the concepts of semantic relative entropy and semantic mutual information.

Semantic Mutual 
Information
Semantic Entropy
Up Semantic MI
Down Semantic MI
( )
s
H U
(
; )
s X Y
I
(
; )
s X Y
I
Semantic Capacity
Semantic Rate Distortion
(
; )
max
s
s
C
I
X Y
=
(
; )
(
)
min
s
s
R
I
X Y
D =
Gaussian Sem. entropy
Gaussian Sem. Capacity
Gaussian Sem. R(D)
2
4
P
C
B
S
N B
0
log
1
s
4
1 log
(
)
2
s
P
R
S D
D






=




=
+










e
H
S

=
2
1
2
log
2
s

Semantic Channel Coding Theorem Semantic Rate Distortion Theorem Semantic Source Coding Theorem

→

R
C
P
If 
,lim
0
,
( )

→





R
H
P

                                                                            
If 
         ,lim
                    0

,
                 ( )

| R   | R   | d   | x x   | D   | D   |
|-----|-----|-----|-------|-----|-----|

)

s
s

→

               n
s
              e
       n

→

               n
s
              e
       n


       

d
    x
        D

x

          (
              ),
                         (
                             ˆ
If 
                   ,
                            ,
                           ˆ
Otherwi e,
                         ,

s

     (
           )
,


        →

P

Otherwise,
             ,lim
                          1

( )


        →

s

P

Otherwise,
,lim
1
( )
→
n e n
→
n e n

Lossless Semantic Source Channel Coding Theorem
Lossy Semantic Source 
Channel Coding Theorem
s
s
H
C
R


(
)
s
s
R D
R
C



Unlike the classic mutual information, it is noted that we use two measures to indicate semantic mutual information, such as the up semantic mutual information Is( ˜X; ˜Y ) =
H(X)+H(Y )−Hs( ˜X, ˜Y ) and the down semantic mutual information Is( ˜X; ˜Y ) = Hs( ˜X)+
Hs(˜Y ) −H(*X, Y* ). Furthermore, given a channel with the transition probability p(y|x), we

can use the semantic capacity Cs = maxfxy maxp(x) Is( ˜X; ˜Y ) to indicate the maximum
transmission rate of semantic information on the channel. Correspondingly, given a source
with the distribution p(x) and the average distortion measure constraint E
h
ds(˜x, ˆ˜x)
i
≤D,
the semantic rate distortion Rs(D) = min{fx,f˜x} minp(ˆx|x):Eds(˜x,ˆ˜x)≤D Is( ˜X; ˆ˜X) reveals the
minimum compression rate of semantic information in the case of lossy source coding.
(4) We investigate the asymptotic equipartition property (AEP) in the semantic sense and introduce the semantically typical set ˜A(n)
ϵ
which presents all the information of semantic variable
˜U. Especially, we prove that, under the synonymous mapping, the syntactically typical set

A(n)
 ϵ
  can be equipartitioned into many synonymous typical sets B(n)
                                   ϵ . By using random coding

and synonymous mapping between a semantic typical sequence to the synonymous typical set B(n)
ϵ , we prove the lossless source coding theorem, that is, given a syntactic source U ∼p(u)
and the associated semantic variable ˜U, if the semantic code rate *R > H*s( ˜U), there exists a semantic source code satisfies P (n)
e
→0 with a sufficiently large code length n, otherwise, any codes cannot satisfy the lossless reconstruction of semantic information. Furthermore, we extend the Kraft inequality to the semantic version and provide an example of semantic Huffman coding. Thanks to the synonymous mapping and coding, the compression rate of semantic lossless source coding can be further lowered and the semantic compressive efficiency can outperform the classic source coding.

(5) Consider the problem of channel transmission, we investigate the jointly asymptotic equipartition property (JAEP) of semantic version and define the corresponding jointly typical set. Particularly, we find that, under the jointly synonymous mapping, the syntactically jointly typical set can be evenly divided into a series of jointly synonymous typical sets. By using random coding on jointly synonymous typical sets and jointly typical decoding, we prove the semantic channel coding theorem. This theorem reveals that given a channel with the
transition probability p(y|x), under the jointly synonymous mapping fxy, if the code rate is
lower than the semantic capacity, i.e., *R < C*s = maxfxy maxp(x) Is( ˜X, ˜Y ), there exists a
semantic channel code can satisfy the requirement of asymptotic error-free transmission, that
is P (n)
e
→0 with sufficiently large n, on the contrary, if *R > C*s, the error probability of any
code can not tend to zero. Since the semantic capacity Cs is no less than the classic capacity
C, i.e., Cs ≥C, we conclude that semantic channel coding can improve the capability of
information transmission. Furthermore, inspired by the jointly synonymous mapping, we
consider the decoding rule of semantic channel code and propose a maximum likelihood group (MLG) decoding algorithm. Unlike traditional ML decoding, the MLG algorithm calculates all the group likelihood probabilities and selects one synonymous codeword of the group with the maximum likelihood probability as the final result. Hence, we define the group Hamming distance as the construction metric and derive the group-wise error

probability to evaluate the performance of the semantic channel code.
(6) Consider the problem of lossy source coding, we define the semantic distortion and the
jointly typical set of source and reconstruction sequence. By using jointly typical encoding
based on synonymous typical set, we prove the semantic rate distortion coding theorem.

This theorem states that given a source X ∼p(x) with the associated semantic source ˜X, the synonymous mappings fx, fˆx, and the bounded semantic distortion function ds(˜x, ˆ˜x), if the code rate *R > R*s(D), there exist a sequences of semantic source codes, the semantic distortion satisfies Eds( ˜X, ˆ˜X) *< D* with sufficiently large n. Conversely, if *R < R*s(D), then the semantic distortion of any code meets Eds( ˜X, ˆ˜X) *> D*. Since the semantic rate distortion Rs(D) is no more than the classic counterpart R(D), that is, Rs(D) ≤R(D), it follows that semantic source coding can further compress the source data and achieve a lower rate than the classic source coding.

(7) We also investigate the measure of semantic information in the continuous case. Given
a continuous random variable U ∼p(u) and a synonymous mapping f, we define the
entropy of the associated semantic variable ˜U as Hs( ˜U) = −
R
p(u) log p(u)du −log S,
where S is named as the average synonymous length. Furthermore, we extend the semantic
conditional/joint entropy and semantic mutual information to the continuous case. Especially,
we derive the semantic capacity of Gaussian channel Cs = 1
σ2

and a lower
2 log

S4 
1 + P
bound Cs = 1
σ2

where P is the signal power and σ2 is the variance of Gaussian
2 log

1 + S4 P
noise. In addition, we obtain the channel capacity formula of time-limited, band-limited
and power-limited Gaussian channel, that is, Cs = B log
h
S4 
1 +
P
N0B
i
where B is the
bandwidth and N0 is the single-sided power spectral density of white Gaussian noise. In
addition, we also obtain the rate distortion function of Gaussian source, that is, Rs(D) =
log
P
S4D where P is the power of signal sample and D is the constraint of semantic distortion.
(8) Finally, we inspect the source and channel coding problem in the semantic sense. Both for
lossless and lossy cases, we prove the semantic source channel coding theorem. We find that
the code rate of semantic communication system satisfies Rs(D)(Hs( ˜U)) ≤R ≤Cs. Compared with the classic communication system, the code rate range of sematic communication can be further extended. This point reveals the significant potential of semantic coding.
The remainder of the paper is organized as follows. Section II presents the systematic model of semantic communication and introduces the concept of synonymous mapping. In Section III, we define the semantic entropy, the semantic joint and conditional entropy and discuss the basic properties of these measures. In Section IV we define the semantic relative entropies and the up/down semantic mutual information and discuss the corresponding properties. Then in Section V, the semantic channel capacity and the semantic rate distortion function are introduced and investigated. Furthermore, we explore the semantically asymptotic equipartition property and prove the semantic source coding theorem in Section VI. We extend the jointly asymptotic equipartition property in the semantic sense. By using random coding and jointly typical decoding, we prove the semantic channel coding theorem in Section VII. Correspondingly, By using jointly typical encoding, we prove the semantic rate distortion coding theorem in Section VIII. In addition, we extend the semantic information measures to the continuous case in Section IX. Especially, we derive the semantic capacity of Gaussian channel and the rate distortion of Gaussian source. In Section X, we prove the semantic source channel coding theorem. Finally, Section XI concludes the paper.

## Ii. Semantic Communication System And Synonymous Mapping

In this section, we first introduce the system model of semantic communication and describe the design criteria of semantic information transmission. Then we clarify the function of synonymous mapping and emphasize its key role in semantic communication.

## A. Notation Conventions

In this paper, calligraphy letters, such as X and Y, are mainly used to denote sets, and the cardinality of X is defined as *|X|*. The Cartesian product of X and Y is written as *X × Y*.

Let X n denote the n-th Cartesian power of X and Qn k=1 Xk denote the Cartesian product of n sets X1, *· · ·* , Xn. We write un to denote an n-dimensional vector (u1, u2, · · · *, u*n). We use the summation convention N[i1:im] to denote the integer summation of Ni1 + · · · + Nij + *· · ·* + Nim, where ∀Nij ∈N.

We use f : *X →Y* to denote a mapping from X to Y. Furthermore. the extended mapping f n : X n →Yn denotes an element-wise sequential mapping from X n to Yn.

We use dH(un, vn) to denote the Hamming distance between the binary vector un and vn.

Given ∀an, bn ∈Rn, let ∥an −bn∥denote the Euclidian distance between the vector an and bn.

Throughout this paper, log (·) means "logarithm to base 2" and ln (·) stands for the natural logarithm. Let (x)+ = max(x, 0) be the non-negative part of x. Let (·)T denote the transpose operation of the vector. Let E(Z) and Var(Z) denote the expectation and the variance of the random variable Z respectively.

## B. Semantic Communication System

The block diagram of semantic communication system is presented in Fig. 5. Compared with the system model of classic communication in Fig. 1, the semantic communication system extends the range of information processing and adds extra modules, such as semantic source, semantic destination, synonymous mapping and demapping.

At the transmitting side, the semantic source ˜U generates a semantic sequence and drives the syntactical source U produces a message. With the help of synonymous mapping f, the encoder transforms the syntactical message into a codeword and sends to the channel. On the other hand, the receiver performs the reverse operations. The decoder recovers the codeword from the received signal and feeds it into the syntactical destination V to reconstruct the message. Then the semantic destination ˜V obtains the message and reasons the meaning.

According to the general understanding of semantic information, the system design of semantic communication should obey the following criteria.

(i) **Invisibility of semantic information** Generally, semantic information is implied in syntactic information and cannot be directly observed. We only observe various data, such as text, speech, image or video so as to understand or infer the meaning of the message. Therefore, we highlight that the semantic information is invisible but perceptible. So only the syntactic information can be encoded and indirectly reveals the semantic information.
(ii) **Synonym of semantic information** The relationship between semantic information and
syntactic information is diverse and complex. However, whether it is single-modal or multi-modal data, synonym, that is, one meaning has multiple manifestations, is a popular phenomenon. The basic characteristic of synonyms is an one-to-many mapping, which means that we can use a set of synonymous data to indicate the same semantic information. Commonly, this mapping is predefined or deterministic.
By the first criterion, the semantic source/destination is a virtual module and implied behind the syntactic source/destination. Although Shannon stated that semantic information is irrelevant to the engineering problem, it is only suitable for the communication in Level A. If we design the communication system of Level B, the semantic source/dstination will indirectly affect the syntactical encoding and decoding process.

Correspondingly, by the second criterion, the synonymous mapping is deterministic rather than statistic, which is constructed by the common knowledge of transmitter/receiver or the system requirement. For the source coding, this mapping is mainly determined by the understanding of message. Although people have different opinions, they all use the same background knowledge. Therefore, knowledge base in the semantic communication system is a representative example. On the other hand, for the channel coding, this mapping can be created based on the transmission requirement. Unlike the traditional communication system, aided by the synonymous mapping, the reliability requirement of semantic communication can be relaxed from one-bit-no-error to tolerance of some error bits. This point will be further explained in Section VII.

## C. Synonymous Mapping

Now we formally introduce the definition of synonymous mapping as follows.

Definition 1. Given a syntactic information set U = {u1, · · · , ui, · · · , uN} and the corresponding semantic information set ˜U = {˜u1, · · · , ˜uis, *· · ·* , ˜u ˜
N}*, the synonymous mapping* f : ˜U →U is defined as the one-to-many mapping between ˜U and U.

Generally, the size of set ˜U is no more than U = {u1, u2, · · · *, u*N}, that is,
˜N ≤N.

Furthermore, under the synonymous mapping f, U is partitioned into a group of synonymous

sets Uis =
             n
              uN[1:(is−1)]+1, · · · , uN[1:(is−1)]+j, · · · , uN[1:(is−1)]+Nis
                                                                   o
                                                                      and ∀is ̸= js, Uis
                                                                                          T Ujs = ∅.

Therefore, we have |Uis| = Nis and U = S ˜
                                        N
                                        is=1 Uis.

Essentially, the synonymous mapping f generates an equivalence class partition of the syntactic

set. So we can construct the quotient set U/f = {Uis}.

For an arbitrary element ˜ui ∈˜U, is = 1, 2, · · · , ˜N, we have f : ˜uis →Uis. In the case of

non-confusion, for this mapping, we can drop out the subscript and present the synonymous set as U˜u, Figure 6 depicts an example of synonymous mapping. Each semantic element can be mapped into an equivalent set of syntactic elements and every set has one or many elements. Further, there is no overlap between any two sets.

Remark 2. In fact, the semantic variable ˜U is also a random variable. However, we emphasize that it is implied behind the random variable U and can deduce the syntactic message u. In many applications of deep learning, after the nonlinear processing of neural networks, the semantic features of input source data are extracted and mapped into a latent space. Here, the nonlinear mapping can be regarded as an instance of synonymous mapping and the latent space can be treated as sample space of semantic variable.

## Iii. Semantic Entropy

In this section, we first give the definition of semantic entropy, that is, the measure of semantic information. Then we discuss the property of semantic joint entropy and semantic conditional entropy.

## A. Semantic Information Measures

Let ˜U = {˜uis}
              ˜
              N
              is=1 and U = {ui}N
                                i=1 be a semantic alphabet and a syntactic alphabet respectively. Let U be a discrete random variable with alphabet U and probability mass function

p(u) = Pr {U = u} , u ∈U. Given a synonymous mapping f : ˜U →U, the semantic information

of a semantic symbol ˜uis ∈˜U can be measured as follows,

$$I_{s}\left(\tilde{u}_{i_{s}}\right)=-\log\left(p\left(\mathcal{U}_{i_{s}}\right)\right)=-\log\left(\sum_{i=N_{\left[1:\left(i_{s}-1\right)\right]}+1}^{N_{\left[1:\left(i_{s}-1\right)\right]}+N_{i_{s}}}p\left(u_{i}\right)\right)\,.\tag{1}$$
Definition 2. Given a discrete random variable U, the corresponding semantic variable ˜U, and the synonymous mapping f : ˜U →U, the semantic entropy of semantic variable ˜U is defined by

$$H_{s}(\tilde{U})=-\sum_{i_{s}=1}^{\tilde{N}}p\left(\mathcal{U}_{i_{s}}\right)\log p\left(\mathcal{U}_{i_{s}}\right)$$ $$=-\sum_{i_{s}=1}^{\tilde{N}}\sum_{i=N_{1}\left[1:\left(i_{s}-1\right)\right]+1}^{N_{i_{s}}}p\left(u_{i}\right)\log\left\langle\sum_{i=N_{1}\left[1:\left(i_{s}-1\right)\right]+1}^{N_{1}\left[1:\left(i_{s}-1\right)\right]+N_{i_{s}}}p\left(u_{i}\right)\right\rangle\tag{2}$$ $$=-\sum_{i_{s}=1}^{\tilde{N}}\sum_{i\in\mathcal{N}_{i_{s}}}p\left(u_{i}\right)\log\left(\sum_{i\in\mathcal{N}_{i_{s}}}p\left(u_{i}\right)\right)\,,$$

where Nis =
             
              N[1:(is−1)] + 1, · · · , N[1:(is−1)] + Nis
                                           	
                                             is the index set associated with Uis.

Essentially, semantic entropy is a functional of the distribution of U and the synonymous mapping f. Similar to information entropy, semantic entropy also indicates the integrity attribute of the

variable ˜U rather than single sample. Furthermore, it depends on the synonymous set partition

determined by the mapping f. In Eq. (2), the log is taken to the base 2 and we name it as the

semantic binary digit, that is, sebit. So the unit of semantic information is expressed as sebit.

The semantic entropy of ˜U can also be interpreted as the expectation of log
1
p(U˜u), that is,

$$H_{s}(\tilde{U})=\mathbb{E}_{p}\left[\log\frac{1}{p\left(\mathcal{U}_{\tilde{u}}\right)}\right].\tag{3}$$
For the semantic entropy, we have the following consequences by the definition.

Lemma 1.

$H_{s}(\tilde{U})\geq0$.

i∈Nis p (ui) ≤1.

Proof: Due to 0 ≤p(ui) ≤1 and PN
i=1 p(ui) = 1, we have 0 ≤p(Uis) = P

So it follows that log
                        1
                      p(Uis) ≥0.
                                                                                                □

Lemma 2. The semantic entropy is no more than the associated information entropy, that is,

$H_{s}(\tilde{U})\leq H(U)$.

Proof: According to the definition of Eq. (2), we have

$$H_{s}(\tilde{U})-H(U)=-\sum_{i_{s}=1}^{\tilde{N}}\sum_{i\in\tilde{N}_{i_{s}}}p\left(u_{i}\right)\log\left(\sum_{i\in\tilde{N}_{i_{s}}}p\left(u_{i}\right)\right)$$ $$+\sum_{i=1}^{N}p\left(u_{i}\right)\log p\left(u_{i}\right)\tag{6}$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{i\in\tilde{N}_{i_{s}}}p\left(u_{i}\right)\log\frac{p\left(u_{i}\right)}{\sum_{i\in\tilde{N}_{i_{s}}}p\left(u_{i}\right)}$$ $$\leq\sum_{i=1}^{N}p\left(u_{i}\right)\log1=0.$$
□
Lemma 3. The semantic entropy H( ˜U) is a concave function of p(u).

The proof is referred to Appendix A.

Theorem 1. Hs( ˜U) ≤log | ˜U|, where | ˜U| = ˜N stands for the number of semantic symbols. The equality holds if and only if the synonymous set Uis is uniformly distributed over U and U has arbitrary distribution over Uis.

| U   | u   |
|-----|-----|
| 1   |     |
| u   |     |
| 2   |     |
| u   |     |
| 3   |     |
| u   |     |
| 4   |     |
| u   |     |
| 5   |     |
| u   |     |
| 6   |     |
| p   | (   |
| ˜   |     |
| U   | ˜   |
| 1   |     |
| →{  |     |
| u   |     |
| 1   |     |
| }   |     |
| ˜   | u   |
| 2   |     |
| →{  |     |
| u   |     |
| 2   |     |
| , u |     |
| 3   |     |
| }   |     |
| ˜   | u   |
| 3   |     |
| →{  |     |
| u   |     |
| 4   |     |
| }   |     |
| ˜   | u   |
| 4   |     |
| →{  |     |
| u   |     |
| 5   |     |
| , u |     |
| 6   |     |
| }   |     |
| p   | (˜  |

Proof: By the maximal entropy theorem of discrete source, we can easily obtain the conclusion and further derive that the equality holds when p (˜uis) =
1
i∈Nis p (ui). So
˜
N = P
the elements in Uis can take an arbitrary distribution.

□
Note that the maximal semantic entropy log | ˜U| is no more than the maximum of information entropy log *|U|*. That means the uncertainty of semantic information can be further reduced with the help of synonymous mapping.

Example 1. Table I gives a probability distribution of source U and the associated semantic variable ˜U under a synonymous mapping. The entropy of source U *is calculated as* H(U) =
−P6
i=1 p(ui) log p(ui) = 2.471 bits. Correspondingly, the semantic entropy of ˜U is calculated as Hs( ˜U) = −P4
is=1 p(˜uis) log p(˜uis) = 1.971 *sebits. Evidently, we observe that* Hs( ˜U) < H(U).

## B. Semantic Joint Entropy And Semantic Conditional Entropy

The definition of semantic entropy can be further extended to a pair of semantic variables.

First, we introduce the jointly/conditionally synonymous mapping as following.

Definition 3. Given a pair of discrete semantic variables ( ˜U, ˜V ) and the corresponding random variable pairs (U, V ), fu : ˜U →U denotes the synonymous mapping from ˜U to U and we have fu : ˜uis →Uis where 1 ≤is ≤| ˜U| = ˜Nu and 1 ≤i ≤|U| = Nu. Similarly, we can define the synonymous mapping fv : ˜V →V. Furthermore, the jointly synonymous mapping fuv : ˜U × ˜V →U × V is defined as

$f_{uv}:(\tilde{u}_{i_{s}},\tilde{v}_{j_{s}})\rightarrow\mathcal{U}_{i_{s}}\times\mathcal{V}_{j_{s}}$.

Correspondingly, give a symbol ui, the conditionally synonymous mapping fv|u : ˜V|u →V|u is defined as

$$f_{v|u}:\tilde{v}_{j_{s}}|u_{i}\rightarrow\mathcal{V}_{j_{s}}|u_{i}.\tag{8}$$
Definition 4. Given a pair of discrete semantic variables ( ˜U, ˜V ) and the corresponding random variable pairs (U, V ) with a joint distribution p(u, v), under the joint mapping fuv : ˜U × ˜V →
U × V, the semantic joint entropy Hs( ˜U, ˜V ) is defined as

$$H_{s}(\tilde{U},\tilde{V})=-\sum_{i_{s}=1}^{\tilde{N}_{u}}\sum_{j_{s}=1}^{\tilde{N}_{v}}p\left(\mathcal{U}_{i_{s}}\times\mathcal{V}_{j_{s}}\right)\log p\left(\mathcal{U}_{i_{s}}\times\mathcal{V}_{j_{s}}\right)\tag{9}$$ $$=-\sum_{i_{s}=1}^{\tilde{N}_{u}}\sum_{j_{s}=1}^{\tilde{N}_{v}}\sum_{(u_{i},v_{j})\in\mathcal{U}_{i_{s}}\times\mathcal{V}_{j_{s}}}p\left(u_{i},v_{j}\right)$$ $$\cdot\log\sum_{(u_{i},v_{j})\in\mathcal{U}_{i_{s}}\times\mathcal{V}_{j_{s}}}p\left(u_{i},v_{j}\right).$$
Lemma 4. The semantic joint entropy is no more than the joint entropy, that is,

$$H_{s}(\tilde{U},\tilde{V})\leq H(U,V)\tag{10}$$
Proof: By the definition of Eq. (9), we have

$$H_{s}\left(\widehat{U},\widehat{V}\right)-H(U,V)$$ $$=-\sum_{i_{s}=1}^{\widehat{N}_{u}}\sum_{j_{s}=1}^{\widehat{N}_{v}}\sum_{(u_{i},v_{j})}p\left(u_{i},v_{j}\right)\log\sum_{(u_{i},v_{j})}p\left(u_{i},v_{j}\right)$$ $$+\sum_{(u_{i},v_{j})}p\left(u_{i},v_{j}\right)\log p\left(u_{i},v_{j}\right)\tag{11}$$ $$=\sum_{(u_{i},v_{j})}p\left(u_{i},v_{j}\right)\log\frac{p\left(u_{i},v_{j}\right)}{\sum_{(u_{i},v_{j})}p\left(u_{i},v_{j}\right)}$$ $$\leq\sum_{(u_{i},v_{j})}p\left(u_{i},v_{j}\right)\log1=0$$
□
We also define the semantic conditional entropy of a semantic variable given another random variable as follows.

Definition 5. Given a pair of discrete semantic variables ( ˜U, ˜V ) and the corresponding random variable pairs (U, V ) ∼p(u, v), under the conditional mapping fv|u : ˜V|u →V|u, the semantic conditional entropy Hs(˜V |U) is defined as

$$H_{s}(\tilde{V}|U)=-\sum_{i=1}^{N_{u}}\sum_{j_{s}=1}^{\tilde{N}_{v}}p\left(u_{i}\right)p\left(\mathcal{V}_{j_{s}}\left|u_{i}\right.\right)\log p\left(\mathcal{V}_{j_{s}}\left|u_{i}\right.\right)\tag{12}$$ $$=-\sum_{i=1}^{N_{u}}\sum_{j_{s}=1}^{\tilde{N}_{v}}\sum_{\left(u_{i},v_{j}\right)}p\left(u_{i},v_{j}\right)\log\sum_{v_{j}\in\mathcal{V}_{j_{s}}\left|u_{i}\right.}p\left(v_{j}\left|u_{i}\right.\right).$$
Similarly, we can also define the semantic conditional entropy Hs( ˜U|V ).

Lemma 5. The semantic conditional entropy is no more than the conditional entropy, that is,

$$H_{s}\left(\tilde{V}|U\right)\leq H\left(V|U\right)\tag{13}$$
Proof: The proof follows along the same lines as Lemma 4.

□
Similarly, we also attain the following lemma.

Lemma 6. If the semantic variables ˜U and ˜V are regarded as a new random variable respectively, the conditional entropy obeys the following relation, that is,

$$H\left(\tilde{V}|\tilde{U}\right)\leq H\left(V|\tilde{U}\right).\tag{14}$$
The chain rule of entropies with two variables can be indicated by the following theorem.

Theorem 2. (Chain Rule of Entropies with Two Variables):

$$H_{s}(\tilde{U})+H_{s}(\tilde{V}\,|U\,)\leq H_{s}(\tilde{U},\tilde{V})\leq H(V)+H_{s}(\tilde{U}\,|V\,)\leq H(U,V)\tag{15}$$
Proof: For the left inequality, due to the property of joint probability, we can write

$$\sum_{(u_{i},v_{j})\in\mathcal{U}_{s}\times\mathcal{V}_{j_{s}}}p(u_{i},v_{j})\leq\left(\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})\right)\left(\sum_{v_{j}\in\mathcal{V}_{s}\,|u_{i}}p(v_{j}\,|u_{i})\right).\tag{16}$$
So we take the negative logarithm and expectation of inequality to prove this inequality. The equality holds if and only if U and V are mutually independent. By using a similar method, we can prove Hs(˜V ) + Hs( ˜U |V ) ≤Hs( ˜U, ˜V ).

Similarly, we can write

$$p\left(v_{j}\right)\sum_{u_{i}\in\mathcal{U}_{i_{k}}\left|v_{j}\right.}p\left(u_{i}\left|v_{j}\right.\right)\leq\sum_{\left(u_{i},v_{j}\right)\in\mathcal{U}_{i_{k}}\times\mathcal{V}_{j_{k}}}p\left(u_{i},v_{j}\right).\tag{17}$$
Take the negative logarithm and expectation of inequality, we prove the medium inequality.

Correspondingly, we also write

$$p\left(u_{i}\right)\sum_{v_{j}\in\mathcal{V}_{j_{\delta}}\left|u_{i}\right.}p\left(v_{j}\left|u_{i}\right.\right)\leq\sum_{\left(u_{i},v_{j}\right)\in\mathcal{U}_{i_{\delta}}\times\mathcal{V}_{j_{\delta}}}p\left(u_{i},v_{j}\right)\tag{18}$$
and conclude that Hs( ˜U, ˜V ) ≤H(U) + Hs(˜V |U).

By using similar methods, we write

$$p(v_{j}\left|u_{i}\right.)\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p\left(u_{i}\right)\leq\sum_{\left(u_{i},v_{j}\right)\in\mathcal{U}_{i_{s}}\times\mathcal{V}_{j_{s}}}p\left(u_{i},v_{j}\right)\tag{19}$$
and conclude that Hs( ˜U, ˜V ) ≤Hs( ˜U) + H(V |U ).

In addition, we also write

$$p\left(u_{i}\left|v_{j}\right.\right)\sum_{v_{j}\in\mathcal{V}_{j_{\delta}}}p\left(v_{j}\right)\leq\sum_{\left(u_{i},v_{j}\right)\in\mathcal{U}_{i_{\delta}}\times\mathcal{V}_{j_{\delta}}}p\left(u_{i},v_{j}\right)\tag{20}$$
and conclude that Hs( ˜U, ˜V ) ≤Hs(˜V ) + H(U |V ).

By Lemma 5, due to H(V )+Hs( ˜U|V )−H(*U, V* ) = H(V )+Hs( ˜U|V )−H(V )−H(U|V ) ≤0, we prove the right inequality. Similarly, we can attain that H(U) + Hs(˜V |U) ≤H(*U, V* ).

□
Definition 6. Given a semantic sequence ( ˜U1, ˜U2, · · · , ˜Un) and the associated syntactic sequence
(U1, U2, · · · , Un), f n : ˜Un →Un denotes the sequential synonymous mapping from ˜U to U and we have

$$f^{n}:(\hat{u}^{n})\rightarrow\prod_{k=1}^{n}\mathcal{U}_{\hat{u}_{k}}.\tag{21}$$

  Correspondingly, give a subvector uk−1
                                     1
                                        , the conditionally synonymous mapping fuk|uk−1
                                                                                     1
                                                                                         :
˜U|uk−1
   1
       →U|uk−1
             1
                 is defined as

$$f_{u_{k}\,|u_{1}^{k-1}}:\tilde{u}_{k}|u_{i}^{k-1}\to{\cal U}_{\tilde{u}_{k}}\,|u_{1}^{k-1}.\tag{22}$$

## Theorem 3. (Chain Rule Of Sequential Entropy):

Given a semantic sequence ( ˜U1, ˜U2, · · · , ˜Un) and the associated syntactic sequence (U1, U2, · · · , Un), under the sequential synonymous mapping f n *and conditional synonymous mappings* fuk|uk−1
1
, we have

$$\sum_{k=1}^{n}H_{s}(\tilde{U}_{k}\left|U_{1}^{k-1}\right.)\leq H_{s}(\tilde{U}^{n})$$

$$\leq\tilde{H}(\tilde{U}_{1}^{n-1},U_{n})\leq\cdots\leq\tilde{H}(\tilde{U}_{1}^{m},U_{m+1}^{n})\tag{23}$$

$$\leq\cdots\leq\tilde{H}(\tilde{U}_{1},U_{2}^{n})$$

$$\leq H(U^{n})=\sum_{k=1}^{n}H(U_{k}\left|U_{1}^{k-1}\right.)$$

where ˜H( ˜U m
         1 , U n
             m+1) = Pm
                      k=1 H( ˜Uk
                               ˜U k−1
                                1
                                    )+Pn
                                        k=m+1 H(Uk
                                                    ˜U m
                                                     1 , U k−1
                                                         m+1) with m = n−1, · · · , 2, 1

denote sequential entropies.

Proof: In order to prove inequality Pn
                                    k=1 Hs( ˜Uk
                                               U k−1
                                                 1
                                                     ) ≤Hs( ˜U n), by using the property

of joint probability and synonymous mappings, we can write

$$\sum_{u_{1}^{n}\in\prod_{k=1}^{n}u_{k_{k}}}p(u_{1},u_{2},\cdots,u_{n})\leq\prod_{k=1}^{n}\sum_{u_{k}\in\mathcal{U}_{\partial_{k}}\left|u_{1}^{k-1}\right.}p(u_{k}\left|u_{1}^{k-1}\right.).\tag{24}$$
So we take the negative logarithm and expectation to prove this inequality. The equality holds if and only if all elements Uk are mutually independent.

  By using the chain rule of sequential entropy, after a permutation of sequence, we have
˜H( ˜U m
    1 , U n
        m+1) = ˜H(U n
                    m+1, ˜U m−1
                          1
                              )+H( ˜Um|U n
                                         m+1, ˜U m−1
                                               1
                                                   ) and ˜H( ˜U m−1
                                                            1
                                                                , U n
                                                                   m) = ˜H(U n
                                                                            m+1, ˜U m−1
                                                                                  1
                                                                                      )+

H(Um|U n
    m+1, ˜U m−1
       1
         ). According to Lemma 6, it follows that H( ˜Um|U n
                                  m+1, ˜U m−1
                                     1
                                       ) ≤H(Um|U n
                                              m+1, ˜U m−1
                                                 1
                                                   ).

So we can attain that

$$\tilde{H}(\tilde{U}_{1}^{m},U_{m+1}^{n})\leq\tilde{H}(\tilde{U}_{1}^{m-1},U_{m}^{n}),\ \ \mbox{for}\ m=n-1,\cdots,2,\tag{25}$$

and

$$\tilde{H}(\tilde{U}_{1},U_{2}^{n})\leq H(U^{n}).\tag{26}$$
Remark 3. In classic information theory, the joint entropy of a pair of random variables is the entropy of one variable plus the conditional entropy of the other variable, that is, H (*U, V* ) =
H(U) + H(V |U). On the other hand, the semantic joint entropy does not satisfy the addition of entropy and degrades to an inequality of information entropy plus semantic conditional entropy, that is, Hs( ˜U, ˜V ) ≤H(U) + Hs(˜V |U ) or Hs( ˜U, ˜V ) ≤H(V ) + Hs( ˜U |V ).

Example 2. Table II gives a joint probability distribution of random variable pair (U, V ). Table III illustrates the distribution of the associated semantic variable pair ( ˜U, ˜V ) under a joint synonymous mapping fuv. Table IV and Table V give the conditional distribution of the semantic variables ˜U|V and ˜V |U. The marginal distributions of the semantic variable ˜U and ˜V are depicted in Table VI.

| (   | U, V   |  )   | (   | u   |
|-----|--------|------|-----|-----|
| 1   |        |      |     |     |
| , v |        |      |     |     |
| 1   |        |      |     |     |
| )   | (      | u    |     |     |
| 1   |        |      |     |     |
| , v |        |      |     |     |
| 2   |        |      |     |     |
| )   | (      | u    |     |     |
| 1   |        |      |     |     |
| , v |        |      |     |     |
| 3   |        |      |     |     |
| )   | (      | u    |     |     |
| 1   |        |      |     |     |
| , v |        |      |     |     |
| 4   |        |      |     |     |
| )   | (      | u    |     |     |
| 1   |        |      |     |     |
| , v |        |      |     |     |
| 5   |        |      |     |     |
| )   |        |      |     |     |
| p   | (      | u, v | )   | 0   |
| (   | U, V   | )    | (   | u   |
| 2   |        |      |     |     |
| , v |        |      |     |     |
| 1   |        |      |     |     |
| )   | (      | u    |     |     |
| 2   |        |      |     |     |
| , v |        |      |     |     |
| 2   |        |      |     |     |
| )   | (      | u    |     |     |
| 2   |        |      |     |     |
| , v |        |      |     |     |
| 3   |        |      |     |     |
| )   | (      | u    |     |     |
| 2   |        |      |     |     |
| , v |        |      |     |     |
| 4   |        |      |     |     |
| )   | (      | u    |     |     |
| 2   |        |      |     |     |
| , v |        |      |     |     |
| 5   |        |      |     |     |
| )   |        |      |     |     |
| p   | (      | u, v | )   | 0   |
| (   | U, V   | )    | (   | u   |
| 3   |        |      |     |     |
| , v |        |      |     |     |
| 1   |        |      |     |     |
| )   | (      | u    |     |     |
| 3   |        |      |     |     |
| , v |        |      |     |     |
| 2   |        |      |     |     |
| )   | (      | u    |     |     |
| 3   |        |      |     |     |
| , v |        |      |     |     |
| 3   |        |      |     |     |
| )   | (      | u    |     |     |
| 3   |        |      |     |     |
| , v |        |      |     |     |
| 4   |        |      |     |     |
| )   | (      | u    |     |     |
| 3   |        |      |     |     |
| , v |        |      |     |     |
| 5   |        |      |     |     |
| )   |        |      |     |     |
| p   | (      | u, v | )   | 0   |
| (   | U, V   | )    | (   | u   |
| 4   |        |      |     |     |
| , v |        |      |     |     |
| 1   |        |      |     |     |
| )   | (      | u    |     |     |
| 4   |        |      |     |     |
| , v |        |      |     |     |
| 2   |        |      |     |     |
| )   | (      | u    |     |     |
| 4   |        |      |     |     |
| , v |        |      |     |     |
| 3   |        |      |     |     |
| )   | (      | u    |     |     |
| 4   |        |      |     |     |
| , v |        |      |     |     |
| 4   |        |      |     |     |
| )   | (      | u    |     |     |
| 4   |        |      |     |     |
| , v |        |      |     |     |
| 5   |        |      |     |     |
| )   |        |      |     |     |
| p   | (      | u, v | )   | 0   |
fuv
(˜u1, ˜v1) →{(u1, v1), (u2, v1)}
(˜u1, ˜v2) →{(u1, v2), (u2, v2)}
p(˜u, ˜v)
0.15
0.15
fuv
(˜u1, ˜v3) →{(u1, v3), (u2, v3)}
(˜u1, ˜v4) →{(u1, v4), (u1, v5), (u2, v4), (u2, v5)}
p(˜u, ˜v)
0.2
0.1
fuv
(˜u2, ˜v1) →{(u3, v1), (u4, v1)}
(˜u2, ˜v2) →{(u3, v2), (u4, v2)}
p(˜u, ˜v)
0.15
0.05
fuv
(˜u2, ˜v3) →{(u3, v3), (u4, v3)}
(˜u2, ˜v4) →{(u3, v4), (u4, v4), (u3, v5), (u4, v5)}
p(˜u, ˜v)
0
0.2

The joint entropy of (U, V ) is calculated as H(U, V ) = −P4
                                                             i=1
                                                                 P5
                                                                   j=1 p(ui, vj) log p(ui, vj)

= 3.5842 bits.

The semantic joint entropy of ( ˜U, ˜V ) is Hs( ˜U, ˜V ) = −P2
                                                         is=1
                                                             P4
                                                               js=1 p(˜uis, ˜vjs) log p(˜uis, ˜vjs)

= 2.7087 sebits.

The conditional entropies are H(U|V ) = −P4
                                              i=1
                                                 P5
                                                    j=1 p(ui, vj) log p(ui|vj) = 1.3377 bits

and H(V |U) = −P5
                 j=1
                    P4
                      i=1 p(ui, vj) log p(vj|ui) = 1.6132 bits.

Correspondingly, the semantic conditional entropies are Hs( ˜U|V ) = −P2
                                                                        is=1
                                                                            P5
                                                                               j=1 p(˜uis, vj)

· log p(˜uis|vj) = 0.6623 sebits and Hs(˜V |U) = −P4
                                                                           js=1
                                                                                 P4
                                                                                     i=1 p(˜vjs, ui) log p(˜vjs|ui) = 1.4755 sebits.

Thus we have Hs( ˜U|V ) = 0.6623 sebits *< H*(U|V ) = 1.3377 *bits and* Hs(˜V |U) = 1.4755 sebits <
H(V |U) = 1.6132 bits.

The entropies of random variables U and V *are calculated as* H(U) = H(0.3, 0.3, 0.2, 0.2) =
1.971 *bits and* H(V ) = H(0.3, 0.2, 0.2, 0.2, 0.1) = 2.2464 bits respectively. Then, the semantic entropies of ˜U and ˜V *are* Hs( ˜U) = Hs(0.6, 0.4) = 0.971 *sebits and* Hs(˜V ) = Hs(0.3, 0.2, 0.2, 0.3) =
1.971 *sebits respectively. So it follows that* Hs( ˜U, ˜V ) = 2.7087 sebits < H(*U, V* ) = 3.5842 bits, Hs(˜V ) + Hs( ˜U|V ) = 2.6633 sebits *< H*s( ˜U, ˜V ) = 2.7087 sebits *< H*(V ) + Hs( ˜U|V ) =
2.9087 *sebits, and* Hs( ˜U) + Hs(˜V |U) = 2.4465 sebits *< H*s( ˜U, ˜V ) = 2.7087 sebits *< H*(U) +
Hs(˜V |U) = 3.4465 sebits.

˜U|V
˜u1|v1 →{u1, u2}|v1
˜u1|v2 →{u1, u2}|v2
˜u1|v3 →{u1, u2}|v3
˜u1|v4 →{u1, u2}|v4
˜u1|v5 →{u1, u2}|v5
p(˜u|v)
0.5
0.75
1
0.5
0
˜U|V
˜u2|v1 →{u3, u4}|v1
˜u2|v2 →{u3, u4}|v2
˜u2|v3 →{u3, u4}|v3
˜u2|v4 →{u3, u4}|v4
˜u2|v5 →{u3, u4}|v5
p(˜u|v)
0.5
0.25
0
0.5
1
˜V |U
˜v1|u1 →{v1}|u1
˜v1|u2 →{v1}|u2
˜v1|u3 →{v1}|u3
˜v1|u4 →{v1}|u4
p(˜v|u)
1/6
1/3
0.5
0.25
˜V |U
˜v2|u1 →{v2}|u1
˜v2|u2 →{v2}|u2
˜v2|u3 →{v2}|u3
˜v2|u4 →{v2}|u4
p(˜v|u)
1/3
1/6
0.25
0
˜V |U
˜v3|u1 →{v3}|u1
˜v3|u2 →{v3}|u2
˜v3|u3 →{v3}|u3
˜v3|u4 →{v3}|u4
p(˜v|u)
0.5
1/6
0
0
˜V |U
˜v4|u1 →{v4, v5}|u1
˜v4|u2 →{v4, v5}|u2
˜v4|u3 →{v4, v5}|u3
˜v4|u4 →{v4, v5}|u4
p(˜v|u)
0
1/3
0.25
0.75
fu
˜u1 →{u1, u2}
˜u2 →{u3, u4}
fv
˜v1 →{v1}
˜v2 →{v2}
˜v3 →{v3}
˜v4 →{v4, v5}
p(˜u)
0.6
0.4
p(˜v)
0.3
0.2
0.2
0.3

## Iv. Semantic Relative Entropy And Mutual Information

In this section, we apply the synonymous mapping to define semantic relative entropy and semantic mutual information. Three semantic relative entropies are measures of the distance between two semantic/syntactic variables. We introduce two measures, such as up/down semantic mutual information to evaluate the reduction in the semantic information of one variable due to the knowledge of the other variable.

## A. Semantic Relative Entropy

In classic information theory, the relative entropy or Kullback Leibler distance D (p∥q) =
P
u∈U p(u) log p(u)
q(u) is used to measure the difference between two probability mass function p(u) and q(u). Similarly, by the synonymous mapping, we can define the semantic relative entropy as following.

Definition 7. Given the semantic variable ˜U and the random variable U with two probability mass function p(u) and q(u), under the synonymous mapping f : ˜U →U, the full semantic relative entropy is defined as

$$D_{s}\left(p_{s}\|q_{s}\right)=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{s}}q(u_{i})}.\tag{27}$$
Two partial semantic relative entropies are defined as

$$D_{s}\left(p_{s}\|q\right)=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})}{q(u_{i})},\tag{28}$$
and

$$D_{s}\left(p\|q_{s}\right)=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})\log\frac{p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{s}}q(u_{i})}.\tag{29}$$

$$u_{i}\in\mathcal{U}_{s_{s}}\ q(u_{i})$$ Hereafter, in order to simplify the presentation, we use $p_s=\sum_{u_i\in\mathcal{U}_s}p(u_i)$ and $q_s=\sum_{u_i\in\mathcal{U}_s}p(u_i)$.

to stand for two different probability distributions of semantic variable ˜U.

Similar to the relative entropy, we have the following basic inequality for the semantic relative entropy.

Theorem 4. (Semantic information inequality): Let p(u), q(u), u ∈U, be two probability mass

function. Given the synonymous mapping $f$, we have_

$$\begin{cases}D_{*}(p_{*}\|q_{*})\geq0\\ D_{*}(p_{*}\|q)\geq D_{*}(p_{*}\|p)\\ D_{*}(p\|q_{*})\geq D_{*}(p\|p_{*})\end{cases}\tag{30}$$

_with equality if and only if_

$$\begin{cases}\sum_{u\in\mathcal{U}_{*}}p(u_{i})=\sum_{u\in\mathcal{U}_{*}}q(u_{i}),\ \ \text{for all}\ \mathcal{U}_{i_{*}}\\ p(u_{i})=q(u_{i}),\ \ \ \ \ \ \text{for all}\ \mathcal{U}_{i_{*}}\\ \sum_{u_{i}\in\mathcal{U}_{*}}p(u_{i})=\sum_{u_{i}\in\mathcal{U}_{*}}q(u_{i}),\ \ \text{for all}\ \mathcal{U}_{i_{*}}\end{cases}\tag{31}$$
Proof: For the first inequality, by Jensen's inequality, we can write

$$-D_{s}\left(p_{s}\|q_{s}\right)=-\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{s}}q(u_{i})}$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{s}}q(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})}$$ $$\leq\log\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})\frac{\sum_{u_{i}\in\mathcal{U}_{s}}q(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{s}}p(u_{i})}\tag{32}$$ $$=\log\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{s}}q(u_{i})$$ $$=\log1=0.$$
For the second inequality, we can write

$$D_{s}\left(p_{s}||q\right)=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{is}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{is}}p(u_{i})}{q(u_{i})}$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{is}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{is}}p(u_{i})}{p(u_{i})}$$ $$+\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{is}}p(u_{i})\log\frac{p(u_{i})}{q(u_{i})}\tag{33}$$ $$\geq\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{is}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{is}}p(u_{i})}{p(u_{i})}$$ $$=D_{s}(p_{s}||p)\geq0.$$
For the third inequality, we further derive as follows

˜ N X X Ds (p∥qs) = is=1 ui∈Uis q(ui) ui∈Uis p(ui) log p(ui) P ˜ N X X = is=1 ui∈Uis p(ui) ui∈Uis p(ui) log p(ui) P P (34) ˜ N X X + ui∈Uis p(ui) P is=1 ui∈Uis q(ui) ui∈Uis p(ui) log ˜ N X X ≥ is=1 ui∈Uis p(ui) ui∈Uis p(ui) log p(ui) P = Ds(p∥ps).
Note that Ds (p∥qs) may be negative. In the practical application, we can take the non-negative value, that is, (Ds (p∥qs))+.

□

$D_{s}(p||q_{s})\leq D_{s}(p_{s}||q_{s})\leq D_{s}(p_{s}||q)$. (35)
Proof: For the left inequality, we can write

$$D_{s}(p||q_{s})-D_{s}(p_{s}||q_{s})$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log\frac{p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}q(u_{i})}$$ $$-\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}q(u_{i})}\tag{36}$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log\frac{p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})}$$ $$\leq\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log1=0.$$
For the right inequality, we can write

$$D_{s}(p_{s}||q_{s})-D_{s}(p_{s}||q)$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{t_{s}}}q(u_{i})}$$ $$-\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})}{q(u_{i})}\tag{37}$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log\frac{q(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{t_{s}}}q(u_{i})}$$ $$\leq\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log1=0.$$
□
Corollary 2.

$$D_{s}(p||q_{s})\leq D(p||q)\leq D_{s}(p_{s}||q).\tag{38}$$
Proof: For the left inequality, we can write

$$D_{s}(p||q_{s})-D(p||q)$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log\frac{p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}q(u_{i})}$$ $$-\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log\frac{p(u_{i})}{q(u_{i})}\tag{39}$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log\frac{q(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}q(u_{i})}$$ $$\leq\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p(u_{i})\log1=0.$$
For the right inequality, we can write

$$D_{s}(p||q)-D_{s}(p_{s}||q)$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log\frac{p(u_{i})}{q(u_{i})}$$ $$-\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log\frac{\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})}{q(u_{i})}\tag{40}$$ $$=\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log\frac{p(u_{i})}{\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})}$$ $$\leq\sum_{i_{s}=1}^{\tilde{N}}\sum_{u_{i}\in\mathcal{U}_{t_{s}}}p(u_{i})\log1=0.$$
□
Theorem 5. Ds(ps∥qs), Ds(ps∥q), and Ds(p∥qs) are convex in the pair (p, q). Equivalently, given (p1, q1) and (p2, q2) are two pairs of probability mass functions, for all 0 ≤θ ≤1, we

have_

$$\begin{cases}D_{s}(\theta p_{s,1}+(1-\theta)p_{s,2}\|\theta q_{s,1}+(1-\theta)q_{s,2})\\ \\ \leq\theta D_{s}(p_{s,1}\|q_{s,1})+(1-\theta)D_{s}(p_{s,2}\|q_{s,2})\\ D_{s}(\theta p_{s,1}+(1-\theta)p_{s,2}\|\theta q_{1}+(1-\theta)q_{2})\\ \\ \leq\theta D_{s}(p_{s,1}\|q_{1})+(1-\theta)D_{s}(p_{s,2}\|q_{2})\\ D_{s}(\theta p_{1}+(1-\theta)p_{2}\|\theta q_{s,1}+(1-\theta)q_{s,2})\\ \\ \leq\theta D_{s}(p_{1}\|q_{s,1})+(1-\theta)D_{s}(p_{2}\|q_{s,2})\end{cases}\tag{41}$$
The proof is referred to Appendix B.

Remark 4. In neural network model, relative entropy or cross entropy is an important cost function used to training. For some deep learning application involved semantic information, such as clustering, classification, recognition, we can use semantic relative entropy to alternate the classic counterpart so as to further improve the system performance.

## B. Semantic Mutual Information

We now introduce up semantic mutual information, which is a partial relative entropy to indicate the large reduction in the semantic information of one variable due to the knowledge of the other.

Definition 8. Consider two semantic variables ˜U and ˜V and two associated random variables U and V with a joint probability mass function p(u, v) and marginal probability mass function p(u) and p(v). Given the jointly synonymous mapping fuv : ˜U × ˜V →U × V, the up semantic mutual information Is( ˜U; ˜V ) is the partial entropy between the joint distribution ps (u, v) and the product distribution p(u)p(v), i.e.,

$$I^{s}(\tilde{U};\tilde{V})=-\sum_{i_{s}=1}^{\tilde{N}_{u}}\sum_{j_{s}=1}^{\tilde{N}_{v}}\sum_{(u_{i},v_{j})\in\mathcal{U}_{s}\times\mathcal{V}_{j_{s}}}p\left(u_{i},v_{j}\right)\tag{42}$$ $$\cdot\log\frac{p\left(u_{i}\right)p\left(v_{j}\right)}{\sum_{(u_{i},v_{j})\in\mathcal{U}_{s}\times\mathcal{V}_{j_{s}}}p\left(u_{i},v_{j}\right)}$$ $$=D_{s}\left(p_{s}\left(u,v\right)\|p(u)p(v)\right)$$ $$=H(U)+H(V)-H_{s}(\tilde{U},\tilde{V}).$$
Similarly, the down semantic mutual information Is( ˜U; ˜V ) is the partial entropy between the joint distribution p (u, v) and the product distribution ps(u)ps(v), i.e.,

$$I_{s}(\tilde{U};\tilde{V})=-\sum_{i_{s}=1}^{\tilde{N}_{u}}\sum_{j_{s}=1}^{\tilde{N}_{v}}\sum_{(u_{i},v_{j})\in\mathcal{U}_{i_{s}}\times\mathcal{V}_{j_{s}}}p\left(u_{i},v_{j}\right)\tag{43}$$ $$\cdot\log\frac{\sum_{u_{i}\in\mathcal{U}_{i_{s}}}p\left(u_{i}\right)\sum_{v_{j}\in\mathcal{V}_{j_{s}}}p\left(v_{j}\right)}{p\left(u_{i},v_{j}\right)}$$ $$=D_{s}\left(p\left(u,v\right)\|p_{s}(u)p_{s}(v)\right)$$ $$=H_{s}(\tilde{U})+H_{s}(\tilde{V})-H(U,V).$$
Correspondingly, the full semantic mutual information ˜Is( ˜U; ˜V ) is defined as,

˜ Nu X ˜ Nv X X ˜Is( ˜U; ˜V ) = − is=1 js=1 (ui,vj)∈Uis×Vjs p (ui, vj) P ui∈Uis p (ui) P · log (44) vj∈Vjs p (vj) P (ui,vj)∈Uis×Vjs p (ui, vj) =Ds (ps (*u, v*) ∥ps(u)ps(v)) =Hs( ˜U) + Hs(˜V ) −Hs( ˜U, ˜V ). ˜Is( ˜U; ˜V ) ≥0

Theorem 6. (Non-negativity of semantic mutual information): For any two semantic variables,
˜U, ˜V , given the jointly synonymous mapping fuv, we have
                      
                      
                      
                      
                      
                      

(45) Is( ˜U; ˜V ) ≥H(*U, V* ) −Hs( ˜U, ˜V ) ≥0 Is( ˜U; ˜V ) ≥Hs( ˜U, ˜V ) −H(*U, V* )      ps (ui) ps (vj) = ps (ui, vj) with equality if and only if       (46) p (ui) p (vj) = p (ui, vj) ps (ui) ps (vj) = ps (ui, vj)     
Proof: By using Theorem 4, we obtain ˜Is( ˜U; ˜V ) = Ds (ps (*u, v*) ∥ps(u)ps(v)) ≥0. So the equality holds if and only if ps (ui) ps (vj) = ps (ui, vj), which means ˜U and ˜V are independent.

Similarly, we can prove the other two inequalities.

□
Note that the down semantic mutual information Is( ˜U; ˜V ) may be negative. Considering the practical case, we can set (Is( ˜U; ˜V ))+.

Theorem 7. The up/down semantic mutual information is a concave function of p(u) for fixed p(v|u) and a convex function of p(v|u) for fixed p(x).

This theorem is proved in Appendix C. We now show the relationship among these mutual information measures.

## Corollary 3.

$$I_{s}(\tilde{U};\tilde{V})\leq I(U;V)\leq H(V)-H_{s}(\tilde{V}\,|U\,)\leq I^{s}(\tilde{U};\tilde{V})\tag{47}$$
Proof: To prove the left inequality, by Lemma 2, we can write

$$I_{s}(\tilde{U};\tilde{V})-I(U;V)=H_{s}(\tilde{U})+H_{s}(\tilde{V})-H(U)-H(V)\leq0.\tag{48}$$
For the medium inequality, due to Lemma 5, we have

$$I(U;V)-(H(V)-H_{s}(\tilde{V}\,|U\,))=-H(V|U)+H_{s}(\tilde{V}\,|U\,)\leq0.\tag{49}$$
For the right inequality, due to Theorem 2, we have

$$H(V)-H_{s}(\tilde{V}\,|U\,)-I^{s}(\tilde{U};\tilde{V})\tag{50}$$ $$=-H_{s}(\tilde{V}\,|U\,)-H(U)+H_{s}(\tilde{U},\tilde{V})\leq0.$$

## Corollary 4.

$$I_{s}(\tilde{U};\tilde{V})\leq\tilde{I}_{s}(\tilde{U};\tilde{V})\leq H(V)-H_{s}(\tilde{V}\,|U\,)\leq I^{s}(\tilde{U};\tilde{V})\tag{51}$$
Proof: By Definition 8 and Lemma 4, we can write the left inequality as

$$I_{s}(\tilde{U};\tilde{V})-\tilde{I}_{s}(\tilde{U};\tilde{V})=-H(U,V)+H_{s}(\tilde{U},\tilde{V})\leq0.\tag{52}$$
Correspondingly, by Definition 8, Lemma 2, and Theorem 2, we can also write the medium

inequality as

$$\begin{split}\tilde{I}_{s}(\tilde{U};\tilde{V})&-(H(V)-H_{s}(\tilde{V}\,|U))\\ &=H_{s}(\tilde{U})+H_{s}(\tilde{V})-H_{s}(\tilde{U},\tilde{V})-H(V)+H_{s}(\tilde{V}\,|U)\\ &\leq H_{s}(\tilde{U})+H_{s}(\tilde{V}\,|U)-H_{s}(\tilde{U},\tilde{V})\leq0.\end{split}\tag{53}$$

## Theorem 8. (Chain Rule Of Sequential Mutual Information):

Given a pair of semantic sequences ( ˜U n, ˜V n) and the associated pair of syntactic sequences
(U n, V n), under the sequential synonymous mapping f n uv and conditional synonymous mappings fuk,vn|uk−1
1
, for the sequential version of down semantic mutual information, we have

$$I_{s}(\tilde{U}^{n};\tilde{V}^{n})\leq I_{s}(\tilde{U}^{n-1}_{1},U_{n};\tilde{V}^{n})$$ $$\leq\cdots\leq I_{s}(\tilde{U}^{m}_{1},U^{n}_{m+1};\tilde{V}^{n})$$ $$\leq\cdots\leq I_{s}(\tilde{U}_{1},U^{n}_{2};\tilde{V}^{n})\tag{54}$$ $$\leq I(U^{n};V^{n})=\sum_{k=1}^{n}I(U_{k};V^{n}\left|U^{k-1}_{1}\right.),$$

where Is( ˜U m
         1 , U n
             m+1; ˜V n) = ˜H( ˜U m
                           1 , U n
                               m+1) + Hs(˜V n) −H(U n, V n) with m = n −1, · · · , 2, 1.

Similarly, for the sequential version of up semantic mutual information, we have

$$I(U^{n};V^{n})=\sum_{k=1}^{n}I(U_{k};V^{n}\left|U_{1}^{k-1}\right.)\tag{55}$$ $$\leq I^{s}(\tilde{U}_{1},U_{2}^{n};\tilde{V}^{n})$$ $$\leq\cdots\leq I^{s}(\tilde{U}_{1}^{m},U_{m+1}^{n};\tilde{V}^{n})$$ $$\leq\cdots\leq I^{s}(\tilde{U}_{1}^{n-1},U_{n};\tilde{V}^{n})\leq I^{s}(\tilde{U}^{n};\tilde{V}^{n}),$$

where Is( ˜U m
         1 , U n
             m+1; ˜V n) = H(U n) + H(V n) −˜H( ˜U m
                                             1 , U n
                                                m+1, ˜V n) with m = 1, 2, · · · , n.

Proof: By Theorem 3, we derive that

$$I_{s}(\tilde{U}^{n};\tilde{V}^{n})=H_{s}(\tilde{U}^{n})+H_{s}(\tilde{V}^{n})-H(U^{n},V^{n})$$ $$\leq H_{s}(\tilde{U}^{n-1}_{1},U_{n})+H_{s}(\tilde{V}^{n})-H(U^{n},V^{n})$$ $$\leq\cdots\leq H_{s}(\tilde{U}^{n}_{1},U^{n}_{m+1})+H_{s}(\tilde{V}^{n})-H(U^{n},V^{n})\tag{56}$$ $$\leq H_{s}(\tilde{U}^{n-1}_{1},U^{n}_{m})+H_{s}(\tilde{V}^{n})-H(U^{n},V^{n})$$ $$\leq\cdots\leq H_{s}(\tilde{U}_{1},U^{n}_{2})+H_{s}(\tilde{V}^{n})-H(U^{n},V^{n})$$ $$\leq H(U^{n})+H(V^{n})-H(U^{n},V^{n})=I(U^{n};V^{n}).$$
So we prove the first chain of inequalities.

Similarly, by Theorem 3, we can attain that

$$I(U^{n};V^{n})=H(U^{n})+H(V^{n})-H(U^{n},V^{n})$$ $$\leq H(U^{n})+H(V^{n})-\tilde{H}(\tilde{U}_{1},U_{2}^{n},\tilde{V}^{n})$$ $$\leq\cdots\leq H(U^{n})+H(V^{n})-\tilde{H}(\tilde{U}_{1}^{n-1},U_{m}^{n},\tilde{V}^{n})\tag{57}$$ $$\leq H(U^{n})+H(V^{n})-\tilde{H}(\tilde{U}_{1}^{n},U_{m+1}^{n},\tilde{V}^{n})$$ $$\leq\cdots\leq H(U^{n})+H(V^{n})-\tilde{H}(\tilde{U}_{1}^{n-1},U_{n},\tilde{V}^{n})$$ $$\leq H(U^{n})+H(V^{n})-H_{s}(\tilde{U}^{n},\tilde{V}^{n})=I^{s}(\tilde{U}^{n};\tilde{V}^{n}).$$
So we prove the second chain of inequalities.

□
Remark 5. In classic information theory, the mutual information satisfies the following equalities

$$I(U;V)=H(U)-H(U|V)\tag{58}$$ $$=H(V)-H(V|U)$$ $$=H(U)+H(V)-H(U;V).$$
On the contrary, in semantic information theory, the semantic mutual information only obeys

the degraded inequalities_

$$\begin{split}&\int I_{s}(\tilde{U};\tilde{V})\leq\tilde{I}_{s}(\tilde{U};\tilde{V})\leq H(V)-H_{s}(\tilde{V}\,|U)\leq I^{s}(\tilde{U};\tilde{V})\\ &\quad\mathsf{I}_{s}(\tilde{U};\tilde{V})\leq\tilde{I}_{s}(\tilde{U};\tilde{V})\leq H(U)-H_{s}(\tilde{U}\,|V)\leq I^{s}(\tilde{U};\tilde{V}).\end{split}\tag{59}$$
Since the size relationship between mutual information and semantic mutual information is uncertain, we only use the up and down semantic mutual information to indicate the upper bound and the lower bound respectively. In fact, by Theorem 2, the up semantic mutual information Is( ˜U; ˜V ) can be further upper bounded by H(U)+H(V )−Hs( ˜U)−Hs(˜V |U ). However, since this bound has not a clear physical meaning, we still use Is( ˜U; ˜V ) as the upper bound.

## Theorem 9. (Semantic Processing Inequality):

Given a Markov chain U →V →W, then we have

$$I_{s}(\tilde{U};\tilde{W})\leq I_{s}(\tilde{U};W)\leq I(U;W)\leq I(U;V)\leq I^{s}(\tilde{U};V)\leq I^{s}(\tilde{U};\tilde{V}).\tag{60}$$
Proof: By the chain rule of semantic mutual information (Theorem 8), we can expand
(down) mutual information in two different ways,

$$I(U;VW)=I(U;V)+I(U;W|V)\tag{61}$$ $$=I(U;W)+I(U;V|W)$$ $$\geq I_{s}(\tilde{U};W)+I(U;V|W)$$ $$\geq I_{s}(\tilde{U};\tilde{W}).$$
Since U and W are conditionally independent give V , we have I(U; W|V ) = 0. Due to I(U; V |W) ≥0, we have

$$I(U;V)\geq I(U;W)\geq I_{s}(\tilde{U};W)\geq I_{s}(\tilde{U};\tilde{W}).\tag{62}$$
Similarly, by the same chain rule, we can expand (up) mutual information in two different

ways,

$$I(U;VW)=I(U;V)+I(U;W|V)\tag{63}$$ $$=I(U;W)+I(U;V|W)$$ $$\leq I^{*}(\hat{U};V)+I(U;W|V)\ \leq I^{*}(\hat{U};\hat{V}).$$
Thus, we have

$$I(U;W)\leq I(U;V)\leq I^{s}(\tilde{U};V)\leq I^{s}(\tilde{U};\tilde{V})\tag{64}$$
and complete the proof.

□
Example 3. According to the joint distribution in Table II, we calculate the mutual information between U *and* V as I(U; V ) = H(U) + H(V ) −H(*U, V* ) = 0.6332 bits. Correspondingly, by using the joint distribution in Table III, we can compute the up semantic mutual information as Is( ˜U; ˜V ) = H(U) + H(V ) −Hs( ˜U; ˜V ) = 1.5087 sebits.

Similarly, by using the distribution in Table VI, we compute the down semantic mutual information as Is( ˜U; ˜V ) = Hs(U) + Hs(V ) −H(U; V ) = −0.6422 sebits. Consider the nonnegative requirement, we set Is( ˜U; ˜V ) = max{−0.6422, 0} = 0 sebits. By using the distribution in Table IV and Table V, we calculate that H(V ) −Hs(˜V |U ) = 0.7709 sebits and H(U) −Hs( ˜U |V ) = 1.3087 *sebits. So we conclude that* Is( ˜U; ˜V ) = 0 sebits *< I*(U; V ) =
0.6332 bits *< H*(V ) −Hs(˜V |U ) = 0.7709 sebits *< H*(U) −Hs( ˜U |V ) = 1.3087 sebits <
Is( ˜U; ˜V ) = 1.5087 sebits.

## V. Semantic Channel Capacity And Semantic Rate Distortion

In this section, we introduce the semantic channel capacity and semantic rate distortion. We use the maximum up semantic mutual information to indicate the former. Similarly, the minimum down semantic mutual information is used to present the latter.

## A. Semantic Channel Capacity

Generally, the discrete channel of semantic communication can be modeled as the form of a five-tuple, that is, n
˜
X, X, Y, ˜Y, p(Y |X)
o
. Here, X and Y are the input and output syntactical alphabet. And ˜
X and ˜Y are the corresponding input and output semantic alphabet. Furthermore, p(Y |X) is the channel transition probability.

Definition 9. Given a discrete memoryless channel n
˜
X, X, Y, ˜Y, p(Y |X)
o
, the semantic channel capacity is defined as

$$C_{s}=\max_{f_{xy}}\max_{p(x)}I^{s}(\tilde{X};\tilde{Y})\tag{65}$$ $$=\max_{f_{xy}}\max_{p(x)}\sum_{i_{s}}\sum_{j_{s}}\sum_{(x_{i},y_{j})\in\mathcal{X}_{s}\times\mathcal{Y}_{j_{s}}}p\left(x_{i}\right)p\left(y_{j}\left|x_{i}\right.\right)$$ $$\cdot\log\frac{\sum_{(x_{i},y_{j})\in\mathcal{X}_{s}\times\mathcal{Y}_{j_{s}}}p\left(x_{i},y_{j}\right)}{p\left(x_{i}\right)p\left(y_{j}\right)}$$ $$=\max_{f_{xy}}\max_{p(x)}\left[H(X)+H(Y)-H_{s}(\tilde{X},\tilde{Y})\right]$$
where the maximum is taken over all possible input distribution p(x) and the jointly synonymous mapping fxy : ˜
X × ˜Y →X × Y.

By Corollary 3, the channel mutual information is no more than the up semantic mutual information, that is, I(X; Y ) ≤Is( ˜X; ˜Y ). Thus, we conclude that the channel capacity is no more than the semantic capacity, i.e., C ≤Cs.

## B. Semantic Rate Distortion

Similar to the classic rate distortion theory, given a semantic/syntactic source distribution and a distortion measure, we also need to investigate the minimum rate description required to achieve a specific distortion.

Definition 10. Given a discrete source X ∼p(x), x ∈X and the associated semantic source
˜X, the decoder outputs an estimate ˆX with the associated semantic variable ˆ˜X. Under the synonymous mapping fx and fˆx, the semantic distortion measure is a mapping

$$d_{s}:\tilde{\cal X}\times\tilde{\cal X}\to\mathbb{R}^{+}\tag{66}$$
from the Cartesian product of semantic source alphabet and reconstruction alphabet into the set of non-negative real numbers. The semantic distortion ds(˜xis, ˆ˜xjs) = ds(Xis, ˆ
Xjs) is a measure of the cost of representing the semantic symbol ˜xis by the symbol ˆ˜xjs, equivalently, which is a cost of representing the syntactical symbol set Xis *by the reproduction set* ˆ
Xjs. For the traditional lossy source coding, the semantic distortion can be measured using Hamming distortion or mean squared error (MSE) distortion. On the other hand, for the lossy coding with neural network model, the semantic distortion can also be evaluated by using word error rate (WER), structure similarity index measure (SSIM), learned perceptual image patch similarity (LPIPS) etc.

So the average semantic distortion is defined as

$$\bar{d}_{s}=\mathbb{E}\left[d_{s}(\hat{x},\hat{\bar{x}})\right]\tag{67}$$ $$=\sum_{i_{s}}\sum_{j_{s}}\sum_{(x_{i},\hat{x}_{j})\in\mathcal{X}_{i_{s}}\times\hat{\mathcal{X}}_{j_{s}}}p\left(x_{i},\hat{x}_{j}\right)d_{s}(\mathcal{X}_{i_{s}},\hat{\mathcal{X}}_{j_{s}}).$$
Furthermore, the test channel set PD is defined as

$$P_{D}=\left\{p(\hat{x}\left|x\right.):\bar{d}_{s}=\mathbb{E}\left[d_{s}\left(\hat{x},\hat{\bar{x}}\right)\right]\leq D\right\}.\tag{68}$$
Next, we give the formal definition of semantic rate distortion.

Definition 11. Given an i.i.d. source X with distribution p(x), the associated semantic source
˜X, and the semantic distortion function ds(˜xis, ˆ˜xjs), the semantic rate distortion is defined as,

$$R_{s}(D)=\min_{I_{x},I_{\hat{x}}}\min_{p(\hat{x}|x)\in P_{D}}I_{s}(\hat{X};\hat{\hat{X}})\tag{69}$$ $$=\min_{I_{x},I_{\hat{x}}}\min_{p(\hat{x}|x)\in P_{D}}\sum_{i_{s}}\sum_{(x_{i},\hat{x}_{j})\in\mathcal{X}_{i_{s}}\times\hat{X}_{j_{s}}}p\left(x_{i},\hat{x}_{j}\right)$$ $$\cdot\log\frac{p\left(x_{i},\hat{x}_{j}\right)}{\sum_{x_{i}\in\mathcal{X}_{i_{s}}}p\left(x_{i}\right)\sum_{\hat{x}_{j}\in\hat{\mathcal{X}}_{i_{s}}}p\left(\hat{x}_{j}\right)}$$ $$=\min_{I_{x},I_{\hat{x}}}\min_{p(\hat{x}|x)\in P_{D}}\left[H_{s}(\hat{X})+H_{s}(\hat{\hat{X}})-H(X,\hat{X})\right].$$
Lemma 7. (Convexity of Rs(D)): The semantic rate distortion function Rs(D) is a non-increasing convex function of D.

Proof: Similar to syntactic rate distortion R(D), Rs(D) is also a non-increasing function in D.

Consider two rate distortion pairs (R1, D1) and (R2, D2), the corresponding joint distribution achieving these pair are p1(x, ˆx) = p(x)p1(ˆx|x) and p2(x, ˆx) = p(x)p2(ˆx|x) respectively. Consider the weighted distribution pθ = θp1 + (1 −θ)p2 and the weighted distortion Dθ = θD1 + (1 −θ)D2. Due to the convexity of semantic mutual information, we have

$$R_{s}(D_{\theta})\leq I_{s,p_{\theta}}(\tilde{X};\hat{\tilde{X}})\tag{70}$$ $$\leq\theta I_{s,p_{1}}(\tilde{X};\hat{\tilde{X}})+(1-\theta)I_{s,p_{2}}(\tilde{X};\hat{\tilde{X}})$$ $$=\theta R_{s}(D_{1})+(1-\theta)R_{s}(D_{2}).$$
So we complete the proof.

□
According to Corollary 3, the down semantic mutual information is no more than the mutual information, that is, Is( ˜X; ˜Y ) ≤I(X; Y ). Thus, we conclude that the semantic rate distortion is no more than the classic rate distortion, i.e., Rs(D) ≤R(D).

## Vi. Semantic Lossless Source Coding

In this section, we discuss the semantic lossless source coding. First, we investigate the asymptotic equipartition property (AEP) of semantic coding and introduce the synonymous typical set. Then we prove the semantic source coding theorem and give the optimal length of semantic coding. Finally, we design the semantic Huffman coding to demonstrate the advantage of semantic data compression.

## A. Asymptotic Equipartition Property And Synonymous Typical Set

Similar to classic information theory, asymptotic equipartition property (AEP) is also an important tool to prove the coding theorem in semantic information theory.

Definition 12. Given a discrete random variable U with the distribution p(u) and the corresponding semantic variable ˜U, when we consider an i.i.d. semantic sequence ( ˜U1, ˜U2, *· · ·* , ˜Un)
and the associated syntactic sequences (U1, U2, · · · , Un), the sequential synonymous mapping is defined as f n : ˜Un →Un. That is to say, given a sequence ˜un*, we have* f n (˜un) = Qn k=1 U˜uk.

Theorem 10. (Semantic and Syntactic AEP): If ( ˜U1, ˜U2, · · · , ˜Un, · · · ) is an i.i.d. semantic sequence and the associated syntactic sequence is (U1, U2, · · · , Un, · · · ), given the sequential

synonymous mapping $f^{n}:\tilde{U}^{n}\to\mathcal{U}^{n}$, then_

$$\begin{split}&\int_{\text{\tiny{$\Pi$}}\to\infty}-\frac{1}{n}\log p(\tilde{U}_{1},\tilde{U}_{2},\cdots,\tilde{U}_{n})=H_{*}(\tilde{U})\\ &\big{\downarrow}_{\text{\tiny{$\Pi$}}\to\infty}-\frac{1}{n}\log p\left(U_{1},U_{2},\cdots,U_{n}\right)=H\left(U\right)\end{split}\tag{71}$$
Proof: For the first equality, since the ˜Uk are i.i.d., so are log p( ˜Uk). By the weak law of large numbers, we have

$$-\frac{1}{n}\log p(\tilde{U}_{1},\tilde{U}_{2},\cdots,\tilde{U}_{n})=-\frac{1}{n}\sum_{k=1}^{n}\log p(\tilde{U}_{k})$$ $$\to\mathbb{E}\left[-\log p(\tilde{U})\right]\tag{72}$$ $$=\mathbb{E}\left[\begin{array}{c}-\log\sum_{u\in\mathcal{U}_{\mathsf{s}}}p(u)\\ \\ =H_{s}(\tilde{U}).\end{array}\right.$$
The second equality holds from the classic information theory [4, Theorem 3.1.1].

□
Base on semantic and syntactic AEP, under the sequential synonymous mapping f n, we have the following synonymous AEP.

Theorem 11. (Synonymous AEP): If ( ˜U1, ˜U2, · · · , ˜Un, · · · ) is an i.i.d. semantic sequence and the associated syntactic sequence is (U1, U2, · · · , Un, · · · ), under the sequential synonymous mapping f n : ˜Un →Un, we have

$$\lim_{n\rightarrow\infty}-\frac{1}{n}\left[\log p\left(U_{1},U_{2},\cdots,U_{n}\right)-\log p(\tilde{U}_{1},\tilde{U}_{2},\cdots,\tilde{U}_{n})\right]\tag{73}$$ $$=H\left(U\right)-H_{s}(\tilde{U}).$$
Proof: By Theorem 10, we have

$$-\frac{1}{n}\left[\log p\left(U_{1},U_{2},\cdots,U_{n}\right)-\log p(\tilde{U}_{1},\tilde{U}_{2},\cdots,\tilde{U}_{n})\right]$$ $$=-\frac{1}{n}\sum_{k=1}^{n}\left[\log p(U_{k})-\log p(\tilde{U}_{k})\right]$$ $$\to\mathbb{E}\left[-\log p(U)\right]-\mathbb{E}\left[-\log p(\tilde{U})\right]\tag{74}$$ $$=\mathbb{E}\left[-\log p(u)\right]-\mathbb{E}\left[-\log\sum_{u\in\tilde{U}_{k}}p(u)\right]$$ $$=H(U)-H_{s}(\tilde{U}).$$
Similar to the definition of syntactically typical set A(n)
ϵ
in [4], we now introduce the seman-

tically typical set ˜A(n)
                     ϵ
                         and the synonymous typical set B(n)
                                                               ϵ
                                                                  (˜un) respectively.

Definition 13. Given the typical sequences {˜un} with respect to the distribution p(u), the

semantically typical set ˜A(n)
                          ϵ
                             is defined as the set of n-sequences with empirical semantic entropy

ϵ-close to the true semantic entropy, that is,

$$\tilde{A}_{\epsilon}^{(n)}=\left\{\tilde{u}^{n}\in\tilde{\mathcal{U}}^{n}:\left|-\frac{1}{n}\log p\left(\tilde{u}^{n}\right)-H_{s}(\tilde{U})\right|<\epsilon\right\},\tag{75}$$
where

$$p\left(\tilde{u}^{n}\right)=\prod_{k=1}^{n}p(\tilde{u}_{k})=\prod_{k=1}^{n}\sum_{u_{k}\in\mathcal{U}_{\tilde{u}_{k}}}p(u_{k}).\tag{76}$$
Definition 14. Given a specific typical sequence {˜un}*, the synonymous typical set* B(n)
ϵ
(˜un) with the syntactically typical sequences {un} is defined as the set of n-sequences with the difference of empirical entropies ϵ-close to the difference of true entropies, that is,

$$B_{\epsilon}^{(n)}\left(\tilde{u}^{n}\right)=\left\{u^{n}\in\mathcal{U}^{n}:\left|-\frac{1}{n}\log p\left(u^{n}\right)-H(U)\right|<\epsilon,\right.$$ $$\left.\left|-\frac{1}{n}\log p\left(\tilde{u}^{n}\right)-H_{s}(\tilde{U})\right|<\epsilon,\right.\tag{77}$$ $$\left.\left|-\frac{1}{n}\log p(\tilde{u}^{n}\to u^{n})\right.\right.$$ $$\left.\left.-\left(H(U)-H_{s}(\tilde{U})\right)\right|<\epsilon\right\},$$

where p(un)=Qn
              k=1 p(uk) and p(˜un)=Qn
                                   k=1
                                      P

uk∈U˜ukp(uk). Here the probability p(˜un →un) is

defined as

$$p(\tilde{u}^{n}\to u^{n})=\begin{cases}\dfrac{p\left(u^{n}\right)}{p\left(\tilde{u}^{n}\right)},&\text{if}u^{n}=f^{n}(\tilde{u}^{n}),\\ 0,&\text{otherwise}.\end{cases}\tag{78}$$
Under the sequential synonymous mapping f n : ˜Un →Un, the syntactically typical set A(n)
ϵ
can be further partitioned into multiple synonymous typical sets B(n)
ϵ
(˜un), that is,

$$A_{\epsilon}^{(n)}=\bigcup\limits_{\hat{u}^{n}\in\hat{A}_{\epsilon}^{(n)}}B_{\epsilon}^{(n)}\left(\hat{u}^{n}\right).\tag{79}$$

And for ∀˜un, ˜vn ∈˜A(n)
                  ϵ , ˜un ̸= ˜vn, we have B(n)
                                        ϵ
                                           (˜un) T B(n)
                                                   ϵ
                                                     (˜vn) = ∅.

Essentially, B(n)
              ϵ
                  (˜un) is an equivalence class of the synonymous typical sequences. Thus for

the syntactically typical set A(n)
                                ϵ , we can obtain a quotient set A(n)
                                                                      ϵ /f n =
                                                                                 n
                                                                                  B(n)
                                                                                    ϵ (˜un)
                                                                                           o
                                                                                              and

construct an one-to-one mapping between ˜A(n)
                                         ϵ
                                            and A(n)
                                                  ϵ /f n.

We now discuss the properties of syntactically and semantically typical sets. In order to the

convenient reading, we first rewrite the properties of syntactically typical set as follows.

Theorem 12. ([4, Theorem 3.1.2]):

(1) If (u1, u2, · · · *, u*n) ∈A(n)
ϵ *, then* H(U) −ϵ ≤−1
n log p (u1, u2, · · · *, u*n) ≤H(U) + ϵ or
equivalently
$$2^{-n(H(U)+\epsilon)}\leq p\left(u_{1},u_{2},\cdots,u_{n}\right)\leq2^{-n(H(U)-\epsilon)}.\tag{80}$$
(2) Pr
n
A(n)
ϵ
o
> 1 −ϵ for n sufficiently large.
(3) (1 −ϵ) 2n(H(U)−ϵ) ≤
A(n)
ϵ
 ≤2n(H(U)+ϵ) for n sufficiently large.
Then as the consequence of the semantic AEP, we give the properties of semantically typical set as below.

## Theorem 13.

(1) If (˜u1, ˜u2, · · · , ˜un) ∈
˜A(n)
ϵ *, then* Hs( ˜U) −ϵ ≤−1
n log p (˜u1, ˜u2, *· · ·* , ˜un) ≤Hs( ˜U) + ϵ,
equivalently,
$$2^{-n\left(H_{s}(U)+\epsilon\right)}\leq p\left(\hat{u}_{1},\hat{u}_{2},\cdots,\hat{u}_{n}\right)\leq2^{-n\left(H_{s}(U)-\epsilon\right)}.\tag{81}$$
(2) Pr
n
˜A(n)
ϵ
o
> 1 −ϵ for sufficiently large n.

(3) (1 −ϵ) 2n(Hs( ˜U)−ϵ) ≤
                            ˜A(n)
                              ϵ
                                 ≤2n(Hs( ˜U)+ϵ) for sufficiently large n.

Proof: The proof of property (1) is directly from the definition of ˜A(n)
                                                                   ϵ . By Theorem 10,

the probability of the event ˜U n ∈˜A(n)
                                    ϵ
                                        tends to 1 as n →∞. So for ∀ϵ > 0, ∃n0, for ∀n ≥n0,

we have

$${\rm Pr}\left\{\left|-\frac{1}{n}\log p(\tilde{U}^{n})-H_{s}(\tilde{U})\right|<\epsilon\right\}>1-\epsilon.\tag{82}$$
Then we complete the proof of property (2).

After summing over the set $\tilde{A}_{i}^{(n)}$, by using property (2), Eq. (81) can be rewritten as

$$\left\{\left|\tilde{A}_{i}^{(n)}\right|2^{-n\left(H_{\varepsilon}(\ell)+\epsilon\right)}\leq\sum_{\tilde{u}^{\text{\tiny{$\circ$}}}\in\tilde{A}_{i}^{(n)}}p\left(\tilde{u}^{n}\right)\leq1\right.\tag{83}$$ $$\left.1-\epsilon\leq\sum_{\tilde{u}^{\text{\tiny{$\circ$}}}\in\tilde{A}_{i}^{(n)}}p\left(\tilde{u}^{n}\right)\leq\left|\tilde{A}_{i}^{(n)}\right|2^{-n\left(H_{\varepsilon}(\ell)-\epsilon\right)}\right.$$

Hence, we can write

$$\left|\tilde{A}_{i}^{(n)}\right|\geq\left(1-\epsilon\right)2^{n\left(H_{\varepsilon}(\ell)-\epsilon\right)}\tag{84}$$

and

$$\left|\tilde{A}_{i}^{(n)}\right|\leq2^{n\left(H_{\varepsilon}(\ell)+\epsilon\right)}\tag{85}$$
respectively and complete the third property.

□
Furthermore, we give the properties of synonymous typical set as below.

## Theorem 14.

(1) Given a semantic sequence (˜u1, ˜u2, · · · , ˜un) ∈˜A(n)
                                                 ϵ , if (u1, u2, · · · , un) ∈B(n)
                                                                          ϵ
                                                                             (˜un), then

H(U) −Hs( ˜U) −ϵ ≤−1
p(˜un) ≤H(U) −Hs( ˜U) + ϵ, equivalently,
n log p(un)
$$2^{-n\left(H(U)-H_{s}(U)+\epsilon\right)}\leq\frac{p\left(u^{n}\right)}{p\left(\tilde{u}^{n}\right)}\leq2^{-n\left(H(U)-H_{s}(U)-\epsilon\right)}.\tag{86}$$
(2) 2n(H(U)−Hs( ˜U)−ϵ) ≤
B(n)
ϵ
(˜un)
 ≤2n(H(U)−Hs( ˜U)+ϵ) for sufficiently large n.
Proof: The proof of property (1) is directly from the definition of B(n)
ϵ
(˜un).

To prove the left inequality of property (2), we write

$$p\left(\tilde{u}^{n}\right)=\sum_{u^{m}\in B_{\epsilon}^{(n)}(u^{n})}p\left(u^{n}\right)\tag{87}$$ $$\leq p\left(\tilde{u}^{m}\right)\sum_{u^{m}\in B_{\epsilon}^{(n)}(\tilde{u}^{n})}2^{-n\left(H(U)-H_{\epsilon}(\tilde{U})-\epsilon\right)}$$ $$=p\left(\tilde{u}^{n}\right)\left|B_{\epsilon}^{(n)}\left(\tilde{u}^{n}\right)\right|2^{-n\left(H(U)-H_{\epsilon}(\tilde{U})-\epsilon\right)}.$$

So we have

$$\left|B_{\epsilon}^{(n)}\left(\tilde{u}^{n}\right)\right|\geq2^{m\left(H(U)-H_{\epsilon}(\tilde{U})-\epsilon\right)}.\tag{88}$$
On the other hand, for the right inequality, we can write

$$p\left(\tilde{u}^{n}\right)=\sum_{u^{n}\in B_{t}^{(n)}(\tilde{u}^{n})}p\left(u^{n}\right)\tag{89}$$ $$\geq p\left(\tilde{u}^{n}\right)\sum_{u^{n}\in B_{t}^{(n)}(\tilde{u}^{n})}2^{-n\left(H(U)-H_{s}(\tilde{t})+_{t}\right)}$$ $$=p\left(\tilde{u}^{n}\right)\left|B_{t}^{(n)}\left(\tilde{u}^{n}\right)\right|2^{-n\left(H(U)-H_{s}(\tilde{t})+_{t}\right)}.$$

Similarly, we have

$$\left|B_{t}^{(n)}\left(\tilde{u}^{n}\right)\right|\leq2^{n\left(H(U)-H_{s}(U)+_{t}\right)}\tag{90}$$
and complete the proof.

□
Remark 6. *Both the probability of syntactically typical set* A(n)
ϵ
and the probability of semantically typical set ˜A(n)
ϵ
trends to 1 with sufficiently large n. Furthermore, all the syntactically and semantically typical sequences are almost equiprobability. Therefore, the number of syntactically

typical sequences is about
                           A(n)
                             ϵ
                                ≈2n(H(U)±ϵ) and that of semantically typical sequences is

about
       ˜A(n)
        ϵ
           ≈2n(Hs( ˜U)±ϵ). Simultaneously, the number of synonymous typical sequences is

about
     B(n)
        ϵ
          (˜un)
               ≈2n(H(U)−Hs( ˜U)±ϵ). In fact, all the synonymous typical sets have almost the

same number of typical sequences. Hence, hereafter, in the case of non-confusion, we abbreviate

B(n)
ϵ
  (˜un) to B(n)
        ϵ .

## B. Semantic Source Coding Theorem

We now discuss the semantic source coding. As shown in Fig. 7, in the side of transmitter, with the help of synonymous mapping f n, the semantic index is is mapped into a syntactic sequence U n and encoded into a source codeword Xn. In the other side, the decoder decides the codeword Xn to an estimated syntactic sequence ˆU n. After de-synonymous mapping gn, we obtain an index estimation of semantic sequence ˆis.

$\begin{array}{c}\includegraphics[height=56.905512pt]{Fig1}\end{array}$

\(\begin{array}{c}\includegraphics[height=56.

Definition 15. An (M, n) code for semantic source coding consists of the following parts:

(1) A semantic index set Is = {1, · · · , is, · · · , Ms} and a syntactic index set I = {1, · · · , i, · · · , M}.
(2) By the synonymous mapping f n : ˜Un →Un, one semantic sequence in the set ˜Un is mapped
into a syntactic sequence.
(3) An encoding function ϕ : Un →X n generates the set of codewords, namely, codebook,
C = {Xn(1), Xn(2), · · · , Xn(M)}. Due to synonymous mapping, this codebook can be partitioned into synonymous codeword subsets Cs(is) = {Xn(is, j), is ∈Is, j = 1, 2, · · · , M
Ms},
where Xn(is, j) denotes the j-th codeword of the is-th subset.
(4) A decoding function ψ : X n →Un outputs the decision syntactic sequence ˆU n.
(5) After de-mapping, gn( ˆU n) = ˆis, the estimated semantic index is obtained. Note that both ψ
and gn are deterministic.
Under the synonymous mapping f n, Cs is an equivalence class of the synonymous codewords.

So we can construct the quotient set C/f n = {Cs}. Without loss of generality, we can assume all the synonymous sets have the same number of codewords, that is, |Cs| = M
Ms = 2nRs, where Rs is called the rate of synonymous codeword set. So we have |C/f n| = Ms. Let R = 1
n log2 Ms denote the semantic code rate. Furthermore, let R′ = R + Rs = 1
n log2 M be the syntactic code rate.

## Definition 16. The Decoding Error Probability Is Defined As

$P_{e}^{(n)}=Pr(g^{n}({\cal C}_{s})\neq i_{s})$. (91)
We now give the formal description of semantic source coding theorem.

Theorem 15. Given the semantic source ˜U and the syntactic source U with the synonymous

mapping f : ˜U →U, for each code rate R > Hs( ˜U), there exists a series of
                                                                   
                                                                    2n(R+Rs), n
                                                                             

codes, when code length n tends to sufficiently large, the error probability is close to zero, i.e.

P (n)
 e
   →0. On the contrary, if R < Hs( ˜U), then for any
                                    
                                    2n(R+Rs), n
                                           
                                             code, the error probability

tends to 1 with n sufficiently large.

Proof: First, we prove the direct part of the theorem. We select ϵ > 0 and construct a

one-to-one mapping from ˜A(n)
                         ϵ
                            to C/f n. So for sufficiently large n, by Theorem 13, we have

$$(1-\epsilon)\,2^{n\big{(}H_{s}(\bar{U})-\epsilon\big{)}}\leq|{\cal C}/f^{n}|=2^{nR}=\left|\tilde{A}_{\epsilon}^{(n)}\right|\leq2^{n\big{(}H_{s}(\bar{U})+\epsilon\big{)}}.\tag{92}$$
Therefore, the semantic code rate satisfies

$$\frac{1}{n}\log\left(1-\epsilon\right)+H_{s}(\tilde{U})-\epsilon\leq R\leq H_{s}(\tilde{U})+\epsilon.\tag{93}$$
Also by Theorem 13, it follows that

$$P_{\epsilon}^{(n)}={\rm Pr}(\tilde{U}^{n}\notin\tilde{A}_{\epsilon}^{(n)})<\epsilon.\tag{94}$$
By Theorem 14, the size of synonymous set satisfies

$$1\leq|{\cal C}_{s}|=2^{nR_{s}}\leq\left|B_{\epsilon}^{(n)}\left(\tilde{u}^{n}\right)\right|\leq2^{n\left(H(U)-H_{s}(\tilde{U})+\epsilon\right)}.\tag{95}$$
Therefore the syntactic code rate satisfies

$$\frac{1}{n}\log\left(1-\epsilon\right)+R_{s}+H_{s}(\hat{U})-\epsilon\leq R^{\prime}\leq H_{s}(\hat{U})+R_{s}+\epsilon.\tag{96}$$
When the synonymous set Cs only has one codeword to represent the semantically typical sequence, that is, Rs = 0, letting ϵ →0, both the code rate R and R′ tend to Hs( ˜U), while P (n)
e tends to 0. On the other hand, substituting Rs = H(U) −Hs( ˜U) into (96) and letting ϵ →0, R →Hs( ˜U) and R′ →H(U). So we prove the direct part of theorem.

Next we prove the converse part. Consider any code with code length n and the number of synonymous sets satisfies 2nR ≤2n(Hs( ˜U)−ζ). So some sets are used to represent the semantically typical sequences gn (Cs) ∈˜A(n)
ϵ
and the rest sets to represent the semantic non-typical sequences gn (Cs) /∈˜A(n)
ϵ . By Theorem 13, for sufficiently large n, the probability of semantic sequences covered by the code is upper bounded by

$$1-P_{e}^{(n)}=2^{n\left(H_{s}(\tilde{U})-\epsilon\right)}2^{-n\left(H_{s}(\tilde{U})-\epsilon\right)}+\Pr(g^{n}\left(\mathcal{C}_{s}\right)\notin\tilde{A}_{\epsilon}^{(n)})\tag{97}$$ $$<2^{-n(\zeta-\epsilon)}+\epsilon.$$
Therefore, we can write the error probability as

$P_{e}^{(n)}>1-2^{-n(\zeta-\epsilon)}-\epsilon$. (98)
This inequality holds when n →∞for *ϵ >* 0 and *ζ > ϵ*. So we have P (n)
e
> 1−2ϵ. Additionally, when n →∞and ϵ →0, P (n)
e
→1. So we complete the proof of the semantic source coding theorem.

□
Remark 7. The semantic source coding theorem is an extended version of the counterpart in the classic information theory. The limitation of semantic compression rate R is Hs( ˜U) while the corresponding syntactic rate R′ is Hs( ˜U)+Rs. If the synonymous set is represented by only one codeword, i.e., Rs = 0, then compression rate R′ tends to the semantic entropy Hs( ˜U). On the other hand, if the synonymous mapping is ignored and all the synonymous typical sequences are distinct, thus Rs = H(U)−Hs( ˜U) and the compression rate R′ achieves the information entropy H(U). Since the distinction between codewords in the synonymous set is no longer regarded as an error, by using semantic coding, the source data can be further compressed and the efficiency is improved.

## C. Semantic Source Coding Method

Similar to the classic source coding, the variable length coding is desired for semantic source coding. Thus we also obtain the semantic version of Kraft inequality as following.

Theorem 16. (Semantic Kraft Inequality): Given a discrete random variable U ∈U = {ui}N
i=1,

the corresponding semantic variable ˜U ∈˜U = {˜uis}
                                                  ˜
                                                  N
                                                  is=1, and the synonymous mapping f : ˜U →

U. For any prefix code over an alphabet of size F exists if and only if the codeword length

l1, l2, · · · , l ˜
               N satisfies
                                                           ˜
                                                          N
                                                        X

is=1 F −lis ≤1. (99)
The proof is similar to that of classic Kraft inequality and omitted. It should be noted that the semantic Kraft inequality has the same form as that of classic counterpart. However, since the semantic prefix code is performed over a synonymous set rather than a single syntactic symbol, the number of codewords is less than classic prefix code, that is, ˜N ≤N.

Syntactic symbol
u1
u2
u3
u4
Probability
1 2
1 4
1 8
1 8
Syn. HC
0
10
110
111

Furthermore, we can obtain the average code length of optimal semantic source code as follows.

Theorem 17. Given the syntactic source distribution p(u) and the synonymous mapping f :
˜U →U, let l∗
           1, l∗
             2, · · · , l∗
                    ˜
                   N denote the optimal code lengths with an F-ary alphabet, then the

expected length ¯L∗of the optimal semantic code satisfies

$$\frac{H_{s}(\tilde{U})}{\log F}\leq\tilde{L}^{*}<\frac{H_{s}(\tilde{U})}{\log F}+1.\tag{100}$$

Proof: Assign the code length as lis =
                                    l
                                     −logF
                                            P

i∈Uis p(ui)
                m
                  . Similar to the classic version,

by semantic Kraft inequality, we have

$$\frac{H_{s}(\tilde{U})}{\log F}\leq\bar{L}^{s}\leq\sum_{i_{s}=1}^{\tilde{N}}p(\hat{u}_{i_{s}})l_{i_{s}}<\frac{H_{s}(\tilde{U})}{\log F}+1.\tag{101}$$
So we prove the theorem.

□
Example 4. (Semantic Huffman Coding): Now we describe an example of semantic Huffman coding. For a syntactic source U with four symbols u1, u2, u3, u4, the probability distribution is listed in Table VII. The information entropy is H(U) = 1.75 bits. By using Huffman coding, the codewords are shown in Table VII and the average code length is ¯L = 1.75 bits = H(U).

If we give a synonymous mapping f, that is, ˜u1 →{u1}, ˜u2 →{u2}, ˜u3 →{u3, u4}, the probability distribution of semantic source ˜U is listed in Table VIII. So the semantic entropy is calculated as Hs( ˜U) = 1.5 sebits. Correspondingly, by the semantic Huffman codewords listed in Table VIII, the average code length is ¯Ls = 1.5 sebits = Hs( ˜U). Distinctly, due to synonymous mapping, the average code length of semantic Huffman code is smaller than that of traditional Huffman code.

Given a syntactic sequence u = (u1u1u3u4u2u3u2), by Table VII, the syntactic Huffman coding is x = (001101111011010). On the other hand, by Table VIII, the semantic Huffman coding is

Semantic symbol
˜u1 →{u1}
˜u2 →{u2}
˜u3 →{u3, u4}
Probability
1 2
1 4
1 4
Sem. HC
0
10
11

xs = (001111101110)*. Hence, the length of syntactic coding is* L(x) = 15 bits and the length of semantic coding is L(xs) = 12 sebits so that the latter is smaller than the former, i.e., L(xs) < L(x). Certainly, since the decoder can select arbitrary symbol from the set {u3, u4}
when decoding ˜u3*, the result may be* ˆu = (u1u1u3u3u2u4u2). Although such decoding sequence is different from the original one u = (u1u1u3u4u2u3u2) in the syntactic sense, the semantic information of the decoding results still keeps the same since u3 and u4 have the same meaning.

Remark 8. For the method of semantic source coding, we have two kinds of design thought.

The first kind thought is to modify the traditional source coding, such as Huffman, arithmetic or universal coding. By using an elaborate synonymous mapping, these coding methods can be devised to further improve the compression efficiency. For the second thought, based on the deep learning method, we can construct a neural network model to perform semantic source coding. In this model, the synonymous mapping and semantic coding can be integrated and optimized to approach the theoretic limitation.

## Vii. Semantic Channel Coding

In this section, we investigate the semantic channel coding. First, we introduce the jointly asymptotic equipartition property (JAEP) in the semantic sense and define the jointly synonymous typical set. Then we prove the semantic channel coding theorem by using JAEP and jointly synonymous typical set, which states that the semantic capacity, maxfxy maxp(x) Is( ˜X; ˜Y ), the maximum up semantic mutual information, is the largest achievable rate of semantic communication. Finally, we consider the semantic channel decoding problem and propose the maximum likelihood group (MLG) decoding algorithm. A simple example of semantic Hamming code is analyzed based on the MLG decoding rule.

## A. Jointly Asymptotic Equipartition Property And Jointly Synonymous Typical Set

Given the semantic channel model n
˜
X, X, Y, ˜Y, p(Y |X)
o
, X and Y are the input and output syntactical alphabet and ˜
X and ˜Y are the corresponding input and output semantic alphabet.

Furthermore, p(Y |X) is the channel transition probability and let fxy : ˜
X × ˜*Y →X × Y* be the jointly synonymous mapping.

Based on this channel model, we extend the jointly asymptotic equipartition property (JAEP)
to the semantic sense and use it to prove the channel coding theorem in semantic information theory.

Definition 17. Given the semantic channel model, the jointly synonymous mapping over the sequence pairs is defined as f n xy : ˜
X n × ˜Yn →X n × Yn. Equivalently, given a sequence pair
(˜xn, ˜yn), we have f n xy (˜xn, ˜yn) = Qn k=1 X˜xk × Y˜yk.

To facilitate the reader's understanding, we first cite the definition of syntactically joint typical set A(n)
ϵ
in [4].

Definition 18. Given the jointly typical sequences {xn, yn} with respect to the distribution p(xn, yn)*, the syntactically jointly typical set* A(n)
ϵ
is defined as the set of n-sequence pairs with empirical entropies ϵ-close to the true entropies, that is,

$$A_{\epsilon}^{(n)}=\Biggr{\{}\left(x^{n},y^{n}\right)\in\mathcal{X}^{n}\times\mathcal{Y}^{n}:$$ $$\left|-\frac{1}{n}\log p\left(x^{n}\right)-H(X)\right|<\epsilon,\tag{102}$$ $$\left|-\frac{1}{n}\log p\left(y^{n}\right)-H(Y)\right|<\epsilon,$$ $$\left|-\frac{1}{n}\log p\left(x^{n},y^{n}\right)-H(X,Y)\right|<\epsilon\Biggr{\}}$$
where p (xn, yn) = Qn k=1 p(xk, yk).

Next, we introduce the semantically jointly typical set ˜A(n)
ϵ .

Definition 19. Given the jointly typical sequences {˜xn, ˜yn} with respect to the distribution p(xn, yn)*, the semantically jointly typical set* ˜A(n)
ϵ
is defined as the set of n-sequence pairs with empirical entropies ϵ-close to the true entropies, that is,

$$\tilde{A}_{\epsilon}^{(n)}=\Biggr\{\,(\tilde{x}^{n},\tilde{y}^{n})\in\tilde{\mathcal{X}}^{n}\times\tilde{\mathcal{Y}}^{n}:$$ $$\left|-\frac{1}{n}\log p\left(\tilde{x}^{n}\right)-H_{s}(\tilde{X})\right.$$ $$\left|-\frac{1}{n}\log p\left(\tilde{x}^{n}\right)-H_{s}(\tilde{X})\right|<\epsilon,$$ $$\left|-\frac{1}{n}\log p\left(\tilde{y}^{n}\right)-H_{s}(\tilde{Y})\right|<\epsilon,$$ $$\left|-\frac{1}{n}\log p\left(\tilde{x}^{n},\tilde{y}^{n}\right)-H_{s}(\tilde{X},\tilde{Y})\right|<\epsilon\right\}$$
where

$$p\left(\tilde{x}^{n},\tilde{y}^{n}\right)=\prod_{k=1}^{n}p(\tilde{x}_{k},\tilde{y}_{k})=\prod_{k=1}^{n}\sum_{(x_{k},y_{k})\in\mathcal{X}_{\tilde{y}_{k}}\times\mathcal{Y}_{\tilde{y}_{k}}}p(x_{k},y_{k}).\tag{104}$$
The semantic and syntactic sequence mapping is depicted in Fig. 8. As described in Section VI,

under the synonymous mapping f n
                             x and f n
                                    y , semantic source set ˜
                                                        X and semantic destination set
˜Y are mapped into synonymous typical sets. Similarly, under the joint mapping f n
                                                                       xy, the typical

sequences in these sets can further compose the jointly synonymous typical set B(n)
                                                                                ϵ
                                                                                   (˜xn, ˜yn).

Definition 20. Given a specifically typical sequence pair {˜xn, ˜yn}, the jointly synonymous typical set B(n)
ϵ
(˜xn, ˜yn) with the syntactically jointly typical sequences {xn, yn} is defined as the set of n-sequence pairs with the difference of empirical entropies ϵ-close to the difference of true

entropies, that is, B(n) ϵ (˜xn, ˜yn) =  (xn, yn) ∈X n × Yn : −1 n log p (˜xn) −Hs( ˜X)  < ϵ, −1 n log p (˜yn) −Hs(˜Y )  < ϵ, −1 (105) n log p (˜xn, ˜yn) −Hs( ˜X, ˜Y )  < ϵ, −1 n log p (xn) −H(X)  < ϵ, −1 n log p (yn) −H(Y )  < ϵ, −1 n log p((˜xn, ˜yn) →(xn, yn)) n log p (xn, yn) −H(*X, Y* )  < ϵ, −1 −  H(*X, Y* ) −Hs( ˜X, ˜Y )  < ϵ o ,
where the probability p((˜xn, ˜yn) →(xn, yn)) is defined as

$$p((\tilde{x}^{n},\tilde{y}^{n})\rightarrow(x^{n},y^{n}))$$ $$=\left\{\begin{array}{ll}\frac{p\left(x^{n},y^{n}\right)}{p\left(\tilde{x}^{n},\tilde{y}^{n}\right)},&\mbox{if}f_{xy}^{n}(\tilde{x}^{n},\tilde{y}^{n})=(x^{n},y^{n}),\\ \\ 0,&\mbox{otherwise}.\end{array}\right.\tag{106}$$
Under the jointly synonymous mapping f n xy : ˜
X n × ˜Yn →X n × Yn, the syntactically jointly typical set A(n)
ϵ
can be partitioned into jointly synonymous typical sets B(n)
ϵ
(˜xn, ˜yn), that is,

$$A_{\epsilon}^{(n)}=\bigcup B_{\epsilon}^{(n)}\left(\tilde{x}^{n},\tilde{y}^{n}\right).\tag{107}$$

For ∀(˜un, ˜vn), (˜xn, ˜yn) ∈˜A(n)
                        ϵ , (˜un, ˜vn) ̸= (˜xn, ˜yn), we have B(n)
                                                       ϵ
                                                          (˜xn, ˜yn) T B(n)
                                                                     ϵ
                                                                        (˜un, ˜vn) = ∅.

Similarly, B(n)
             ϵ
                (˜xn, ˜yn) also is an equivalence class of the jointly synonymous typical sequences. Thus we can construct a quotient set A(n)
                                               ϵ /f n
                                                    xy =
                                                         n
                                                           B(n)
                                                            ϵ (˜xn, ˜yn)
                                                                       o
                                                                         from the syntactically

jointly typical set A(n)
                     ϵ , and establish an one-to-one mapping between ˜A(n)
                                                                               ϵ
                                                                                  and A(n)
                                                                                         ϵ /f n
                                                                                              xy.

We now discuss the properties of syntactically and semantically jointly typical sets. We first rewrite the properties of syntactically jointly typical set in [4] as follows.

Theorem 18. (Syntactically joint AEP [4, Theorem 7.6.1]): Let (Xn, Y n) be a sequence pair with length n obeying the i.i.d. distribution p(xn, yn), then

(1) Pr((Xn, Y n) ∈A(n)
ϵ ) > 1 −ϵ for n sufficiently large. Or equivalently, if (xn, yn) ∈A(n)
ϵ ,
then
$$2^{-n(H(X,Y)+\epsilon)}\leq p\left(x^{n},y^{n}\right)\leq2^{-n(H(X,Y)-\epsilon)}.\tag{108}$$
(2) (1 −ϵ) 2n(H(X,Y )−ϵ) ≤
A(n)
ϵ
 ≤2n(H(X,Y )+ϵ) for n sufficiently large.
(3) If
˙Xn and ˙Y n are independent sequences with the same marginals as xn and yn, i.e.,
( ˙Xn, ˙Y n) ∼p(xn)p(yn), for n sufficiently large, we have
$$(1-\epsilon)2^{-n(I(X;Y)+3\epsilon)}\leq Pr\left((\dot{X}^{n},\dot{Y}^{n})\in A^{(n)}_{\epsilon}\right)\tag{109}$$ $$\leq2^{-n(I(X;Y)-3\epsilon)}.$$
Then as the consequence of the semantic JAEP, we present the properties of semantically jointly typical set as following.

Theorem 19. (Semantically joint AEP): Let ( ˜Xn, ˜Y n) be a semantic sequence pair with length n drawn i.i.d. according to p(˜xn, ˜yn), by using jointly synonymous mapping f n xy, the associated syntactic sequence pair is (Xn, Y n) ∼p(xn, yn), then

(1) Pr(( ˜Xn, ˜Y n) ∈˜A(n)
ϵ ) > 1 −ϵ for n *sufficiently large. Or equivalently if* (˜xn, ˜yn) ∈˜A(n)
ϵ ,
then
$$2^{-n\left(H_{s}(\tilde{X},\tilde{Y})+\epsilon\right)}\leq p\left(\tilde{x}^{n},\tilde{y}^{n}\right)\leq2^{-n\left(H_{s}(\tilde{X},\tilde{Y})-\epsilon\right)}.\tag{110}$$
(2) (1 −ϵ) 2n(Hs( ˜
X, ˜Y )−ϵ) ≤
 ˜A(n)
ϵ
 ≤2n(Hs( ˜
X, ˜Y )+ϵ) for n sufficiently large.
(3) If
˙Xn and ˙Y n are independent sequences with the same marginals as Xn and Y n, i.e.,
( ˙Xn, ˙Y n) ∼p(xn)p(yn), and the corresponding semantic sequence is ( ˙˜Xn, ˙˜Y n), for n
sufficiently large, we have
$$(1-\epsilon)2^{-n(I^{s}(\tilde{X};\tilde{Y})+3\epsilon)}\leq Pr((\dot{\tilde{X}}^{n},\dot{\tilde{Y}}^{n})\in\tilde{A}^{(n)}_{\epsilon})\tag{111}$$ $$\leq2^{-n(I^{s}(\tilde{X};\tilde{Y})-3\epsilon)}.$$
Proof: Similar to Theorem 18, the proof of property (1) is directly from the weak law of large numbers and Definition 19.

According to property (1), Eq. (10) can be rewritten as

$$\left\{\left|\tilde{A}_{\ell}^{(n)}\right|2^{-n\left(H_{s}(\tilde{X},\tilde{Y})+\epsilon\right)}\leq\sum_{(\hat{x}^{0}\neq\epsilon)\in\tilde{A}_{\ell}^{(n)}}p\left(\tilde{x}^{n},\tilde{y}^{n}\right)\leq1\right.\tag{112}$$ $$\left.\left\{\begin{array}{l}1-\epsilon\leq\sum_{(\hat{x}^{0}\neq\hat{y}^{0})\in\tilde{A}_{\ell}^{(n)}}p\left(\tilde{x}^{n},\tilde{y}^{n}\right)\leq\left|\tilde{A}_{\ell}^{(n)}\right|2^{-n\left(H_{s}(\tilde{X},\tilde{Y})-\epsilon\right)}\right.\end{array}\right.$$

So the cardinality of semantically jointly typical set can be written as

$$\left|\tilde{A}_{\ell}^{(n)}\right|\geq\left(1-\epsilon\right)2^{n\left(H_{s}(\tilde{X},\tilde{Y})-\epsilon\right)}\tag{113}$$

and

$$\left|\tilde{A}_{\ell}^{(n)}\right|\leq2^{n\left(H_{s}(\tilde{X},\tilde{Y})+\epsilon\right)}\tag{114}$$
respectively. We complete the proof of the second property.

Now assume ˙Xn and ˙Y n are independent but have the same marginals as Xn and Y n, and after de-mapping, ( ˙Xn, ˙Y n) is mapped into a semantic sequence pair ( ˙˜Xn, ˙˜Y n), so we can establish an one-to-one mapping (xn, yn) ↔( ˙xn, ˙yn) ↔( ˙˜xn, ˙˜yn), then we have

$$\Pr\left((\dot{X}^{n},\dot{Y}^{n})\in\bar{A}^{(n)}_{\epsilon}\right)$$ $$=\Pr\left((\dot{X}^{n},\dot{Y}^{n})\in B^{(n)}_{\epsilon},\dot{X}^{n}\in A^{(n)}_{\epsilon}(X^{n}),\dot{Y}^{n}\in A^{(n)}_{\epsilon}(Y^{n})\right)$$ $$=\sum_{(x^{n},y^{n})\leftarrow(\dot{x}^{\bar{n}},\dot{y}^{n})\in\bar{A}^{(n)}_{\epsilon}}\tag{115}$$ $$\leq2^{n\left(H_{s}(\bar{X},\bar{Y})+\epsilon\right)}2^{-n\left(H(X)-\epsilon\right)}2^{-n\left(H(Y)-\epsilon\right)}$$ $$=2^{-n\left(I^{s}(\bar{X},\bar{Y})-3\epsilon\right)}.$$
Using a similar thought, we can also derive that

$$\Pr\left((\dot{\tilde{X}}^{n},\dot{\tilde{Y}}^{n})\in\tilde{A}^{(n)}_{\epsilon}\right)$$ $$=\sum_{\tilde{A}^{(n)}_{\epsilon}}p(x^{n})p(y^{n})\tag{116}$$ $$\geq(1-\epsilon)2^{n(H_{s}(\tilde{X};\tilde{Y})-\epsilon)}2^{-n(H(X)+\epsilon)}2^{-n(H(Y)+\epsilon)}$$ $$=(1-\epsilon)2^{-n\left(I^{s}(\tilde{X},\tilde{Y})+3\epsilon\right)}.$$
Thus we complete the proof of the theorem.

□
Furthermore, we give the properties of jointly synonymous typical set as below.

## Theorem 20.

(1) Given a semantic sequence pair (˜xn, ˜yn) ∈˜A(n)
                                              ϵ , if (xn, yn) ∈B(n)
                                                                ϵ
                                                                   (˜xn, ˜yn), then

$$2^{-n\left(H(X,Y)-H_{s}(\tilde{X},\tilde{Y})+\epsilon\right)}\leq\frac{p\left(x^{n},y^{n}\right)}{p\left(\tilde{x}^{n},\tilde{y}^{n}\right)}\tag{117}$$ $$\leq2^{-n\left(H(X,Y)-H_{s}(\tilde{X},\tilde{Y})-\epsilon\right)}.$$
(2) 2n(H(X,Y )−Hs( ˜
X, ˜Y )−ϵ) ≤
B(n)
ϵ
(˜xn, ˜yn)
 ≤2n(H(X,Y )−Hs( ˜
X, ˜Y )+ϵ) for sufficiently large n.
Proof: The proof of property (1) is directly from the definition of B(n)
ϵ
(˜xn, ˜yn).

To prove the left inequality of property (2), we write

p (˜xn, ˜yn) = X (xn,yn)∈B(n) ϵ (˜xn,˜yn) p (xn, yn) (118) ≤p (˜xn, ˜yn) X (xn,yn)∈B(n) ϵ (˜xn,˜yn) 2−n(H(X,Y )−Hs( ˜ X, ˜Y )−ϵ) = p (˜xn, ˜yn) B(n) ϵ (˜xn, ˜yn)  2−n(H(X,Y )−Hs( ˜ X, ˜Y )−ϵ). So it follows that B(n) ϵ (˜xn, ˜yn)  ≥2n(H(X,Y )−Hs( ˜ X, ˜Y )−ϵ) (119)
On the other hand, we can write

p (˜xn, ˜yn) = X (xn,yn)∈B(n) ϵ (˜xn,˜yn) p (xn, yn) (120) ≥p (˜xn, ˜yn) X (xn,yn)∈B(n) ϵ (˜xn,˜yn) 2−n(H(X,Y )−Hs( ˜ X, ˜Y )+ϵ) = p (˜xn, ˜yn) B(n) ϵ (˜xn, ˜yn)  2−n(H(X,Y )−Hs( ˜ X, ˜Y )+ϵ). Similarly, we have B(n) ϵ (˜xn, ˜yn)  ≤2n(H(X,Y )−Hs( ˜ X, ˜Y )+ϵ) (121)
and complete the proof.

□
The relationships of typical sets, such as typical set, jointly typical set and synonymous typical set, are shown in Fig. 9. By using synonymous mapping f n x or f n y , syntactically typical set

A(n)
 ϵ (Xn) or A(n)
        ϵ (Y n) can be partitioned into many synonymous typical sets B(n)
                                           ϵ (Xn) or B(n)
                                                  ϵ (Y n).

1 The syntactically jointly typical set A(n)
ϵ (Xn, Y n) consists of some syntactically typical sequences simultaneously belonging to the typical set A(n)
ϵ (Xn) and A(n)
ϵ (Y n). Note that not all pairs of syntactically typical Xn and Y n are jointly typical.

The probability of jointly typical set A(n)
ϵ (Xn, Y n) with sufficiently large n is close to 1. In addition, since all the jointly typical sequences are almost equiprobability, the number of syn-

tactically jointly typical sequences is about
                                             A(n)
                                                ϵ (Xn, Y n)
                                                             ≈2n(H(X,Y )±ϵ). Similar conclusion

holds for the jointly typical set ˜A(n)
                                   ϵ ( ˜Xn, ˜Y n) and the corresponding number of jointly typical

sequences is about
                   ˜A(n)
                     ϵ ( ˜Xn, ˜Y n)
                                 ≈2n(Hs( ˜
                                          X, ˜Y )±ϵ).

Furthermore, under the joint mapping f n
                                      xy, as circled by dashed lines, some jointly typical

sequences constitute the jointly synonymous typical sets B(n)
                                                          ϵ (Xn, Y n). Since the number of

jointly synonymous typical sequences is about 2n(H(X,Y )−Hs( ˜
X, ˜Y )±ϵ), all the synonymous typical sets have almost the same number of typical sequences. In each set, the black circle marked by a red box denotes the representative typical sequence. If we randomly choose a typical sequence pair, the probability that this pair falls in a jointly synonymous typical set (equivalently represents a semantically jointly typical sequence) is about 2−nIs( ˜
X; ˜Y ). This means that there are about
2nIs( ˜
X; ˜Y ) distinguishable sequences ˜Xn in the semantic sense.

## B. Semantic Channel Coding Theorem

We now discuss the problem of semantic channel coding. As shown in Fig. 10, with the help of synonymous mapping f n, a semantic index is is mapped and encoded into the channel codeword Xn. Here, the semantic message ˜Xn(is) is a broad concept, which can be a real semantic sequence or a syntactic sequence with some semantic constraints. After going through the channel, we obtain the received sequence Y n. Then the decoder outputs the decoded codeword
ˆXn. After de-synonymous mapping gn, we obtain an estimation of semantic index ˆis.

si
n
Y
ˆ n
X
n
X
si
ˆ
Encoder
Decoder
n
f
n
g


Channel
(
)
p y x

For a discrete memoryless channel, let p(Y |X) be the channel transition probabilities and X
and Y denote the input and output syntactical alphabet of channel respectively. Then the channel transition probabilities for the n-th extension of the channel can be written as

$$p(y^{n}\left|x^{n}\right.)=\prod_{k=1}^{n}p(y_{k}\left|x_{k}\right.).\tag{122}$$
Definition 21. An (M, n) code for the semantic channel n
˜
X, X, Y, ˜Y, p(Y |X)
o consists of the following parts:

(1) A semantic index set Is = {1, · · · , is, · · · , Ms} and a syntactic index set I = {1, · · · , i, · · · , M}.
(2) An encoding function ϕ : ˜
X n →X n generates the set of codewords, namely, codebook,
C = {Xn(1), Xn(2), · · · , Xn(M)}. Due to the synonymous mapping f n, this codebook can
be partitioned into synonymous codeword subsets Cs.
(3) A decoding function ψ : Yn →X n outputs the decision syntactic codeword ˆXn.
(4) After de-mapping, gn( ˆXn) = ˆis, the estimated semantic index is obtained. Note that both ψ
and gn are deterministic.
According to the synonymous mapping f n, Cs is an equivalence class consisting of the synonymous codewords. So we can construct a quotient set C/f n = {Cs} with |C/f n| = Ms. Let R = 1
n log2 Ms denote the semantic code rate of channel coding. We configure each synonymous set with the same number of codewords, that is, |Cs| = 2nRs =
M
Ms, where Rs is named as the rate of synonymous set. Furthermore, let R′ = R+Rs = 1
n log2 M being the syntactic code rate.

Definition 22. Assume a semantic index is is mapped into a syntactic codeword Xn(i) ∈Cs(is), the conditional decoding error probability given the index is is defined as

$$\lambda_{i_{s}}=Pr\left(g^{n}\left(\psi\left(Y^{n}\right)\right)\neq i_{s}\left|X^{n}=X^{n}(i)\leftrightarrow\tilde{X}^{n}=\tilde{X}^{n}(i_{s})\right.\right)\tag{123}$$ $$=Pr\left(\tilde{X}^{n}(i)\notin\mathcal{C}_{s}(i_{s})\left|X^{n}=X^{n}(i)\leftrightarrow\tilde{X}^{n}=\tilde{X}^{n}(i_{s})\right.\right)$$ $$=\sum_{y^{n}}p\left(y^{n}(i)\left|x^{n}(i)\right.\right)I\left(\psi(y^{n})\notin\mathcal{C}_{s}(i_{s})\right)$$
where I(·) is the indicator function. Assume the index is is chosen uniformly on the set Is, the average error probability P (n)
e for an (M, n) code is defined as

$$P_{e}^{(n)}=\frac{1}{M_{s}}\sum_{i_{s}=1}^{M_{s}}\lambda_{i_{s}}.\tag{124}$$
We now give the formal description of semantic channel coding theorem.

## Theorem 21. (Semantic Channel Coding Theorem)

Given the semantic channel n
˜
X, X, Y, ˜Y, p(Y |X)
o
, for each code rate R < Cs, there exists

a sequence of
             
              2n(R+Rs), n
                         
                           codes consisting of synonymous codeword set with the rate 0 ≤

Rs ≤H(X, Y )−Hs( ˜X, ˜Y ), when code length n tends to sufficiently large, the error probability

tends to zero, i.e. P (n)
                     e
                         →0.

On the contrary, if R > Cs, then for any
                                          
                                           2n(R+Rs), n
                                                      
                                                        code, the error probability tends to 1

with sufficiently large n.

Proof: We first prove the achievability part of the theorem and the converse will be left in

the next part.

Given the source distribution p(x) and the synonymous mapping fx, a
                                                                       
                                                                        2n(R+Rs), n
                                                                                   
                                                                                     code can

be generated randomly according to the distribution p(x). Note that 2n(R+Rs) codewords can be

independently generated based on the distribution

$$p(x^{n})=\prod_{k=1}^{n}p(x_{k}).\tag{125}$$
Furthermore, these codewords can be uniformly divided into 2nR groups according to the synonymous mapping

$$f_{x}^{n}(\tilde{x}^{n})=\prod_{k=1}^{n}\mathcal{X}_{\tilde{x}_{k}}\,.\tag{126}$$
Thus all the 2n(R+Rs) codewords can be listed as a matrix

$$\mathcal{C}=\left[\begin{array}{ccccc}x_{1}(1)&x_{2}(1)&\cdots&x_{n}(1)&\Big{]}\\ \vdots&\vdots&\vdots&\vdots&\vdots\\ x_{1}(2^{nR_{s}})&x_{2}(2^{nR_{s}})&\cdots&x_{n}(2^{nR_{s}})\\ \vdots&\vdots&\ddots&\vdots\\ x_{1}(2^{n(R+R_{s})})&x_{2}(2^{n(R+R_{s})})&\cdots&x_{n}(2^{n(R+R_{s})})&\mathcal{C}_{s}(2^{nR})\end{array}\right]\tag{127}$$
The probability of generating the synonymous codeword set Cs(1) is

$$\Pr\left(\mathcal{C}_{s}(1)\right)=\prod_{i=1}^{2^{nR_{S}}}\prod_{k=1}^{n}p(x_{k}(i))\tag{128}$$
All the codeword sets Cs(is) have the same generating probability. Furthermore, the probability of generating a particular code C is

$$\Pr\left(\mathcal{C}\right)=\prod_{i=1}^{2^{n\left(R+R_{s}\right)}}\prod_{k=1}^{n}p(x_{k}(i)).\tag{129}$$
Similar to the idea in [4], we also use jointly typical decoding for the semantic channel code.

If a codeword ˆXn(i) is decided, it must satisfy the following conditions.

1) ( ˆXn(i), Y n) is syntactically jointly typical, due to gn( ˆXn(i)) = ˆis, ( ˜Xn(ˆis), ˜Y n) is also
semantically jointly typical. Equivalently, ( ˆXn(i), Y n) is jointly synonymous typical. Hence,
the decision codeword ˆXn(i) may be not equal to the transmit codeword Xn(i) but both
ˆXn(i) and Xn(i) belong to Cs(ˆis).
2) There is no other index m, satisfying ( ˆXn(m), Y n) ∈A(n)
ϵ
or gn( ˆXn(m)) = ˆis.
We now calculate the average error probability of jointly typical decoding. Generally, this error probability should be averaged over all the codebooks and all the codewords. However, based on the symmetry of the code construction, due to averaging over all codes, the error probability is not dependent on the specific the semantic index is and the syntactic index i. We relabel the codewords in a synonymous set as Xn(is, j) ∈Cs(is), j ∈

1, 2, *· · ·* , 2nRs	
.

Without loss of generality, we can assume is = 1 and i = 1, that is, the codeword Xn(1, 1)
is sent. Therefore, the average error probability can be written as

$$\Pr(\mathcal{E})=\frac{1}{2^{nR}}\sum_{i_{s}=1}^{2^{nR}}\sum_{\mathcal{C}}P(\mathcal{C})\lambda_{i_{s}}(\mathcal{C})\tag{130}$$ $$=\sum_{\mathcal{C}}P(\mathcal{C})\lambda_{1}(\mathcal{C})$$ $$=\Pr\left(\mathcal{E}\left|X^{n}(1,1)\right.\right).$$
Given the received sequence Y n when sending the first codeword Xn(1, 1), we define the

following events:

$$\begin{split}E_{i_{s}}=\big{\{}(X^{n}(i_{s},j),Y^{n})&\in B_{i}^{(n)},X^{n}(i_{s},j)\in\mathcal{C}_{s}(i_{s})\big{\}},\\ i_{s}&\in\mathcal{I}_{s},j\in\big{\{}1,2,\cdots,2^{nR_{s}}\big{\}}.\end{split}\tag{131}$$
Here, the event Eis means that the codewords in the is-th synonymous set Cs(is) and Y n are jointly synonymous typical.

When sending the first codeword Xn(1, 1) and receiving the received sequence Y n, by using jointly typical decoding, two kinds of error will occur. The first error event is Ec
1 which means that all the codewords in Cs(1) and Y n are not jointly typical. By the syntactically joint AEP, we have

$P(E_{1}^{c})\leq\epsilon,$ for sufficiently large $n$. (132)
On the other hand, the second error event is Eis, is ∈

2, *· · ·* , 2nR	
which means that a codeword in a wrong synonymous set is jointly typical with the received sequence Y n. Due to the code construction process, Xn(1, 1), Xn(is, j), (is ̸= 1), and Y n are mutually independent.

Hence, by using semantically joint AEP (Theorem 19), the probability that Xn(is, j) and Y n are jointly synonymous typical is written as

$$P(E_{i_{s}})\leq2^{-n(I^{s}(\bar{X};\bar{Y})-3\epsilon)},i_{s}\in\left\{2,\cdots,2^{nR}\right\}.\tag{133}$$
Consequently, combining (132) and (133), the error probability can be derived as

$$\Pr(\mathcal{E})=\Pr\left(\mathcal{E}\left|X^{n}(1,1)\right.\right)$$ $$\leq P(E_{1}^{c})+\sum_{i_{s}=2}^{2^{nR}}P(E_{i_{s}})$$ $$\leq\epsilon+\sum_{i_{s}=2}^{2^{nR}}2^{-n(I^{s}(\bar{X};\bar{Y})-3\epsilon)}\tag{134}$$ $$=\epsilon+\left(2^{nR}-1\right)2^{-n(I^{s}(\bar{X};\bar{Y})-3\epsilon)}$$ $$\leq\epsilon+2^{-n\left(I^{s}(\bar{X};\bar{Y})-R-3\epsilon\right)}$$ $$\leq2\epsilon.$$
This formula holds for sufficiently large n and Is( ˜X; ˜Y ) −R −3ϵ > 0.

Therefore, if the semantic code rate satisfies *R < I*s( ˜X; ˜Y ), the error probability can tend to zero with the suitable ϵ and n. In addition, by Theorem 20, the size of synonymous codeword

set satisfies

$$1\leq2^{nR_{n}}\leq2^{n\big{(}H(X;Y)-H_{n}(\hat{X};\hat{Y})\big{)}}\tag{135}$$ $$=2^{n\big{(}I^{*}(\hat{X};\hat{Y})-I(X;Y)\big{)}}$$
for sufficiently large n. Thus we have 0 ≤Rs ≤(Is( ˜X; ˜Y ) −I(X; Y )) and derive that

$$I(X;Y)+R_{s}\leq I^{s}(\tilde{X};\tilde{Y}).\tag{136}$$
Then the syntactic code rate R′ = R + Rs can be upper bounded by

$$R^{\prime}\leq2I^{s}(\tilde{X};\tilde{Y})-I(X;Y)\tag{137}$$ $$=I^{s}(\tilde{X};\tilde{Y})+H(X,Y)-H_{s}(\tilde{X},\tilde{Y}).$$
In summary, for any code rate below the semantic capacity Cs = maxfxy maxp(x) Is( ˜X; ˜Y ), we can construct a code with the error probability being close to zero for sufficiently large n. This proves the achievability of theorem.

□
Remark 9. In the classic channel coding theorem, in order to satisfying the requirement of reliable communication, the code rate must be lower than the channel capacity C. On the contrary, in the semantic channel coding, the code rate can be further increased to the semantic channel capacity Cs under the condition of keeping the semantic reliability. Using synonymous codeword set to present the semantic sequence is the key technique to achieve this goal. By
(136), due to C + Rs ≤Cs, with the increasing of the number of codewords in a synonymous codeword set, the semantic code rate gradually grows and approaches the semantic capacity. Similar to the classic counterpart, although the proof of the semantic channel coding theorem is also an existence method due to using the random coding, it may provide some hints to construct the channel codes approaching the semantic capacity.

In order to prove the converse, we first illustrate the relationship between sequential syntactic mutual information and sequential semantic mutual information.

Lemma 8. Assume ˜Xn is the transmitted semantic sequence over a discrete memoryless channel and the received sequence is Y n, we have

$I(\tilde{X}^{n};Y^{n})\leq I^{s}(\tilde{X}^{n};\tilde{Y}^{n}),$ for all $p(x^{n}).$_ (138)
Proof: Due to the definition of discrete memoryless channel, we can write the sequential mutual information as

$$I(\tilde{X}^{n};Y^{n})-I^{s}(\tilde{X}^{n};\tilde{Y}^{n})$$ $$=H(\tilde{X}^{n})+H(Y^{n})-H(\tilde{X}^{n},Y^{n})$$ $$-\left[H(X^{n})+H(Y^{n})-H_{s}(\tilde{X}^{n},\tilde{Y}^{n})\right]\tag{139}$$ $$=\sum_{k=1}^{n}\left[H_{s}(\tilde{X}_{k})-H(X_{k})+H_{s}(\tilde{X}_{k},\tilde{Y}_{k})-H(\tilde{X}_{k},Y_{k})\right]\leq0.$$
Due to H( ˜Xk) = Hs( ˜Xk) ≤H(Xk) and Hs( ˜Xk, ˜Yk) ≤H( ˜Xk, Yk) (Theorem 2), we prove the lemma.

□
Then we investigate the sequential semantic mutual information many times using of discrete memoryless channel.

Lemma 9. Assume Xn is the transmitted codeword over a discrete memoryless channel and the received sequence is Y n, under a jointly synonymous mapping f n xy, we have

$I^{s}(\tilde{X}^{n};\tilde{Y}^{n})\leq nC_{s},$ for all $p(x^{n}).$_ (140)
Proof: Due to the definitions of discrete memoryless channel and up-semantic mutual information, we can write

$$I^{s}(\tilde{X}^{n};\tilde{Y}^{n})=H(X^{n})+H(Y^{n})-H_{s}(\tilde{X}^{n};\tilde{Y}^{n})\tag{141}$$ $$\leq\sum_{k=1}^{n}\left[H(X_{k})+H(Y_{k})\right]-\sum_{k=1}^{n}H_{s}(\tilde{X}_{k},\tilde{Y}_{k})$$ $$=\sum_{k=1}^{n}\left[H(X_{k})+H(Y_{k})-H_{s}(\tilde{X}_{k},\tilde{Y}_{k})\right]$$ $$\leq nC_{s}.$$
The first inequality is from the property of sequential entropy and the second is from the definition of semantic channel capacity.

□
We now prove the converse to the semantic channel coding theorem.

Proof: (Converse to Theorem 21, (Semantic Channel Coding Theorem)):
Let Ws denote a semantic index uniformly drawn from

1, 2, *· · ·* , 2nR	
. The error probability can be written as P (n)
e
= Pr( ˆWs ̸= Ws). So we have

$$nR=H(W_{s})=H(W_{s}|Y^{n})+I(W_{s};Y^{n})$$ $$\stackrel{{(a)}}{{\leq}}H(W_{s}|Y^{n})+I(\tilde{X}^{n}(W_{s});Y^{n})$$ $$\stackrel{{(b)}}{{\leq}}1+P_{e}^{(n)}nR+I(\tilde{X}^{n}(W_{s});Y^{n})\tag{142}$$ $$\stackrel{{(c)}}{{\leq}}1+P_{e}^{(n)}nR+I^{s}(\tilde{X}^{n}(W_{s});\tilde{Y}^{n})$$ $$\stackrel{{(d)}}{{\leq}}1+P_{e}^{(n)}nR+nC_{s}.$$
Inequality (a) holds since ˜Xn(Ws) is the function of Ws and (b) is from Fano's inequality. In equality (c) is from Lemma 8 and (d) from Lemma 9.

So the semantic code rate satisfies

$$R\leq C_{s}+P_{e}^{(n)}R+\frac{1}{n},\tag{143}$$
which means R ≤Cs for n →∞. On the other hand, we can rewrite (143) as

$$P_{e}^{(n)}\geq1-\frac{1}{nR}-\frac{C_{s}}{R}.\tag{144}$$
This formula indicates that if *R > C*s, the error probability is larger than zero for sufficiently large n and the reliable transmission of semantic information can not be fulfilled. So we complete the proof.

□
In classic communication systems, the channel capacity is a fundamental limitation of data reliable transmission. In the past seventy years, people invented many powerful channel codes to approach capacity, such as turbo, LDPC and polar codes. Similarly, the semantic capacity is also a key parameter for the semantic transmission. In the future, the construction of channel codes approaching semantic capacity will become one core issue of semantic communication.

## C. Semantic Channel Coding Method

We now investigate the semantic channel coding method. Given a
                                                              
                                                               2n(R+Rs), n
                                                                          
                                                                           channel code

with length n and semantic rate R, the codebook C can be divided into 2nR synonymous codeword

groups Cs(is), is ∈{1, 2, · · · , 2nR} and each group has 2nRs synonymous codewords. Consider

the synonymous mapping, we propose a new method, named as maximum likelihood group (MLG) decoding algorithm to decode this semantic code. The basic idea is to calculate the likelihood probability of the received signal on a synonymous group and compare all the group likelihood probabilities so as to select a group with the maximum probability as the final decoding result.

Definition 23. Assume one codeword xn ∈Cs(is) is transmitted on the discrete memoryless

channel with the transition probability p(yn|xn), the group likelihood probability is defined as

$$P(y^{n}|{\cal C}_{s}(i_{s}))\stackrel{{\triangle}}{{=}}\prod_{l=1}^{2^{nR_{s}}}p(y^{n}|x^{n}(i_{s},l)).\tag{145}$$
So the maximum likelihood group decoding rule is written as

$$\dot{i}_{s}=\arg\max_{i_{s}}P(y^{n}|{\cal C}_{s}(i_{s}))\tag{146}$$ $$=\arg\max_{i_{s}}\prod_{l=1}^{2^{nR_{s}}}p(y^{n}|x^{n}(i_{s},l)).$$
Equivalently, this rule can also presented as a logarithmic version,

$$\dot{i}_{s}=\arg\max_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\ln p(y^{n}|x^{n}(i_{s},l)).\tag{147}$$
Hence, we can calculate all the group likelihood probabilities and select one group with the maximum probability as the final decision. The index ˆis indicates the estimation of semantic information ˆ˜xn.

Next, we discuss the MLG decoding in the additive white Gaussian noise (AWGN) channel.

When a signal is transmitted over the AWGN channel, the received signal can be represented by an equivalent low-pass signal sampled at time k:

$y_{k}=s_{k}+z_{k}$ (148)
where sk =

±√Es
	
is the binary phase shifted key (BPSK) signal, zk is a sample of a zeromean complex Gaussian noise process with variance σ2 = N0/2. Let Es be the symbol energy and N0 denote the single-sided power spectral density of the additive white noise. So the symbol signal-to-noise ratio (SNR) is defined as Es N0.

Assume one codeword xn(is, l) is mapped into a transmitted signal vector sn(is, l) =
√Es (1n −2xn(is, l)) where 1n is the all-one vector with the length of n, by using MLG rule, we can write

$$\hat{i}_{s}=\arg\max_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\ln p(y^{n}|x^{n}(i_{s},l))\tag{149}$$ $$=\arg\max_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\ln\left[\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{\|y^{n}-s^{n}(i_{s},l)\|^{2}}{2\sigma^{2}}}\right]$$ $$=\arg\min_{i_{s}}\sum_{l=1}^{2^{nR_{s}}}\left\|y^{n}-s^{n}(i_{s},l)\right\|^{2}$$ $$=\arg\min_{i_{s}}d_{\rm E}^{2}\left(y^{n},{\cal C}_{s}(i_{s})\right),$$
where d2
E (yn, Cs(is)) = P2nRs l=1 ∥yn −sn(is, l)∥2 is the squared Euclidian distance between the receive vector yn and the code group Cs(is). Thus the MLG rule in AWGN channel is transformed into the minimum distance grouping decoding rule.

Now we investigate the group-wise error probability (GEP) of semantic channel code.

Theorem 22. Given a semantic channel code C with equipartition code groups Cs, the GEP
between Cs(is) and Cs(js) is upper bounded by

$$P\left(\mathcal{C}_{s}(i_{s})\rightarrow\mathcal{C}_{s}(j_{s})\right)\leq\exp\left\{-d_{GH}(\mathcal{C}_{s}(i_{s}),\mathcal{C}_{s}(j_{s}))\frac{E_{s}}{N_{0}}\right\},\tag{150}$$
where dGH denotes the group Hamming distance which is defined as following dGH(Cs(is), Cs(js)) =

$$\min_{m}\left[\sum_{l=1}^{2^{nR_{k}}}d_{H}(x^{n}(i_{s},m),x^{n}(j_{s},l))-\sum_{l=1,l\neq m}^{2^{nR_{k}}}d_{H}(x^{n}(i_{s},m),x^{n}(i_{s},l))\right]^{2}\tag{151}$$ $$\frac{\parallel\sum_{l=1}^{2^{nR_{k}}}\left(x^{n}(j_{s},l)-x^{n}(i_{s},l)\right)\parallel^{2}}{\parallel\sum_{l=1}^{2^{nR_{k}}}\left(x^{n}(j_{s},l)-x^{n}(i_{s},l)\right)\parallel^{2}}.$$
Proof: Assume one codeword xn(1, 1) in the code group Cs(1) is transmitted, the received signal vector can be represented as follows

$y^{n}=s^{n}(1,1)+z^{n}$, (152)
where zn ∼N(0, σ2I) is the Gaussian noise vector.

Suppose a codeword xn(js, l) ∈Cs(js), js ̸= 1 is mapped into the signal vector sn(js, l). By using the MLG rule, if a group-wise error occurs, the Euclidian distance between the received vector and the transmitted signal vector group satisfy the inequality

$$d_{\rm E}^{2}\left(y^{n},{\cal C}_{s}(i_{s})\right)>d_{\rm E}^{2}\left(y^{n},{\cal C}_{s}(j_{s})\right).\tag{153}$$
Substituting (149) and (152) into this inequality, we have

$$\sum_{l=1}^{2^{nRs}}\|y^{n}-s^{n}(1,l)\|^{2}$$ $$>\sum_{l=1}^{2^{nRs}}\|y^{n}-s^{n}(j_{s},l)\|^{2}\Rightarrow\tag{154}$$ $$\|z^{n}\|^{2}+\sum_{l=2}^{2^{nRs}}\|s^{n}(1,1)-s^{n}(1,l)+z^{n}\|^{2}$$ $$>\sum_{l=1}^{2^{nRs}}\|s^{n}(1,1)-s^{n}(j_{s},l)+z^{n}\|^{2}\,.$$
After some manipulations, the error decision region can be written as

$$\mathcal{H}=\left\{z^{n}:\left|\sum_{l=1}^{2^{nRs}}\left(s^{n}(j_{s},l)-s^{n}(1,l)\right)\right|\,\left(z^{n}\right)^{T}>\right.$$ $$\left.\frac{1}{2}\left|\sum_{l=1}^{2^{nRs}}\left\|s^{n}(1,1)-s^{n}(j_{s},l)\right\|^{2}\right.\right.\tag{155}$$ $$\left.-\sum_{l=2}^{2^{nRs}}\left\|s^{n}(1,1)-s^{n}(1,l)\right\|^{2}\right\}.$$
Let d2
E (sn(1, 1), Cs(js)) = P2nRs l=1 ∥sn(1, 1) −sn(js, l)∥2 denote the distance between the transmit vector sn(1, 1) and the code group Cs(js) and d2
E (sn(1, 1), Cs(1)) = P2nRs l=2 ∥sn(1, 1) −sn(1, l)∥2
denote the inner distance of code group Cs(1).

So the codeword-to-group error probability can be derived as

$$P\left(x^{n}(1,1)\rightarrow\mathcal{C}_{s}(j_{s})\right)$$ $$=Q\,\left|\,\sqrt{\frac{\left(d_{\mathbb{E}}^{2}\left(s^{n}(1,1),\mathcal{C}_{s}(j_{s})\right)-d_{\mathbb{E}}^{2}\left(s^{n}(1,1),\mathcal{C}_{s}(1)\right)\right)^{2}}{\left\|\sum_{l=1}^{2^{n}B_{s}}\left(s^{n}(j_{s},l)-s^{n}(1,l)\right)\right\|^{2}2N_{0}}}\right|\,,\tag{156}$$
where Q(x) =
1
√

$\frac{1}{2\pi}\int_{x}^{\infty}e^{-t^{2}/2}dt$ is the tail distribution function of the standard normal distribution.

Furthermore, due to (sn(1, 1) −sn(js, l)) = 2√Es (xn(js, l) −xn(1, 1)), then we have ∥sn(1, 1) −sn(js, l)∥2

= 4EsdH(xn(1, 1), xn(js, l)). Additionally, we can derive that
                                                  

P2nRs
                                                     l=1 (sn(js, l) −sn(1, l))
                                                                        

                                                                         2
                                                                           =

4Es
   

P2nRs
      l=1 (xn(js, l) −xn(1, l))
                           

                            2
                             = 4Es∥∆(Cs(js), Cs(1))∥2. Thus the error probability can be

further written as

$$P\left(x^{n}(1,1)\rightarrow{\cal C}_{s}(j_{s})\right)=Q\left[\begin{array}{c}/\\ d_{\rm GH}(x^{n}(1,1),{\cal C}_{s}(j_{s}))\frac{2E_{s}}{N_{0}}\end{array}\right]\,,\tag{157}$$
where dGH(xn(1, 1), Cs(js)) =[P2nRs l=1 dH(xn(1, 1), xn(js, l))
−P2nRs l=2 dH(xn(1, 1), xn(1, l))]2/∥∆(Cs(js), Cs(1))∥2 denotes the codeword-to-group Hamming distance.

Furthermore, using the inequality Q(x) ≤e−x2
2 , the codeword-to-group error probability can be upper bounded by

$$P\left(x^{n}(1,1)\to{\cal C}_{s}(j_{s})\right)\leq e^{-d_{\rm OH}(x^{n}(1,1),{\cal C}_{s}(j_{s}))}\frac{E_{s}}{N_{0}}\,.\tag{158}$$
Averaging over all the codewords of the group Cs(1), we obtain the upper bound of GEP as follows

$$P\left(\mathcal{C}_{s}(1)\rightarrow\mathcal{C}_{s}(j_{s})\right)\leq\sum_{l=1}^{2^{nR_{s}}}\frac{1}{2^{nR_{s}}}e^{-d_{\mathrm{GH}}(x^{n}(1,l),\mathcal{C}_{s}(j_{s}))}\frac{E_{s}}{N_{0}}\tag{159}$$ $$\leq\exp\left\{-d_{\mathrm{GH}}(\mathcal{C}_{s}(1),\mathcal{C}_{s}(j_{s}))\frac{E_{s}}{N_{0}}\right\}\,.$$
So we complete the proof.

□
In the ML decoding, the minimum Hamming distance dH,min determines the error performance of one linear channel code. Similarly, in the MLG decoding, the minimum group Hamming distance dGH,min = min dGH(Cs(is), Cs(js)) dominates the performance of semantic channel code.

Example 5. We now give an example of semantic code constructed based on (7,4) Hamming code with synonymous mapping and MLG decoding. The codebook is shown in Table IX. All the sixteen codewords are divided into eight code groups and each group has two synonymous codewords. For an instance, Cs(1) *has two codewords* (0000000) *and* (1101000) and its semantic sequence is (000). So this code can be regarded as a (7,3) semantic Hamming code with code rate R = 3
7.

7 and Rs = 1
By using ML decoding, the union bound of the error probability is

$$P_{e}\leq\sum_{d=d_{H_{min}}}^{n}A_{d}Q\left(\begin{array}{c}/_{2}\frac{E_{s}}{N_{0}}\\ \end{array}\right)\leq\sum_{d=d_{H_{min}}}^{n}A_{d}e^{-d\frac{E_{s}}{N_{0}}}.\tag{160}$$
Since the minimum Hamming distance of this code is d*H,min* = 3 and distance spectrum is
{A3 = 8, A4 = 6, A7 = 1}, the error probability of ML decoding is upper bounded by

$$P_{e}^{\it ML}\leq8e^{-3\frac{E_{\pi}}{N_{0}}}+6e^{-4\frac{E_{\pi}}{N_{0}}}+e^{-7\frac{E_{\pi}}{N_{0}}}.\tag{161}$$
Let {Ad1,d2} denote the group distance spectrum and d1 and d2 mean the codeword-to-group Hamming distance. By using MLG decoding, the union bound of the error probability is

" ! !#  r  r Ad1,d2 n X Q + Q Pe ≤ 2 2d1 Es N0 2d2 Es N0 d1,d2=dGH,min (162) Ad1,d2 n X ≤  e−d1 Es N0 + e−d2 Es N0  . 2 d1,d2=dGH,min
So the minimum group Hamming distance of this code is d*GH,min* = 2 and group distance spectrum is {A2,2 = 6, A4,4 = 1}. The corresponding upper bound of the MLG decoding is

$$P_{e}^{MLG}\leq6e^{-2\frac{E_{x}}{N_{0}}}+e^{-4\frac{E_{x}}{N_{0}}}.\tag{163}$$
Compare with (161) and (163), we find that the minimum distance of semantic Hamming code is decreased. However, for a long code length and well-designed synonymous mapping, the error performance of MLG decoding will be better than that using ML decoding.

Remark 10. From the viewpoint of practical application, semantic channel codes are a new kind of channel codes. Synonymous mapping provides a valuable idea for the construction and decoding of semantic channel codes. Unlike the traditional channel codes, semantic channel codes should optimize the minimum group Hamming distance. How to design an optimal synonymous mapping to cleverly partition the code group is significant for the design of semantic codes. Non-equipartition mapping may be more flexible than the equipartition mapping. On the other hand, the optimal decoding of semantic codes is the MLG rule rather ML rule. However,

Index is
Semantic sequence
Hamming code group Cs(is)
1
000
{0000000, 1101000}
2
001
{0110100, 1011100}
3
010
{1110010, 0011010}
4
011
{1000110, 0101110}
5
100
{1010001, 0111001}
6
101
{1100101, 0001101}
7
110
{0100011, 1001011}
8
111
{0010111, 1111111}

due to the exponent complexity of MLG decoding algorithm, it is not practical for application. So we should pursuit lower complexity decoding algorithms for the semantic channel codes in the future.

## Viii. Semantic Lossy Source Coding

In this section, we mainly discuss the semantic lossy source coding. Firstly, we investigate the semantic distortion measure and extend the concept of jointly typical sequence in the semantic sense. Then we prove the semantic rate-distortion coding theorem by using JAEP
and synonymous typical set, which means that the semantic rate distortion function, Rs(D) =
minfx,fˆx minp(Y |X):Eds( ˜
X, ˆ˜
X)≤D Is( ˜X; ˆ˜X), namely the minimum down semantic mutual information, is the lowest compression rate achieving by semantic lossy source coding.

## A. Semantic Distortion And Jointly Typical Set

Assume a discrete source X ∼p(x), x ∈X with the associated semantic source ˜X produces a sequence Xn ∼p(xn), after the quantization and reproduction, equivalently, through a test channel with the transition probabilities p(ˆxn|xn), we obtain the representative sequence ˆXn ∼
p(xn) p(xn)p(ˆxn|xn) denotes the distribution of reproduction sequences.

p(ˆxn). Here, p(ˆxn) = P
Definition 24. For a semantic sequence ˜xn, under the synonymous mapping f n x , the associated source sequence is xn. After going through a test channel with the transition probabilities p(ˆxn|xn), we obtain the reconstruction sequence ˆxn. Under the de-synonymous mapping gn
ˆx, the associated semantic sequence is ˆ˜xn. Thus the semantic distortion between sequences ˜xn and
ˆ˜xn is defined by

$$d_{s}(\hat{x}^{n},\hat{x}^{n})=\frac{1}{n}\sum_{k=1}^{n}d_{s}(\hat{x}_{k},\hat{x}_{k})\tag{164}$$ $$=\frac{1}{n}\sum_{k=1}^{n}d_{s}(\mathcal{X}_{\hat{x}_{k}},\hat{\mathcal{X}}_{\hat{x}_{k}}).$$
Then the average semantic distortion over the semantic sequences is defined as

$$\mathbb{E}\left[d_{s}(\hat{X}^{n},\hat{\hat{X}}^{n})\right]=\sum_{(x^{n},\hat{x}^{n})}p\left(x^{n}\right)p(\hat{x}^{n}|x^{n})d_{s}(\hat{x}^{n},\hat{\hat{x}}^{n}).\tag{165}$$

Given the synonymous mapping f n
                              x , let A(n)
                                     ϵ (Xn) and ˜A(n)
                                                  ϵ ( ˜Xn) denote the syntactically

and semantically typical set of source sequence respectively. Correspondingly, the synonymous

typical set is denoted as B(n)
                           ϵ (˜xn →Xn). According to Theorem 14, we have

$$2^{n\left(H(X)-H_{s}(\bar{X})-\epsilon\right)}\leq\left|B_{\epsilon}^{(n)}(\tilde{x}^{n}\to X^{n})\right|\leq2^{n\left(H(X)-H_{s}(\bar{X})+\epsilon\right)}\tag{166}$$
for sufficiently large n.

Furthermore, given a test channel with the transition probabilities p(ˆxn|xn), the distribution p(xn) p(xn)p(ˆxn|xn). Similarly, given the of reproduction sequence is written as p(ˆxn) = P

synonymous mapping f n
                   ˆx , let A(n)
                          ϵ ( ˆXn) and ˜A(n)
                                      ϵ ( ˆ˜Xn) denote the syntactically and semantically

typical set of reproduction sequence respectively. Moreover, the associated synonymous typical

set is addressed as B(n)
                     ϵ (ˆ˜xn →ˆXn). By Theorem 14, we also have

$$2^{n\left(H(\hat{X})-H_{s}(\hat{X})-\epsilon\right)}\leq\left|B_{\epsilon}^{(n)}(\hat{x}^{n}\to\hat{X}^{n})\right|\leq2^{n\left(H(\hat{X})-H_{s}(\hat{X})+\epsilon\right)}\tag{167}$$
for sufficiently large n. Since all the synonymous typical sets have almost the same size, we can abbreviate them as B(n)
ϵ ( ˆXn) and B(n)
ϵ (Xn) respectively.

Given the joint distribution p(x, ˆx) on *X ×* ˆ
X, let A(n)
ϵ (Xn, ˆXn) denote the the syntactically jointly typical set of source sequence and reproduction sequence and ˜A(n)
ϵ
be the semantically jointly typical set.

The typical sequence mapping for lossy source coding is depicted in Fig. 11. Under the synonymous mappings f n x and f n
ˆx , the semantic source sequence ˜xn and semantic reconstruction sequence ˆ˜xn are separately mapped into synonymous typical sets B(n)
ϵ (Xn) and B(n)
ϵ ( ˆXn) .

Furthermore, by the conditional probability p(ˆxn|xn), the typical sequences in these sets can compose the jointly typical sequence.

Then as the consequence of the JAEP, we present the properties of semantically jointly typical set as following.

Theorem 23. Let (Xn, ˆXn) be a sequence pair with length n drawn i.i.d. according to p(xn, ˆxn).

By using synonymous mapping f n x (f n
ˆx ), the semantic sequence ˜Xn ( ˆ˜Xn) is mapped into a syntactic sequence Xn ( ˆXn).

(1) Pr

( ˜Xn, ˆ˜Xn) ∈˜A(n)
ϵ

> 1 −ϵ for n sufficiently large.
(2) (1 −ϵ) 2
n

Hs( ˜
X, ˆ˜
X)−ϵ

≤
 ˜A(n)
ϵ
 ≤2
n

Hs( ˜
X, ˆ˜
X)+ϵ

for n sufficiently large.
(3) Given ˜Zn and ˆ˜Zn are two independent semantic sequences, if Zn and ˆZn are two associated
syntactic sequences with the same distributions as Xn and ˆXn, i.e., Zn ∼p(xn) and ˆZn ∼
p(ˆxn), for n sufficiently large, we have
$$(1-\epsilon)2^{-n(I_{s}(\hat{X};\hat{\hat{X}})+3\epsilon)}\leq Pr\left((\hat{Z}^{n},\hat{\hat{Z}}^{n})\in\hat{A}^{(n)}_{\epsilon}\right)\tag{168}$$ $$\leq2^{-n(I_{s}(\hat{X};\hat{\hat{X}})-3\epsilon)}.$$
Proof: Property (1) and (2) are restatements of semantically jointly typical set.

Assume ˜Zn and ˆ˜Zn are two independent semantic sequences, the associated syntactic sequences Zn and ˆZn are independent but have the same distributions as Xn and ˆXn. Thus we can establish two one-to-one mappings ˜zn ↔zn ↔xn and ˆ˜zn ↔ˆzn ↔ˆxn, then,

$$\Pr\left((\hat{Z}^{n},\hat{\hat{Z}}^{n})\in\bar{A}^{(n)}_{\epsilon}\right)$$ $$=\Pr\left(Z^{n}\in B^{(n)}_{\epsilon}(X^{n}),\hat{Z}^{n}\in B^{(n)}_{\epsilon}(\hat{X}^{n}),(Z^{n},\hat{Z}^{n})\in A^{(n)}_{\epsilon}\right)$$ $$=\sum_{\langle\hat{x}^{n},\hat{x}^{n}\rangle\leftrightarrow(\hat{x}^{n},\hat{x}^{n})\in A^{(n)}_{\epsilon}}\tag{169}$$ $$\leq2^{n\big{(}H(X,\hat{X})+\epsilon\big{)}}2^{-n\big{(}H_{\epsilon}(\hat{X})-\epsilon\big{)}}2^{-n\big{(}H_{\epsilon}(\hat{X})-\epsilon\big{)}}$$ $$=2^{-n\big{(}L_{\epsilon}(\hat{X};\hat{Y})-3\epsilon\big{)}}.$$
Using a similar method, we can also derive that

$$\Pr\left((\tilde{Z}^{n},\hat{\tilde{Z}}^{n})\in\tilde{A}^{(n)}_{\epsilon}\right)$$ $$=\sum_{\begin{subarray}{c}A^{(n)}_{\epsilon}\end{subarray}}p(\tilde{x}^{n})p(\hat{\tilde{x}}^{n})\tag{170}$$ $$\geq(1-\epsilon)2^{n(H(X;\tilde{X})-\epsilon)}2^{-n\left(H_{s}(\tilde{X})+\epsilon\right)}2^{-n\left(H_{s}(\tilde{X})+\epsilon\right)}$$ $$=(1-\epsilon)2^{-n\left(I_{s}(\tilde{X};\tilde{X})+3\epsilon\right)}.$$
Thus we complete the proof of the theorem.

□
Remark 11. It should be noted that the probability of Eq. (168) in Theorem 23 is different from that of Eq. (111) in Theorem 19. For the former, it indicates the probability of selecting two synonymous typical sequences (equivalently, representing two semantically typical sequences) constituting a syntactically jointly typical pair which is used to evaluate the error probability of jointly typical encoding. On the other hand, for the latter, it represents the probability of selecting two syntactically typical sequences consisting a jointly synonymous typical pair (equivalently, a semantically jointly typical pair) so that it reveals the error probability of jointly typical decoding.

## B. Semantic Rate Distortion Coding Theorem

We now investigate the problem of semantic lossy source coding. As depicted in Fig. 12, with the help of synonymous mapping f n x , a semantic index is is mapped into the syntactic source sequence Xn(i). Considering the distortion requirement, the encoder selects a suitable codeword
ˆXn(j) to represent the source sequence Xn(i) then send the index i to the receiver. Then in the side of receiver, the decoder outputs the reproduction sequence ˆXn(j). After de-mapping gn, we obtain an estimation of semantic index ˆis.

$i_{s}$$\left|\begin{array}{c}\includegraphics[height=36.135pt]{./figures/.eps}\end{array}\right|$$X^{n}\left(i\right)$$i\in\left\{1,\cdots,2^{n\left(n_{i}+R_{i}\right)}\right\}$$\left|\begin{array}{c}\includegraphics[height=36.135pt]{./figures/.eps}\end{array}\right|$$\left|\begin{array}{c}\includegraphics[height=36.135pt]{./figures/.eps}\end{array}\right|$$\left|\begin{array}{c}\includegraphics[height=36.135pt]{./figures/.eps}\end{array}\right|$$\left|\begin{array}{c}\includegraphics[height=36.135pt]{./figures/.eps}\end{array}\right|$\(\left|\begin{array}{c}\includegraphics[height=36.

For a lossy source coding system, let p( ˆX|X) be the conditional probability and X and
ˆ
X denote the source and reproduction syntactical alphabet respectively. Then the conditional probabilities for the n-th extension can be written as

$$p(\hat{x}^{n}\,|x^{n}\,)=\prod_{k=1}^{n}p(\hat{x}_{k}\,|x_{k}\,).\tag{171}$$
Definition 25. An (M, n) code for the semantic lossy source coding consists of the following parts:

(1) Two semantic index sets Is = {1, · · · , Ms} and Ir = {1, · · · , Mr}. Two syntactic index sets
I = {1, · · · , M} and I′ = {1, · · · , M ′}.
(2) A synonymous mapping f n
x :
˜
X n →X n generates the set of source sequences, namely,
semanticbook, S = {Xn(1), Xn(2), · · · , Xn(M ′)}. This semanticbook can be partitioned
into synonymous sequences subsets Sr.
(3) An encoding function ϕ : X n →ˆ
X n generates the set of codewords, namely, codebook,
C = { ˆXn(1), ˆXn(2), · · · , ˆXn(M)}. Due to the synonymous mapping f n
ˆx , this codebook can
be partitioned into synonymous codeword subsets Cs.
(4) An decoding function ψ : ˆ
X n →ˆ
X n outputs the decision syntactic codeword ˆXn.
(5) After de-mapping, gn
ˆx( ˆXn) = ˆis, the estimated semantic index is obtained. Note that both ψ
and gn
ˆx are deterministic.
According to the synonymous mappings f n x and f n
ˆx , Sr is an equivalence class consisting of the synonymous sequences and Cs is an equivalence class consisting of the synonymous codewords.

So we can construct the quotient sets S/f n
                                         x = {Sr} with |S/f n
                                                             x | = Mr and C/f n
                                                                               ˆx = {Cs} with

|C/f n
ˆx | = Ms. Let R = 1
n log2 Ms denote the semantic rate of source coding and R0 = 1
n log2 Mr denote the semantic rate of source sequences. We configure each synonymous set with the same number of sequences or codewords, that is, |Sr| = M′
Mr = 2nRr and |Cs| = M
Ms = 2nRs.

We now give the formal description of semantic rate-distortion coding theorem.

## Theorem 24. (Semantic Rate-Distortion Coding Theorem):

Given an i.i.d. syntactic source X ∼p(x) with the associated semantic source ˜X under the synonymous mapping f and the bounded semantic distortion function ds(˜x, ˆ˜x), for each code

rate R > Rs(D), there exists a sequence of
                                              
                                               2n(R+Rs), n
                                                          
                                                             codes, when code length n tends to

sufficiently large, the semantic distortion satisfies Eds( ˜X, ˆ˜X) < D.

On the contrary, if R < Rs(D), then for any
                                              
                                               2n(R+Rs), n
                                                           
                                                            code, the semantic distortion meets

Eds( ˜X, ˆ˜X) > D with sufficiently large n.

Proof: We first prove the achievability part of the theorem and the converse will be left in

the next part.

Given the desired distortion D and the conditional probability distribution p(ˆx|x), set the

rate-distortion function as Rs( D

x p(x)p(ˆx|x). A
                 
                  2n(R0+Rr), n
                              
                                semantic-

1+ϵ) and let p(ˆx) = P

book S =
         
          Xn(1), Xn(2), · · · , Xn(2n(R0+Rr))
                                      	
                                        can be generated randomly according to the

distribution

$$p(x^{n})=\prod_{k=1}^{n}p(x_{k}).\tag{172}$$
This set consists of the syntactic sequences to represent the semantic source sequence. Furthermore, these sequences can be uniformly divided into 2nR0 groups according to the synonymous

mapping f n
        x (˜xn) = Qn
                  k=1 X˜xk. Let Sr(ir) ⊂B(n)
                                      ϵ (Xn) denote the ir-th synonymous sequence

set.

Similarly, a
            
              2n(R+Rs), n
                         
                           code C =
                                      n
                                       ˆXn(1), · · · , ˆXn(2n(R+Rs))
                                                                  o
                                                                     can be generated randomly

according to the distribution

$$p(\hat{x}^{n})=\prod_{k=1}^{n}p(\hat{x}_{k})=\prod_{k=1}^{n}\sum_{x_{k}}p(x_{k})p(\hat{x}_{k}|x_{k}).\tag{173}$$
Correspondingly, these codewords can be uniformly divided into 2nR groups according to the

synonymous mapping f n
                   ˆx (ˆ˜xn) = Qn
                             k=1 ˆ
                                Xˆ˜xk. Let Cs(is) ⊂B(n)
                                                ϵ ( ˆXn) denote the is-th synonymous

codeword set. Thus the semanticbook S and the codebook C are produced and shared in the encoder and the decoder.

Similar to the idea in [5], we also use jointly typical encoding for the semantic lossy source code. Given a semantic index wr, base on the synonymous mapping f n x , we determine a syntactic source sequence xn. Furthermore, find an index ws such that (˜xn, ˆ˜Xn(ws)) ∈˜A(n)
ϵ . Equivalently, we can select one codeword ˆXn(ws, l) in a synonymous set Cs(ws) to present the semantic reconstruction sequence ˆ˜Xn(ws) so as to satisfy (xn, ˆXn(ws, l)) ∈A(n)
ϵ . If there is more than one semantic index, choose the smallest one. If no such semantic index exists, let ws = 1. After sending (ws, l) to the decoder, the decoder produces the reconstruction sequence ˆxn = ˆXn(ws, l).

Then under the de-synonymous mapping gn
ˆx, we obtain the estimated semantic index ˆwr.

We now analyze the expected distortion by using semantic coding. Let Ws denote the semantic index chosen by the encoder. We can bound the semantic distortion averaged over the random choice of the semanticbook S and the codebook C. The encoding error events can be expressed as

$${\cal E}=\left\{\left(X^{n},\hat{X}^{n}(W_{s},l)\right)\notin A^{(n)}_{\epsilon},\hat{X}^{n}(W_{s},l)\in{\cal C}_{s}(W_{s})\right\}.\tag{174}$$
This error event can be divided into two types of events, that is, E = E1
S E2, where

$$\mathcal{E}_{1}=\left\{X^{n}\notin A_{i}^{(n)}(X^{n})\right\}\tag{175}$$

and

$$\mathcal{E}_{2}=\left\{X^{n}\in\mathcal{S}_{r}(w_{r}),\hat{X}^{n}(w_{s},l)\in\mathcal{C}_{s}(w_{s}),\right.$$ $$\left.\left(X^{n},\hat{X}^{n}(w_{s},l)\right)\notin A_{i}^{(n)}(X^{n},\hat{X}^{n}),\right.\tag{176}$$ $$\left.\text{for all}w_{s}\in\left\{1,2,\cdots,2^{nR}\right\},l\in\left\{1,2,\cdots,2^{nR_{s}}\right\}\right\}.$$
So the error probability can be upper bounded by P (n)
e
= Pr(E) ≤P(E1) + P(E2). For the first term, by the weak law of large numbers, the probability tends to zero as n →∞. For the second term, recall that Sr(wr) ⊂B(n)
ϵ (Xn) and Cs(ws) ⊂B(n)
ϵ ( ˆXn), we derive the probability shown as follows

p(xn) P(E2) = X xn∈A(n) ϵ · P  Xn ∈Sr(wr), ˆXn(ws, l) ∈Cs(ws), (Xn, ˆXn(ws, l)) /∈A(n) ϵ , ∀ws ∈Is|Xn = xn 2nR Y = X p(xn) ws=1 P  Xn ∈Sr(wr), ˆXn(ws, l) ∈Cs(ws), (Xn, ˆXn(ws, l)) /∈A(n) ϵ  xn∈A(n) ϵ = X p(xn) h P  Xn ∈Sr(wr), ˆXn(1, l) ∈Cs(1), (Xn, ˆXn(1, l)) /∈A(n) ϵ i2nR . xn∈A(n) ϵ (177)

Since xn ∈A(n)
            ϵ
               and ˆXn(1, l) ∼Qn
                               k=1 p(ˆxk), by Theorem 23, for sufficiently large n, we have

P
 
  Xn ∈Sr(wr), ˆXn(1, l) ∈Cs(1), (Xn, ˆXn(1, l)) ∈A(n)
                                   ϵ
                                     

$$\geq(1-\epsilon)2^{-n(I_{s}(\hat{X};\hat{X})+3\epsilon)}.\tag{178}$$
Furthermore, since (1 −x)m ≤e−mx for x ∈[0, 1] and m ≥0, we have

$$P({\cal E}_{2})\leq\left(1-(1-\epsilon)2^{-n(I_{s}(\tilde{X};\hat{\tilde{X}})+3\epsilon)}\right)^{2^{nR}}\tag{179}$$ $$\leq\exp\left(-2^{nR}(1-\epsilon)2^{-n(I_{s}(\tilde{X};\hat{\tilde{X}})+3\epsilon)}\right)$$ $$=\exp\left(-(1-\epsilon)2^{n(R-I_{s}(\tilde{X};\hat{\tilde{X}})-3\epsilon)}\right)\,,$$
which tends to zero as n →∞and ϵ →0 if *R > I*s( ˜X; ˆ˜X). Consequently, the error probability P (n)
e
→0.

Hence, we can drive the expectation of semantic distortion as follows

$$\mathbb{E}\left[d_{s}(\hat{X}^{n},\hat{\hat{X}}^{n}(W_{s}))\right]=P(\mathcal{E})\mathbb{E}\left[d_{s}(\hat{X}^{n},\hat{\hat{X}}^{n}(W_{s}))|\mathcal{E}\right]\tag{180}$$ $$+P(\mathcal{E}^{c})\mathbb{E}\left[d_{s}(\hat{X}^{n},\hat{\hat{X}}^{n}(W_{s}))|\mathcal{E}^{c}\right]$$ $$\leq P_{e}^{(n)}d_{\max}$$ $$+(1-P_{e}^{(n)})(1+\epsilon)\mathbb{E}(d_{s}(\hat{X},\hat{\hat{X}})),$$
where dmax is the maximum semantic distortion.

By the assumption that E(ds( ˜X, ˆ˜X)) ≤
D
1+ϵ, we have E
h ds( ˜Xn, ˆ˜Xn(Ws))
i
≤D for sufficiently large n if *R > I*s( ˜X; ˆ˜X) + 3ϵ.

Furthermore, by Theorem 14, we have 1 ≤2nRr ≤2n(H(X)−Hs( ˜
                                                         X)) and 1 ≤2nRs ≤2n(H( ˆ
                                                                                 X)−Hs( ˆ˜
                                                                                       X)).

Hence, it follows that Is( ˜X; ˆ˜X) ≤R′ = R + Rr + Rs ≤Is( ˜X; ˆ˜X) + H(X) −Hs( ˜X) + H( ˆX) −
Hs( ˆ˜X) = I(X; ˆX). Thus we can conclude that if Rr = Rs = 0 with sufficiently large n, the compression rate R′ →Rs(D). On the other hand, if Rr and Rs gradually increase, then the compression rate R′ →R(D). This complete the proof of achievability.

□
Next, we prove the converse of the theorem. In order to prove the converse, we first illustrate the relationship between the sequential syntactic mutual information and the sequential semantic mutual information.

Lemma 10. Assume Xn is the syntactic sequence of discrete memoryless source and the reconstruction semantic sequence is ˆ˜Xn, we have

$I(X^{n};\hat{X}^{n})\geq I_{s}(\hat{X}^{n};\hat{X}^{n}),$ for all $p(x^{n}).$_ (181)
Proof: Due to the definitions of discrete memoryless source, we can write the sequential mutual information as

$$I(X^{n};\hat{X}^{n})-I_{s}(\hat{X}^{n};\hat{X}^{n})$$ $$=H(X^{n})+H(\hat{X}^{n})-H(X^{n},\hat{X}^{n})$$ $$-\left[H_{s}(\hat{X}^{n})+H_{s}(\hat{X}^{n})-H(X^{n};\hat{X}^{n})\right]\tag{182}$$ $$=\sum_{k=1}^{n}\left[H(X_{k})-H_{s}(\hat{X}_{k})+H(X_{k},\hat{X}_{k})-H(X_{k},\hat{X}_{k})\right]\geq0.$$
Due to H( ˆ˜Xn) = Hs( ˆ˜Xn), H(Xk) ≥Hs( ˜Xk), and H(Xk, ˆXk) ≥H(Xk, ˆ˜Xk) (Theorem 2), we prove the lemma.

□
Proof: (Converse to Theorem 24, (Semantic Rate Distortion Coding Theorem)):

In order to prove limn→∞E
                            h
                             ds( ˜Xn, ˆ˜Xn)
                                        i
                                          ≤D for any sequence of (2n(R+Rs), n) codes, we

must have R ≥Rs(D). So we consider the following inequality,

$nR\geq H(W_{s})\geq I(W_{s};X^{n})$

$$\geq I(\hat{\hat{X}}^{n}(W_{s});X^{n})$$ $$\stackrel{{(a)}}{{\geq}}I_{s}(\hat{X}^{n};\hat{\hat{X}}^{n})$$ $$\stackrel{{(b)}}{{=}}\sum_{k=1}^{n}I_{s}(\tilde{X}_{k};\hat{\hat{X}}_{k})\tag{183}$$ $$\stackrel{{(c)}}{{\geq}}\sum_{k=1}^{n}R_{s}\left(\mathbb{E}\left[d_{s}(\tilde{X}_{k};\hat{\hat{X}}_{k})\right]\right)$$ $$\stackrel{{(d)}}{{\geq}}nR_{s}\left(\mathbb{E}\left[d_{s}(\tilde{X}^{n};\hat{\hat{X}}^{n})\right]\right),$$
where (a) follows by Lemma 10, (b) follows from the property of discrete memoryless source,
(c) follows by the definition of Rs(D), and (d) follows by the convexity of Rs(D). So we conclude that Rs(D) ≤R for sufficiently large n and complete the proof of the converse.

□
Remark 12. For the conventional lossy source coding, we can use quantization, linear prediction, and transform coding to approach the rate-distortion function. By now, many efficient methods based on deep learning are applied in lossy source compression, such as convolutional neural network, transformer based network and so on. Heuristically, these new coding methods sufficiently utilize the semantic information of source and demonstrate better performance than the conventional ones. However, there is not a mature theoretic framework to design and optimize these deep learning based source coding methods. Semantic rate distortion may reveal some insights for future lossy source coding. By integrating the synonymous set into the traditional lossy source coding or neural network model, we believe that the semantic lossy source coding will provide a new solution for source compression in speech, image, and video.

## Ix. Semantic Information Measure Of Continuous Message

In this section, we extend the semantic information measures, such as semantic entropy, semantic mutual information to the continuous message. First we give the definitions of semantic entropy and mutual information in the continuous case. Then we investigate the capacity of Gaussian channel in the semantic sense and obtain the semantic channel capacity formula of band-limited Gaussian channel. Finally, we derive the semantic rate-distortion function for the Gaussian source.

## A. Semantic Entropy And Semantic Mutual Information For Continuous Message

In order to indicate the semantic entropy in the continuous case, we first define the synonymous mapping for the continuous variable as following.

Definition 26. Given a continuous random variable U with a probability density distribution p(u), u ∈Ω, the associated discrete semantic variable is ˜U, the synonymous mapping between
˜U and U is defined as

$f:\dot{\cal U}\to\Omega$, (184)

where ˜U = {˜uis}
               ˜
               N
               is=1 is the semantic alphabet and Ω= S ˜
                                                    N
                                                    is=1 Ωis is the support set of the

random variable with ∀is ̸= js, Ωis
                                T Ωjs = ∅. Specifically, for any ˜uis ∈˜U, it can be mapped

into a subset Ωis ⊂Ω. Hence under the mapping f, the support set is partitioned into a series of

synonymous intervals and |Ωis| = Lis is named as the synonymous length of the is-th interval.

Definition 27. Given a continuous random variable U with a probability density function p(u),

u ∈Ω, under a synonymous mapping f, for the associated semantic variable ˜U, the semantic

entropy is defined as

$$H_{s}(\tilde{U})=-\int\limits_{\Omega}p(u)\log p(u)du-\mathbb{E}(\log L),\tag{185}$$
Ωis p(u)du log Lis is the expectation of logarithm of synonymous where E(log L) = P∞
is=−∞
R
interval length.

We now illustrate the relationship between the continuous semantic entropy and the discrete counterpart. Let L[−∞:is] = Pis js=−∞Ljs. Suppose the range of U is divided into synonymous in-

tervals Ωis =
              
               L[−∞:is−1], L[−∞:is]
                                   
                                     of length Lis under the synonymous mapping f. Furthermore,

each interval is divided into bins of length ∆so that Lis = Jis∆. Assume the density function

is continuous in bins, by the integration mean value theorem, there is a value uJ[−∞:is−1]+j such

that

$$p(u_{J_{[-\infty;i_{\delta}-1]}+j})\Delta=\int_{L_{[-\infty;i_{\delta}]}+j\Delta}^{L_{[-\infty;i_{\delta}]}+(j+1)\Delta}p(u)du.\tag{186}$$
Considering the synonymous mapping f, we can write

$$\sum_{j=0}^{J_{i_{s}}-1}p(u_{J_{[-\infty:i_{s}-1]}+j})\Delta=p(\hat{u}_{i_{s}})J_{i_{s}}\Delta.\tag{187}$$
So the semantic entropy of the quantized version is

$$H_{s}(\tilde{U}^{\Delta})=-\sum_{i_{s}=-\infty}^{\infty}p(\tilde{u}_{i_{s}})J_{i_{s}}\Delta\log\left[p(\tilde{u}_{i_{s}})J_{i_{s}}\Delta\right]\tag{188}$$ $$=-\sum_{i_{s}=-\infty}^{\infty}\sum_{j=0}^{J_{i_{s}-1}}p(u_{J_{[-\infty,i_{s}-1]+j}})\Delta\log\left[\sum_{j=0}^{J_{i_{s}-1}}p(u_{J_{[-\infty,i_{s}-1]+j}})\Delta\right]$$ $$\Rightarrow\sum_{i_{s}=-\infty}^{\infty}\int_{L_{[-\infty,i_{s}-1]}}^{L_{[-\infty,i_{s}]}}p(u)\log\left[\int_{L_{[-\infty,i_{s}-1]}}^{L_{[-\infty,i_{s}]}}p(v)dv\right]\,du,$$
where the last equality is from the Riemann integrability.

By using the integration mean value theorem, there is a value ζ in the interval
                                                                             
                                                                              L[−∞:is−1], L[−∞:is]
                                                                                                 

such that R L[−∞:is]
L[−∞:is−1] p(v)dv = Lisp(ζ). So the quantized semantic entropy can be further approximated as

$$-\sum_{i_{s}=-\infty}^{\infty}\int_{L_{1-\infty,i_{s}-1}}^{L_{1-\infty,i_{s}}}p(u)\log\left[\begin{array}{c}\int_{L_{1-\infty,i_{s}-1}}^{L_{1-\infty,i_{s}}}p(v)dv\end{array}\right]du$$ $$=-\sum_{i_{s}=-\infty}^{\infty}\int_{L_{1-\infty,i_{s}-1}}^{L_{1-\infty,i_{s}}}p(u)\log\left[L_{i_{s}}p(\zeta)\right]du$$ $$\approx-\sum_{i_{s}=-\infty}^{\infty}\int_{\Omega_{s}}p(u)\log\left[L_{i_{s}}p(u)\right]du\tag{189}$$ $$=-\int_{-\infty}^{\infty}p(u)\log p(u)du-\sum_{i_{s}=-\infty}^{\infty}\int_{\Omega_{s}}p(u)du\,\log L_{i_{s}}=H_{s}(\tilde{U}).$$
Although Eq. (185) is an approximation form, it has a concise expression and clear physical meaning. So we use this formula to present the semantic entropy in the continuous case.

Corollary 5. Given a continuous random variable U *with a probability density function* p(u)
and the associated semantic variable ˜U under a synonymous mapping f, the semantic entropy is lower bounded by

$$H_{s}(\tilde{U})\geq-\int\limits_{\Omega}p(u)\log p(u)du-\log S,\tag{190}$$
where S = E(L) is the average length of synonymous interval. The equality holds when the optimal mapping f is a proportional partition based on the probability of synonymous interval.

Proof: The semantic entropy can be written as

$$H_{s}(\tilde{U})=-\int\limits_{\Omega}p(u)\log p(u)du-\sum\limits_{i_{s}=1}^{\tilde{N}}\int\limits_{\Omega_{i_{s}}}p(u)du\log\frac{L_{i_{s}}}{|\Omega|}-\log|\Omega|.\tag{191}$$
Let hR
|Ω| . Since D(ps||qs) ≥0, we have
Ωis p(u)du i
= ps,is and qs,is = Lis

$$-\sum_{i_{s}=1}^{\tilde{N}}\big{[}\begin{array}{cc}t&p(u)du\\ \Omega_{s_{s}}&\end{array}\big{]}\log\frac{L_{i_{s}}}{|\Omega|}\geq-\sum_{i_{s}=1}^{\tilde{N}}\big{[}\begin{array}{cc}t&p(u)du\\ \Omega_{s_{s}}&\end{array}\big{]}\log\big{[}\begin{array}{cc}t&p(u)du\\ \Omega_{s_{s}}&\end{array}\big{]}.\tag{192}$$

The equality holds when the following condition is satisfied

$$\int\limits_{\Omega_{s}}p(u)du=\frac{L_{i_{s}}^{*}}{|\Omega|},i_{s}=1,2,\cdots,\tilde{N}.\tag{193}$$
This condition means that the synonymous length L∗
is of interval Ωis is proportional to the probability of synonymous interval R
Ωis p(u)du for the optimal mapping f.

Furthermore, by using Jensen's inequality, we can derive that

$$H_{s}(\tilde{U})\geq-\ \int_{\Omega}\,p(u)\log p(u)du-\sum_{i_{s}=1}^{\tilde{N}}\left[\begin{array}{cc}&\\ &\\ \end{array}\right]_{\Omega_{i_{s}}}p(u)du\ \log L_{i_{s}}^{*}$$ $$\geq-\ \int_{\Omega}\,p(u)\log p(u)du-\log\sum_{i_{s}=1}^{\tilde{N}}\left[\begin{array}{cc}&\\ &\\ \end{array}\right]_{\Omega_{i_{s}}}p(u)du\ \right]L_{i_{s}}^{*}\tag{194}$$ $$=-\ \int_{\Omega}\,p(u)\log p(u)du-\log S,$$
where S is the average length of synonymous interval, which is defined as

$$S=\mathbb{E}(L)=\sum_{i_{s}=1}^{\tilde{N}}\left[\begin{array}{cc}&\\ &\\ \Omega_{i_{s}}&\end{array}p(u)du\right]_{i_{s}}L_{i_{s}}^{*}.\tag{195}$$
So we complete the proof.

□
In fact, under the synonymous mapping, if S →0, Hs( ˜U) = −
R ∞
−∞p(u) log p(u)du−log S →
∞, the first term is the differential entropy of U, that is, h(U) = −
R ∞
−∞p(u) log p(u)du.

Specifically, if S = 1, then Hs( ˜U) = h(U), that is, the semantic entropy is equal to the differential entropy. On the other hand, if S →∞, then Hs( ˜U) *→−∞*. That means if the synonymous length goes sufficiently large we can obtain no extra semantic information.

Remark 13. The average synonymous length S indicates the identification ability of information.

If S = 1, the semantic variable ˜U obtains the same identification result as the random variable U. On the other hand, if S > 1, the former loses some identification ability and attains a smaller semantic entropy than the latter.

We now give the definition of joint/conditional synonymous mapping as following.

Definition 28. Given a continuous random variable pair (U, V ) with a probability density distribution p(u, v), u ∈Ωu, v ∈Ωv, the associated discrete semantic variable pair is ( ˜U, ˜V ), the joint synonymous mapping is defined as

$$f_{uv}:\dot{\cal U}\times\dot{\cal V}\to\Omega_{u}\times\Omega_{v},\tag{196}$$

where ( ˜U, ˜V) = {(˜uis, ˜vjs)}
                        ˜
                       Nu, ˜
                         Nv
                       is,js=1 is the semantic alphabet and ∀is, js,
                                                            Ω(is,js)
                                                                   = LisLjs. Here

Lis and Ljs are the synonymous lengths. Similarly, the conditional synonymous mapping is

defined as

$f_{v|u}:\dot{\mathcal{V}}|U\to\Omega_{v}|U$, (197)
where for all js, |Ωjs|U| = Ljs.

Thus we give the definitions of semantic conditional entropy and semantic joint entropy as following.

Definition 29. Given a pair of semantic variables ( ˜U, ˜V ) and the associated continuous random variable pairs (U, V ) with a joint density function p(u, v), under a joint mapping fuv, the semantic joint entropy Hs( ˜U, ˜V ) is defined as

$$H_{s}(\bar{U},\bar{V})=-\ \big{|}_{\Omega_{v}}\ \big{|}_{\Omega_{u}}\ p(u,v)\log p(u,v)dudv-\mathbb{E}\left[\log(L_{u}L_{v})\right],\tag{198}$$
where Lu and Lv are the synonymous lengths of random variables U and V respectively.

Correspondingly, under an conditional mapping fv|u*, the semantic conditional entropy* Hs(˜V |U)
is defined as

$$H_{s}(\tilde{V}|U)=-\int\limits_{\Omega_{v}}\int\limits_{\Omega_{u}}p(u,v)\log p(v|u)dudv-\mathbb{E}(\log L_{v}),\tag{199}$$
where Lv is the synonymous length of random variable V .

Example 6. Consider a random variable U *with the uniform distribution* p(u) =
1
b−a, u ∈[a, b].

Assume the synonymous mapping f evenly partitions the interval [a, b] into ˜N parts, so the synonymous length is S = b−a
˜
N . Hence the semantic entropy of the associated variable ˜U is

$$H_{s}(\tilde{U})=-\int_{a}^{b}\frac{1}{b-a}\log\frac{1}{b-a}du-\log\frac{b-a}{\tilde{N}}=\log\tilde{N}\ \mbox{sebits}.\tag{200}$$
Example 7. Let U *denote the Gaussian random variable with the density function* p(u) =
1
√
2σ2 . Under a proportional-partition mapping f with the synonymous length S, the
2πσ2e−u2
semantic entropy is written as

$$H_{s}(\tilde{U})=\mathbb{E}[-\log p(U)]-\log S\tag{201}$$ $$=\mathbb{E}\left(\log\sqrt{2\pi\sigma^{2}}+(\log e)\,\frac{U^{2}}{2\sigma^{2}}\,\right)-\log S$$ $$=\frac{1}{2}\log2\pi\sigma^{2}+(\log e)\,\frac{\mathbb{E}U^{2}}{2\sigma^{2}}-\log S$$ $$=\frac{1}{2}\log\frac{2\pi e\sigma^{2}}{S^{2}}\text{{sebits}}.$$
We now give the definition of up/down semantic mutual information as follows.

Definition 30. Given a pair of semantic variables ( ˜U, ˜V ) and the associated continuous random variable pair (U, V ) with a joint density function p(u, v), under a joint mapping fuv, the up semantic mutual information Is( ˜U; ˜V ) is defined as

$$I^{s}(\hat{U};\hat{V})=H(U)+H(V)-H_{s}(\hat{U},\hat{V})\tag{202}$$ $$=-\int\limits_{\Omega_{u}}\int\limits_{\Omega_{u}}p(u,v)\log\frac{p(u)p(v)}{p(u,v)}dudv+\mathbb{E}\left[\log(L_{u}L_{v})\right],$$
where Lu and Lv is the synonymous length of random variable U and V . Similarly, the down semantic mutual information Is( ˜U; ˜V ) is defined as

$$I_{s}(\hat{U};\tilde{V})=H_{s}(\tilde{U})+H_{s}(\tilde{V})-H(U,V)\tag{203}$$ $$=-\int\limits_{\Omega_{v}}\int\limits_{\Omega_{u}}p(u,v)\log\frac{p(u)p(v)}{p(u,v)}dudv-\mathbb{E}\left[\log(L_{u}L_{v})\right].$$
Clearly, in (202) and (203), the first term is the classic mutual information. The main difference between semantic and syntactic mutual information is the logarithmic production of synonymous lengths. If E [log(LuLv)] ≥0, we have

$$I_{s}(\tilde{U};\tilde{V})\leq I(U;V)\leq I^{s}(\tilde{U};\tilde{V}).\tag{204}$$
Similar to the discrete case, the down semantic mutual information may be negative. Consider the practical condition, we can set (Is( ˜U; ˜V ))+.

Analog to the typical set for the discrete case, asymptotic equipartition property also holds for the continuous case and we can introduce the typical set for this case. Conceptually, the volume of continuous typical set for the semantic variable can be approximated as 2nHs( ˜U) = 2nH(U)
Sn
. So the synonymous length S can be interpreted as the reduced proportion in each side length. Similar interpretation can be applied for the semantic conditional/joint entropy and mutual information.

Due to the page length limitation, we will not discuss the details of the continuous typical set in the semantic sense.

Remark 14. In the algorithm of signal detection and estimation, we often make a decision based on observations in an interval. This process can be modeled as a synonymous mapping. Thus, we can handle some problems in the radar signal detection, hypothesis testing, integrated sensing and communication from the viewpoint of semantic information processing. Therefore, semantic information theory may provide a theoretical explanation and establish the fundamental limits for these signal processing problems.

## B. Semantic Capacity Of Gaussian Channel

Consider a Gaussian channel model, the received signal yk at time k can be written as

$y_{k}=x_{k}+z_{k}$, (205)
where zk ∼N(0, σ2) is the noise sample drawn i.i.d. from a Gaussian distribution with variance
σ2. Under a joint synonymous mapping fxy, the associated semantic variables are ˜Y , ˜X and ˜Z
respectively. Given the average power constraint EX2 ≤P, by using Jensen's inequality, the semantic capacity of the Gaussian channel can be derived as

$$C_{s}=\max_{f_{xy}}\max_{(p(x);\mathbb{E}X^{2}\leq P)}I^{s}(\tilde{X};\tilde{Y})$$ $$=\max_{p(x);f_{xy}}H(X)+H(Y)-H_{s}(\tilde{X},\tilde{Y})$$ $$=\max_{p(x)}H(X)+H(Y)-H(X,Y)+\max_{f_{xy}}\mathbb{E}\left[\log(L_{x}L_{y})\right]\tag{206}$$ $$=\frac{1}{2}\log\left(1+\frac{F}{\sigma^{2}}\right)+\log\left[\mathbb{E}(L_{x})\mathbb{E}(L_{y})\right]$$ $$=\frac{1}{2}\log\left(1+\frac{F}{\sigma^{2}}\right)+\log(S^{2}),$$
where we assume Sx = E(Lx) = Sy = E(Ly) = S and S ≥1.

Remark 15. Like the classic information theory, the semantic capacity of Gaussian channel is achieved when X ∼N(0, P) and the synonymous mapping fxy is a proportional-partition mapping. Specifically speaking, if the transmitted variable X (the received signal Y ) is partitioned based on an equiprobability mapping, the semantic capacity may be achieved.

Furthermore, we can also obtain a lower bound of the semantic capacity, that is

$$C_{s}=\frac{1}{2}\log\left\langle\,P+\sigma^{2}\,\right\rangle\geq\frac{1}{2}\log\left\langle\,P+\frac{\sigma^{2}}{S^{4}}\,\right\rangle\tag{207}$$ $$=\frac{1}{2}\log\left(1+S^{4}\frac{F}{\sigma^{2}}\,\right)=\underline{C}_{s}.$$
Theorem 25. Given a Gaussian channel with power constraint P, noise variance σ2 and average synonymous length S ≥1, the achievable semantic rate is

$$C_{s}=\frac{1}{2}\log\left(1+\frac{F}{\sigma^{2}}\right)+\log(S^{2})\mbox{sebits per transmission.}\tag{208}$$
Similarly, the lower bound of semantic capacity is

$$\underline{C}_{s}=\frac{1}{2}\log\left(1+S^{4}\frac{F}{\sigma^{2}}\right)\ \mbox{sebits per transmission}.\tag{209}$$
By using random coding, synonymous mapping, and joint typicality decoding in the continuous case, we can prove this theorem. The details is omitted due to the limitation of page length.

Remark 16. We now give a geometric interpretation for this theorem as shown in Fig. 13.

Given signal power P and noise variance σ2, and a codeword of length n, for the classic channel coding, the transmitted codeword is normally scattered in a sphere of radius
√
nP and the decoding region of each codeword is confined to a sphere of radius
√
nσ2 (labeled by red circle) with high probability. Since the energy of received vectors is no more than p n(P + σ2), n(P + σ2). Therefore, the volume of received vector sphere is An(n(P +σ2))
n
2 with the radius p in order to avoid the intersection of the decoding region, the maximum number of decoding

spheres in this volume is limited as_

$$\frac{A_{n}(n(P+\sigma^{2}))^{\frac{n}{2}}}{A_{n}(n\sigma^{2})^{\frac{n}{2}}}=2^{\frac{n}{2}\log(1+\frac{P}{\sigma^{2}})}.\tag{210}$$
In the syntactic sense, such a maximum number of codewords with no error probability decoding cannot be surpassed.

On the contrary, in the semantic sense, thanks to the synonymous mapping, the radius of a decoding sphere is further reduced to p nσ2/S4. Due to the radius decreasing, the semantic decoding sphere (labeled by black circle) has a smaller volume An(nσ2/S4)
n
2 . Hence, the maximum number of semantic decoding spheres in received vector volume is limited as

$$A_{n}(n(P+\sigma^{2}))^{\frac{n}{2}}\over A_{n}(n\sigma^{2}/S^{4})^{\frac{n}{2}}}=2^{\frac{n}{2}\log\left(S^{4}\left(1+\frac{P}{\sigma^{2}}\right)\right)}.\tag{211}$$
Furthermore, if we consider a conservative estimate for the received vector volume, we can limit the energy of the received vector as p n(P + σ2/S4). So the number of semantic decoding

spheres is bounded by

$$\frac{A_{n}(n(P+\sigma^{2}/S^{4}))^{\frac{n}{2}}}{A_{n}(n\sigma^{2}/S^{4})^{\frac{n}{2}}}=2^{\frac{n}{2}\log\left(1+S^{4}\frac{P}{\sigma^{2}}\right)}.\tag{212}$$
Compare with the classic coding in Gaussian channel, when the synonymous length is equal to one, the semantic capacity is the same as the classic capacity. However, with the growth of synonymous length, we find that semantic coding based on the synonymous mapping can further reduce the uncertainty range of decoding region so that the volume of decoding sphere can be decreased. Therefore, semantic channel coding can pack larger number of codewords in the same volume of received vector sphere than the traditional method. In this sense, the capacity of Gaussian channel can be further improved by using semantic coding.

## C. Semantic Capacity Of Band-Limited Gaussian Channel

We now investigate the semantic capacity of band-limited Gaussian channel. Given a Gaussian noise channel with limited bandwidth B and two-sided power spectrum N0/2, we transmit signals on this channel with a limited time interval [0, T] and limited power P. So Shannon's channel capacity formula is written as

$$C=B\log\left(1+\frac{P}{N_{0}B}\right)\ \mbox{bits per second.}\tag{213}$$
Theorem 26. If a signal S(t) in a time interval [0, T] with the power constraint P is transmitted on the Gaussian noise channel with limited bandwidth B, under a synonymous mapping f with the average synonymous length S ≥1, the semantic channel capacity can be written by

$$C_{s}=B\log\left[S^{4}\left(1+\frac{P}{N_{0}B}\right.,\right.\tag{214}$$ $$=B\log\left(1+\frac{P}{N_{0}B}\right)+4B\log S\text{sebits per second.}$$
Correspondingly, the lower bound of semantic capacity can be written by

$$\underline{C}_{s}=B\log\left(1+S^{4}\frac{F}{N_{0}B}\right)\ \ \mbox{sebits per second.}\tag{215}$$
Proof: By using the Shannon-Nyquist sampling theorem, the signal S(t) is decomposed into a series of i.i.d. samples. Hence the semantic capacity per sample is 1

2 log
     h
       S4 
            1 +
                  P
                 N0B
                     i
                        .

Since there are 2B samples each second, the semantic capacity of the channel can be rewritten

as Cs = B log
              h
               S4 
                   1 +
                         P
                        N0B
                           i
                              . A similar method can also be applied to derive the lower bound

of semantic capacity. So we complete the proof.
                                                                                          □

Figure 14 depicts the comparison of semantic and classic capacity of band-limited Gaussian

channel at various bit signal-to-noise ratios (Eb/N0). For the semantic cases, we draw the

semantic capacity and the lower bound for different synonymous lengths, such as S = 8, S = 4, and S = 2. We can see that the semantic capacity of Gaussian channel is significantly larger

than the classic capacity due to using the synonymous mapping and semantic coding. With the increasing of average synonymous length, the improvement of capacity will become more noticeable.

If the bandwidth tend to infinity, for the lower bound of semantic channel capacity, we obtain

the limitation as following,

$$\lim_{B\to\infty}\frac{C_{s}}{\ln2}\cdot\frac{P}{N_{0}}\approx1.44S^{4}\frac{P}{N_{0}}.\tag{216}$$
On the other hand, let η = Cs B be the spectrum efficiency. When η →0. we obtain the limitation of Eb/N0 as

$$\lim_{\eta\to0}\frac{E_{b}}{N_{0}}=\frac{\ln2}{S^{4}}\approx\frac{0.693}{S^{4}}.\tag{217}$$
Next, we explore the minimum energy per sebit. Let P = ER be the signal power and µ = R
B
be the spectrum efficiency. Since R ≤B log(1 + S4 ER
N0B), we can derive the minimum energy needed for semantic communication at spectrum efficiency µ, that is,

$$E(\mu)=\frac{1}{S^{4}\mu}(2^{\mu}-1).\tag{218}$$
Here we assume the Gaussian noise power N0 = 1. Figure 15 shows the minimum energy needed for semantic and classic communication under various spectrum efficiency with the average synonymous lengths S = 2 and S = 4. Compared with the classic communication, we can observe that the minimum energy of semantic communication is dramatically reduced. So it follows that semantic communication may be an important method to implement the green communication.

All these theoretic results show an extraordinary advantage of semantic coding over the classic coding. They reveal the tremendous potential of semantic channel coding in the communication

## Application. D. Semantic Rate Distortion Of Gaussian Source

As a dual problem, we now consider the semantic rate-distortion of Gaussian source and have the following theorem.

Theorem 27. Given a Gaussian source X ∼N(0, P) and the reconstruction signal ˆX, under the synonymous mapping fx and fˆx, the associated semantic variables are ˜X and ˆ˜X respectively, with the mean squared error (MSE) distortion E[( ˜X−ˆ˜X)2] ≤D, the signal model can be written

$$\hat{X}=\hat{X}+Z,\tag{219}$$
where Z is the noise sample drawn i.i.d. from a Gaussian distribution with variance D. So the

semantic rate-distortion of the Gaussian source is_

$$R_{*}(D)=\begin{cases}\frac{1}{2}\log\left(\frac{F}{S^{4}D}\right),&0\leq D\leq\frac{F}{S^{4}},\\ 0,&D>\frac{F}{S^{4}}.\end{cases}\tag{220}$$
where S is the average synonymous length.

Proof: For the Gaussian source X ∼N(0, P), by using Corollary 5, we can write the down semantic mutual information as

$$I_{s}(\hat{X};\hat{\hat{X}})=H_{s}(\hat{X})+H_{s}(\hat{\hat{X}})-H(X,\hat{X})$$ $$=h(X)-\mathbb{E}\log(L_{x})+h(\hat{X})-\mathbb{E}\log(L_{\hat{x}})-H(X,\hat{X})$$ $$\geq I(X;\hat{X})-\log\mathbb{E}(L_{x})-\log\mathbb{E}(L_{\hat{x}})$$ $$\geq\frac{1}{2}\log\left(\frac{F}{D}\right)-\log(S_{x}S_{y})\tag{221}$$ $$=\frac{1}{2}\log\left(\frac{F}{D}\right)-\log(S^{2})$$ $$=\frac{1}{2}\log\left(\frac{F}{S^{4}D}\right),$$
where we assume Sx = E(Lx) = Sˆx = E(Lˆx) = S and S ≥1. Like the classic information theory, this semantic rate-distortion function is achieved when Z ∼N(0, D) and fx (fˆx) is an equiprobability partition synonymous mapping. If 0 ≤D ≤P/S4, Rs(D) ≥0, otherwise Rs(D) = 0.

□
We now give a geometric interpretation for this theorem as shown in Fig. 16. For the classic lossy source code, we should use a group of encoding spheres of radius
√
nD to cover the source volume of radius
√
nP. So the minimum number of the source codewords required is 2nR(D) =
 P
D
n/2. On the contrary, due to the synonymous mappings for the source and reconstruction sequence, the equivalent volume of source space can be reduced to a sphere of radius p nP/S4 so that the minimum number of codewords is 2nRs(D) =

P/S4
D
n/2
.

Figure 17 shows the semantic and syntactic rate distortion function of a Gaussian source. Here, the source signal power is P = 1 and the synonymous lengths are set to S = 1.5 and S = 2
respectively. We observe that the semantic rate distortion functions dramatically decrease with the growth of synonymous length and become significantly lower than the classic counterparts.

These results manifest that semantic lossy source coding has enormous potential for the source compression in the future.

## X. Semantic Joint Source Channel Coding

In this section, we consider the semantic joint source channel coding. Similar to the classic information theory, we can tie together two basic methods of semantic communication: semantic source coding and semantic channel coding. Figure 18 gives the system model of semantic joint source channel coding. Let ˜U denote the discrete memoryless semantic source with entropy Hs( ˜U) and ds(˜u, ˆ˜u) be a semantic measure with rate-distortion function Rs(D). Furthermore, let X denote the coded symbol after synonymous mapping f and n
˜
X( ˜U), X, Y, ˜Y( ˆ˜U), p(Y |X)
o be a discrete memoryless channel with semantic capacity Cs.

## Theorem 28. (Semantic Source Channel Coding Theorem In Lossy Case):

Given a discrete memoryless source U ∼p(u) and a discrete memoryless channel p(y|x), the source is associated with a semantic variable ˜U under the synonymous mapping f and the codeword Xn is transmitted on the channel. In the case of lossy transmission, if the code rate satisfies Rs(D) < R < Cs, there exists a sequence of

2nR, n semantic source channel codes, when code length n tends to sufficiently large, then semantic distortion satisfies Eds( ˜X, ˆ˜X) < D.

On the contrary, if Rs(D) > Cs, then for any

2nR, n code, with sufficiently large n, the semantic distortion meets Eds( ˜X, ˆ˜X) > D.

Proof: We use separate semantic lossy source coding and semantic channel coding to prove the achievability.

(1) Source coding: Give any *ϵ >* 0, there exists a sequence of semantic lossy source codes with rate R ≥Rs(D) + ϵ that satisfies Eds(˜u, ˆ˜u) *< D*. The index of each code can be regarded as a semantic message to be sent to the channel.

(2) Channel coding: The source indices can be encoded and reliably transmitted over the channel if R ≤Cs −ϵ.

In the channel decoder, when n →∞, the error probability P (n)
e
→0 so that the source decoder can reconstruct sequence and de-mapping the semantic index ˆis and the average distortion meets Eds( ˜X, ˆ˜X) ≤D.

Next we prove the converse. By the converse proof of semantic rate distortion coding theorem (Theorem 24), we have Rs(D) ≤Is( ˜U n, ˆ˜U n). Furthermore, by Corollary 3, we have Is( ˜U n, ˆ˜U n) ≤Is( ˜U n, ˆ˜U n). Then we can derive Is( ˜U n, ˆ˜U n) ≤Cs. So we complete the proof of the theorem.

□

## Theorem 29. (Semantic Source Channel Coding Theorem In Lossless Case):

Given a discrete memoryless source U ∼p(u) and a discrete memoryless channel p(y|x), the source is associated with a semantic variable ˜U under the synonymous mapping f and the codeword Xn is transmitted on the channel. In the case of lossless transmission, for each code rate Hs( ˜U) < R < Cs, there exists a sequence of

2nR, n semantic source channel codes, when code length n tends to sufficiently large, the error probability tends to zero.

On the contrary, if Hs( ˜U) > Cs, then for any

2nR, n code, with sufficiently large n, the error probability cannot achieve arbitrarily low.

The proof of this theorem is similar to Theorem 28 by setting D = 0. In principle, the separate semantic source and channel coding is asymptotically optimal for lossless or lossy transmission.

Therefore, the fundamental criteria for the semantic communication are summarized as following

$$\begin{split}&\int H_{*}(\bar{U})\leq R\leq C_{*},\ \ \ \text{for lossless transmission,}\\ &\big{\{}R_{*}(D)\leq R\leq C_{*},\ \ \ \ \ \text{for lossy transmission.}\end{split}\tag{222}$$
On the contrary, for classic communication, the code rate should be confined in H(U) ≤R ≤C
or R(D) ≤R ≤C. In these common intervals, both classic communication and semantic communication can work well. On the other hand, if H(U) > C or R(D) *> C*, the classic communication cannot work yet the semantic communication still works well as long as Hs( ˜U) ≤
Cs or Rs(D) ≤Cs. Thus, we conclude that semantic source-channel coding can extend the range of code rates and provide new insight to improve the performance of the communication system.

## Xi. Conclusions

In this paper, we develop an information-theoretic framework of semantic communication.

We start from the synonym, a fundamental property of semantic information, to build the semantic information measures including semantic entropy, up/down semantic mutual information, semantic channel capacity, and semantic rate distortion function. Then we extend the asymptotic equipartition property to the semantic sense and introduce the synonymous typical set to prove three significant coding theorems, that is, semantic source coding theorem, semantic channel coding theorem, and semantic rate distortion coding theorem. Additionally, we investigate the semantic information measures in the continuous case and derive the semantic capacity of Gaussian channel and semantic rate distortion of Gaussian source. All these works uncover the critical features of semantic communication and constitute the theoretic basis of semantic information theory.

For the theoretic analysis, the semantic information theory needs further development. In this paper, we only consider the semantic information measure and the fundamental limitation in the discrete or continuous memoryless case. In the future, we can further investigate the measure and limitation of semantic information in various memory source or channel cases, such as stationary and ergodic process (e.g. Markov process) or non-stationary non-ergodic process. Strong asymptotic equipartition property and strong typicality in the semantic sense should be further explored. On the other hand, the analysis of semantic capacity or semantic rate distortion with finite block length may also be an interesting research topic. In addition, in various multiuser communication scenarios, such as multiple access, broadcasting, relay etc., we can further analyze and derive the corresponding measure and performance limit of semantic information.

Guided by the classic information theory, in the past seventy years, the source coding and channel coding techniques have approached the theoretic limitation. On the contrary, the semantic information theory paves a new way for the coding techniques. From the viewpoint of semantic processing, with the help of synonymous mapping, the lossless source coding has much space to improve and the existing coding methods can be further modified and polished. The construction of semantic channel codes may be centered on the group Hamming distance and the optimization of decoding algorithms will be concentrated on the group decoding so that the information transmission techniques will usher in a new era that surpasses the classic limitation and approaches the semantic capacity. By the optimization of synonymous mapping, the classic lossy source coding techniques, such as vector quantization, prediction coding, and transform coding, will demonstrate new advantages to further improve the compression efficiency. Briefly, the performance bottleneck of classic communication will be broken and the traditional communication will naturally evolve to the semantic communication.

For the new coding techniques based on deep learning (DL), the semantic information theory will lift its mystery veil and provide a systematic design and optimization tool. The synonymous mapping will provide a reasonable explanation for the semantic information extracted by the deep neural network. The basic structures of mainstream DL models, such as convolutional neural networks, transformer model, variational auto-encoder and so on, may be analyzed and optimized based on the semantic information measures. Furthermore, the system architecture of semantic communication based on deep learning can be simplified or optimized guided by the semantic information theory.

In summary, the theoretic framework proposed in this paper may help understanding the essential features of semantic information and shed light on some ambiguity problems in semantic communication. We believe that the semantic information theory will uncover a new chapter of information theory and have a profound impact on many fields such as communication, signal detection and estimation, deep learning and machine learning, and integrated sensing and communication etc.

## Appendix A. Proof Of Lemma 3

Proof: Suppose two probability distributions p1(u) and p2(u), for all 0 ≤θ ≤1, we have pθ(u) = θp1(u) + (1 −θ)p2(u). Using Jensen's inequality, we can write

$$\theta H_{s}(p_{1}(u))+(1-\theta)H_{s}(p_{2}(u))-H_{s}(p_{\theta}(u))$$ $$=\theta\sum_{i_{s}}\sum_{i\in\mathcal{N}_{i_{s}}}p_{1}(u_{i})\log\frac{\sum_{i\in\mathcal{N}_{i_{s}}}p_{\theta}(u_{i})}{\sum_{i\in\mathcal{N}_{i_{s}}}p_{1}(u_{i})}$$ $$+(1-\theta)\sum_{i_{s}}\sum_{i\in\mathcal{N}_{i_{s}}}p_{2}(u_{i})\log\frac{\sum_{i\in\mathcal{N}_{i_{s}}}p_{\theta}(u_{i})}{\sum_{i\in\mathcal{N}_{i_{s}}}p_{2}(u_{i})}\tag{223}$$ $$\leq\theta\log\sum_{i_{s}}\sum_{i\in\mathcal{N}_{i_{s}}}p_{\theta}(u_{i})+(1-\theta)\log\sum_{i_{s}}\sum_{i\in\mathcal{N}_{i_{s}}}p_{\theta}(u_{i})$$ $$=\theta\log1+(1-\theta)\log1=0.$$
So we prove the concavity of semantic entropy.

□

## B. Proof Of Theorem 5

Proof: To prove the first inequality, by using Jensen's inequality, we have

Ds(θps,1 + (1 −θ)ps,2∥θqs,1 + (1 −θ)qs,2) −θDs(ps,1∥qs,1) −(1 −θ)Ds(ps,2∥qs,2) X = θ X is ui∈Uis p1(ui) P P · log ui∈Uis θp1(ui) + (1 −θ)p2(ui) P ui∈Uis q1(ui) P (224) ui∈Uis θq1(ui) + (1 −θ)q2(ui) ui∈Uis p1(ui) X + (1 −θ) X is ui∈Uis p2(ui) P P · log ui∈Uis θp1(ui) + (1 −θ)p2(ui) P ui∈Uis q2(ui) P ui∈Uis θq1(ui) + (1 −θ)q2(ui) ui∈Uis p2(ui) X ≤log X is ui∈Uis θp1(ui) + (1 −θ)p2(ui) = log 1 = 0.
The other two inequalities can also be proved by using similar methods. So we prove the theorem.

□

## C. Proof Of Theorem 7

Proof: First, we prove Is( ˜U; ˜V ) is a concave function of p(u) for fixed p(v|u). Since Is( ˜U; ˜V ) = H(U)+H(V )−Hs( ˜U, ˜V ), the entropies of H(U) and H(V ) are concave functions of p(u) for fixed p(v|u). Furthermore, the semantic joint entropy Hs( ˜U, ˜V ) is also a concave function of p(u). So we conclude that Is( ˜U; ˜V ) is also a concave function of p(u).

Second, we prove Is( ˜U; ˜V ) is a convex function of p(v|u) for fixed p(u). Since Is( ˜U; ˜V ) =
Ds (p (*u, v*) ∥ps(u)ps(v)), due to the convexity of semantic relative entropy, we conclude that Is( ˜U; ˜V ) is a convex function of p(v|u).

By using the similar methods, we can prove Is( ˜U; ˜V ) is a convex function of p(v|u) for fixed p(u) and Is( ˜U; ˜V ) is a concave function of p(u) for fixed p(v|u).

□

## References

[1] C. E. Shannon, "A Mathematical Theory of Communication," The Bell System Technical
Journal, vol. 27, pp. 379-423, 623-656, July, Oct., 1948.
[2] C. E. Shannon and W. Weaver, *The Mathematical Theory of Communication*, The University
of Illinois Press, 1949.
[3] W. Weaver, "Recent Contributions to the Mathematical Theory of Communication," ETC:
A Review of General Semantics, pp. 261-81, 1953.
[4] T. Cover and J. Thomas, *Elements of Information Theory*, New York: Wiley, 1991.
[5] A. El Gamal and Y. H. Kim, *Network Information Theory*, Cambridge: Cambridge
University Press, 2011.
[6] R. Carnap, Y. Bar-Hillel, "An Outline of A Theory of Semantic Information," RLE Technical
Reports 247, Research Laboratory of Electronics, Massachusetts Institute of Technology,
Cambridge MA,1952.
[7] L. Floridi, "Outline of A Theory of Strongly Semantic Information," *Minds and machines*,
vol. 14, no. 2, pp. 197-221, 2004.
[8] N. J. Nilsson, "Probabilistic Logic," *Artificial Intelligence*, vol. 28, no. 1, pp. 71-87, 1986.
[9] J. Bao, P. Basu, M. Dean, et al., "Towards A Theory of Semantic Communication," IEEE
Network Science Workshop, West Point, NY, USA, Jun. 2011.
[10] A. D. Luca, S. Termini, "A Definition of A Non-probabilistic Entropy In The Setting of
Fuzzy Sets," *Information and Control*, vol. 20, pp. 301-312, 1972.
[11] A. D. Luca, S. Termini, "Entropy of L-Fuzzy Sets," *Information and Control*, vol. 24, pp.
55-73, 1974.
[12] W. Wu, "General Source and General Entropy," Journal of Beijing University of Posts and
Telecommunications, vol. 5, no. 1, pp. 29-41, 1982.
[13] J. Liu, W. Zhang, and H. V. Poor, "A Rate-Distortion Framework for Characterizing
Semantic Information," 2021 IEEE International Symposium on Information Theory (ISIT), Melbourne, Australia,2021.
[14] T. Guo, Y. Wang, et al., "Semantic Compression with Side Information: A Rate-Distortion
Perspective," arXiv preprint arXiv:2208.06094, 2022.
[15] Y. Shao, Q. Cao, and D. Gunduz, "A Theory of Semantic Communication," arXiv preprint
arXiv:2212.01485, 2022.
[16] J. Tang, Q. Yang, and Z. Zhang, "Information-Theoretic Limits on Compression of Semantic
Information," arXiv preprint arXiv:2306.02305, 2023.
[17] P. Zhang, W. Xu, H. Gao, et al., "Toward Wisdom-Evolutionary and Primitive-Concise 6G:
A New Paradigm of Semantic Communication Networks," *Engineering*, vol. 8, no. 1, pp.

60-73, 2022.

[18] G. Shi, Y. Xiao, Y. Li, et al., "From Semantic Communication to Semantic-aware
Networking: Model, Architecture, and Open Problems," *IEEE Communications Magazine*,
vol. 59, no. 8, pp. 44-50, 2021.
[19] Z. Qin, X. Tao, J. Lu, et al., "Semantic Communications: Principles and Challenges," arXiv
preprint arXiv:2201.01389, 2021.
[20] H. Xie, Z. Qin, X. Tao, et al., "Task-oriented Multi-user Semantic Communications," IEEE
Journal on Selected Areas in Communications, vol. 40, no. 9, pp. 2584-2597, 2022.
[21] D. G¨ud¨uz, Z. Qin, et al., "Beyond Transmitting Bits: Context, Semantics, and Task-oriented
Communications," *IEEE Journal on Selected Areas in Communications*, vol. 41, no. 1, pp.
5-41, Jan. 2023.
[22] K. Niu, J. Dai, S. Yao, et al., "A Paradigm Shift Towards Semantic Communications," IEEE
Communications Magazine, vol. 60, no. 11, pp. 113-119, 2022.
[23] E. Arıkan, "Channel polarization: a method for constructing capacity achieving codes for
symmetric binary-input memoryless channels," *IEEE Trans. Inf. Theory*, vol. 55, no. 7, pp.
3051-3073, July 2009.