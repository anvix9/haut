{
    "language": "English",
    "filetype": "pdf",
    "toc": [
        [
            1,
            "Introduction",
            3
        ],
        [
            1,
            "Pretraining",
            4
        ],
        [
            2,
            "Data",
            4
        ],
        [
            2,
            "Tokenization",
            6
        ],
        [
            2,
            "Architecture",
            6
        ],
        [
            2,
            "Training",
            7
        ],
        [
            2,
            "Context Length Extension",
            7
        ],
        [
            2,
            "Experimental Results",
            8
        ],
        [
            1,
            "Alignment",
            9
        ],
        [
            2,
            "Supervised Finetuning",
            9
        ],
        [
            3,
            "Data",
            10
        ],
        [
            3,
            "Training",
            10
        ],
        [
            2,
            "Reinforcement Learning from Human Feedback",
            10
        ],
        [
            3,
            "Reward Model",
            10
        ],
        [
            3,
            "Reinforcement Learning",
            11
        ],
        [
            2,
            "Automatic and Human Evaluation of Aligned Models",
            11
        ],
        [
            2,
            "Tool Use, Code Interpreter, and Agent",
            13
        ],
        [
            1,
            "Code-Qwen: Specialized Model for Coding",
            16
        ],
        [
            2,
            "Code Pretraining",
            16
        ],
        [
            2,
            "Code Supervised Fine-Tuning",
            17
        ],
        [
            2,
            "Evaluation",
            17
        ],
        [
            1,
            "Math-Qwen: Specialized Model for Mathematics Reasoning",
            17
        ],
        [
            2,
            "Training",
            17
        ],
        [
            2,
            "Evaluation",
            20
        ],
        [
            1,
            "Related Work",
            20
        ],
        [
            2,
            "Large Language Models",
            20
        ],
        [
            2,
            "Alignment",
            20
        ],
        [
            2,
            "Tool Use and Agents",
            21
        ],
        [
            2,
            "LLM for Coding",
            21
        ],
        [
            2,
            "LLM for Mathematics",
            22
        ],
        [
            1,
            "Conclusion",
            22
        ],
        [
            1,
            "Appendix",
            36
        ],
        [
            2,
            "More Training Details",
            36
        ],
        [
            3,
            "Data Format for Qwen-Chat",
            36
        ],
        [
            2,
            "Evaluation",
            36
        ],
        [
            3,
            "Automatic Evaluation",
            36
        ],
        [
            3,
            "Human Evaluation",
            40
        ],
        [
            2,
            "Analysis of Code Interpreter",
            58
        ]
    ],
    "pages": 59,
    "ocr_stats": {
        "ocr_pages": 0,
        "ocr_failed": 0,
        "ocr_success": 0
    },
    "block_stats": {
        "header_footer": 0,
        "code": 33,
        "table": 18,
        "equations": {
            "successful_ocr": 0,
            "unsuccessful_ocr": 0,
            "equations": 0
        }
    },
    "postprocess_stats": {
        "edit": {}
    }
}