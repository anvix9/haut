{
    "language": "English",
    "filetype": "pdf",
    "toc": [
        [
            1,
            "Introduction",
            1
        ],
        [
            1,
            "Related Work",
            2
        ],
        [
            1,
            "Preliminaries",
            3
        ],
        [
            1,
            "Direct Preference Optimization",
            4
        ],
        [
            1,
            "Theoretical Analysis of DPO",
            5
        ],
        [
            2,
            "Your Language Model Is Secretly a Reward Model",
            5
        ],
        [
            2,
            "Instability of Actor-Critic Algorithms",
            6
        ],
        [
            1,
            "Experiments",
            7
        ],
        [
            2,
            "How well can DPO optimize the RLHF objective?",
            8
        ],
        [
            2,
            "Can DPO scale to real preference datasets?",
            9
        ],
        [
            2,
            "Generalization to a new input distribution",
            9
        ],
        [
            2,
            "Validating GPT-4 judgments with human judgments",
            10
        ],
        [
            1,
            "Discussion",
            10
        ],
        [
            1,
            "Mathematical Derivations",
            15
        ],
        [
            2,
            "Deriving the Optimum of the KL-Constrained Reward Maximization Objective",
            15
        ],
        [
            2,
            "Deriving the DPO Objective Under the Bradley-Terry Model",
            16
        ],
        [
            2,
            "Deriving the DPO Objective Under the Plackett-Luce Model",
            16
        ],
        [
            2,
            "Deriving the Gradient of the DPO Objective",
            17
        ],
        [
            2,
            "Proof of Lemma 1 and 2",
            17
        ],
        [
            2,
            "Proof of Theorem 1",
            18
        ],
        [
            1,
            "DPO Implementation Details and Hyperparameters",
            19
        ],
        [
            1,
            "Further Details on the Experimental Set-Up",
            20
        ],
        [
            2,
            "IMDb Sentiment Experiment and Baseline Details",
            20
        ],
        [
            2,
            "GPT-4 prompts for computing summarization and dialogue win rates",
            20
        ],
        [
            2,
            "Unlikelihood baseline",
            21
        ],
        [
            1,
            "Additional Empirical Results",
            22
        ],
        [
            2,
            "Performance of Best of N baseline for Various N",
            22
        ],
        [
            2,
            "Sample Responses and GPT-4 Judgments",
            22
        ],
        [
            2,
            "Human study details",
            27
        ]
    ],
    "pages": 27,
    "ocr_stats": {
        "ocr_pages": 0,
        "ocr_failed": 0,
        "ocr_success": 0
    },
    "block_stats": {
        "header_footer": 0,
        "code": 20,
        "table": 1,
        "equations": {
            "successful_ocr": 31,
            "unsuccessful_ocr": 0,
            "equations": 31
        }
    },
    "postprocess_stats": {
        "edit": {}
    }
}