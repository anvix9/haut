# Introduction

1
Meshes and points are the most common 3D scene representations
because they are explicit and are a good fit for fast GPU/CUDA-based
rasterization. In contrast, recent Neural Radiance Field (NeRF) meth-
ods build on continuous scene representations, typically optimizing
a Multi-Layer Perceptron (MLP) using volumetric ray-marching for
novel-view synthesis of captured scenes. Similarly, the most efficient
radiance field solutions to date build on continuous representations
by interpolating values stored in, e.g., voxel [Fridovich-Keil and Yu
et al. 2022] or hash [M√ºller et al. 2022] grids or points [Xu et al. 2022].
While the continuous nature of these methods helps optimization,
the stochastic sampling required for rendering is costly and can
result in noise. We introduce a new approach that combines the best
of both worlds: our 3D Gaussian representation allows optimization
with state-of-the-art (SOTA) visual quality and competitive training
times, while our tile-based splatting solution ensures real-time ren-
dering at SOTA quality for 1080p resolution on several previously
published datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch
et al. 2017] (see Fig. 1).

Our goal is to allow real-time rendering for scenes captured with
multiple photos, and create the representations with optimization
times as fast as the most efficient previous methods for typical
real scenes. Recent methods achieve fast training [Fridovich-Keil

ACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.

Ground TruthInstantNGP (9.2  fps) Plenoxels (8.2 fps) Train: 7min, PSNR: 22.1Train: 26min, PSNR: 21.9Mip-NeRF360 (0.071 fps) Train: 48 h, PSNR: 24.3Ours (135  fps) Train: 6 min, PSNR: 23.6Ours (93  fps) Train: 51min, PSNR: 25.2

1:2

‚Ä¢ Bernhard Kerbl, Georgios Kopanas, Thomas Leimk√ºhler, and George Drettakis

and Yu et al. 2022; M√ºller et al. 2022], but struggle to achieve the
visual quality obtained by the current SOTA NeRF methods, i.e.,
Mip-NeRF360 [Barron et al. 2022], which requires up to 48 hours of
training time. The fast ‚Äì but lower-quality ‚Äì radiance field methods
can achieve interactive rendering times depending on the scene
(10-15 frames per second), but fall short of real-time rendering at
high resolution.

Our solution builds on three main components. We first intro-
duce 3D Gaussians as a flexible and expressive scene representation.
We start with the same input as previous NeRF-like methods, i.e.,
cameras calibrated with Structure-from-Motion (SfM) [Snavely et al.
2006] and initialize the set of 3D Gaussians with the sparse point
cloud produced for free as part of the SfM process. In contrast to
most point-based solutions that require Multi-View Stereo (MVS)
data [Aliev et al. 2020; Kopanas et al. 2021; R√ºckert et al. 2022], we
achieve high-quality results with only SfM points as input. Note
that for the NeRF-synthetic dataset, our method achieves high qual-
ity even with random initialization. We show that 3D Gaussians
are an excellent choice, since they are a differentiable volumetric
representation, but they can also be rasterized very efficiently by
projecting them to 2D, and applying standard ùõº-blending, using an
equivalent image formation model as NeRF. The second component
of our method is optimization of the properties of the 3D Gaussians
‚Äì 3D position, opacity ùõº, anisotropic covariance, and spherical har-
monic (SH) coefficients ‚Äì interleaved with adaptive density control
steps, where we add and occasionally remove 3D Gaussians during
optimization. The optimization procedure produces a reasonably
compact, unstructured, and precise representation of the scene (1-5
million Gaussians for all scenes tested). The third and final element
of our method is our real-time rendering solution that uses fast GPU
sorting algorithms and is inspired by tile-based rasterization, fol-
lowing recent work [Lassner and Zollhofer 2021]. However, thanks
to our 3D Gaussian representation, we can perform anisotropic
splatting that respects visibility ordering ‚Äì thanks to sorting and ùõº-
blending ‚Äì and enable a fast and accurate backward pass by tracking
the traversal of as many sorted splats as required.
To summarize, we provide the following contributions:

‚Ä¢ The introduction of anisotropic 3D Gaussians as a high-quality,

unstructured representation of radiance fields.

‚Ä¢ An optimization method of 3D Gaussian properties, inter-
leaved with adaptive density control that creates high-quality
representations for captured scenes.

‚Ä¢ A fast, differentiable rendering approach for the GPU, which
is visibility-aware, allows anisotropic splatting and fast back-
propagation to achieve high-quality novel view synthesis.

Our results on previously published datasets show that we can opti-
mize our 3D Gaussians from multi-view captures and achieve equal
or better quality than the best quality previous implicit radiance
field approaches. We also can achieve training speeds and quality
similar to the fastest methods and importantly provide the first
real-time rendering with high quality for novel-view synthesis.

