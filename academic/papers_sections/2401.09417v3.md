# Abstract

Recently the state space models (SSMs) with ef-
ficient hardware-aware designs, i.e., the Mamba
deep learning model, have shown great poten-
tial for long sequence modeling. Meanwhile
building efficient and generic vision backbones
purely upon SSMs is an appealing direction. How-
ever, representing visual data is challenging for
SSMs due to the position-sensitivity of visual
data and the requirement of global context for
visual understanding. In this paper, we show that
the reliance on self-attention for visual represen-
tation learning is not necessary and propose a
new generic vision backbone with bidirectional
Mamba blocks (Vim), which marks the image
sequences with position embeddings and com-
presses the visual representation with bidirec-
tional state space models. On ImageNet classi-
fication, COCO object detection, and ADE20k

*Equal contribution 1School of EIC, Huazhong University
of Science & Technology 2Institute of Artificial Intelligence,
Huazhong University of Science & Technology 3Horizon Robotics
4Beijing Academy of Artificial Intelligence. Correspondence to:
Xinggang Wang <xgwang@hust.edu.cn>.

Proceedings of the 41 st International Conference on Machine
Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).

1

semantic segmentation tasks, Vim achieves higher
performance compared to well-established vision
transformers like DeiT, while also demonstrating
significantly improved computation & memory
efficiency. For example, Vim is 2.8× faster than
DeiT and saves 86.8% GPU memory when per-
forming batch inference to extract features on
images with a resolution of 1248×1248. The
results demonstrate that Vim is capable of over-
coming the computation & memory constraints
on performing Transformer-style understanding
for high-resolution images and it has great poten-
tial to be the next-generation backbone for vision
foundation models. Code and models are released
at https://github.com/hustvl/Vim

# Introduction

Recent research advancements have led to a surge of inter-
est in the state space model (SSM). Originating from the
classic Kalman filter model (Kalman, 1960), modern SSMs
excel at capturing long-range dependencies and benefit from
parallel training. Some SSM-based methods, such as the lin-
ear state-space layers (LSSL) (Gu et al., 2021b), structured
state space sequence model (S4) (Gu et al., 2021a), diag-
onal state space (DSS) (Gupta et al., 2022), and S4D (Gu
et al., 2022), are proposed to process sequence data across a

4243444546DetectionmAP(%)3637383940Ins. Seg.mAP(%)71737577ClassificationTop-1Acc.(%)38394041Sem. Seg.mIoU(%)(a)AccuracyComparison11.41.82.22.651264073810241248FPSw/logscaleResolutionDeiT-TiVim-Ti2.542.252.051.571.262.292.071.911.71(b)SpeedComparison02040608051264073810241248GPUMemory(GB)ResolutionDeiT-TiVim-Ti4.564.2212.488.1311.148.095.0340.09OOM(c)GPUMemoryComparison3.32DeiT-TiVim-TiFasterSmaller2.8×faster-86.8%memory

Vision Mamba: Efficient Visual Representation Learning with State Space Model

wide range of tasks and modalities, particularly on modeling
long-range dependencies. They are efficient in processing
long sequences because of convolutional computation and
near-linear computation. 2-D SSM (Baron et al., 2023),
SGConvNeXt (Li et al., 2022b), and ConvSSM (Smith et al.,
2023a) combine SSM with CNN or Transformer architec-
ture to process 2-D data. The recent work, Mamba (Gu &
Dao, 2023), incorporates time-varying parameters into the
SSM and proposes a hardware-aware algorithm to enable
very efficient training and inference. The superior scaling
performance of Mamba indicates that it is a promising alter-
native to Transformer in language modeling. Nevertheless,
a generic pure-SSM-based backbone network has not been
explored for processing visual data, such as images and
videos.

Vision Transformers (ViTs) have achieved great success
in visual representation learning, excelling in large-scale
self-supervised pre-training and high performance on down-
stream tasks. Compared with convolutional neural networks,
the core advantage lies in that ViT can provide each image
patch with data/patch-dependent global context through self-
attention. This differs from convolutional networks that use
the same parameters, i.e., the convolutional filters, for all
positions. Another advantage is the modality-agnostic mod-
eling by treating an image as a sequence of patches without
2D inductive bias, which makes it the preferred architecture
for multimodal applications (Bavishi et al., 2023; Li et al.,
2023; Liu et al., 2023). At the same time, the self-attention
mechanism in Transformers poses challenges in terms of
speed and memory usage when dealing with long-range vi-
sual dependencies, e.g., processing high-resolution images.

Motivated by the success of Mamba in language model-
ing, it is appealing that we can also transfer this success
from language to vision, i.e., to design a generic and ef-
ficient visual backbone with the advanced SSM method.
However, there are two challenges for Mamba, i.e., unidi-
rectional modeling and lack of positional awareness. To
address these challenges, we propose the Vision Mamba
(Vim) model, which incorporates the bidirectional SSMs for
data-dependent global visual context modeling and position
embeddings for location-aware visual recognition. We first
split the input image into patches and linearly project them
as vectors to Vim. Image patches are treated as the sequence
data in Vim blocks, which efficiently compresses the vi-
sual representation with the proposed bidirectional selective
state space. Furthermore, the position embedding in Vim
block provides the awareness for spatial information, which
enables Vim to be more robust in dense prediction tasks. In
the current stage, we train the Vim model on the supervised
image classification task using the ImageNet dataset and
then use the pretrained Vim as the backbone to perform
sequential visual representation learning for downstream
dense prediction tasks, i.e., semantic segmentation, object

detection, and instance segmentation. Like Transformers,
Vim can be pretrained on large-scale unsupervised visual
data for better visual representation. Thanks to the better
efficiency of Mamba, the large-scale pretraining of Vim can
be achieved with lower computational cost.

Compared with other SSM-based models for vision tasks,
Vim is a pure-SSM-based method and models images in a
sequence manner, which is more promising for a generic and
efficient backbone. Thanks to the bidirectional compressing
modeling with positional awareness, Vim is the first pure-
SSM-based model to handle dense prediction tasks. Com-
pared with the most convincing Transformer-based model,
i.e., DeiT (Touvron et al., 2021a), Vim achieves superior
performance on ImageNet classification. Furthermore, Vim
is more efficient in terms of GPU memory and inference
time for high-resolution images. The efficiency in terms
of memory and speed empowers Vim to directly perform
sequential visual representation learning without relying on
2D priors (such as the 2D local window in ViTDet (Li et al.,
2022c)) for high-resolution visual understanding tasks while
achieving higher accuracy than DeiT.

Our main contributions can be summarized as follows:

• We propose Vision Mamba (Vim), which incorporates
bidirectional SSM for data-dependent global visual
context modeling and position embeddings for location-
aware visual understanding.

• Without the need of attention, the proposed Vim has
the same modeling power as ViT while it only has
subquadratic-time computation and linear memory
complexity. Specifically, Vim is 2.8× faster than DeiT
and saves 86.8% GPU memory when performing batch
inference to extract features on images at the resolution
of 1248×1248.

• We conduct extensive experiments on ImageNet classi-
fication and dense prediction downstream tasks. The
results demonstrate that Vim achieves superior perfor-
mance compared to the well-established and highly-
optimized plain vision Transformer, i.e., DeiT.

# Method

The goal of Vision Mamba (Vim) is to introduce the ad-
vanced state space model (SSM), i.e., Mamba (Gu & Dao,
2023), to computer vision. This section begins with a de-
scription of the preliminaries of SSM. It is followed by an
overview of Vim. We then detail how the Vim block pro-
cesses input token sequences and proceed to illustrate the
architecture details of Vim. The section concludes with an

3

Vision Mamba: Efficient Visual Representation Learning with State Space Model

analysis of the efficiency of the proposed Vim.

3.1. Preliminaries

The SSM-based models, i.e., structured state space sequence
models (S4) and Mamba are inspired by the continuous
system, which maps a 1-D function or sequence x(t) ∈
R (cid:55)→ y(t) ∈ R through a hidden state h(t) ∈ RN. This
system uses A ∈ RN×N as the evolution parameter and
B ∈ RN×1, C ∈ R1×N as the projection parameters. The
continuous system works as follows: h′(t) = Ah(t) +
Bx(t) and y(t) = Ch(t).

The S4 and Mamba are the discrete versions of the con-
tinuous system, which include a timescale parameter ∆
to transform the continuous parameters A, B to discrete
parameters A, B. The commonly used method for trans-
formation is zero-order hold (ZOH), which is defined as
follows:

A = exp (∆A),
B = (∆A)−1(exp (∆A) − I) · ∆B.

(1)

After the discretization of A, B, the discretized version
using a step size ∆ can be rewritten as:

ht = Aht−1 + Bxt,
yt = Cht.

(2)

At last, the models compute output through a global convo-
lution.

K = (CB, CAB, . . . , CA

y = x ∗ K,

M−1

B),

(3)

where M is the length of the input sequence x, and K ∈ RM
is a structured convolutional kernel.

3.2. Vision Mamba

An overview of the proposed Vim is shown in Fig. 2. The
standard Mamba is designed for the 1-D sequence. To
process the vision tasks, we first transform the 2-D image
t ∈ RH×W×C into the flattened 2-D patches xp ∈ RJ×(P2·C),
where (H, W) is the size of input image, C is the number of
channels, P is the size of image patches. Next, we linearly
project the xp to the vector with size D and add position
embeddings Epos ∈ R(J+1)×D, as follows:

T0 = [tcls; t1

pW; t2

pW; · · · ; tJ

pW] + Epos,

(4)

p is the j-th patch of t, W ∈ R(P2·C)×D is the learn-
where tj
able projection matrix. Inspired by ViT (Dosovitskiy et al.,
2020) and BERT (Kenton & Toutanova, 2019), we also use
class token to represent the whole patch sequence, which is
denoted as tcls. We then send the token sequence (Tl−1)
to the l-th layer of the Vim encoder, and get the output

4

Tl. Finally, we normalize the output class token T0
L and
feed it to the multi-layer perceptron (MLP) head to get the
final prediction ˆp, as follows: Tl = Vim(Tl−1) + Tl−1,
f = Norm(T0
L), and ˆp = MLP(f ), where Vim is the
proposed vision mamba block, L is the number of layers,
and Norm is the normalization layer.

3.3. Vim Block

The original Mamba block is designed for the 1-D sequence,
which is not suitable for vision tasks requiring spatial-aware
understanding. In this section, we introduce the Vim block,
which incorporates the bidirectional sequence modeling for
the vision tasks. The Vim block is shown in Fig. 2.

Specifically, we present the operations of Vim block in
Algo. 1. The input token sequence Tl−1 is first normalized
by the normalization layer. Next, we linearly project the
normalized sequence to the x and z with dimension size
E. Then, we process the x from the forward and backward
directions. For each direction, we first apply the 1-D convo-
lution to the x and get the x′
o. We then linearly project the
x′
o to the Bo, Co, ∆o, respectively. The ∆o is then used to
transform the Ao, Bo, respectively. Finally, we compute the
yf orward and ybackward through the SSM. The yf orward
and ybackward are then gated by the z and added together
to get the output token sequence Tl.

3.4. Architecture Details

In summary, the hyper-parameters of our architecture are
listed as follows: L denotes the number of blocks, D denotes
the hidden state dimension, E denotes the expanded state
dimension, and N denotes the SSM dimension. Following
ViT (Dosovitskiy et al., 2020) and DeiT (Touvron et al.,
2021b), we first employ 16×16 kernel size projection layer
to get a 1-D sequence of non-overlapping patch embeddings.
Subsequently, we directly stack L Vim blocks. By default,
we set the number of blocks L to 24, SSM dimension N
to 16. To align with the model sizes of DeiT series, we
set the hidden state dimension D to 192 and expanded state
dimension E to 384 for the tiny-size variant. For the small-
size variant, we set D to 384 and E to 768.

3.5. Efficiency Analysis

Traditional SSM-based methods leverage the fast Fourier
transform to boost the convolution operation as shown in
Eq. (3). For data-dependent methods, such as Mamba, the
SSM operation in Line 11 of Algo. 1 is no longer equivalent
to convolution. To address this problem, Mamba and the
proposed Vim choose a modern-hardware-friendly way to
ensure efficiency. The key idea of this optimization is to
avoid the IO-bound and memory-bound of modern hardware
accelerators (GPUs).

Vision Mamba: Efficient Visual Representation Learning with State Space Model

The overview of the proposed Vim model. We first split the input image into patches, and then project them into
patch tokens. Last, we send the sequence of tokens to the proposed Vim encoder. To perform ImageNet classification, we
concatenate an extra learnable classification token to the patch token sequence. Different from Mamba for text sequence
modeling, Vim encoder processes the token sequence with both forward and backward directions.

IO-Efficiency. The high bandwidth memory (HBM) and
SRAM are two important components for GPUs. Among
them, SRAM has a larger bandwidth and HBM has a bigger
memory size. The standard implementation of Vim’s SSM
operation with HBM requires the number of memory IO on
the order of O(BMEN). Inspired by Mamba, Vim first reads
in O(BME + EN) bytes of memory (∆o, Ao, Bo, Co) from
slow HBM to fast SRAM. Then, Vim gets the discrete Ao,
Bo of a size of (B, M, E, N) in SRAM. Last, Vim performs
SSM operations in SRAM and writes the output of a size of
(B, M, E) back to HBM. This method can help to reduce IOs
from O(BMEN) to O(BME + EN).

Memory-Efficiency. To avoid out-of-memory problems
and achieve lower memory usage when dealing with long
sequences, Vim chooses the same recomputation method
as Mamba. For the intermediate states of size (B, M, E, N) to
calculate the gradient, Vim recomputes them at the network
backward pass. For intermediate activations such as the
output of activation functions and convolution, Vim also
recomputes them to optimize the GPU memory requirement,
as the activation values take a lot of memory but are fast for
recomputation.

Computation-Efficiency. SSM in Vim block (Line 11 in
Algo.1) and self-attention in Transformer both play a key
role in providing global context adaptively. Given a visual
sequence T ∈ R1×M×D and the default setting E = 2D, the
computation complexity of a global self-attention and SSM
are:

lengths.

4. Experiment

Method

ResNet-18
ResNet-50
ResNet-101
ResNet-152

image
size

Convnets
2242
2242
2242
2242

ResNeXt50-32×4d

2242

#param.

ImageNet
top-1 acc.

12M
25M
45M
60M

25M

21M

2242
Transformers
3842
3842

86M
307M

2242
2242
2242
SSMs
2242

2242
2242

2242
2242

2242
2242

6M
22M
86M

89M

7M
7M

26M
26M

98M
98M

69.8
76.2
77.4
78.3

77.6

80.0

77.9
76.5

72.2
79.8
81.8

80.4

76.1
78.3 +2.2

80.3
81.4 +1.1

81.9
83.2 +1.3

RegNetY-4GF

ViT-B/16
ViT-L/16

DeiT-Ti
DeiT-S
DeiT-B

S4ND-ViT-B

Vim-Ti
Vim-Ti†

Vim-S
Vim-S†

Vim-B
Vim-B†

Ω(self-attention) = 4MD2 + 2M2D,
Ω(SSM) = 3M(2D)N + M(2D)N,

(5)

(6)

Table 1: Comparison with different backbones on ImageNet-
1K validation set. † represents the model is fine-tuned with
our long sequence setting.

where self-attention is quadratic to sequence length M, and
SSM is linear to sequence length M (N is a fixed parameter,
set to 16 by default). The computational efficiency makes
Vim scalable for gigapixel applications with large sequence

4.1. Image Classification

Settings. We benchmark Vim on the ImageNet-1K
dataset (Deng et al., 2009), which contains 1.28M training

5

EmbeddedPatchesNorm𝑥ForwardConv1dBackwardConv1dForwardSSMBackwardSSML×VisionMambaEncoderInput ImageVisionMamba EncoderFlatten&Linear ProjectionProjectionLayerPatch TokensPositionEmbed.ClassToken01*VisionMamba(Vim)Activation𝑧MLP&Prediction012345*6789𝑦ℎforwardℎbackwardVision Mamba: Efficient Visual Representation Learning with State Space Model

(B, M, E) ← log(1 + exp(Linear∆

o (x′

o) +

Algorithm 1 Vim Block Process

l−1 */

Require: token sequence Tl−1 : (B, M, D)
Ensure: token sequence Tl : (B, M, D)
1: /* normalize the input sequence T′
2: T′
l−1 : (B, M, D) ← Norm(Tl−1)
3: x : (B, M, E) ← Linearx(T′
4: z : (B, M, E) ← Linearz(T′
5: /* process with different direction */
6: for o in {forward, backward} do
7:
8: Bo : (B, M, N) ← LinearB
9: Co : (B, M, N) ← LinearC
10:
11: ∆o

/* softplus ensures positive ∆o */

o (x′
o)
o (x′
o)

l−1)
l−1)

x′

:

o : (B, M, E) ← SiLU(Conv1do(x))

Parameter∆
/* shape of ParameterA

o ))

o

o is (E, N) */
(cid:78) ParameterA
(cid:78) Bo
/* initialize ho and yo with 0 */
ho : (B, E, N) ← zeros (B, E, N)
yo : (B, M, E) ← zeros (B, M, E)
/* SSM recurrent */
for i in {0, ..., M-1} do

12:
13: Ao : (B, M, E, N) ← ∆o
14: Bo : (B, M, E, N) ← ∆o
15:
16:
17:
18:
19:
20:
21:
22:
23: end for
24: /* get gated y */
25: y′
26: y′
27: /* residual connection */
28: Tl : (B, M, D) ← LinearT(y′
29: Return: Tl

f orward : (B, M, E) ← yf orward
backward : (B, M, E) ← ybackward

ho = Ao[:, i, :, :] (cid:74) ho + Bo[:, i, :, :] (cid:74) x′
(cid:78) Co[:, i, :]
yo[:, i, :] = ho

(cid:74) SiLU(z)

f orward + y′

end for

(cid:74) SiLU(z)

o[:, i, :, None]

backward) + Tl−1

images and 50K validation images from 1,000 categories.
All models are trained on the training set, and top-1 accu-
racy on the validation set is reported. For fair comparisons,
our training settings mainly follow DeiT (Touvron et al.,
2021b). Specifically, we apply random cropping, random
horizontal flipping, label-smoothing regularization, mixup,
and random erasing as data augmentations. When training
on 2242 input images, we employ AdamW (Loshchilov &
Hutter, 2019) with a momentum of 0.9, a total batch size of
1024, and a weight decay of 0.05 to optimize models. We
train the Vim models for 300 epochs using a cosine sched-
ule, 1×10−3 initial learning rate, and EMA. During testing,
we apply a center crop on the validation set to crop out 2242
images. Experiments are performed on 8 A800 GPUs.

Long Sequence Fine-tuning To make full use of the effi-
cient long sequence modeling power of Vim, we continue to
fine-tune Vim with a long sequence setting for 30 epochs af-
ter pretraining. Specifically, we set a patch extraction stride
of 8 while keeping the patch size unchanged, a constant
learning rate of 10−5, and a weight decay of 10−8.

