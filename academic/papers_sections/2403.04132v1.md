# Abstract

Large Language Models (LLMs) have unlocked
new capabilities and applications; however, evalu-
ating the alignment with human preferences still
poses significant challenges. To address this is-
sue, we introduce Chatbot Arena, an open plat-
form for evaluating LLMs based on human pref-
erences. Our methodology employs a pairwise
comparison approach and leverages input from
a diverse user base through crowdsourcing. The
platform has been operational for several months,
amassing over 240K votes. This paper describes
the platform, analyzes the data we have collected
so far, and explains the tried-and-true statistical
methods we are using for efficient and accurate
evaluation and ranking of models. We confirm
that the crowdsourced questions are sufficiently
diverse and discriminating and that the crowd-
sourced human votes are in good agreement with
those of expert raters. These analyses collectively
establish a robust foundation for the credibility
of Chatbot Arena. Because of its unique value
and openness, Chatbot Arena has emerged as
one of the most referenced LLM leaderboards,
widely cited by leading LLM developers and
companies. Our demo is publicly available at
https://chat.lmsys.org.

# Introduction

Recent advancements in large language models (LLMs)
have significantly expanded their capabilities beyond tradi-
tional natural language processing boundaries, addressing a
broad array of general tasks (OpenAI, 2023; Gemini et al.,
2023; Touvron et al., 2023). These developments underscore
the potential of LLMs but also have raised concerns with re-
spect to performance evaluation. Current benchmarks often
fail to capture the nuanced and diverse aspects of these mod-
els, particularly in assessing their alignment with human

*Equal contribution 1UC Berkeley 2Stanford 3UCSD. Corre-

spondence to: Wei-Lin Chiang <weichiang@berkeley.edu>.

Classification of LLM benchmarks: We categorize
along two dimensions: whether the questions are from a static
dataset or a live, fresh source, and whether the evaluation met-
ric relies on ground truth or (approximated) human preferences.
MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019),
GSM-8K (Cobbe et al., 2021), MT-Bench (Zheng et al., 2023b),
and AlpacaEval (Li et al., 2023) are common examples of static
benchmarks. Chatbot Arena is the platform introduced in this
paper.

preferences in real-world, open-ended tasks.

To assess the performance of LLMs, the research community
has introduced a variety of benchmarks. These benchmarks
can be categorized based on two factors: the source of ques-
tions (either static or live) and the evaluation metric (either
ground truth or human preference). According to these fac-
tors, benchmarks can be classified into four categories, as
shown in While a range of benchmarks is benefi-
cial, the most prevalent current method for evaluating LLMs
remains a static, ground-truth-based evaluation, partly be-
cause such evaluations are inexpensive and reproducible.

However, these static, ground-truth-based benchmarks ex-
hibit several limitations. Firstly, the questions within these
benchmarks are not open-ended, hindering the ability to
capture the flexible and interactive use found in real-world
settings (Zheng et al., 2023b). Secondly, the test sets in
these benchmarks are static, meaning they can become con-
taminated over time, which undermines the reliability of
the evaluation results (Yang et al., 2023). Furthermore, for
many complex tasks, establishing a definitive ground truth
is not only challenging but sometimes unattainable. Conse-
quently, current benchmarks fail to adequately address the
needs of state-of-the-art LLMs, particularly in evaluating
user preferences. Thus, there is an urgent necessity for an
open, live evaluation platform based on human preference
that can more accurately mirror real-world usage.

Creating such a benchmark platform entails significant chal-
lenges. It requires the collection of live, fresh, and diverse
user questions to accurately represent real-world scenarios.

1

LiveStaticCodeforces Weekly ContestsMMLU, HellaSwag, GSM-8KGround TruthChatbot ArenaMT-Bench, AlpacaEvalHuman PreferenceQuestion SourceEvaluationMetric

Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

Additionally, developing scalable, incremental, and efficient
ranking systems is essential for evaluating a large number
of models. Moreover, ensuring the quality of human evalua-
tions is crucial given the noisy nature of human preferences.

To this end, we introduce Chatbot Arena, a benchmarking
platform for LLMs that features anonymous, randomized
battles in a crowdsourced setting. Chatbot Arena is a free
website open to all users.1 On this website, a user can ask a
question and get answers from two anonymous LLMs. Af-
terward, the user casts a vote for the model that delivers the
preferred response, with the models’ identities revealed only
after voting. This crowdsourced method effectively gathers
a diverse array of fresh user prompts, accurately reflecting
real-world LLM applications. Armed with this data, we
employ a suite of powerful statistical techniques, ranging
from the statistical model of Bradley & Terry (1952) to the
E-values of Vovk & Wang (2021), to estimate the ranking
over models as reliably and sample-efficiently as possible.
With these tools in hand, we have designed efficient sam-
pling algorithms specifically to select model pairs in a way
that accelerates the convergence of rankings while retaining
statistical validity.

We conduct a thorough analysis of the collected data to en-
sure the credibility of our platform. We demonstrate that
the user-generated questions are sufficiently diverse to en-
compass a wide range of LLM use cases and are sufficiently
challenging to differentiate between models. Furthermore,
we confirm that the crowd-sourced votes are highly consis-
tent with expert evaluations.

We have been running our system since Apr 2023 and have
received over 240K votes from about 90K users in over
100 different languages as of Jan 2024. To encourage user
engagement, we have made over 50 state-of-the-art models
available for free. We also collaborate with leading model
developers such as OpenAI, Google, Anthropic, Mistral,
Hugging Face, and various universities, incorporating their
latest models into our platform. We keep the community
engaged by routinely updating the leaderboard, publishing
analytical blogs, releasing datasets, and sharing information
via tweets. Because of its unique and significant value, our
leaderboard has emerged as one of the most referenced in
the LLM field and has become a benchmark for the industry.
We commit to making our data and code available, ensuring
that this platform is open-source and open-accessible.

We make the following contributions:

• We build the first large-scale crowd-sourced live LLM

evaluation platform with over 1M users visit.2

1https://chat.lmsys.org
2The number was estimated by Google Analytics as of March
2024. Note that user visit may not convert to votes as our website
also offers “direct chat” mode.

• We conduct an in-depth analysis of the collected data,
including prompt diversity, quality, vote quality, and
insights on human feedback.

• We will publicly release a human preference dataset with
over 100K pairwise votes collected from Chatbot Arena.

• We design an efficient sampling algorithm that actively
chooses which model pairs to show, such that our sample
efficiency improves, sometimes to a large degree.

