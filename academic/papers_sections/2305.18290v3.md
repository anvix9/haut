# Abstract

While large-scale unsupervised language models (LMs) learn broad world knowl-
edge and some reasoning skills, achieving precise control of their behavior is
difficult due to the completely unsupervised nature of their training. Existing
methods for gaining such steerability collect human labels of the relative quality of
model generations and fine-tune the unsupervised LM to align with these prefer-
ences, often with reinforcement learning from human feedback (RLHF). However,
RLHF is a complex and often unstable procedure, first fitting a reward model that
reflects the human preferences, and then fine-tuning the large unsupervised LM
using reinforcement learning to maximize this estimated reward without drifting
too far from the original model. In this paper we introduce a new parameterization
of the reward model in RLHF that enables extraction of the corresponding optimal
policy in closed form, allowing us to solve the standard RLHF problem with only a
simple classification loss. The resulting algorithm, which we call Direct Prefer-
ence Optimization (DPO), is stable, performant, and computationally lightweight,
eliminating the need for sampling from the LM during fine-tuning or performing
significant hyperparameter tuning. Our experiments show that DPO can fine-tune
LMs to align with human preferences as well as or better than existing methods.
Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sen-
timent of generations, and matches or improves response quality in summarization
and single-turn dialogue while being substantially simpler to implement and train.

# Introduction

Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabili-
ties . However, these models are trained on data generated by humans with a wide variety
of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for
example, while we may want our AI coding assistant to understand common programming mistakes
in order to correct them, nevertheless, when generating code, we would like to bias our model toward
the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want
our language model to be aware of a common misconception believed by 50% of people, but we
certainly do not want the model to claim this misconception to be true in 50% of queries about it!
In other words, selecting the model’s desired responses and behavior from its very wide knowledge
and abilities is crucial to building AI systems that are safe, performant, and controllable . While
existing methods typically steer LMs to match human preferences using reinforcement learning (RL),

∗Equal contribution; more junior authors listed earlier.

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods
for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and
human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.
In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification
objective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.

we will show that the RL-based objective used by existing methods can be optimized exactly with a
simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.

At a high level, existing methods instill the desired behaviors into a language model using curated
sets of human preferences representing the types of behaviors that humans find safe and helpful. This
preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on
a large text dataset. While the most straightforward approach to preference learning is supervised
fine-tuning on human demonstrations of high quality responses, the most successful class of methods
is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; ). RLHF methods fit
a reward model to a dataset of human preferences and then use RL to optimize a language model
policy to produce responses assigned high reward without drifting excessively far from the original
model. While RLHF produces models with impressive conversational and coding abilities, the RLHF
pipeline is considerably more complex than supervised learning, involving training multiple LMs and
sampling from the LM policy in the loop of training, incurring significant computational costs.

In this paper, we show how to directly optimize a language model to adhere to human preferences,
without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimiza-
tion (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms
(reward maximization with a KL-divergence constraint) but is simple to implement and straight-
forward to train. Intuitively, the DPO update increases the relative log probability of preferred to
dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents
the model degeneration that we find occurs with a naive probability ratio objective. Like existing
algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; ) that
measures how well a given reward function aligns with empirical preference data. However, while
existing methods use the preference model to define a preference loss to train a reward model and
then train a policy that optimizes the learned reward model, DPO uses a change of variables to define
the preference loss as a function of the policy directly. Given a dataset of human preferences over
model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective,
producing the optimal policy to an implicit reward function fit to the preference data.

Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for
training language models from preferences. Our experiments show that DPO is at least as effective
as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as
sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.

