# Abstract

We aim to produce a smaller language model that is aligned to user intent. Pre-
vious research has shown that applying distilled supervised fine-tuning (dSFT)
on larger models significantly improves task accuracy; however, these models are
unaligned, i.e. they do not respond well to natural prompts. To distill this prop-
erty, we experiment with the use of preference data from AI Feedback (AIF).
Starting from a dataset of outputs ranked by a teacher model, we apply distilled
direct preference optimization (dDPO) to learn a chat model with significantly
improved intent alignment. The approach requires only a few hours of training
without any additional sampling during fine-tuning. The final result, ZEPHYR-
7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models,
and requires no human annotation.
In particular, results on MT-Bench show
that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHF-
based model. Code, models, data, and tutorials for the system are available at
https://github.com/huggingface/alignment-handbook.

Model performance on MT-Bench. We compare ZEPHYR-7B, trained with distilled direct
preference optimization (dDPO), to proprietary models as well as larger, open-access models like
LLAMA2-CHAT-70B that were additionally trained using reinforcement learning on a large amount
of human feedback.

∗Equal contribution.

1

WritingRoleplayReasoningMathCodingExtractionSTEMHumanities0246810modelLlama-2-70b-chatGPT-3.5-turboClaude 1GPT-4Zephyr 7b

Technical Report

# Introduction

Smaller, open large language models (LLMs) have greatly increased in ability in recent years, from
early GPT-2-like models (Wang & Komatsuzaki, 2021) to accurate and compact models (Touvron
et al., 2023; Penedo et al., 2023; Jiang et al., 2023) that are trained on significantly more tokens than
the “compute-optimal” amount suggested by the Chincilla scaling laws (De Vries, 2023). In addi-
tion, researchers have shown that these models can be further trained through distilled supervised
fine-tuning (dSFT) based on proprietary models to increase their accuracy (Taori et al., 2023). In
this approach, the output of a more capable teacher model is used as supervised data for the student
model.

Distillation has proven to be an effective tool for improving open models on a range of differ-
ent tasks (Chiang et al., 2023); however, it does not reach the performance of the teacher mod-
els (Gudibande et al., 2023). Users have noted that these models are not “intent aligned”, i.e. they
do not behave in a manner that aligns with human users’ preferences. This property often leads to
outputs that do not provide correct responses to queries.

Intention alignment has been difficult to quantify, but recent work has led to the development of
benchmarks like MT-Bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023) that specifically
target this behavior. These benchmarks yield scores that correlate closely with human ratings of
model outputs and confirm the qualitative intuition that proprietary models perform better than open
models trained with human feedback, which in turn perform better than open models trained with
distillation. This motivates careful collection of human feedback for alignment, often at enormous
cost at scale, such as in LLAMA2-CHAT (Touvron et al., 2023).

In this work, we consider the problem of aligning a small open LLM entirely through distillation.
The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference
data, and apply distilled direct preference optimization as the learning objective (Rafailov et al.,
2023). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling
compared to using other approaches like proximal preference optimization (PPO) (Schulman et al.,
2017). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter
of hours on 16 A100s (80GB).

To validate this approach, we construct ZEPHYR-7B, an aligned version of Mistral-7B (Jiang
et al., 2023). We first use dSFT, based on the UltraChat (Ding et al., 2023) dataset. Next we
use the AI feedback data collected in the UltraFeedback dataset (Cui et al., 2023). Finally, we
apply dDPO based on this feedback data. Experiments show that this 7B parameter model can
achieve performance comparable to 70B-parameter chat models aligned with human feedback.
Results show improvements both in terms of standard academic benchmarks as well as bench-
marks that take into account conversational capabilities. Analysis shows that the use of prefer-
ence learning is critical in achieving these results. Models, code, and instructions are available at
https://github.com/huggingface/alignment-handbook.

We note an important caveat for these results. We are primarily concerned with intent alignment
of models for helpfulness. The work does not consider safety considerations of the models, such
as whether they produce harmful outputs or provide illegal advice (Bai et al., 2022). As distillation
only works with the output of publicly available models this is technically more challenging to do
because of added challenges in curating that type of synthetic data, and is an important subject for
future work.

# Method

The goal of this work is to align an open-source large-language model to the intent of the user.
Throughout the work we assume access to a larger teacher model πT which can be queried by
prompted generation. Our goal is be to produce a student model πθ and our approach follows
similar stages as InstructGPT (Ouyang et al., 2022) as shown in 3

Step 1 - dSFT

Generate multi-turn AI dialogues
Prompt sampled from
dataset of prompts.LLM simulates multi-turn
user-assistant interactions.Dialogues are used for
supervised fine-tuning.LLMLLMCreate a scenario
for a game about
space explorationStep 2 - AIF

Response generation and AI ranking
Prompt sampled from
dataset of prompts.4 different language models
generate responses.GPT-4 ranks the responses.
Mix flour, sugar and eggAdd chocolateBakeMelt chocolate and butteMix dry ingredient....Mix flour, sugar and eggAdd chocolateBakeMelt chocolate and butteMix dry ingredient.... Describe how to make chocolate browniesMix flour, sugar and eggAdd chocolateBakeMix flour, sugar and eggAdd chocolateBakeMix flour, sugar and eggAdd chocolateBakeMix flour, sugar and eggAdd chocolateBakeMix flour, sugar and eggAdd chocolateBakeMix flour, sugar and eggAdd chocolateBakePrompt sampled from
dataset of prompts.Best and another random
response are selected.Direct Preference
OptimizationMix flour, sugar and eggAdd chocolateBake Describe how to make chocolate browniesStep 3 - dDPO

Distillation of AI preferences
Mix flour, sugar and eggAdd chocolateBakeTechnical Report

Distilled Supervised Fine-Tuning (dSFT) Starting with a raw LLM, we first need to train it to
respond to user prompts. This step is traditionally done through supervised fine tuning (SFT) on
a dataset of high-quality instructions and responses (Chung et al., 2022; Sanh et al., 2021). Given
access to a teacher language models, we can instead have the model generate instructions and re-
sponses (Taori et al., 2023), and train the model directly on these. We refer to this as distilled SFT
(dSFT).
Approaches to dSFT follow the self-instruct protocol (Wang et al., 2023). Let x0
J be a set
of seed prompts, constructed to represent a diverse set of topical domains. A dataset is constructed
through iterative self-prompting where the teacher is used to both respond to an instruction and
refine the instruction based on the response. For each x0, we first sample response y0 ∼ πT(·|x0),
and then refine by sampling a new instruction (using a prompt for refinement), x1 ∼ πT(·|x0, y0).
The end point is a final dataset, C = {(x1, y1), . . . , (xJ , yJ )}. Distillation is performed by SFT,

1, . . . , x0

πdSFT = max

π

E
(x,y)∼C

log π(y|x)

AI Feedback through Preferences (AIF) Human feedback (HF) can provide additional signal
to align LLMs. Human feedback is typically given through preferences on the quality of LLM
responses (Ouyang et al., 2022). For distillation, we instead use AI preferences from the teacher
model on generated outputs from other models.

We follow the approach of UltraFeedback (Cui et al., 2023) which uses the teacher to provide pref-
erences on model outputs. As with SFT, the system starts with a set of prompts x1, . . . , xJ . Each
prompt x is fed to a collection of four models π1, . . . , π4, e.g. Claude, Falcon, Llama, etc, each of
which yield a response y1 ∼ π1(·|x), . . . , y4 ∼ π4(·|x). These responses are then fed to the teacher
model, e.g. GPT-4, which gives a score for the response s1 ∼ πT (·|x, y1), . . . , s4 ∼ πT (·|x, y4).
After collecting the scores for a prompt x, we save the highest scoring response as yw and a random
lower scoring prompt as yl. The final feedback dataset D consists of a set of these triples (x, yw, yl).

Distilled Direct Preference Optimization (dDPO) The goal of the final step is to refine the πdSFT
by maximizing the likelihood of ranking the preferred yw over yl in a preference model. The prefer-
ence model is determined by a reward function rθ(x, y) which utilizes the student language model
πθ. Past work using AI feedback has primarily focused on using RL methods such as proximal
policy optimization (PPO) to optimize θ with respect to this reward. These approaches optimize θ
by first training the reward and then sampling from the current policy to compute updates.

Direct preference optimization (DPO) uses a simpler approach to directly optimize the preference
model from the static data (Rafailov et al., 2023). The key observation is to derive the optimal
reward function in terms of the optimal LLM policy π∗ and the original LLM policy πdSFT. Under
an appropriate choice of preference model they show, for constant β and partition function Z that,
π*(y|x)
πdSFT(y|x)
By plugging this function of the reward into the preference model, the authors show that the objective
can be written as,

r∗(x, y) = β

+ β log Z(x)

πθ = max

π

E
(x,yw,yl) ∼D

(cid:18)

log σ

β log

π(yw|x)
πdSFT(yw|x)

− β log

π(yl|x)
πdSFT(yl|x)

(cid:19)

.

(1)

While this term looks complex, we note that it implies a simple training procedure. Starting with
the dSFT version of the model, we iterate through each AIF triple (x, yw, yl).

1. Compute the probability for (x, yw) and (x, yl) from the dSFT model (forward-only).
2. Compute the probability for (x, yw) and (x, yl) from the dDPO model.
3. Compute Eq 1 and backpropagate to update. Repeat.

