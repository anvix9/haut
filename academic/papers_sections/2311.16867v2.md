# Abstract

We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-
only models trained on a diverse high-quality corpora predominantly assembled
from web data. The largest model, Falcon-180B, has been trained on over 3.5
trillion tokens of text–the largest openly documented pretraining run. Falcon-180B
significantly outperforms models such as PaLM or Chinchilla, and improves upon
concurrently developed models such as LLaMA 2 or Inflection-1. It nears the
performance of PaLM-2-Large at a reduced pretraining and inference cost, making
it, to our knowledge, one of the three best language models in the world along
with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep
dive into the methods and custom tooling employed to pretrain Falcon. Notably,
we report on our custom distributed training codebase, allowing us to efficiently
pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with
limited interconnect. We release a 600B tokens extract of our web dataset, as well
as the Falcon-7/40/180B models under a permissive license to foster open-science
and accelerate the development of an open ecosystem of large language models.

The Falcon series of models achieves competitive performance, with Falcon-180B
nearly matching the 1-shot performance of PaLM-2 Large. 1-shot performance of PaLM (Chowd-
hery et al., 2022), PaLM-2 (Anil et al., 2023), and Falcon-180B, on a set of tasks from Brown et al.
(2020). These evaluation results are only a small snapshot of our evaluations, see Section 6 for details
and comparisons with GPT-3.5/4, LLaMA-1/2, and Inflection-1.

1Authors listed alphabetically, contributions detailed in Appendix A. Correspondence to falconllm@tii.ae

PaLMPaLM-2SmallPaLM-2MediumPaLM-2LargeFalcon-180B6065707580Aggregate1-shotperformance[acc.%]

1

# Introduction

The on-going Cambrian explosion of language models has been primarily fueled by the unique
scalability of popular Transformer-based recipes. This scalability manifests over multiple axes:

• Performance scalability (and predictability). Increase in pretraining compute budgets
systematically yield improvements in language modeling capabilities, in a consistent and
predictable way (Kaplan et al., 2020). Falcon-180B is the first publicly documented GPT-3-
sized model to follow the updated scaling law recommendations of Hoffmann et al. (2022),
with a total pretraining length of 3,500 billion tokens, without any upsampling.

• Data scalability. To scale-up pretraining efficiently, and to decouple pretraining and
inference compute, increasingly large models should be trained for longer, on larger corpora.
To sustain the Falcon series, we developed RefinedWeb (Penedo et al., 2023), a 5 trillion
tokens high-quality filtered and deduplicated web dataset–the largest publicly documented.
• Hardware scalability. Transformer models (Vaswani et al., 2017) are naturally suited for
modern GEMM optimized hardware, allowing their training and inference to be efficiently
distributed over a large number of accelerators (Narayanan et al., 2021b; Pope et al., 2023).
With Falcon-180B, we demonstrate scaling-up pretraining to 4,096 A100 40GB with only
50 Gbps interconnect per accelerator on cost-efficient AWS cloud infrastructure.

Building upon these fundamentals, increasingly large language models give rise to so-called emergent
capabilities (Wei et al., 2022a). These capabilities can be further tailored to human preference, to
build instruction-following or chatty models (Ouyang et al., 2022). All together these methods
have lead to the widespread deployment of large language models in customer-facing applications,
such as ChatGPT (GPT-3.5/4, OpenAI (2023a)), Claude, Bard (PaLM-2, Anil et al. (2023)), or Pi
(Inflection-1, Inflection (2023)). In this paper, we primarily report on the pretraining alone of the
Falcon series of models, and leave further downstream finetuning and alignment to future works.

The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We
assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on
RefinedWeb (Penedo et al., 2023)–a massive filtered and deduplicated web dataset. The architecture
of the models is based on PaLM (Chowdhery et al., 2022), although we independently validated each
decision, ultimately resulting in some minor tweaks–see Section 4 for details. The Falcon series
leverages extensive custom tooling (i.e., pretraining codebase, data pipeline), of which development
started in August 2022, with training of the models kicked-off in December 2022. In-depth evaluation
shows that the Falcon series is competitive across scale, and that Falcon-180B nears the performance
of PaLM-2 Large, positioning it as the best open model and in the top-3 of the best language models.

Contributions. With this paper and the Falcon series, we make the following contributions:

• Public documentation of the pretraining of a large-scale model. Recent state-of-the-art
models have been scarcely documented, hindering further research and progress in the field.
At variance with these works, we extensively document the pretraining of the Falcon series.
• Open data and models. To accelerate research, and to enable community-driven improve-
ments of large language models, we openly release Falcon-7/40/180B, and a 600 billion
tokens extract of the RefinedWeb dataset: https://huggingface.co/tiiuae/.

Table 1: The Falcon series of models covers a wide range of capabilities and inference require-
ments, enabled by large-scale web data. Falcon-7B can efficiently run on consumer hardware
(e.g., Apple M2), while Falcon-180B typically requires dedicated inference infrastructure (e.g.,
8 × A100 80GB ). We report steady zero-shot performance gains across the entire Falcon series.

Falcon-7B Falcon-40B

Falcon-180B

Pretraining [tokens]
Compute [PF-days]
Training [A100s]
Availability
Agg. performance (Section 6.5)
Closest model

1,500B
730
384

1,000B
2,800
384

Apache 2.0 Apache 2.0

60.8
<GPT-3

67.1
Chinchilla

3,500B
43,500
4,096
Responsible use license
70.3
PaLM-2 Large

2

Contents

1 Introduction

2 State-of-the-art: from language modeling to frontier models

3 Design philosophy

4 Experiments and motivations for data, architecture, and hyperparameters

4.1 Setup for small-scale experiments

. . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Data: web vs curated, code and multilinguality impact on English performance . .

4.2.1 Web data alone can outperform curated corpora . . . . . . . . . . . . . . .

2

5

6

7

7

8

8

4.2.2 Against a strong web baseline, curated data can even be detrimental

. . . .

10

4.2.3

Limited code and multilingual data do not strongly degrade English performance 11

4.3 Architecture and pretraining: validating popular recipes, and inference optimizations

12

4.3.1 Extending multiquery into multigroup for tensor parallel training and inference 12
4.3.2 Rotary positionnal embeddings may only offer a limited edge over ALiBi
14
4.3.3 The extra memory cost of GLU may not be worth it for cost-efficient training 14

.

4.3.4

Small tweaks help scalability: parallel layers and no biases in linear layers

15

4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search 15

4.4 Further experimentation required: ideas that did not make the cut . . . . . . . . . .

4.5 Wrapping-it up: validating overall dataset and architecture recipes . . . . . . . . .

5 Implementation

5.1 The Falcon dataset: predominantly web, with added curated and conversational data

5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset . . . . . .

5.1.2 The Microdata curated corpora and conversational masking . . . . . . . . .
5.2 The Falcon architecture and recipe for efficient inference and (stable) training . . .

17

18

19

19

20

21

22

5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up 22

5.2.2 Large language model alchemy: hyperparameters for pretraining . . . . . .

5.3 Large-scale distributed training on cloud infrastructure with Gigatron . . . . . . .

5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability

24

24

25

28
State-of-the-art throughput with dedicated Triton kernels . . . . . . . . . .
5.3.2
5.3.3 Efficient memory use via selective recomputation implemented as a monolayer 28
5.3.4 Numerical precision: all you need is bfloat16? . . . . . . .

. . . . . .

29

.

5.3.5 Quality-of-life features for improved flexibility and reliability . . . . . . .

5.4 Run management: keeping large-scale infrastructure running . . . . . . . . . . . .

6 Results

6.1 To prompt or not to prompt: comparing evaluations across codebases . . . . . . . .

6.2 Comparisons with PaLM on a natural language tasks aggregate . . . . . . . . . . .

6.3 Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks . . . . . . . . . . .

29

29

30

31

33

34

3

6.4 State-of-the-art comparisons on common sense, question answering, and code tasks

6.5 Comparison with other models using the EleutherAI Evaluation Harness . . . . . .

7 Limitations

7.1 Limitations of our findings and ablations . . . . . . . . . . . . . . . . . . . . . . .

7.2 Limitations of the Falcon models . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Conclusion

A Contributions

B Acknowledgements

C Model card

D Datasheet

E Comparisons with undocumented models

F Pseudocode samples

F.1 Measurement plan to measure all to all bandwidths/latencies efficiently . . . . .

. .

F.2 Converting tree token depth into an attention mask:

. . . . . . . . . . . . . . . . .

F.3

Zero-1 pseudo-code .

.

. . . .

. . .

. . . . . . . . . . . . . . . . . . . . . . . .

G Prompts

34

36

37

37

38

39

51

51

52

53

53

53

53

53

53

54

4

2 State-of-the-art: from language modeling to frontier models

We provide in this section an overview of general trends and works adjacent to the Falcon series. For
an in-depth literature review of individual technical components, see the relevant sections.
Language modeling. Beyond corpus/task-specific approaches, the first large-scale vector-based word
embeddings methods (Mikolov et al., 2013; Pennington et al., 2014) pioneered unsupervised learning
from massive unstructured text corpora. The integration of deep recurrent neural architectures enabled
models to deal with polysemy and to integrate contextual information (Peters et al., 2018); up to the
emergence of the transfer learning paradigm, leveraging universal models specialized to downstream
tasks through finetuning (Howard and Ruder, 2018). Despite the existence of many of the first
principles currently used, early scaling attempts (Jozefowicz et al., 2016) only had mixed success,
partly due to the fastidiousness and poor scalability on common hardware of recurrent approaches.

Transfomer models. The introduction of the attention-based Transformer architecture Vaswani
et al. (2017) sparked an explosion in the number of recipes to produce efficient, generalist models:
from embedding and classification focused encoder-only BERTs (Kenton and Toutanova, 2019), to
causal decoder-only GPTs (Radford et al., 2018). Specifically, GPT-2 (Radford et al., 2019) was the
first series of models to popularize emergent few-shot generalization abilities, allowing a model to
understand and perform arbitrary tasks simply from in-context instructions and demonstrations.

Large language models. The aforementioned works laid out the key components to current models;
the last ingredient, scaling, was demonstrated by GPT-3 (Brown et al., 2020), and consecrated by
the outlining of scaling laws (Kaplan et al., 2020). As increasingly large amounts of compute are
spent to pretrain models, commensurate gains are made in language modeling performance. The
tantalizing prospect of a systematic and direct path to more capable language models lead to a "scaling
frenzy": first with reproductions of GPT-3 with Jurassic-1 (Lieber et al., 2021) or PanGu-Alpha
(Zeng et al., 2021), and with open efforts such as GPT-J (Wang and Komatsuzaki, 2021), OPT (Zhang
et al., 2022), or BLOOM (Scao et al., 2022a); second, with works pushing further the limits of
scaling with Gopher (Rae et al., 2021), MT-NLG (Smith et al., 2022), or PaLM (Chowdhery et al.,
2022). Increased development and adoption of large language models also lead to improvements
of pretraining methods. Notably, Hoffmann et al. (2022) demonstrated with Chinchilla that optimal
scaling should actually jointly increase model size and pretraining dataset. For deployment in the
real-world, it may even be desirable to train far past so-called optimality, to decouple training and
inference compute and reduce serving costs. This is illustrated with the LLaMA models (Touvron
et al., 2023a,b), with 7B/13B/30B/70B parameters models trained on up to 2 trillion tokens.

Frontier models. Concurrently to this work, so-called frontier models have emerged, under the
shifting definition of "large-scale machine-learning models that exceed the capabilities currently
present in the most advanced existing models, and can perform a wide variety of tasks". Although a
moving goal, we attribute recent works on GPT-4 (OpenAI, 2023a) and PaLM-2 (Anil et al., 2023) as
early contributions to this category. These stand out by their significantly increased compute budget,
and improved capabilities. See Appendix E for details on our approach to undocumented models.

Although open models lag behind closed models in pretraining compute (by ∼18
months), the gap is not widening. After a 1 year lag, GPT-3 sparked a "scaling frenzy", with the
emergence of numerous LLMs. Models in the >10,000 PF-days range remain rare for open-source.

5

2021−01−012022−01−012023−01−01Publisheddate[arXiv]102103104105Compute[PF-days]GPT-3GPT-Neo-1.3BGPT-JGPT-NeoX-20BOPTBLOOMLLaMALLaMA2Falcon-180BFalcon-40BPaLM-2GPT-4PaLMChinchillaGopherLaMDAMT-NLGJurassic-1PanGu-AlphaGLMMPTCommercial/privateOpenOpen-sourceOpen-accessCommercialPrivate3 Design philosophy

Inspired by the bitter lesson (Sutton, 2019), we believe that scalable methods that best leverage
compute are ultimately the most effective. Accordingly, in designing the Falcon series of models, we
focused primarily on scalability, across three axes: performance, data, and hardware.

Performance scalability. The sustained increase in the scale of large language models (Fig. 2)
has been primarily motivated by so-called scaling laws: with increased pretraining compute comes
commensurate improvements in language modeling capabilities (Hestness et al., 2017; Kaplan et al.,
2020). This systematic path to model improvement has proved far more reliable than waiting for the
occasional research epiphany to manifest a paradigm-shifting method. But upstream and downstream
performance are not simply motivators of scaling, they are also a powerful driver. Indeed, quantifying
the impact of modeling interventions (e.g., architecture tweaks, data sourcing, hyperparameters
selection) is critical to sustaining the feedback loop provided by small-scale ablations. However,
the question of what to quantify is not trivial: upstream performance alone can be at odds with
downstream tasks (Tay et al., 2021), and even downstream metrics may not be aligned with human
preferences (Stiennon et al., 2020). This is made even more challenging by the fundamental contrast
between pretraining objective (i.e., predict the next word on a predominantly web-based corpora) and
common downstream use (i.e., follow users’ instructions in a helpful, harmless, and honest way) (Bai
et al., 2022a). As we focus on the pretraining stage of the Falcon models, we choose to center our
evaluations on measuring zero/few-shot generalization on large aggregates of natural language
tasks with the EleutherAI Harness (Gao et al., 2021)–similar to the setup of Le Scao et al. (2022);
Wang et al. (2022a). We build aggregates enabling comparisons with other state-of-the-art models,
but note the difficulty in executing principled comparisons: practices diverge widely across papers in
terms of task selection, prompting, and even mode of evaluation–standardized benchmarks remain
only scarcely adopted. We discuss in Section 7 limitations of our evaluation setup.

Data scalability. An increase in pretraining compute budget can be either spent towards a larger
model, or towards longer training. While Kaplan et al. (2020) had first found optimal scaling to be
mostly model size driven, Hoffmann et al. (2022) revised this finding and found that joint scaling is
preferable–see Fig. 3 to see impact. Furthermore, growing model size also increases inference burden;
where as increasing the pretraining dataset size is decoupled from inference costs. Recent open
models have been trained on datasets of up to two trillions tokens (Touvron et al., 2023b); because
naive repetition risks degrading the model (Hernandez et al., 2022; Muennighoff et al., 2023), this has
led to concerns about the sustainability of data scaling (Villalobos et al., 2022). These concerns are
exacerbated by the widely belief that curated corpora are necessary to build state-of-the-art models,
requiring manual addition of individual sources such as arXiv papers, books, and more (Brown et al.,
2020; Gao et al., 2020). For the Falcon series of models, we choose to focus on scaling high-quality
web data through stringent filtering and deduplication, enabling us to collect an English web
dataset of 5,000 billion tokens and to not repeat any data during training. We extensively report on
this work in Penedo et al. (2023), but also provide some key elements in this paper.

Hardware scalability. Large-scale training requires thousands of hardware accelerators to work
efficiently in unison; making the best use of these accelerators requires in turn principled distributed
training methods (Shoeybi et al., 2019). Methods that are able to best run efficiently and leverage
large-scale compute are often the ones that gain the most traction in the community (Hooker, 2021),
as best evidenced by the Transformer architecture itself (Vaswani et al., 2017). Furthermore, it
is difficult to find architectural improvements that significantly improve the task performance of
models, compared to the impact of data for instance (Le Scao et al., 2022). Accordingly, we
focus architectural decisions not on improving task performance, but on improving hardware
scalability and throughput. We are also concerned with inference scalability (Pope et al., 2023),
leading us to the adoption of tweaks such as a (revised) multiquery attention scheme (Shazeer, 2019).
Finally, beyond scalability, we are also concerced with cost-efficiency and relying on proven
approaches. We reimplement a 3D parallelism strategy (Narayanan et al., 2021b) combined with
optimizer sharding (Rajbhandari et al., 2020), enabling us to run on a more cost-efficient cloud AWS
infrastructure with limited interconnect. We also put a strong focus on memory-saving methods,
enabling us to run on cheaper 40GB A100. Reimplementing our data preprocessing and pretraining
pipelines from scratch allow us to extensively verify them, rather than relying on an unknown external
codebase. We also do not explore radical departures from the classic designs of large language
models, such as state-space models (Fu et al., 2022), as these have usually not been proven at scale.

6

4 Experiments and motivations for data, architecture, and hyperparameters

We first focus on a series of small-scale experiments with models in the 1B-3B parameters range
to validate recommended practices, as well as to identify interesting tweaks. We also conducted
extensive experiments to validate our web data pipeline, reported in Penedo et al. (2023). With each
subject of interest, we also outline common practices and our motivations for exploring this direction.

4.1 Setup for small-scale experiments

Small models. For these ablations, we seek to be able to rapidly iterate with limited compute costs.
This leads us to train 1/3 billion parameters models, for 30/60 billion parameters respectively. We
choose these short training lengths to be illustrative of the optimality regime from Hoffmann et al.
(2022) under which our larger models are likely to be trained. We base our reference architecture and
hyperparameters on the one described for GPT-3 (Brown et al., 2020), with the caveat of using ALiBi
positionnal embeddings as our baseline (Press et al., 2022). With reasonable resources (32-64 A100),
these ablation models can be trained overnight or in a few days, enabling rapid iterations. We note
that a caveat of using smaller models is that they may not be illustrative of some of the behaviours
of larger models: for instance, Dettmers et al. (2022) found that outlier features emerge at the 6B
scale, impacting quantization; concerns around data duplication and memorization have also shown
to disprotionately affect larger models (Carlini et al., 2022; Hernandez et al., 2022)

Dedicated aggregates. Although small models enable rapid iterations, they also have limited zero-
shot capabilities; naive bulk evaluation would result in most tasks being close to the random baseline,
and in significant noise in the evaluations. Using models trained on subsets of The Pile, we identified
tasks that showed both reasonable performance at small-scale, and limited variability across runs.
We independently quantified variance and average performance on over 50 tasks across 5 runs with
a different seed (we use this σ for architecture experiments) and across 10 runs with different data
subsets and shuffling (for data experiments). Based on this analysis, we source 11 tasks from the
evaluation setup of Brown et al. (2020); Le Scao et al. (2022); Srivastava et al. (2023) for our ablations
(zs-main/data/web). Note that differences between these three subsets are mostly due to differing
practices across time and between teams. zs-comp is a subset of main for comparisons with other
models, based on commonly reported tasks. We also report perplexities on The Pile (Gao et al., 2020)
(ppl-pile) for architectures (for data experiments, we found perplexities on The Pile to mostly
illustrate differences in formatting rather than content), and on a restricted subset of 3 NLP tasks with
low variance (zs-small). For our small-scale evaluations, we use both the EleutherAI harness (Gao
et al., 2021) and BigBench (Srivastava et al., 2023); note that our inference and evaluation codebase
differ significantly between this setup and the final results we report in Section 6, so results are not
directly comparable. We present an outline of the aggregates in Table 2.

Table 2: To evaluate small models used in ablations (1/3B models trained on 30/60B tokens), we
build four aggregates across 11 tasks on which to measure zero-shot performance and perplexity.
We trained 15 reference models on subsets of The Pile and with random seeds to identify tasks with
performance better than random and low variablity at this scale. All evaluations leverage the EAI
Harness (Gao et al., 2021) except date which is taken from BigBench (Srivastava et al., 2023). main
was built for architecture and hyperparameters ablations; data for data mixtures experiments; web
for small-scale ablations on web data; and core for its low variance on a reduced number of tasks.
This setup only covers natural language abilities. For main, data, and web, differences are mostly
due to individual preferences at the time of the experiments.

Tasks

Type

Random

main

comp

data

web

core

pile

LAMBADA (Paperno et al., 2016)
RACE (Lai et al., 2017)
HellaSwag (Zellers et al., 2019)
Winogrande (Sakaguchi et al., 2019)
PIQA (Bisk et al., 2020)
BoolQ (Clark et al., 2019)
COPA (Gordon et al., 2012)
Date (Srivastava et al., 2023)
ARC (Clark et al., 2018)
OpenBookQA (Mihaylov et al., 2018)
SciQ (Johannes Welbl, 2017)
The Pile (Gao et al., 2020)

Reading Comprehension
Reading Comprehension
Common Sense
Common Sense
Common Sense
Common Sense
Common Sense
Common Sense
Question Answering
Question Answering
Question Answering
Language Modeling

0.0
25.0
25.0
50.0
50.0
50.0
50.0
25.0
25.0
25.0
25.0

✓
✓
✓

✓
✓
✓
✓
✓

✓

✓
✓
✓

✓
✓

✓
✓
✓
✓
✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

7

4.2 Data: web vs curated, code and multilinguality impact on English performance

4.2.1 Web data alone can outperform curated corpora

Our motivations for predominantly training on web data, details of the processing pipeline, and
extensive evaluations are detailed in our dedicated RefinedWeb paper (Penedo et al., 2023). In
this section, we only highlight key ablations guiding our decision to focus our efforts on web data.

