# Abstract

Diffusion models create data from noise by invert-
ing the forward paths of data towards noise and
have emerged as a powerful generative modeling
technique for high-dimensional, perceptual data
such as images and videos. Rectified flow is a re-
cent generative model formulation that connects
data and noise in a straight line. Despite its better
theoretical properties and conceptual simplicity, it
is not yet decisively established as standard prac-
tice. In this work, we improve existing noise sam-
pling techniques for training rectified flow mod-
els by biasing them towards perceptually relevant
scales. Through a large-scale study, we demon-

*Equal contribution . <first.last>@stability.ai.

strate the superior performance of this approach
compared to established diffusion formulations
for high-resolution text-to-image synthesis. Ad-
ditionally, we present a novel transformer-based
architecture for text-to-image generation that uses
separate weights for the two modalities and en-
ables a bidirectional flow of information between
image and text tokens, improving text comprehen-
sion, typography, and human preference ratings.
We demonstrate that this architecture follows pre-
dictable scaling trends and correlates lower vali-
dation loss to improved text-to-image synthesis as
measured by various metrics and human evalua-
tions. Our largest models outperform state-of-the-
art models, and we will make our experimental
data, code, and model weights publicly available.

1

Scaling Rectified Flow Transformers for High-Resolution Image Synthesis

