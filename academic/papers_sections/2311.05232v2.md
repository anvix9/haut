# Introduction

Recently, the emergence of large language models (LLMs) , exemplified by LLaMA ,
Claude , Gemini  and GPT-4 , has ushered in a significant paradigm shift in natural
language processing (NLP), achieving unprecedented progress in language understanding ,
generation  and reasoning . Furthermore, the extensive factual
knowledge encoded within LLMs has demonstrated considerable advancements in leveraging
LLMs for information seeking , potentially reshaping the landscape of information retrieval
systems . Nevertheless, in tandem with these remarkable advancements, concerns have arisen
about the tendency of LLMs to generate hallucinations , resulting in seemingly plausible yet
factually unsupported content. Further compounding this issue is the capability of LLMs to generate
highly convincing and human-like responses , which makes detecting these hallucinations
particularly challenging, thereby complicating the practical deployment of LLMs, especially real-
world information retrieval (IR) systems that have integrated into our daily lives like chatbots
, search engines , and recommender systems . Given that the information
provided by these systems can directly influence decision-making, any misleading information has
the potential to spread false beliefs, or even cause harm.

Notably, hallucinations in conventional natural language generation (NLG) tasks have been
extensively studied , with hallucinations defined as generated content that is either
nonsensical or unfaithful to the provided source content. These hallucinations are categorized into
two types: intrinsic hallucination, where the generated output contradicts the source content, and
extrinsic hallucination, where the generated output cannot be verified from the source. However,
given their remarkable versatility across tasks , understanding hallucinations in LLMs
presents a unique challenge compared to models tailored for specific tasks. Besides, as LLMs
typically function as open-ended systems, the scope of hallucination encompasses a broader concept,
predominantly manifesting factual errors. This shift necessitates a reevaluation and adjustment
of the existing taxonomy of hallucinations, aiming to enhance its adaptability in the evolving
landscape of LLMs.

In this survey, we propose a redefined taxonomy of hallucination tailored specifically for applica-
tions involving LLMs. We categorize hallucination into two primary types: factuality hallucination
and faithfulness hallucination. Factuality hallucination emphasizes the discrepancy between gen-
erated content and verifiable real-world facts, typically manifesting as factual inconsistencies.
Conversely, faithfulness hallucination captures the divergence of generated content from user input
or the lack of self-consistency within the generated content. This category is further subdivided
into instruction inconsistency, where the content deviates from the user’s original instruction;
context inconsistency, highlighting discrepancies from the provided context; and logical incon-
sistency, pointing out internal contradictions within the content. Such categorization refines our
understanding of hallucinations in LLMs, aligning it closely with their contemporary usage.

Delving into the underlying causes of hallucinations in LLMs is essential not merely for enhanc-
ing the comprehension of these phenomena but also for informing strategies aimed at alleviating
them. Recognizing the multifaceted sources of LLM hallucinations, our survey identifies potential
contributors into three main aspects: data, training, and inference stages. This categorization allows
us to span a broad spectrum of factors, providing a holistic view of the origins and mechanisms by
which hallucinations may arise within LLM systems. Furthermore, we comprehensively outline a
variety of effective detection methods specifically devised for detecting hallucinations in LLMs, as
well as an exhaustive overview of benchmarks related to LLM hallucinations, serving as appropriate
testbeds to assess the extent of hallucinations generated by LLMs and the efficacy of detection
methods. Beyond evaluation, significant efforts have been undertaken to mitigate hallucinations of

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:3

LLMs. These initiatives are comprehensively surveyed in our study, in accordance with the corre-
sponding causes, spanning from data-related, training-related, and inference-related approaches. In
addition, the effectiveness of retrieval-augmented generation (RAG) in mitigating hallucinations has
garnered tremendous attention within the field. Despite the considerable potential of RAG, current
systems inherently face limitations and even suffer from hallucinations. Accordingly, our survey
undertakes an in-depth analysis of these challenges, aiming to provide valuable insights aimed
at developing more robust RAG systems. We also highlight several promising avenues for future
research, such as hallucinations in large vision-language models and understanding of knowledge
boundaries in LLM hallucinations, paving the way for forthcoming research in the field.

Comparing with Existing Surveys. As hallucination stands out as a major challenge in gener-
ative AI, numerous research  has been directed towards hallucinations.
While these contributions have explored LLM hallucination from various perspectives and provided
valuable insights, our survey seeks to delineate their distinct contributions and the comprehensive
scope they encompass. Ji et al.  primarily shed light on hallucinations in pre-trained models
for NLG tasks, leaving LLMs outside their discussion purview. Tonmoy et al.  mainly focused
on discussing the mitigation strategies combating LLM hallucinations. Besides, Liu et al.  took
a broader view of LLM trustworthiness without delving into specific hallucination phenomena,
whereas Wang et al.  provided an in-depth look at factuality in LLMs. However, our work nar-
rows down to a critical subset of trustworthiness challenges, specifically addressing factuality and
extending the discussion to include faithfulness hallucinations. To the best of our knowledge, Zhang
et al.  presented research closely aligned with ours, detailing LLM hallucination taxonomies,
evaluation benchmarks, and mitigation strategies. However, our survey sets itself apart through
a unique taxonomy and organizational structure. We present a detailed, layered classification of
hallucinations and conduct a more comprehensive analysis of the causes of hallucinations. Crucially,
our proposed mitigation strategies are directly tied to these causes, offering a targeted and coherent
framework for addressing LLM hallucinations.

Organization of this Survey. In this survey, we present a comprehensive overview of the latest
developments in LLM hallucinations, as shown in Fig 1. We commence by constructing a taxonomy
of hallucinations in the realm of LLM (§2). Subsequently, we analyze factors contributing to LLM
hallucinations in depth (§3), followed by a review of various strategies and benchmarks employed
for the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches
designed to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by
current RAG systems (§6) and delineate potential pathways for forthcoming research (§7).

2 DEFINITIONS
For the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a
succinct introduction to LLMs (§2.1), delineating the scope of this survey. Subsequently, we delve
into the training stages of LLMs (§2.2), as a thorough understanding of the training mechanisms
contributes significantly to elucidating the origins of hallucinations. Lastly, we expound upon the
concept of hallucinations in LLMs (§2.3), further categorizing it into two distinct types.

2.1 Large Language Models
Before delving into the causes of hallucination, we first introduce the concept of LLMs. Typically,
LLMs refer to a series of general-purpose models that leverage the transformer-based language
model architecture and undergo extensive training on massive textual corpora with notable ex-
amples including GPT-3 , PaLM , LLaMA , GPT-4  and Gemini . By scaling
the amount of data and model capacity, LLMs raise amazing emergent abilities, typically including
in-context learning (ICL) , chain-of-thought prompting  and instruction following .

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:4

s
l
e
d
o
M
e
g
a
u
g
n
a
L
e
g
r
a
L
n
i

s
n
o
i
t
a
n
i
c
u

l
l
a
H

Misinformation and
Biases

e.g. Bender et al. , Lee et al. , Lin et al. 

Hallucination from Data

Knowledge Boundary

e.g. Katz et al. , Onoe et al. , Singhal et al. 

Huang, et al.

Hallucination
Causes (§3)

Hallucination from
Training

Hallucination from
Inference

Hallucination
Detection and
Benchmarks(§4)

Hallucination Detection

Hallucination Benchmarks

Inferior Alignment Data

e.g. Gekhman et al. , Li et al. 

Hallucination from
Pre-training

e.g. Li et al. , Liu et al. , Wang and Sennrich 

Hallucination from SFT

e.g. Schulman , Yang et al. , Zhang et al. 

Hallucination from RLHF

e.g. Cotra , Perez et al. , Sharma et al. , Wei et al. 

Imperfect Decoding
Strategies

e.g. Holtzman et al. , Stahlberg and Byrne 

Over-confidence

e.g. Chen et al. , Liu et al. , Miao et al. 

Softmax Bottleneck

e.g. Chang and McCallum , Miao et al. 

Reasoning Failure

e.g. Berglund et al. , Zheng et al. 

Factuality Hallucination
Detection

Faithfulness Hallucination
Detection

Hallucination Evaluation
Benchmarks

Hallucination Detection
Benchmarks

e.g. Dhuliawala et al. , Manakul et al. , Min et al. 

e.g. Fabbri et al. , Maynez et al. , Scialom et al. 

e.g. TruthfulQA , HalluQA , HaluEval-2.0 

e.g. SelfCheckGPT-Wikibio , HaluEval , FELM 

Data Filtering

e.g. Abbas et al. , Gunasekar et al. , Touvron et al. 

Mitigating Data-related
Hallucinations

Model Editing

e.g. Dai et al. , Huang et al. , Mitchell et al. 

Hallucination
Mitigation (§5)

Mitigating Training-related
Hallucinations

Mitigating Inference-related
Hallucinations

Retrieval-Augmented
Generation

Mitigating Pre-training-
related Hallucination

Mitigating Misalignment
Hallucination

Factuality Enhanced
Decoding

Faithfulness Enhanced
Decoding

e.g. Gao et al. , Ram et al. , Yu et al. 

e.g. Li et al. , Liu et al. , Shi et al. 

e.g. Rimsky , Sharma et al. , Wei et al. 

e.g. Chuang et al. , Lee et al. , Li et al. 

e.g. Chang et al. , Shi et al. , Wan et al. 

Fig. 1. The main content flow and categorization of this survey.

2.2 Training Stages of Large Language Models
The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs
undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination
origins in LLMs, as each stage equips the model with specific capabilities.

2.2.1 Pre-training. Pre-training is widely acknowledged as a foundational stage for LLM to acquire
knowledge and capabilities . During this phase, LLMs engage in autoregressive prediction of
subsequent tokens within sequences. Through self-supervised training on extensive textual corpora,
LLMs acquire knowledge of language syntax, world knowledge, and reasoning abilities, thereby
laying a solid groundwork for further fine-tuning. Besides, recent research  suggests that
predicting subsequent words is akin to losslessly compressing significant information. The essence

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:5

of LLMs lies in predicting the probability distribution for upcoming words. Accurate predictions
indicate a profound grasp of knowledge, translating to a nuanced understanding of the world.

Supervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during
2.2.2
the pre-training stage, it’s crucial to recognize that pre-training primarily optimizes for completion.
Consequently, pre-trained LLMs fundamentally serve as completion machines, which can lead to
a misalignment between the next-word prediction objective of LLMs and the user’s objective of
obtaining desired responses. To bridge this gap, SFT  has been introduced, which involves
further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting
in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies 
have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on
unseen tasks, showcasing their remarkable generalization abilities.

2.2.3 Reinforcement Learning from Human Feedback. While the SFT process successfully enables
LLMs to follow user instructions, there is still room for them to better align with human preferences.
Among various methods that utilize human feedback, RLHF stands out as an representative solution
for aligning with human preferences through reinforcement learning . Typically, RLHF
employs a preference model  trained to predict preference rankings given a prompt alongside a
pair of human-labeled responses. To align with human preferences, RLHF optimizes the LLM to
generate outputs that maximize the reward provided by the trained preference model, typically
employing a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) .
Such integration of human feedback into the training loop has proven effective in enhancing the
alignment of LLMs, guiding them toward producing high-quality and harmless responses.

2.3 Hallucinations in Large Language Models
The concept of hallucination traces its roots to the fields of pathology and psychology and is
defined as the perception of an entity or event that is absent in reality . Within the realm
of NLP, hallucination is typically referred to as a phenomenon in which the generated content
appears nonsensical or unfaithful to the provided source content . This concept bears a
loose resemblance to the phenomenon of hallucination observed in human psychology. Generally,
hallucinations in natural language generation tasks can be categorized into two primary types:
intrinsic hallucination and extrinsic hallucination .

Specifically, intrinsic hallucinations pertain to the model outputs that directly conflict with the
provided source context. On the other hand, extrinsic hallucinations involve outputs that cannot be
verified using the provided source context or external knowledge bases. This means the generated
text is neither supported by nor directly contradicts the available information, rendering the output
unverifiable and potentially misleading.

However, in the era of LLMs, the versatile capabilities of these models have facilitated their
widespread use across diverse fields, highlighting limitations in existing task-specific categorization
paradigms. Considering that LLMs place a significant emphasis on user-centric interactions and pri-
oritize alignment with user directives, coupled with the fact that their hallucinations predominantly
surface at factual levels, we introduce a more granular taxonomy building upon the foundational
work by Ji et al. . This refined taxonomy seeks to encapsulate the distinct intricacies associ-
ated with LLM hallucinations. To provide a more intuitive illustration of our definition of LLM
hallucination, we present examples for each type of hallucination in Table 1, namely factuality
hallucination and faithfulness hallucination.

Factuality Hallucination. The emergence of LLMs marks a significant shift from traditional
2.3.1
task-specific toolkits to AI assistants that have a heightened focus on open-domain interactions.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:6

Huang, et al.

Table 1. Examples of each category of LLM hallucinations. Content marked in Red represents the hallucinatory
output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM
hallucination.

Category

Type

Example Dialog

.

Explanation

Factual
Contradiction

Instruction: What are Thomas Edison’s main contributions
to science and technology?

Response: Thomas Edison developed the first practical tele-
phone and invented the light bulb.

The response is factually incorrect. In reality, Edison
improved the light bulb, building on earlier desi-
gns by others, and Alexander Graham Bell inven-
ted the telephone.

Factuality
Hallucination

Factual
Fabrication

Instruction: What are the major environmental impacts of
the construction of the Eiffel Tower?

Response: The construction of the Eiffel Tower in 1889 led
to the extinction of the Parisian tiger, a species that played a
crucial role in the region’s ecosystem. Additionally, it is wid-
ely recognized as the event that sparked the global green
architecture movement.

Instruction
Inconsistency

Instruction: Translate the English question into Spanish:
"What is the capital of France?"

Response: The capital of France is Paris.

Faithfulness
Hallucination

Context
Inconsistency

Retrieved context: The Nile is a major north-flowing river
in northeastern Africa. It’s traditionally considered the
longest river in the world. Originating in the Great Lakes
region of central Africa, it flows through several countries
before emptying into the Mediterranean Sea.

Instruction: Summarize the retrieval document above.

Response: The Nile originates from the mountain ranges
of central Africa and flows into the Mediterranean.

Instruction: Solve the equation 2x + 3 = 11 step by step.

Logical
Inconsistency

Response: Step 1: Subtract 3 from both sides to get: 2x = 8.
Step 2: Divide both sides by 2 to get: x = 3. So the result
of the equation is x=3.

The response erroneously states that the construction
of the Eiffel Tower led to the extinction of the ‘Parisian
tiger’ and sparked the global green architecture move-
ment. In fact, there is no evidence of a ‘Parisian tig-
er’ ever existing, making this a fabricated claim.
Moreover, attributing the origins of green architec-
ture to the Eiffel Tower is an exaggeration, as this
movement has diverse roots and cannot be traced back
to a single event.

The user explicitly provided an instruction to translate
the English question into Spanish. Instead of perform-
ing the translation as instructed, the LLM chose to
answer the question.

The summary incorrectly states that the Nile originates
from the "mountain ranges of central Africa", while the
paragraph mentions that the Nile originates in the
Great Lakes region of central Africa.

The first step correctly isolates the term with x to get
2x = 8. However, the next step inaccurately divides
8 by 2 to yield a result of x = 3, which is inconsistent
with the earlier reasoning.

This shift is primarily attributed to their vast parametric factual knowledge. However, existing LLMs
occasionally exhibit tendencies to produce outputs that are either inconsistent with real-world
facts or unverifiable , posing challenges to the trustworthiness of artificial intelligence. In this
context, we categorize these factuality hallucinations into two primary types:

Factual Contradiction refers to situations where the LLM’s output contains facts that can be
grounded in real-world information, but present contradictions. This type of hallucination occurs
most frequently and arises from diverse sources, encompassing the LLM’s capture, storage, and
expression of factual knowledge. Depending on the error type of contradictions, it can be further
divided into two subcategories: entity-error hallucination and relation-error hallucination.

• Entity-error hallucination refers to the situations where the generated text of LLMs
contains erroneous entities. As shown in Table 1, when asked about "the inventor of the
telephone", the model erroneously states "Thomas Edison", conflicting with the real fact that it
was "Alexander Graham Bell".

• Relation-error hallucination refers to instances where the generated text of LLMs contains
wrong relations between entities. As shown in Table 1, when inquired about "the inventor of
the light bulb", the model incorrectly claims "Thomas Edison", despite the fact that he improved
upon existing designs and did not invent it.

Factual Fabrication refers to instances where the LLM’s output contains facts that are unveri-
fiable against established real-world knowledge. This can be further divided into unverifiability
hallucination and overclaim hallucination.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:7

• Unverifiability hallucination pertains to statements that are entirely non-existent or
cannot be verified using available sources. As shown in Table 1, when asked about "the major
environmental impacts of the construction of the Eiffel Tower", the model incorrectly states that
"the construction led to the extinction of the Parisian tiger", a species that does not exist and
thus, this claim cannot be substantiated by any historical or biological record.

• Overclaim hallucination involves claims that lack universal validity due to subjective biases.
As shown in Table 1, the model claims that "the Eiffel Tower’s construction is widely recognized
as the event that sparked the global green architecture movement." This is an overclaim, as
there is no broad consensus or substantial evidence to support the statement.

Faithfulness Hallucination. LLMs are inherently trained to align with user instructions. As
2.3.2
the use of LLMs shifts towards more user-centric applications, ensuring their consistency with
user-provided instructions and contextual information becomes increasingly vital. Furthermore,
LLM’s faithfulness is also reflected in the logical consistency of its generated content. From this
perspective, we categorize three subtypes of faithfulness hallucinations:

Instruction inconsistency refers to the LLM’s outputs that deviate from a user’s directive.
While some deviations might serve safety guidelines, the inconsistencies here signify unintentional
misalignment with non-malicious user instructions. As described in Table 1, the user’s actual
intention is translation, However, the LLM erroneously deviated from the user’s instruction and
performed a question-answering task instead.

Context inconsistency points to instances where the LLM’s output is unfaithful with the user’s
provided contextual information. For example, as shown in Table 1, the user mentioned the Nile’s
source being in the Great Lakes region of central Africa, yet the LLM’s response contradicted the
context.

Logical inconsistency underscores when LLM outputs exhibit internal logical contradictions,
often observed in reasoning tasks. This manifests as inconsistency both among the reasoning steps
themselves and between the steps and the final answer. For example, as shown in Table 1, while
the reasoning step of dividing both sides of the equation by 2 is correct, the final answer of x=4 is
inconsistent with the reasoning chain, leading to an incorrect result.

3 HALLUCINATION CAUSES
LLM hallucinations have multifaceted origins, spanning the entire spectrum of LLMs’ capability
acquisition process. In this section, we delve into the root causes of hallucinations in LLMs, primarily
categorized into three key aspects: (1) Data (§3.1), (2) Training (§3.2), and (3) Inference (§3.3).

3.1 Hallucination from Data
Data for training LLMs are comprised of two primary components: (1) pre-training data, through
which LLMs acquire their general capabilities and factual knowledge , and (2) alignment
data, which teach LLMs to follow user instructions and align with human preferences .
Although these data constantly expand the capability boundaries of LLMs, they inadvertently
become the principal contributors to LLM hallucinations. This primarily manifests in three aspects:
the presence of misinformation and biases in the flawed pre-training data sources (§3.1.1), the
knowledge boundary inherently bounded by the scope of the pre-training data (§3.1.2), and the
hallucinations induced by inferior alignment data (§3.1.3).

3.1.1 Misinformation and biases. Neural networks possess an intrinsic tendency to memorize
training data , and this memorization tendency grows with model size . In general, the
inherent memorization capability is a double-edged sword in the fight against hallucinations. On
the one hand, the capacities of LLMs to memorize suggests their potential to capture profound

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:8

Huang, et al.

Table 2. Examples of Hallucinations from Misinformation and Biases. The table categorizes hallucinations
arising from flawed data sources into imitative falsehoods, duplication bias, and social biases. Each category
is accompanied by a premise outlining the data issue, user input, and the LLM’s hallucinatory output, and an
explanation for the occurrence, aiding comprehension of these complex phenomena.

Type

Premise

Input

Model Ouput

Explanation

Imitative
Falsehood

Within the LLM’s
training data, a fre-
referenced
quently
fact
but
appears
multiple
times: Thomas Edison
invented the light bulb.

incorrect

Societal
Biases

data,

In the model’s train-
ing
certain
names may frequently
be associated with
specific nationalities.

Who is credited with the
invention of the light bulb?

Thomas Edison is cred-
ited with the invention
of the light bulb.

Provide a summary of the
following news: Dr. Kim re-
cently published a break-
through paper on quantum
computing. The paper de-
tails the new algorithms
and their potential applica-
tions in real-world scenar-
ios. It has been hailed as
revolutionary by experts.

Dr. Kim from South Ko-
rea recently published
a paper on quantum
computing, detailing
new algorithms and
their applications.

While Edison im-
proved
light
the
bulb’s design, he was
not its sole inventor.
The model’s answer
reflects a misconception
prevalent in its training
data, exemplifying an
imitative falsehood.

The model
erro-
neously added "South
Korea" based on the
name "Kim" revealing
its bias in linking cer-
tain names to specific
nationalities, which can
cause hallucinations in
the summary.

world knowledge. On the other hand, it becomes problematic in the context of misinformation and
biases present within pre-training data and may inadvertently be amplified, manifesting as imitative
falsehood  and the reinforcement of societal biases. For a more comprehensive understanding,
detailed examples are presented in Table 2.

Imitative Falsehood. Misinformation such as fake news and unfounded rumors has been widely
spread among social media platforms and gradually serves as a significant contributor to LLM
hallucinations. The increasing demand for large-scale corpora for pre-training necessitates the
employment of heuristic data collection methods. While facilitating the acquisition of extensive
data, challenges arise in maintaining consistent data quality, which inevitably introduces such
misinformation in pre-training data . In this situation, LLMs will increase the likelihood
of generating such false statements due to their remarkable memorization capabilities, leading to
imitative falsehoods. The issue is further exacerbated by the fact that LLMs have drastically lowered
the barriers to content creation, posing considerable risks to the trustworthiness of public discourse
and internet ecosystems.

Societal Biases. In addition to misinformation, biases are also deeply rooted in social media
platforms, showing up variously, like biased hiring, prejudiced news, and hate-spewing. As a purely
negative phenomenon, biases and hallucinations have obvious differences, mainly manifested
as stereotypes spreading and social inequalities. However, certain biases are intrinsically tied to
hallucinations, especially those related to gender  and nationality . For instance,
LLMs might associate the profession of nursing with females, even when gender isn’t explicitly
mentioned in the user-provided context, exemplifying context inconsistency as discussed in Section
(§2.3). Such biases can be inadvertently acquired from internet-based texts, which are rife with
diverse and biased viewpoints, and subsequently be propagated into the generated content .

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:9

Table 3. Example of Knowledge Boundary. Content marked in Red represents the hallucinatory output.

Type

Input

Model Ouput

Explanation

Long-tail Knowledge

What causes phenylke-
tonuria?

Up-to-date Knowledge

Who will be the Demo-
cratic Party candidate
for the 2024 US presi-
dential election?

Copyright-sensitive
Knowledge

Describe the ending of
the novel "The Secrets
of the Alchemist" that
was published last year.

Phenylketonuria is caused by
a mutation in the hemoglobin
gene, which results in an
interaction with
unusual
hemoglobin,
its
affecting
ability to transport oxygen in
the bloodstream.

The correct cause of phenylke-
tonuria is mutations in the PAH
gene. Such specialized medical
information is less frequently
encountered in the training
corpus, making LLMs struggle
to memorize.

has

an-
Michelle Obama
nounced her candidacy and
is leading in the polls, with
significant endorsements from
key Democratic figures such
as Barack Obama and Bernie
Sanders. She has also outlined
a policy platform focusing on
healthcare reform and climate
change initiatives.

After President Joe Biden an-
nounced his withdrawal from the
race on July 21, 2024, he endorsed
Kamala Harris as the Democratic
Party candidate. Such hallucina-
tions arise because LLMs’ para-
metric knowledge cannot be
updated in real-time, leading
to potential fabrication regard-
ing current events.

In the dramatic conclusion of
"The Secrets of the Alchemist",
the protagonist discovers the
true secret behind the philoso-
pher’s stone, leading to a peace-
ful resolution with all charac-
ters sharing in the wisdom.

The novel "The Secrets of the
Alchemist" is under copyright
protection, and LLMs have not
been trained directly on such
copyrighted materials. Thus,
the model’s output fabricates de-
tails about the book’s ending.

3.1.2 Knowledge Boundary. While the vast pre-training corpora have empowered LLMs with
extensive factual knowledge, they inherently possess knowledge boundaries. These boundaries arise
primarily from two sources: (1) the inability of LLMs to memorize all factual knowledge encountered
during pre-training, especially the less frequent long-tail knowledge; and (2) the intrinsic boundary
of the pre-training data itself, which does not include rapidly evolving world knowledge or content
restricted by copyright laws. Consequently, when LLMs encounter information that falls outside
their limited knowledge boundaries, they are more susceptible to generating hallucinations. We
present detailed examples for clear illustration in Table 3.

Long-tail Knowledge. The distribution of knowledge within the pre-training corpora is in-
herently non-uniform, which results in LLMs demonstrating varying levels of proficiency across
different types of knowledge. Recent studies have highlighted a strong correlation between the
model’s accuracy on general domain questions and the volume of relevant documents  or
entity popularity  within the pre-training corpora. Furthermore, given that LLMs are pre-
dominantly trained on extensive general domain corpora , they may exhibit deficits
in domain-specific knowledge. This limitation becomes particularly evident when LLMs are con-
fronted with tasks that require domain-specific expertise, such as medical  and legal
 questions, these models may exhibit pronounced hallucinations, often manifesting as
factual fabrication.

Up-to-date Knowledge. Beyond the shortfall in long-tail knowledge, another intrinsic limitation
concerning the knowledge boundaries within LLMs is their constrained capacity for up-to-date
knowledge. The factual knowledge embedded within LLMs exhibits clear temporal boundaries

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:10

Huang, et al.

and can become outdated over time . Once these models are trained, their internal
knowledge is never updated. This poses a challenge given the dynamic and ever-evolving nature of
our world. When confronted with queries that transcend their temporal scope, LLMs often resort
to fabricating facts or providing answers that might have been correct in the past but are now
outdated.

Copyright-sensitive Knowledge. Due to licensing restrictions , existing LLMs are legally
constrained to training on corpora that are publicly licensed  or otherwise available for use
without infringing copyright laws . This limitation significantly impacts the breadth and
diversity of knowledge that LLMs can legally acquire. A significant portion of valuable knowledge,
encapsulated in copyrighted materials such as recent scientific research, proprietary data, and
copyrighted literary works, remains inaccessible to LLMs. This exclusion creates a knowledge gap,
leading to potential hallucinations when LLMs attempt to generate information in domains where
their training data is inaccessible .

Inferior Alignment Data. After the pre-training stage, LLMs have embedded substantial
3.1.3
factual knowledge within their parameters, thereby establishing obvious knowledge boundaries.
During the supervised fine-tuning (SFT) stage, LLMs are typically trained on instruction pairs
labeled by human annotators, potentially introducing new factual knowledge that extends beyond
the knowledge boundary established during pre-training. Gekhman et al.  analyzed the training
dynamics of incorporating new factual knowledge during the SFT process and found that LLMs
struggle to acquire such new knowledge effectively. Most importantly, they discovered a correlation
between the acquisition of new knowledge through SFT and increased hallucinations, suggesting
that introducing new factual knowledge encourages LLMs to hallucinate. Additionally, Li et al. 
conducted extensive analysis on the effect of instructions in producing hallucinations. Findings
indicated that task-specific instructions which primarily focus on task format learning, tend to yield
a higher proportion of hallucinatory responses. Moreover, overly complex and diverse instructions
also lead to increased hallucinations.

3.2 Hallucination from Training
As detailed in Section 2.2, the distinct stages of training impart various capabilities to LLMs,
with pre-training focusing on acquiring general-purpose representations and world knowledge,
and alignment enables LLMs to better align with user instructions and preferences. While these
stages are critical for equipping LLMs with remarkable capabilities, shortfalls in either stage can
inadvertently pave the way for hallucinations.

3.2.1 Hallucination from Pre-training. Pre-training constitutes the foundational stage for LLMs,
predominantly utilizing a transformer-based architecture following the paradigm established by
GPT , and further developed by OPT, Falcon , and Llama-2 . This
stage employs a causal language modeling objective, where models learn to predict subsequent
tokens solely based on preceding ones in a unidirectional, left-to-right manner. While facilitating
efficient training, it inherently limits the ability to capture intricate contextual dependencies,
potentially increasing risks for the emergence of hallucination . Moreover, recent research
has exposed that LLMs can occasionally exhibit unpredictable reasoning hallucinations spanning
both long-range and short-range dependencies, which potentially arise from the limitations of
soft attention , where attention becomes diluted across positions as sequence length
increases. Notably, the phenomenon of exposure bias  has been a longstanding and serious
contribution to hallucinations, resulting from the disparity between training and inference in the
auto-regressive generative model. Such inconsistency can result in hallucinations , especially

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:11

when an erroneous token generated by the model cascades errors throughout the subsequent
sequence, akin to a snowball effect .

3.2.2 Hallucination from Supervised Fine-tuning. LLMs have inherent capability boundaries es-
tablished during pre-training. SFT seeks to utilize instruction data and corresponding responses
to unlock these pre-acquired abilities. However, challenges arise when the demands of annotated
instructions exceed the model’s pre-defined capability boundaries. In such cases, LLMs are trained
to fit responses beyond their actual knowledge boundaries. As discussed in §3.1.3, over-fitting
on new factual knowledge encourages LLMs prone to fabricating content, amplifying the risk of
hallucinations . Moreover, another significant reason lies in the models’ inability to reject.
Traditional SFT methods typically force models to complete each response, without allowing them
to accurately express uncertainty . Consequently, when faced with queries that exceed
their knowledge boundaries, these models are more likely to fabricate content rather than reject it.
This misalignment of knowledge boundaries, coupled with the inability to express uncertainty, are
critical factors that contribute to the occurrence of hallucinations during the SFT stage.

3.2.3 Hallucination from RLHF. Several studies  have demonstrated that LLM’s activations
encapsulate an internal belief related to the truthfulness of its generated statements. Nevertheless,
misalignment can occasionally arise between these internal beliefs and the generated outputs. Even
when LLMs are refined with human feedback , they can sometimes produce outputs that
diverge from their internal beliefs. Such behaviors, termed as sycophancy , underscore the
model’s inclination to appease human evaluators, often at the cost of truthfulness. Recent studies
indicate that models trained via RLHF exhibit pronounced behaviors of pandering to user opinions.
Such sycophantic behaviors are not restricted to ambiguous questions without definitive answers
, like political stances, but can also arise when the model chooses a clearly incorrect answer,
despite being aware of its inaccuracy . Delving into this phenomenon, Sharma et al. 
suggested that the root of sycophancy may lie in the training process of RLHF models. By further
exploring the role of human preferences in this behavior, the research indicates that the tendency
for sycophancy is likely driven by both humans and preference models showing a bias towards
sycophantic responses over truthful ones.

3.3 Hallucination from Inference
Decoding plays an important role in manifesting the capabilities of LLMs after pretraining and
alignment. However, certain shortcomings in decoding strategies can lead to LLM hallucinations.

Imperfect Decoding Strategies. LLMs have demonstrated a remarkable aptitude for generating
3.3.1
highly creative and diverse content, a proficiency that is critically dependent on the pivotal role of
randomness in their decoding strategies. Stochastic sampling  is currently the prevailing
decoding strategy employed by these LLMs. The rationale for incorporating randomness into
decoding strategies stems from the realization that high likelihood sequences often result in
surprisingly low-quality text, which is called likelihood trap . The diversity
introduced by the randomness in decoding strategies comes at a cost, as it is positively correlated
with an increased risk of hallucinations . An elevation in the sampling temperature results
in a more uniform token probability distribution, increasing the likelihood of sampling tokens
with lower frequencies from the tail of the distribution. Consequently, this heightened tendency to
sample infrequently occurring tokens exacerbates the risk of hallucinations .

3.3.2 Over-confidence. Prior studies in conditional text generation  have highlighted the
issue of over-confidence which stems from an excessive focus on the partially generated content,
often prioritizing fluency at the expense of faithfully adhering to the source context. While LLMs,

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:12

Huang, et al.

primarily adopting the causal language model architecture, have gained widespread usage, the
over-confidence phenomenon continues to persist. During the generation process, the prediction
of the next word is conditioned on both the language model context and the partially generated
text. However, as demonstrated in prior studies , language models often exhibit a
localized focus within their attention mechanisms, giving priority to nearby words and resulting in
a notable deficit in context attention . Furthermore, this concern is further amplified in LLMs
that exhibit a proclivity for generating lengthy and comprehensive responses. In such cases, there
is even a heightened susceptibility to the risk of instruction forgetting . This insufficient
attention can directly contribute to faithfulness hallucinations, wherein the model outputs content
that deviates from the original context.

Softmax Bottleneck. The majority of language models utilize a softmax layer that operates on
3.3.3
the final layer’s representation within the language model, in conjunction with a word embedding,
to compute the ultimate probability associated with word prediction. Nevertheless, the efficacy
of Softmax-based language models is impeded by a recognized limitation known as the Softmax
bottleneck , wherein the employment of softmax in tandem with distributed word embeddings
constrains the expressivity of the output probability distributions given the context which prevents
LMs from outputting the desired distribution. Additionally, Chang and McCallum  discovered
that when the desired distribution within the output word embedding space exhibits multiple
modes, language models face challenges in accurately prioritizing words from all the modes as the
top next words, which also introduces the risk of hallucination.

3.3.4 Reasoning Failure. Beyond the challenges with long-tail knowledge, effective utilization of
knowledge is inextricably linked with reasoning capabilities. For instance, in multi-hop question-
answering scenarios, even if the LLM possesses the necessary knowledge, it may struggle to produce
accurate results if multiple associations exist between questions, due to its limitations in reasoning
. Furthermore, Berglund et al.  unveiled a specific reasoning failure in LLMs termed the
Reversal Curse. Specifically, while the model can correctly answer when the question is formulated
as "A is B", it exhibits a failed logical deduction when asked the converse "B is A". This discrepancy
in reasoning extends beyond simple deductions.

4 HALLUCINATION DETECTION AND BENCHMARKS
The issue of hallucinations within LLMs has garnered considerable attention, raising concerns
about the reliability of LLMs and their deployment in practical applications. As LLMs become
increasingly adept at generating human-like text, accurately distinguishing between hallucinated
versus factual content becomes increasingly vital. Moreover, effectively measuring the level of
hallucination in LLM is crucial for improving their reliability. Thus, in this section, we delve into
hallucination detection approaches (§4.1) and benchmarks for assessing LLM hallucinations (§4.2).

4.1 Hallucination Detection
Existing strategies for detecting hallucinations in LLMs can be categorized based on the type of
hallucination: (1) factuality hallucination detection, which aims to identify factual inaccuracies in
the model’s outputs, and (2) faithfulness hallucination detection, which focuses on evaluating the
faithfulness of model’s outputs to the contextual information provided.

Factuality Hallucination Detection. Factuality hallucination detection involves assessing
4.1.1
whether the output of LLMs aligns with real-world facts. Typical methods generally fall into
two categories: fact-checking, which involves verifying the factuality of the generated response

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:13

against trusted knowledge sources, and uncertainty estimation, which focuses on detecting factual
inconsistency via internal uncertainty signals.

Fact-checking. Given that the output of LLMs is typically comprehensive and consists of
multiple factual statements, the fact-checking approach is generally divided into two primary steps:
(1) fact extraction, which involves extracting independent factual statements within the model’s
outputs (2) fact verification, which aims at verifying the correctness of these factual statements
against trusted knowledge sources. Depending on the type of knowledge sources employed for
verification, fact-checking methodologies can be broadly categorized into two distinct parts: external
retrieval and internal checking.

• External retrieval: The most intuitive strategy for fact verification is external retrieval. Min et al.
 developed FACTSCORE, a fine-grained factual metric tailored for evaluating long-form
text generation. It first decomposes the generation content into atomic facts and subsequently
computes the percentage supported by reliable knowledge sources. Expanding on this concept,
Chern et al.  proposed a unified framework that equips LLMs with the capability to identify
factual inaccuracies by utilizing a collection of external tools dedicated to evidence gathering.
In addition to retrieving supporting evidence solely based on decomposited claims, Huo et al.
 improved the retrieval process through query expansion. By combining the original
question with the LLM-generated answer, they effectively addressed the issue of topic drift,
ensuring that the retrieved evidence aligns with both the question and the LLM’s response.
• Internal checking: Given the extensive factual knowledge encoded in their parameters, LLMs
have been explored as factual knowledge sources for fact-checking. Dhuliawala et al. 
introduced the Chain-of-Verification (CoVe), where an LLM first generates verification ques-
tions for a draft response and subsequently leverages its parametric knowledge to assess
the consistency of the answer against the original response, thereby detecting potential
inconsistencies.Kadavath et al.  and Zhang et al.  calculates the probability 𝑝 (𝑇𝑟𝑢𝑒)
to assess the factuality of the response to a boolean question, relying exclusively on the
model’sinternal knowledge. Additionally, Li et al.  observed that most atomic statements
are interrelated, some may serve as contextual backgrounds for others, which potentially
leads to incorrect judgments. Thus, they instruct the LLM to directly predict hallucination
judgments considering all factual statements. However, as LLMs are not inherently reliable
factual databases , solely relying on LLMs’ parametric knowledge for fact-checking may
result in inaccurate assessments.

Uncertainty Estimation. While many approaches to hallucination detection rely on external
knowledge sources for fact-checking, several methods have been devised to address this issue in
zero-resource settings, thus eliminating the need for retrieval. The foundational premise behind
these strategies is that the origin of LLM hallucinations is inherently tied to the model’s uncertainty.
Therefore, by estimating the uncertainty of the factual content generated by the model, it becomes
feasible to detect hallucinations. The methodologies in uncertainty estimation can broadly be
categorized into two approaches: based on LLM internal states and LLM behavior, as shown in Fig. 2.
• LLM internal states: The internal states of LLMs can serve as informative indicators of their
uncertainty, often manifested through metrics like token probability or entropy. Varshney et al.
 determined the model’s uncertainty towards key concepts quantified by considering
the minimal token probability within those concepts. The underlying rationale is that a
low probability serves as a strong indicator of the model’s uncertainty, with less influence
from higher probability tokens present in the concept. Similarly, Luo et al.  employed
a self-evaluation-based approach for uncertainty estimation by grounding in the rationale
that a language model’s ability to adeptly reconstruct an original concept from its generated

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:14

Huang, et al.

Fig. 2. Taxonomy of Uncertainty Estimation Methods in Factual Hallucination Detection, featuring a) LLM In-
ternal States and b) LLM Behavior, with LLM Behavior encompassing two main categories: Self-Consistency
and Multi-Debate.

explanation is indicative of its proficiency with that concept. By initially prompting the model
to generate an explanation for a given concept and then employing constrained decoding
to have the model recreate the original concept based on its generated explanation, the
probability score from the response sequence can serve as a familiarity score for the concept.
Furthermore, Yao et al.  interpreted hallucination through the lens of adversarial attacks.
Utilizing gradient-based token replacement, they devised prompts to induce hallucinations.
Notably, they observed that the first token generated from a raw prompt typically exhibits
low entropy, compared to those from adversarial attacks. Based on this observation, they
proposed setting an entropy threshold to define such hallucination attacks.

• LLM behavior: However, when systems are only accessible via API calls ,
access to the output’s token-level probability distribution might be unavailable. Given this
constraint, several studies have shifted their focus to probing a model’s uncertainty, either
through natural language prompts  or by examining its behavioral manifestations.
For instance, by sampling multiple responses from an LLM for the same prompt, Manakul et al.
 detected hallucinations via evaluating the consistency among the factual statements.
However, these methods predominantly rely on direct queries that explicitly solicit informa-
tion or verification from the model. Agrawal et al. , inspired by investigative interviews,
advocated for the use of indirect queries. Unlike direct ones, these indirect counterparts often
pose open-ended questions to elicit specific information. By employing these indirect queries,
consistency across multiple model generations can be better evaluated. Beyond assessing
uncertainty from the self-consistency of a single LLM’s multiple generations, one can embrace
a multi-agent perspective by incorporating additional LLMs. Drawing inspiration from legal
cross-examination practices, Cohen et al.  introduced the LMvLM approach. This strategy
leverages an examiner LM to question an examinee LM, aiming to unveil inconsistencies of
claims during multi-turn interaction.

Faithfulness Hallucination Detection. Ensuring the faithfulness of LLMs to provide context or
4.1.2
user instructions is pivotal for their practical utility in IR applications, from conversational search
to interactive dialogue systems. We categorize existing hallucination detection metrics tailored
to faithfulness into the following groups, with an overview shown in Fig. 3: (1) Fact-based (2)
Classifier-based (3) QA-based (4) Uncertainty-based (5) LLM-based.

Fact-based Metrics. In the realm of assessing faithfulness, one of the most intuitive methods
involves measuring the overlap of pivotal facts between the generated content and the source
content. Given the diverse manifestations of facts, faithfulness can be measured based on n-gram,

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

(1) Self-ConsistencyMount Everest stands as the tallestpeak in the world.As far as I know, the highest peakin the world is Mount Fuji in Japan.The highest peak is Mount Everestlocated in the Himalayas.ConsistencyThe highest peak in the worldis Mount Fuji.I stand corrected, you are right.I must correct you. Mount Fuji is the highestpeak in Japan. The highest peak in the worldis Mount Everest in the Himalayas range.(2) Multi-Debate(a) LLM Internal StatesQuestion: What is the highest peak in the world?Large Language ModelThe highest peak in the world  is Mount Fuji.low uncertaintyhigh uncertainty(b) LLM BehaviorA Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:15

Fig. 3. The illustration of detection methods for faithfulness hallucinations: a) Fact-based Metrics, which
assesses faithfulness by measuring the overlap of facts between the generated content and the source
content; b) Classifier-based Metrics, utilizing trained classifiers to distinguish the level of entailment
between the generated content and the source content; c) QA-based Metrics, employing question-answering
systems to validate the consistency of information between the source content and the generated content; d)
Uncertainty Estimation, which assesses faithfulness by measuring the model’s confidence in its generated
outputs; e) Prompting-based Metrics, wherein LLMs are induced to serve as evaluators, assessing the
faithfulness of generated content through specific prompting strategies.

entities, and relation triples. Traditional n-gram-based metrics, such as BLEU , ROUGE 
and PARENT-T , typically fall short in differentiating the nuanced discrepancies between the
generated content and the source content . Entity-based metrics  make a step further by
calculating the overlap of entities, as any omission or inaccurate generation of these key entities
could lead to an unfaithful response. Notably, even if entities match, the relations between them
might be erroneous. Thus, relation-based metrics  focus on the overlap of relation tuples and
introduce a metric that computes the overlap of relation tuples extracted using trained end-to-end
fact extraction models.

Classifier-based Metrics. Beyond computing fact overlap, another straightforward approach
to assessing the faithfulness of the model generation involves utilizing classifiers trained on data
from related tasks such as natural language inference (NLI) and fact-checking, or data comprised of
synthetically task-specific hallucinated and faithful content. A foundational principle for assessing
the faithfulness of generated text is anchored on the idea that genuinely faithful content should
inherently be entailed by its source content. In line with this, numerous studies  have
trained classifiers on NLI datasets to identify factual inaccuracies, especially in the context of
abstract summarization. However, Mishra et al.  highlighted that the mismatch in input
granularity between conventional NLI datasets and inconsistency detection datasets limits their
applicability for effectively detecting inconsistencies. Building on this, more advanced studies have
proposed methods such as fine-tuning on adversarial datasets , decomposing the entailment
decisions at the dependency arc level , and segmenting documents into sentence units then

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

LLM GenerationUser QueryInformationExtractionInformationExtractionFactsFact OverlapQuestionAnsweringAnswerOverlapQuestionGenerationAnswersAnswerSelectionAnswersQuestionsbinaryjudgementk-pointLikert scaleLLM GenerationUser QueryEntailmentNLI ModelLLM GenerationUser QueryUser QueryLLM Generation(a) Fact-based Metric(b) Classiﬁer-based Metric(e) Prompting-based Metric(c) QA-based Metric(d) Uncertainty EstimationFactsUser QueryEOSLarge Language Modelhigh uncertaintylow uncertainty1:16

Huang, et al.

aggregating scores between sentence pairs . While using data from related tasks to fine-
tune the classifier has shown promise in evaluating faithfulness, it’s essential to recognize the
inherent gap between related tasks and the downstream task. The scarcity of annotated data further
constrains their applicability. In response to this challenge, a surge of research explores leveraging
data-augmentation methods to construct synthetical data for fine-tuning the classifier, either by
rule-based perturbation  or generation .

QA-based Metrics. In contrast to classifier-based metrics, QA-based metrics 
have recently garnered attention for their enhanced ability to capture information overlap between
the model’s generation and its source. These metrics operate by initially selecting target answers
from the information units within the LLM’s output, and then questions are generated by the
question-generation module. The questions are subsequently used to generate source answers based
on the user context. Finally, the faithfulness of the LLM’s responses is calculated by comparing
the matching scores between the source and target answers. Although these methodologies share
a common thematic approach, they exhibit variability in aspects like answer selection, question
generation, and answer overlap, leading to diverse performance outcomes. Building on this founda-
tional work, Fabbri et al.  conducted an in-depth evaluation of the components within QA-based
metrics, yielding further enhancements in faithfulness evaluation.

Uncertainty-based Metrics. Drawing parallels with the uncertainty-based approaches em-
ployed for detecting factuality hallucinations (§4.1.1), the application of uncertainty estimation
in assessing faithfulness has been widely explored, typically characterized by entropy and log-
probability. For entropy-based uncertainty, Xiao and Wang  has revealed a positive correlation
between hallucination likelihood in data-to-text generation and predictive uncertainty, which is
estimated by deep ensembles . In a related vein, Guerreiro et al.  leveraged the variance
in hypotheses yielded by Monte Carlo Dropout  as an uncertainty measure within neural
machine translation. More recently, van der Poel et al.  employed conditional entropy 
to assess model uncertainty in abstractive summarization. Regarding log-probability, it can be
applied at different levels of granularity, such as word or sentence level. Notably, several studies
 have adopted length-normalized sequence log-probability to measure model confi-
dence. Furthermore, considering the hallucinated token can be assigned high probability when the
preceding context contains the same hallucinated information, Zhang et al.  focused on the
most informative and important keywords and introduced a penalty mechanism to counteract the
propagation of hallucinated content.

LLM-based Judgement. Recently, the remarkable instruction-following ability of LLMs has
underscored their potential for automatic evaluation . Exploiting this capability,
researchers have ventured into novel paradigms for assessing the faithfulness of model-generated
content . By providing LLMs with concrete evaluation guidelines and feeding
them both the model-generated and source content, they can effectively assess faithfulness. The
final evaluation output can either be a binary judgment on faithfulness  or a k-point Likert
scale indicating the degree of faithfulness . For prompt selection, evaluation prompt can either
be direct prompting, chain-of-thought prompting , using in-context-learning  or allowing
the model to generate evaluation results accompanying with explanations .

4.2 Hallucination Benchmarks
In this section, we present a comprehensive overview of existing hallucination benchmarks, which
can be categorized into two primary domains: Hallucination Evaluation Benchmarks (§4.2.1), which
assess the extent of hallucinations generated by existing cutting-edge LLMs, and Hallucination

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:17

Detection Benchmarks (§4.2.2), designed specifically to evaluate the performance of existing hallu-
cination detection methods. Collectively, these benchmarks establish a unified framework, enabling
a nuanced and thorough exploration of hallucinatory patterns in LLMs.

Table 4. An overview of existing hallucination benchmarks. For Attribute, Factuality and Faithfulness
represent whether the benchmark is used to evaluate LLM’s factuality or to detect faithfulness hallucination,
and Manual represents whether the inputs in the data are handwritten.

Benchmark

Datasets

Data Size

Language

Factuality

Faithfulness Manual

Task Type

Input

Label

Metric

Attribute

Task

TruthfulQA

REALTIMEQA

SelfCheckGPT-Wikibio

-

-

-

817

English

Dynamic

English

1,908

English

HaluEval

Med-HALT

FACTOR

BAMBOO

Task-specific
General

-

Wiki-FACTOR
News-FACTOR

SenHallu
AbsHallu

ChineseFactEval

-

Misleading
Misleading-hard
Knowledge

Never-changing
Slow-changing
Fast-changing
False-premise

30,000
5,000

4,916

2,994
1,036

200
200

125

175
69
206

150
150
150
150

English
English

Multilingual

English
English

English
English

Chinese

Chinese
Chinese
Chinese

English
English
English
English

HaluQA

FreshQA

FELM

PHD

RealHall

LSum

SAC3

HaluEval 2.0

ScreenEval

-

-

3,948

English

PHD-LOW
PHD-Meidum
PHD-High

COVID-QA
DROP
Open Assistant
TriviaQA

100
100
100

52

N/A
N/A
N/A
N/A

English
English
English

English

English
English
English
English

-

6,166

English

HotpotQA
NQ-Open

Biomedicine
Finance
Science
Education
Open domain

250
250

1,535
1,125
1,409
1,701
3,000

English
English

English
English
English
English
English

✔

✔

✗

✗
✗

✔

✔
✔

✗
✗

✔

✔
✔
✔

✔
✔
✔
✔

✔

✗
✗
✗

✗

✗
✗
✗
✗

✗

✗
✗

✔
✔
✔
✔
✔

✗

✗

✔

✔
✔

✗

✗
✗

✔
✔

✗

✗
✗
✗

✗
✗
✗
✗

✔

✔
✔
✔

✔

✔
✔
✔
✔

✔

✔
✔

✗
✗
✗
✗
✗

✔

✔

✗

✗
✗

✗

✗
✗

✗
✗

✔

✔
✔
✔

✔
✔
✔
✔

✗

✗
✗
✗

✗

✗
✗
✗
✗

✗

✗
✗

✗
✗
✗
✗
✗

Generative QA
Multi-Choice QA

Multi-Choice QA
Generative QA

Question

Answer

Question

Answer

LLM-Judge &
Human

Acc
EM & F1

Detection

Detection
Detection

Paragraph &
Concept

Passage

AUROC

Query
Task Input

Response
Response

Acc
Acc

Multi-Choice QA

Question

Choice

Multi-Choice QA
Multi-Choice QA

Question
Question

Answer
Answer

Detection
Detection

Paper
Paper

Summary
Summary

Pointwise Score
& Acc

likelihood
likelihood

P & R & F1
P & R & F1

Generative QA

Question

-

Score

Generative QA
Generative QA
Generative QA

Generative QA
Generative QA
Generative QA
Generative QA

Question
Question
Question

Question
Question
Question
Question

Answer
Answer
Answer

Answer
Answer
Answer
Answer

Detection

Question

Response

Detection
Detection
Detection

Entity
Entity
Entity

Response
Response
Response

LLM-Judge
LLM-Judge
LLM-Judge

Human
Human
Human
Human

Balanced
Acc & F1

P & R & F1
P & R & F1
P & R & F1

Detection

Document

Summary

AUROC

Detection
Detection
Detection
Detection

Question
Question
Question
Question

Answer
Answer
Answer
Answer

AUROC
AUROC
AUROC
AUROC

Detection

Document

Summary

Balanced Acc

Detection
Detection

Generative QA
Generative QA
Generative QA
Generative QA
Generative QA

Question
Question

Question
Question
Question
Question
Question

Answer
Answer

Answer
Answer
Answer
Answer
Answer

AUROC
AUROC

MiHR & MaHR
MiHR & MaHR
MiHR & MaHR
MiHR & MaHR
MiHR & MaHR

4.2.1 Hallucination Evaluation Benchmarks. Hallucination evaluation benchmarks are devised
to quantify the tendency of LLMs to generate hallucinations, particularly emphasizing factual
inaccuracies and inconsistency from the given contexts. Given the adeptness of LLMs at memorizing
high-frequency count knowledge, the primary focus of current hallucination evaluation benchmarks
targets long-tailed knowledge and challenging questions that can easily elicit imitative falsehood.
As for evaluating, these benchmarks typically utilize multiple choice QA, where performance is
measured through accuracy metrics, or generative QA, evaluated either through human judgment
or scores given by proxy models.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:18

Huang, et al.

Long-tail Factual Knowledge. The selection criteria for gathering long-tail factual question-
answering samples typically include the frequency of appearance, recency, and specific domains.
Regarding the frequency of appearance, benchmarks such as PopQA  and Head-to-Tail 
are constructed based on entity popularity derived directly from Wikipedia. Considering that world
knowledge is constantly evolving, it becomes crucial to validate the LLM’s factuality concerning
the current world. Among benchmarks characterized by ever-changing, REALTIMEQA  and
FreshQA  stands out. REALTIMEQA offers real-time, open-domain multiple-choice questions
that are regularly updated to reflect the latest developments. These questions are derived from newly
published news articles, encompassing a broad spectrum of topics, including politics, business,
sports, and entertainment. Similarly, FreshQA challenges LLMs with questions designed to represent
varying degrees of temporal change—categorized into never-changing, slow-changing, and fast-
changing world knowledge. This benchmark is further enriched by including questions based on
false premises, requiring debunking, thus comprising a total of 600 meticulously hand-crafted
questions. Moreover, long-tail knowledge often pertains to specific domains. For instance, Med-
HALT  is distinguished by its focus on the medical domain, challenging LLMs with multiple-
choice questions derived from a variety of countries. Additionally, Malaviya et al.  collected
expert-curated questions across 32 fields of study, resulting in a high-quality long-form QA dataset
with 2,177 questions.

Imitative Falsehood Knowledge. Imitative falsehood knowledge is specifically designed to
challenge LLMs through adversarial prompting. This approach crafts questions in such a way
that they are prone to misleading LLMs due to false beliefs or misconceptions. The two most
representative benchmarks are TruthfulQA  and HalluQA . TruthfulQA comprises 817
questions that span 38 diverse categories, such as health, law, finance, and politics. Crafted using an
adversarial methodology, it aims to elicit "imitative falsehoods"—misleading responses that models
might generate due to their frequent presence in training data. The benchmark is divided into two
parts, one of which contains manually curated questions that were further refined by filtering out
those correctly answered by GPT-3, resulting in 437 filtered questions. The other part includes
380 unfiltered non-adversarial questions. Drawing from the construction approach of TruthfulQA,
HalluQA is crafted to specifically assess hallucinations in Chinese LLMs, focusing on imitative
falsehoods and factual errors. The benchmark comprises 450 handcrafted adversarial questions
across 30 domains and is categorized into two parts. The misleading section captures questions that
successfully deceive GLM-130B, while the knowledge section retains questions that both ChatGPT
and Puyu consistently answer incorrectly. To comprehensively evaluate LLM hallucinations across
various domains, Li et al.  constructed an upgraded hallucination evaluation benchmark,
HaluEval 2.0, based on . This benchmark includes 8,770 questions that LLMs are prone to
hallucination, across five domains: biomedicine, finance, science, education, and open domain.

4.2.2 Hallucination Detection Benchmarks. For hallucination detection benchmarks, most prior
studies have primarily concentrated on task-specific hallucinations, such as abstractive summariza-
tion , data-to-text, and machine translation . However,
the content generated in these studies often originates from models with lesser capabilities, such
as BART  and PEGASUS . As a result, they may not accurately reflect the effectiveness of
hallucination detection strategies, underlining the necessity for a significant shift toward developing
benchmarks that encapsulate more complex scenarios reflective of the era of LLMs.

For example, SelfCheckGPT-Wikibio  offers a sentence-level dataset created by generating
synthetic Wikipedia articles with GPT-3, manually annotated for factuality, highlighting the chal-
lenge of detecting hallucinations in the biography domain. Complementing this, HaluEval 
combines automated generation with human annotation to evaluate LLMs’ ability to recognize

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:19

hallucinations across 5,000 general user queries and 30,000 task-specific samples, leveraging a
"sampling-then-filtering" approach. Building upon existing research predominantly focused on
short documents, BAMBOO  and ScreenEval  extend the scope in long-form hallucination
detection. Further, FELM , distinguishes itself by assessing factuality across diverse domains
including world knowledge, science, and mathematics, producing 817 samples annotated for various
facets of factual accuracy, thereby addressing the need for cross-domain evaluation of factuality
in LLM-generated content. On a different note, PHD , shifts the focus towards passage-level
detection of non-factual content by analyzing entities from Wikipedia, thus offering a nuanced
view on the knowledge depth of LLMs. RealHall  and SAC3  align closely with real-world
applications focusing on open-domain question-answering, whereas LSum  concentrating on
summarization tasks.

5 HALLUCINATION MITIGATION
In this section, we present a comprehensive review of contemporary methods aimed at mitigating
hallucinations in LLMs. Drawing from insights discussed in Hallucination Causes (§3), we systemat-
ically categorize these methods based on the underlying causes of hallucinations. Specifically, we
focus on approaches addressing Data-related Hallucinations (§5.1), Training-related Hallucinations
(§5.2) and Inference-related Hallucinations (§5.3), each offering tailored solutions to tackle specific
challenges inherent to their respective cause.

5.1 Mitigating Data-related Hallucinations
As analyzed in §3.1, data-related hallucinations generally emerge as a byproduct of misinformation,
biases, and knowledge gaps, which are fundamentally rooted in the pre-training data. Several
methods are proposed to mitigate such hallucinations, primarily categorized into three distinct
parts: (1) data filtering aiming at selecting high-quality data to avoid introducing misinformation and
biases, (2) model editing focusing on injecting up-to-date knowledge by editing model’s parameters,
and (3) retrieval-augmented generation leveraging external non-parametric database for knowledge
supplying.

5.1.1 Data Filtering. To reduce the presence of misinformation and biases, an intuitive approach
involves the careful selection of high-quality pre-training data from reliable sources. In this way, we
can ensure the factual correctness of data while also minimizing the introduction of social biases. As
early as the advent of GPT-2, Radford et al.  underscored the significance of exclusively scraping
web pages that had undergone rigorous curation and filtration by human experts. However, as
pre-training datasets continue to scale, manual curation becomes a challenge. Given that academic
or specialized domain data is typically factually accurate, gathering high-quality data emerges as a
primary strategy. Notable examples include the Pile  and “textbook-like” data sources .
Additionally, up-sampling factual data during the pre-training phase has been proven effective in
enhancing the factual correctness of LLMs , thus alleviating hallucination.

In addition to strictly controlling the source of data, deduplication serves as a crucial procedure.
Existing practices typically fall into two categories: exact duplicates and near-duplicates. For exact
duplicates, the most straightforward method involves exact substring matching to identify identical
strings. However, given the vastness of pre-training data, this process can be computationally
intensive, a more efficient method utilizes the construction of a suffix array , enabling effective
computation of numerous substring queries in linear time. Regarding near-duplicates, the identifi-
cation often involves approximate full-text matching, typically utilizing hash-based techniques to
identify document pairs with significant n-gram overlap. Furthermore, MinHash  stands out as
a prevalent algorithm for large-scale deduplication tasks . Additionally, SemDeDup  makes

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:20

Huang, et al.

use of embeddings from pre-trained models to identify semantic duplicates, which refers to data
pairs with semantic similarities but not identical.

Discussion. Since data filtering works directly at the source of hallucinations, it effectively
mitigates hallucinations by ensuring the use of high-quality, factually accurate sources. Despite
its effectiveness, the efficiency and scalability of current data filtering methods pose significant
challenges as data volumes expand. Additionally, these methods often overlook the influence
of LLM-generated content, which can introduce new risks and inaccuracies. To advance, future
research must focus on developing more efficient, automated data filtering algorithms that can
keep pace with the rapid expansion of datasets and the complexities of LLM-generated content.

5.1.2 Model Editing. Model editing  has garnered rising attention from researchers,
which aims to rectify model behavior by incorporating additional knowledge. Current model editing
techniques can be categorized into two classes: locate-then-edit and meta-learning.

Locate-then-edit. Locate-then-edit methods  consist of two stages, which first locate the
“buggy” part of the model parameters and then apply an update to them to alter the model’s behavior.
For example, ROME  located the edits-related layer by destroying and subsequently restoring
the activations and then updates the parameters of FFN in a direct manner to edit knowledge. MEMIT
 employed the same knowledge locating methods as ROME, enabling the concurrent updating
of multiple layers to facilitate the simultaneous integration of thousands of editing knowledge.
However, Yao et al.  found that these methods lack non-trivial generalization capabilities
and varying performance and applicability to different model architectures. The best-performing
methods ROME and MEMIT empirically only work well on decoder-only LLMs.

Meta-learning. Meta-learning methods  train an external hyper-network to predict the
weight update of the original model. Nevertheless, meta-learning methods often require additional
training and memory cost, where MEND  utilized a low-rank decomposition with a specialized
design to reduce the size of hyper-networks. Notably, MEND would exhibit a cancellation effect,
where parameter shifts corresponding to different keys significantly counteract each other. MAL-
MEN  further addressed this issue by framing the parameter shift aggregation as a least squares
problem rather than a simple summation, thereby greatly enhancing its capacity for extensive
editing. While these methods can fine-grainedly adjust the behavior of the model, modifications to
the parameters could have a potentially harmful impact on the inherent knowledge of the model.
Discussion. Model editing provides a precise way to mitigate hallucinations induced by specific
misinformation without extensive retraining. However, these methods struggle with large-scale
updates and can adversely affect the model’s overall performance, particularly when continuous
edits are applied. Consequently, future research should focus on improving model editing to handle
large-scale knowledge updates more efficiently and address hallucinations caused by social biases.

5.1.3 Retrieval-Augmented Generation. Typically, retrieval-augmented generation (RAG)  follows a retrieve-then-read pipeline, where relevant knowledge is firstly retrieved by a retriever
 from external sources, and then the final response is generated by a generator conditioning
on both user query and retrieved documents. By decoupling external knowledge from LLM, RAG
can effectively alleviate the hallucination caused by the knowledge gap without affecting the
performance of LLM. Common practices can be divided into three parts, as shown in Fig 4: one-time
retrieval, iterative retrieval, and post-hoc retrieval, depending on the timing of retrieval.

One-time Retrieval. One-time retrieval aims to directly prepend the external knowledge
obtained from a single retrieval to the LLMs’ prompt. Ram et al.  introduced In-context RALM,
which entails a straightforward yet effective strategy of prepending chosen documents to the
input text of LLMs. Beyond conventional knowledge repositories such as Wikipedia, ongoing
research endeavors have explored alternative avenues, specifically the utilization of knowledge

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:21

Fig. 4. The illustration of three distinct approaches for Retrieval-Augmented Generation: a) One-time
Retrieval, where relevant information is retrieved once before text generation; b) Iterative Retrieval,
involving multiple retrieval iterations during text generation for dynamic information integration; and c)
Post-hoc Retrieval, where the retrieval process happens after an answer is generated, aiming to refine and
fact-check the generated content.

graphs (KGs). These KGs serve as a pivotal tool for prompting LLMs, facilitating their interaction
with the most recent knowledge, and eliciting robust reasoning pathways . Varshney
et al.  introduce the Parametric Knowledge Guiding (PKG) framework, enhancing LLMs with
domain-specific knowledge. PKG employs a trainable background knowledge module, aligning it
with task knowledge and generating relevant contextual information.

Iterative Retrieval. When confronted with intricate challenges like multi-step reasoning 
and long-form question answering , traditional one-time retrieval may fall short. Address-
ing these demanding information needs, recent studies have proposed iterative retrieval, which
allows for continuously gathering knowledge throughout the generation process. Recognizing the
substantial advancements chain-of-thought prompting  has brought to LLMs in multi-step
reasoning, numerous studies  try to incorporate external knowledge at each reasoning
step and further guide retrieval process based on ongoing reasoning, reducing factual errors in
reasoning chains. Building upon chain-of-thought prompting, Press et al.  introduced self-ask.
Diverging from the conventional continuous, undelineated chain-of-thought prompting, self-ask
delineates the question it intends to address at each step, subsequently incorporating a search
action based on the follow-up question. Instead of solely depending on chain-of-thought prompting
for retrieval guidance, both Feng et al.  and Shao et al.  employed an iterative retrieval-
generation collaborative framework, where a model’s response serves as an insightful context to
procure more relevant knowledge, subsequently refining the response in the succeeding iteration.
Beyond multi-step reasoning tasks, Jiang et al.  shifted their emphasis to long-form generation.
They proposed an active retrieval augmented generation framework, which iteratively treats the
upcoming prediction as a query to retrieve relevant documents. If the prediction contains tokens
of low confidence, the sentence undergoes regeneration. In addition to using iterative retrieval
to improve intermediate generations, Zhang et al.  presented MixAlign, which iteratively
refines user questions using model-based guidance and seeking clarifications from users, ultimately
enhancing the alignment between questions and knowledge.

Post-hoc Retrieval. Beyond the traditional retrieve-then-read paradigm, a line of work has
delved into post-hoc retrieval, refining LLM outputs through subsequent retrieval-based revisions.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

RetrieveQueryGenerateLarge LanguageModelAnswerQueryAnswerQueryGenerateLarge LanguageModelRertieveAnswerReviseRevisior(a) One-time Retrieval(b) Iterative Retrieval(c) Post-hoc Retrieval...LLMOutputLLMOutputIteration Iteration ...Revision1:22

Huang, et al.

To enhance the trustworthiness and attribution of LLMs, Gao et al.  adopted the research-
then-revise workflow, which initially research relevant evidence and subsequently revise the initial
generation based on detected discrepancies with the evidence. Similarly, Zhao et al.  introduced
the verify-and-edit framework to enhance the factual accuracy of reasoning chains by incorporating
external knowledge. For reasoning chains that show lower-than-average consistency, the framework
generates verifying questions and then refines the rationales based on retrieved knowledge, ensuring
a more factual response. Yu et al.  enhanced the post-hoc retrieval method through diverse
answer generation. Instead of generating just a single answer, they sample various potential
answers, allowing for a more comprehensive retrieval feedback. Additionally, by employing an
ensembling technique that considers the likelihood of the answer before and after retrieval, they
further mitigate the risk of misleading retrieval feedback.

Discussion. One crucial advantage of retrieval-augmented generation methodology is its ef-
fectiveness in mitigating hallucinations caused by knowledge gaps, and their generality, which
allows for application across any domain. This flexibility is further enhanced by the modularity of
the approach, treating external knowledge bases like plug-ins that can be swapped or modified as
needed. In terms of the drawbacks, it can be easily impacted by irrelevant retrievals, which may
decrease the overall performance by introducing noise or incorrect information into the response
generation process. Furthermore, the current paradigm exhibits shallow interactions between the
retriever and generator components, leading to suboptimal knowledge utilization. Hence, future
research should focus on developing a robust RAG system that minimizes the impact of irrelevant
retrieval, as well as integrating adaptive learning components that can dynamically adjust retrieval
strategies based on the context of the query and the performance of previous interactions.

5.2 Mitigating Training-related Hallucination
Training-related hallucinations typically arise from the intrinsic limitations of the architecture
and training strategies adopted by LLMs. In this context, we discuss various optimization methods
ranging from training stages (§5.2.1) and alignment stages (SFT & RLHF) (§5.2.2), aiming to mitigate
hallucinations within the training process.

5.2.1 Mitigating Pretraining-related Hallucination. One significant avenue of research in mitigat-
ing pretraining-related hallucination centers on the limitations inherent in model architectures,
especially unidirectional representation and attention glitches. In light of this, numerous studies have
delved into designing novel model architectures specifically tailored to address these flaws. To
address the limitations inherent in unidirectional representation, Li et al.  introduced BATGPT
which employs a bidirectional autoregressive approach. This design allows the model to predict
the next token based on all previously seen tokens, considering both past and future contexts, thus
capturing dependencies in both directions. Building on this idea, Liu et al.  highlighted the
potential of encoder-decoder models to make better use of their context windows, suggesting a
promising direction for future LLMs architecture design. Besides, recognizing the limitations of soft
attention within self-attention-based architecture, Liu et al.  proposed attention-sharpening
regularizers. This plug-and-play approach specifies self-attention architectures using differentiable
loss terms  to promote sparsity, leading to a significant reduction in reasoning hallucinations.
In the pre-training phase of LLMs, the choice of objective plays a pivotal role in determining the
model’s performance. However, conventional objectives can lead to fragmented representations and
inconsistencies in model outputs. Recent advancements have sought to address these challenges by
refining pre-training strategies, ensuring richer context comprehension, and circumventing biases.
Addressing the inherent limitations in training LLMs, where unstructured factual knowledge at a
document level often gets chunked due to GPU memory constraints and computational efficiency,

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:23

leading to fragmented information and incorrect entity associations, Lee et al.  introduced a
factuality-enhanced training method. By appending a TOPICPREFIX to each sentence in factual
documents, the approach transforms them into standalone facts, significantly reducing factual
errors and enhancing the model’s comprehension of factual associations. Similarly, considering that
randomly concatenating shorter documents during pre-training might introduce inconsistencies in
model outputs, Shi et al.  proposed In-Context Pretraining, an innovative approach in which
LLMs are trained on sequences of related documents. By altering the document order, this method
aims to maximize similarity within the context windows. It explicitly encourages LLMs to reason
across document boundaries, potentially bolstering the logical consistency between generations.
Discussion. Strategies designed to mitigate pretraining-related hallucinations typically are
fundamental, potentially yielding significant improvements. However, they typically involve modifi-
cations to pre-training architectures and objectives, which are computationally intensive. Moreover,
these integrations may lack broad applicability. Moving forward, the focus should be on developing
adaptable and efficient strategies that can be universally applied without extensive system overhaul.

5.2.2 Mitigating Misalignment Hallucination. Hallucinations induced during alignment often stem
from capability misalignment and belief misalignment. However, defining the knowledge boundary
of LLMs proves challenging, making it difficult to bridge the gap between LLMs’ inherent capabilities
and the knowledge presented in human-annotated data. While limited research addresses capability
misalignment, the focus mainly shifts toward belief misalignment.

Hallucinations stemming from belief misalignment often manifest as sycophancy, a tendency of
LLMs to seek human approval in undesirable ways. This sycophantic behavior can be attributed to
the fact that human preference judgments often favor sycophantic responses over more truthful
ones , paving the way for reward hacking . To address this, a straightforward strategy is
to improve human preference judgments and, by extension, the preference model. Recent research
 has investigated the use of LLMs to assist human labelers in identifying overlooked flaws.
Additionally, Sharma et al.  discovered that aggregating multiple human preferences enhances
feedback quality, thereby reducing sycophancy.

Besides, modifications to LLMs’ internal activations have also shown the potential to alter model
behavior. This can be achieved through methods like fine-tuning  or activation steering during
inference . Specifically, Wei et al.  proposed a synthetic-data intervention, fine-
tuning language models using synthetic data where the claim’s ground truth is independent of a
user’s opinion, aiming to reduce sycophantic tendencies.

Another avenue of research  has been to mitigate sycophancy through activation
steering. This approach involves using pairs of sycophantic/non-sycophantic prompts to generate
the sycophancy steering vector, derived from averaging the differences in intermediate activations.
During inference, subtracting this vector can produce less sycophantic LLM outputs.

Discussion. Mitigating hallucinations through post-training methods represents a direct and
effective approach, bypassing the complexities associated with data sourcing and pre-training.
However, a notable gap in current research is the limited attention given to capability misalign-
ment within LLMs. Future research should prioritize understanding the knowledge boundaries in
capability alignment to address hallucinations effectively.

5.3 Mitigating Inference-related Hallucination
Decoding strategies in LLMs play a pivotal role in determining the factuality and faithfulness of
the generated content. However, as analyzed in Section §3.3, imperfect decoding often results in
outputs that might lack factuality or stray from the original context. In this subsection, we explore

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:24

Huang, et al.

two advanced strategies aimed at refining the decoding strategy to enhance both the factuality and
faithfulness of the LLMs’ outputs.

Factuality Enhanced Decoding. Factuality Enhanced Decoding aims to improve the reliability
5.3.1
of outputs from LLMs by prioritizing the factuality of the information they generate. This line
of methods focuses on aligning model outputs closely with established real-world facts, thereby
minimizing the risk of disseminating false or misleading information.

Factuality Decoding. Considering the randomness in the sampling process can introduce
non-factual content into open-ended text generation, Lee et al.  introduced the factual-nucleus
sampling algorithm that dynamically adjusts the nucleus probability 𝑝 throughout sentence genera-
tion. By dynamically adjusting the nucleus probability based on decay factors and lower boundaries
and resetting the nucleus probability at the beginning of every new sentence, the decoding strategy
strikes a balance between generating factual content and preserving output diversity. Moreover,
some studies  posit that the activation space of LLMs contains interpretable structures
related to factuality. Building on this idea, Li et al.  introduced Inference-Time Intervention
(ITI). This method first identifies a direction in the activation space associated with factually cor-
rect statements and then adjusts activations along the truth-correlated direction during inference.
By repeatedly applying such intervention, LLMs can be steered towards producing more factual
responses. Similarly, Chuang et al.  delved into enhancing the factuality of LLM’s decoding
process from a perspective of factual knowledge storage. They exploit the hierarchical encoding
of factual knowledge within transformer LLMs, noting that lower-level information is captured
in earlier layers and semantic information in the later ones. Drawing inspiration from , they
introduce DoLa, a strategy that dynamically selects and contrasts logits from different layers to
refine decoding factuality. By placing emphasis on knowledge from higher layers and downplaying
that from the lower layers, DoLa showcases its potential to make LLMs more factual, thus reducing
hallucinations.

Post-editing Decoding. Unlike methods that directly modify the probability distribution to
prevent hallucinations during the initial decoding, post-editing decoding seeks to harness the
self-correction capabilities of LLMs  to refine the originally generated content without relying
on an external knowledge base. Dhuliawala et al.  introduced the Chain-of-Verification (COVE),
which operates under the assumption that, when appropriately prompted, LLMs can self-correct
their mistakes and provide more accurate facts. Starting with an initial draft, it first formulates
verification questions and then systematically answers those questions in order to finally produce an
improved revised response. Similarly, Ji et al.  focused on the medical domain and introduced
an iterative self-reflection process. This process leverages the inherent ability of LLMs to first
generate factual knowledge and then refine the response until it aligns consistently with the
provided background knowledge.

Discussion. Factuality decoding methods, which typically assess the factuality at each decoding
step, can offer substantial improvements. Furthermore, due to their plug-and-play nature, they
allow for application without the need for computation-intensive training. Nevertheless, one of
the primary limitations of these methods lies in balancing factual accuracy with maintaining the
diversity and informativeness of the generated content, which can sometimes lead to compromises
in either aspect. On the other hand, post-editing decoding strategies, despite their effectiveness,
heavily rely on the self-correction capabilities of LLMs, which may be unreliable. Furthermore,
applying self-reflection can be time-consuming, limiting their practicality for real-time applications.
Hence, it is crucial to achieve an optimal balance between factuality and computational efficiency.

Faithfulness Enhanced Decoding. On the other hand, Faithfulness Enhanced Decoding priori-
5.3.2
tizes alignment with the provided context and also emphasizes enhancing the consistency within

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:25

the generated content. Thus, in this section, we summarize existing work into two categories,
including Context Consistency and Logical Consistency.

Context Consistency. In the era of LLMs, the issue of faithfulness hallucination typically lies in
insufficient attention to the given context, which inspired numerous research to design inference-
time strategies to enhance context consistency. Shi et al.  proposed context-aware decoding
(CAD), which modifies the model’s original output distribution in a contrastive formulation .
By amplifying the difference between output probabilities with and without context, CAD encour-
ages the LLM to focus more on contextual information rather than over-rely on prior knowledge.
However, due to the inherent trade-off between diversity and context attribution , overem-
phasizing contextual information can reduce diversity. To address this, Chang et al.  introduced
a dynamic decoding algorithm to bolster faithfulness while preserving diversity. Specifically, the
algorithm involves two parallel decoding steps, one with the context and one without. During the
decoding, the KL divergence between two token distributions serves as a guiding signal, indicating
the relevance of the source context. This signal is utilized to dynamically adjust the sampling
temperature to improve source attribution when the source is relevant. In a parallel line of work,
Choi et al.  introduced knowledge-constrained decoding (KCD), which employed a token-level
hallucination detection discriminator to identify contextual hallucinations and then guides the
faithful generation process by reweighing the token distribution. In addition to modifying output
distribution in place to enhance contextual attention, another line of work has explored a generic
post-edit approach to enhance faithfulness. Gao et al.  adopted a research-and-revise workflow,
where the research stage raises questions about various aspects of the model’s initial response and
gathers evidence for each query, while the revision stage detects and revises any disagreements
between the model’s response and the evidence. Similarly, Lei et al.  first detected contex-
tual hallucinations at both the sentence and entity levels and then incorporated the judgments
to refine the generated response. Moreover, several studies have explored methods to overcome
the softmax bottleneck, which constrains the expression of diversity and faithful representations.
These approaches include employing a mixture of Softmax, which uses multiple hidden states to
compute softmax multiple times and merge the resulting distributions  and incorporating
pointer networks, which enables LLMs to copy the context words , thereby reducing context
hallucinations.

Logical Consistency. Inspired by the human thinking process, chain-of-thought  has been
introduced to encourage LLMs to decompose complex problems into explicitly intermediate steps,
thereby enhancing the reliability of the reasoning process . Despite effective, recent research
 demonstrated that the intermediate rationales generated by LLMs do not faithfully capture
their underlying behavior. A branch of research has been inspired to improve the consistency of
intermediate rationales generated by LLMs, particularly in multi-step reasoning  and logical
reasoning . To enhance the self-consistency in chain-of-thought, Wang et al.  employed
a knowledge distillation framework. They first generate a consistent rationale using contrastive
decoding  and then fine-tune the student model with a counterfactual reasoning objective,
which effectively eliminates reasoning shortcuts  that derive answers without considering the
rationale. Furthermore, by employing contrastive decoding directly, LLMs can reduce surface-level
copying and prevent missed reasoning steps . In addition, Li et al.  conducted a deep
analysis of the causal relevance among the context, CoT, and answer during unfaithful reasoning.
Analysis revealed that the unfaithfulness issue lies in the inconsistencies in the context information
obtained by the CoT and the answer. To address this, they proposed inferential bridging, which takes
the attribution method to recall contextual information as hints to enhance CoT reasoning and filter
out noisy CoTs that have low semantic consistency and attribution scores to the context. Paul et al.
 decomposed the reasoning process into two modules: an inference module, which employs

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:26

Huang, et al.

Direct Preference Optimization  to align the LLM towards preferring correct reasoning chains
over counterfactual chains, and a reasoning module, which encourages the LLM to reason faithfully
over the reasoning steps using a counterfactual and causal preference objective. Compared to
natural language reasoning, logical reasoning demands rigorous logical calculation, whereas plain
text often lacks precise logical structure, leading to unfaithful reasoning. To address this, Xu et al.
 introduced Symbolic CoT (SymbCoT), which incorporates symbolic expressions within CoT
to describe intermediate reasoning steps. Specifically, SymbCoT translates the natural language
context into a symbolic representation and then formulates a step-by-step plan to address the
logical reasoning problem, followed by a verifier to check the translation and reasoning chain,
thereby ensuring faithful logical reasoning.

Discussion. Faithfulness Enhanced Decoding significantly advances the alignment of LLM
outputs with provided contexts and enhances the internal consistency of the generated content.
However, strategies such as context-aware decoding often lack adaptive mechanisms, limiting their
effectiveness in scenarios that demand dynamic attention to context. Furthermore, many decoding
strategies require the integration of additional models that do not focus on context, introducing
significant computational overhead and reducing efficiency.

6 HALLUCINATIONS IN RETRIEVAL AUGMENTED GENERATION
Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucina-
tions and improve the factuality of LLM outputs . By incorporating large-scale
external knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus
reducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs
. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still
produce hallucinations . Hallucinations in RAG present considerable complexities, manifesting
as outputs that are either factually inaccurate or misleading. These hallucinations occur when
the content generated by the LLM does not align with real-world facts, fails to accurately reflect
the user’s query, or is not supported by the retrieved information. Such hallucinations can stem
from two primary factors: retrieval failure (§6.1) and generation bottleneck (§6.2). Through
a comprehensive analysis of the limitations present in current RAG systems, we aim to shed
light on potential improvements for retrieval-augmented LLMs, paving the way for more reliable
information retrieval systems.

6.1 Retrieval Failure
The retrieval process is a crucial initial step in the RAG framework, tasked with retrieving the most
relevant information for information-seeking queries. Consequently, failures in the retrieval stage
can have serious downstream effects on the RAG pipeline, leading to hallucinations. These failures
typically stem from three primary parts: the formulation of user queries, the reliability and scope
of retrieval sources, and the effectiveness of the retriever.

6.1.1 User Queries. User queries play a fundamental role in guiding the retrieval process with RAG
systems. The specificity and clarity of these queries critically influence the effectiveness of retrieval
outcomes. In this section, we discuss factors that may contribute to hallucinations from three
perspectives: blind retrieval, misinterpretation of ambiguous queries, and the challenges in accurate
retrieval of complex queries. Some examples are presented in Table 5 for a better understanding.
Retrieval Intent Decisions. Not all queries necessitate retrieval. Blind retrieval for queries
that do not require external knowledge can counterproductively lead to misleading responses. As
shown in Table 5, the query about "the boiling point of water at sea level" pertains to a basic scientific
fact that the model could address without external retrieval. However, the retrieval system was

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:27

Table 5. Examples of each category of User Queries that may lead to LLM hallucinations. Content marked in
Red represents the hallucinatory output.

Type

Example Dialog

Explanation

Retrieval
Intent

Retrieved context: At high altitudes, water boils at lower
temperatures than at sea level, such as 95°C at elevations
above 2,000 meters. This altitude-induced reduction can affect
cooking times and methods.

Query: What is the boiling point of water at sea level?

Response: The boiling point of water at sea level is 95°C.

Retrieved context: The Hennessey Venom GT famously
set a speed record by completing a mile on a racetrack in
32 seconds, highlighting its performance in motorsport events.

Query: What’s the record for the fastest mile run on track?

Ambiguous
Queries

Response: The fastest mile on a track was completed in
32 seconds by the Hennessey Venom GT.

Retrieved context: Alexander Fleming, known for his ground-
breaking work in discovering penicillin, significantly influenced
modern antibiotics. Fleming’s discovery saved countless lives.

Query: Which university did the scientist who discovered
penicillin graduate from?

Complex
Queries

Response: Alexander Fleming graduated from Oxford University.

This query involves a basic scientific fact that should be
available within the model’s parametric knowledge.
However, the system inappropriately retrieves and uses
altitude-specific information about boiling temperatures,
leading to an incorrect response for sea-level conditions.

The query is ambiguous because it does not specify whe-
ther it refers to a human runner or a vehicle. The retrieval
system erroneously pulls information related to car racing
records instead of athletics.

The retrieval system only manages to fetch information
about Fleming’s professional achievements in the discovery
of penicillin. However, the document does not provide informa-
tion about his educational background, thus the model generates a
hallucinatory answer.

inappropriately activated, blindly retrieving inaccurate information and consequently leading to
an undesirable response. Consequently, several studies  have proposed to make a
shift from passive retrieval to adaptive retrieval. In general, these strategies can be divided into
two categories: heuristic-based and self-aware judgment. Heuristic-based methods employ heuristic
rules to determine the necessity of retrieval. For instance, Mallen et al.  observed a positive
correlation between LLMs’ memorization capabilities and entity popularity and suggested triggering
retrieval only when the entity popularity in the user query falls below a certain threshold. Similarly,
Jeong et al.  determined the timing of retrieval based on the query complexity, whereas Asai
et al.  considered whether the query is factual relevant. Self-aware judgment leverages the
models’ intrinsic judgment to decide the necessity for information retrieval. Feng et al. , Ren
et al.  and Wang et al.  directly prompted LLMs for retrieval decisions, recognizing
that LLMs possess a certain level of awareness regarding their knowledge boundaries .
Moreover, Jiang et al.  introduced an active retrieval strategy that triggers retrieval only
when the LLM generates low-probability tokens. Similarly, Su et al.  not only considered the
uncertainty of each token but also its semantic contribution and impact on the subsequent context.
More recently, Cheng et al.  proposed four orthogonal criteria for determining the retrieval
timing, which include intent-aware, knowledge-aware, time-sensitive-aware, and self-aware.

Ambiguous Queries. Ambiguous user queries, containing omission, coreference, and ambiguity,
significantly complicate the retrieval system’s ability to fetch precisely relevant information, thereby
increasing the likelihood of generating undesirable responses. As shown in Table 5, due to the
ambiguity of the query about "the record for the fastest mile run on track", the retrieval system
erroneously retrieved information from automobile racing events, which led the model to generate
a response suited for vehicles instead of athletes. A prevalent mitigation strategy is query rewriting,
where queries are refined and decontextualized to better match relevant documents. Wang et al.
 and Jagerman et al.  have explored prompting approaches where the LLM is prompted
to generate a pseudo-document or rationale based on the original query, which is then used for
further retrieval. Additionally, Ma et al.  introduced a trainable rewriter which is trained using
the feedback from the LLM via reinforcement learning. Mao et al.  employed the feedback
signals from the reranker to train the rewrite model, thus eliminating the reliance on annotated data.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:28

Huang, et al.

However, the challenges deepen in conversational search, which encounters a more complex issue
of context-dependent query understanding with the lengthy conversational history. Addressing
this, Yoon et al.  proposed a similar framework for optimizing the LLM to generate retriever-
preferred query rewrites. This operated by generating a variety of queries and then using the
preference of the rank of retrieved passage to optimize the query rewriting model.

Complex Queries. Complex user queries, characterized by requiring intensive reasoning 
or encompassing multiple aspects , pose significant challenges to the retrieval system.
Such queries require advanced understanding and decomposition capabilities, which may exceed
the current capabilities of the current retrieval methods based on keyword or semantic match-
ing, often leading to partial or incorrect retrievals. For example, as shown in Table 5, due to the
multi-step nature of the query about "Which university did the scientist who discovered penicillin
graduate from?", direct retrieval often leads to incomplete results, thereby resulting in hallucinatory
responses. A common approach involves query decomposition, where the complex query is decom-
posed into sub-queries to facilitate more accurate information retrieval. For instance, Wang et al.
 implemented a sub-aspect explorer that utilizes the extensive world knowledge embedded
LLMs to identify potential sub-aspects of user queries, thereby providing explicit insights into
the user’s underlying intents. Similarly, Shao et al.  concentrated on the demanding task
of expository writing, aiming at retrieving comprehensive information to compose Wikipedia-
like articles from scratch on a specific topic. This approach involves decomposing the topic into
various perspectives and simulating multi-turn conversations with LLMs, each personified with
different perspectives for question asking. Additionally, Cao et al.  and Chu et al.  explored
knowledge-intensive complex reasoning and employed a divide-and-conquer strategy. This strategy
begins with decomposing complex questions into question trees, where at each node, the LLM
retrieves and aggregates answers from diverse knowledge sources.

6.1.2 Retrieval Sources. The reliability and scope of retrieval sources are crucial determinants of
the efficacy of RAG systems. Effective retrieval depends not only on the clarity of the user queries
but also on the quality and comprehensiveness of the sources from which information is retrieved.
When these sources contain factually incorrect or outdated information, the risk of retrieval failures
increases significantly, potentially leading to the generation of incorrect or misleading information.
As the landscape of content creation evolves with the rapid advancement of Artificial Intelligence
Generated Content (AIGC) , an increasing volume of LLM-generated content is permeating the
internet, subsequently becoming integrated into retrieval sources . This integration is reshaping
the dynamics of information retrieval, as evidenced by recent empirical studies  suggesting
that modern retrieval models tend to favor LLM-generated content over human-authored content.
Recent research  has explored the implications of progressively integrating LLM-generated
content into RAG systems. The findings indicate that, without appropriate intervention, human-
generated content may progressively lose its influence within RAG systems. Additionally, Tan
et al.  investigated the performance of RAG systems when incorporating LLM-generated into
retrieved contexts, revealing a significant bias favoring generated contexts. This bias stems from the
high similarity between generated context and questions, as well as the semantic incompleteness
of retrieved contexts. More seriously, the propensity of LLMs to produce factually inaccurate
hallucinations exacerbates the reliability issues of retrieval sources. As LLM-generated content
often contains factual errors, its integration into retrieval sources can mislead retrieval systems,
further diminishing the accuracy and reliability of the information retrieved.

To combat these biases, several approaches have been explored. Inspired by common practice in
pre-training data processing , Asai et al.  proposed a scenario that incorporates a quality
filter designed to ensure the high quality of the retrieval datastore. Additionally, Pan et al. 

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:29

proposed Credibility-aware Generation (CAG), which equips LLMs with the ability to discern and
handle information based on its credibility. This approach assigns different credibility levels to
information, considering its relevance, temporal context, and the trustworthiness of its source, thus
effectively reducing the impact of flawed information in RAG systems.

6.1.3 Retriever. When the user query is explicit and the retrieval source is reliable, the effectiveness
of the retrieval process depends crucially on the performance of the retriever. In such scenarios,
the retriever’s effectiveness is significantly compromised by improper chunking and embedding
practices.

Chunking. Given the extensive nature of retrieval sources, which often encompass lengthy
documents like web pages, it poses significant challenges for LLMs with limited context length. Thus,
chunking emerges as an indispensable step in RAG, which involves segmenting these voluminous
documents into smaller, more manageable chunks to provide precise and relevant evidence for LLMs.
According to actual needs, the chunking granularity ranges from documents to paragraphs, even
sentences. However, inappropriate retrieval granularity can compromise the semantic integrity
and affect the relevance of retrieved information , thereby affecting the performance of
LLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified
length such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,
which is widely used in RAG systems . Considering fixed-size chunking falls short in
capture structure and dependency of lengthy documents, Sarthi et al.  proposed RAPTOR, an
indexing and retrieval system. By recursively embedding, clustering, and summarizing chunks of
text, RAPTOR constructs a tree to capture both high-level and low-level details. When retrieval,
RAPTOR enables LLMs to integrate information from different levels of abstraction, providing
a more comprehensive context for user queries. Instead of chunking text with a fixed chunk
size, semantic chunking adaptively identifies breakpoints between sentences through embedding
similarity, thereby preserving semantic continuity . Furthermore, Chen et al.  pointed
out the limitations of the existing retrieval granularity. On the one hand, while a coarser retrieval
with a longer context can theoretically provide a more comprehensive context, it often includes
extraneous details that could potentially distract LLMs. On the other hand, a fine-grain level can
provide more precise and relevant information, it has limitations such as not being self-contained
and lacking necessary contextual information. To address these shortcomings, Chen et al. 
introduced a novel retrieval granularity, proposition, which is defined as atomic expressions within
the text, each encapsulating a distinct factoid and presented in a concise self-contained natural
language format.

Embedding. Once the retrieval text is chunked, text chunks are subsequently transformed into
vector representation via an embedding model. Such a representation scheme is supported by the
well-known data structure of vector database , which systematically organizes data as key-
value pairs for efficient text retrieval. In this manner, the relevance score can be computed according
to the similarity function between the text representation and query representation. However, a
sub-optimal embedding model may compromise performance, which affects the similarity and
matching of chunks to user queries, potentially misleading LLMs. Typically, a standard embedding
model  learns the query and text representations with encoder-based architecture
(e.g. BERT , RoBERTa ) via contrastive learning , where the loss is constructed by
contrasting a positive pair of query-document against a set of random negative pairs. However, these
embeddings showcase their limitations when applied to new domains, such as medical and financial
applications . In these cases, recent studies  propose to fine-tune the
embedding models on domain-specific data to enhance retrieval relevance. For example, REPLUG
 utilizes language modeling scores of the answers as a proxy signal to train the dense retriever.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:30

Huang, et al.

More recently, Muennighoff et al.  have introduced generative representational instruction
tuning where a single LLM is trained to handle both generative and embedding tasks, which
largely reduces inference latency in RAG by caching representations. Despite these advancements,
the field faces challenges, particularly with the fine-tuning of high-performing yet inaccessible
embedding models, such as OpenAI’s text-embedding-ada-002. Addressing this gap, Zhang et al.
 introduced a novel approach for fine-tuning a black-box embedding model by augmenting it
with a trainable embedding model which significantly enhances the performance of the black-box
embeddings.

6.2 Generation Bottleneck
After the retrieval process, the generation stage emerges as a pivotal point, responsible for generat-
ing content that faithfully reflects the retrieved information. However, this stage can encounter
significant bottlenecks that may lead to hallucinations. We summarize two key capabilities of LLMs
that are closely related to these bottlenecks: contextual awareness and contextual alignment. Each
plays an important role in ensuring the reliability and credibility of the RAG system.

6.2.1 Contextual Awareness. Contextual awareness involves understanding and effectively utilizing
contextual information retrieved. This section discusses the key factors that impact the LLM’s
ability to maintain contextual awareness, which can be categorized into three main parts: (1) the
presence of noisy retrieval in context, (2) context conflicts, and (3) insufficient utilization of context
information.

Noisy Context. As emphasized in §6.1, the failure in the retrieval process may inevitably intro-
duce irrelevant information, which will propagate into the generation stage. When the generator is
not robust enough to these irrelevant retrievals, it will mislead the generator and even introduce
hallucinations .

Yoran et al.  conducted a comprehensive analysis on the robustness of current retrieval-
augmented LLMs, revealing a significant decrease in performance with random retrieval. While
using an NLI model to filter out irrelevant passages is effective, this method comes with the trade-off
of inadvertently discarding some relevant passages. A more effective solution is to train LLMs to
ignore irrelevant contexts by incorporating irrelevant contexts in training data. Similarly, Yu et al.
 introduced Chain-of-Note, which enables LLMs to first generate reading notes for retrieved
contexts and subsequently formulate the final answer. In this way, LLMs can not only filter irrelevant
retrieval to improve noise robustness but also respond with unknown when retrieval is insufficient
to answer user queries. In addition to improving LLM robustness by learning to ignore irrelevant
content in the context, several studies  propose to compress the context to filter
out irrelevant information. Specifically, Li  and Jiang et al.  made use of small language
models to compute self-information and perplexity for prompt compression, finding the most
informative content. Similarly, Wang et al.  proposed to filter out irrelevant content and leave
precisely supporting content based on lexical and information-theoretic approaches. Besides, efforts
have been also made to employ summarization models as compressors. Xu et al.  presented
both extractive and abstractive compressors, which are trained to improve LLMs’ performance while
keeping the prompt concise. Liu et al.  involved summarization compression and semantic
compression, where the former achieves compression by summarizing while the latter removes
tokens with a lower impact on the semantic.

Context Conflict. Retrieval-augmented LLMs generate answers through the combined effect of
parametric knowledge and contextual knowledge. As discussed in §3.3.2, LLMs may sometimes
exhibit over-confidence, which can bring new challenges to the faithfulness of RAG systems when
facing knowledge conflicts. Knowledge conflicts in RAG are situations where contextual knowledge

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:31

contradicts LLMs’ parametric knowledge. Longpre et al.  first investigated knowledge conflicts
in open-domain question answering, where conflicts are automatically created by replacing all
spans of the gold answer in the retrieval context with a substituted entity. Findings demonstrate that
generative QA reader models (e.g. T5) tend to trust parametric memory over contextual information.
By further training the retriever to learn to trust the contextual evidence with augmented training
examples by entity substitution, the issue of over-reliance on parametric knowledge is mitigated.
Similar findings are also reported by Li et al.  who demonstrated that fine-tuning LLMs
on counterfactual contexts can effectively improve the controllability of LLMs when dealing
with contradicts contexts. Also building upon counterfactual data augmentation, Neeman et al.
 trained models to predict two disentangled answers, one based on contextual knowledge
and the other leveraging parametric knowledge to address knowledge conflicts. Besides, Zhou
et al.  introduced two effective prompting-based strategies, namely opinion-based prompts
and counterfactual demonstrations. Opinion-based prompts transform the context to narrators’
statements, soliciting the narrators’ opinions, whereas counterfactual demonstrations employ
counterfactual instances to improve faithfulness in situations of knowledge conflict. While Longpre
et al.  and Li et al.  concentrated their research on the context of a limited single evidence
setting, Chen et al.  further expanded this study to consider a more realistic scenario in which
models consider multiple evidence passages and find models rely almost exclusively on contextual
evidence.

Considering previous studies  mostly focused on smaller models, Xie et al. 
raised doubts about the applicability of their conclusions in the era of LLMs. Such heuristic entity-
level substitution may lead to incoherent counter-memory, thereby making it trivial for LLMs
to overlook the construct knowledge conflicts. By directly eliciting LLMs to generate a coherent
counter-memory that factually conflicts with the parametric memory, LLMs exhibit their high
receptivity to external evidence.

Context Utilization. Despite successfully retrieving evidence relevant to factoid queries, LLMs
can encounter a significant performance degradation due to insufficient utilization of the context,
especially for information located in the middle of the long context window, a notable issue
known as the lost-in-the-middle phenomenon . Beyond factoid QA, recent studies have further
demonstrated such a middle-curse also holds in abstractive summarization , long-form QA 
and passage ranking . One potential explanation lies in the use of rotary positional embedding
(RoPE) , which is widely used in open-source LLMs, due to its excellent performance in length
extrapolation . As a representative relative position embedding, RoPE features a long-term
decay property, which inherently biases the LLM to give precedence to current or proximate tokens,
thereby diminishing its attention on those that are more distant. Another contributing factor is
that the most salient information often resides at the beginning or the end of pre-training data,
a characteristic commonly observed in news reports . Such an issue brings forth challenges
in retrieval-augmented LLMs, as retrieval-augmented LLMs are typically designed with extensive
lengths to accommodate more retrieval documents.

To mitigate this crucial issue, He et al.  introduced several tasks specially designed for
information seeking to enhance the capability of information utilization by explicitly repeating the
question and extracting the index of supporting documents before generating answers. Furthermore,
Zhang et al.  introduced Multi-scale Positional Encoding (Ms-PoE), which mitigates the long-
term decay effect characteristic of RoPE by rescaling position indices. Ms-PoE provides a plug-
and-play solution to enhance the ability of LLMs to effectively capture information in the middle
of the context without the need for additional fine-tuning. Besides, Ravaut et al.  proposed
hierarchical and incremental summarization, which effectively preserves the salient information
and compresses the length of context to avoid the middle-curse.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:32

Huang, et al.

6.2.2 Contextual Alignment. Contextual alignment ensures that LLM outputs faithfully align with
relevant context. This section outlines the primary components of contextual alignment, which
include: (1) source attribution and (2) faithful decoding.

Source Attribution. Source attribution  in retrieval-augmented LLMs refers to the process
by which the model identifies and utilizes the origins of information within its generation process.
This component is crucial for ensuring that the outputs of RAG systems are not only relevant but
also verifiable and grounded in credible sources.

To achieve source attribution in RAG systems, recent studies have been explored, which can be
categorized into three lines based on the type of attribution. (1) Plan-then-Generate: Fierro et al.
 introduced the blueprint model for attribution, which conceptualizes text plans as a series
of questions that serve as blueprints for generation process, dictating both the content and the
sequence of the output. Compared with abstractive questions, Huang et al.  enabled the model
to first ground to extractive evidence spans, which guides the subsequent generation process.
Leveraging either abstract questions or extractive spans as planning facilitates a built-in attribution
mechanism, as they provide a natural link between retrieved information and the subsequent
generation. Similarly, Slobodkin et al.  broke down the conventional end-to-end generation
process into three intuitive stages: content selection, sentence planning, and sentence fusion. By
initially identifying relevant source segments and subsequently conditioning the generation process
on them, the selected segments naturally serve as attributions. (2) Generate-then-Reflect: Asai et al.
 proposed training the LLM to generate text with reflection tokens. These reflection tokens
empower the LLM to decide whether to retrieve, assess the relevance of the retrieved document,
and critique its own generation to ensure attributability. By critiquing its generation. Furthermore,
Ye et al.  introduced AGREE, designed to facilitate self-grounding in LLMs. AGREE trains
LLMs to generate well-grounded claims with citations and identify claims that lack verification.
An iterative retrieval process is then employed to actively seek additional information for these
unsupported statements. (3) Self-Attribution: In addition to leveraging external supervised signals
for attribution, Qi et al.  proposed a self-attribution mechanism that utilizes model-internal
signals. It operates by first identifying context-sensitive answer tokens, which are then paired with
retrieved documents that contributed to the model generation via saliency methods.

Faithful Decoding. Despite significant optimizations in the RAG pipeline that facilitate the
incorporation of highly relevant content into the model’s context, current LLMs still cannot
guarantee faithful generation. The unfaithful utilization of relevant context by LLMs undermines
the reliability of their outputs, even when the sources of information are verifiably accurate. Wu
et al.  analyzed the model’s knowledge preference when internal knowledge conflicts with
contextual information and observed the tug-of-war between the LLM’s internal prior and external
evidence. To tackle this issue, recent research  has focused on faithful decoding within
RAG systems, aiming to improve the models’ ability to generate content that faithfully aligns
with contextual information. Shi et al.  presented context-aware decoding, which modifies
the model’s original output probability distribution into the pointwise mutual information (PMI)
formulation. The strategy operates by amplifying the difference between the output probabilities
when a model is used with and without context, thereby enhancing the faithfulness of LLMs to
the provided context. Li et al.  adopted a semi-parametric language modeling approach 
which facilitates the integration of contextual spans of arbitrary length into LM generations. The
generation is then verified via speculative decoding, further ensuring model faithfulness. More
recently, Wu et al.  proposed faithfulness-oriented decoding, which leverages a lightweight
faithfulness detector to monitor the beam-search process. The detector leverages fine-grained
decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and
semantic alignment to synchronously detect unfaithful sentences. When an unfaithful generation

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:33

is detected, it triggers the backtrack operation and selects the beam with the more faithful score,
thus ensuring greater faithfulness to the retrieval sources.

7 FUTURE DISCUSSION
As the field of research on hallucinations in LLMs continues to evolve, our focus shifts towards
the next horizon of inquiry. We explore prospective areas of study, notably the phenomenon of
hallucinations in vision-language models (§7.1) and the challenge of delineating and understanding
knowledge boundaries within LLMs (§7.2).

7.1 Hallucination in Large Vision-Language Models
Enabling the visual perception ability, along with exceptional language understanding and gen-
eration capabilities, Large Vision-Language Models (LVLMs) have exhibited remarkable vision-
language capabilities . Unlike previous pre-trained multi-modal
models that gain limited vision-language abilities from large-scale visual-language pre-training
datasets , LVLMs exploit advanced LLMs to unleash the power of interacting
with humans and the environment. The consequent diverse applications of LVLMs also bring new
challenges to maintaining the reliability of such systems. Recent studies have revealed that current
LVLMs are suffering from multi-modal hallucinations, where models provide responses misaligned
with the corresponding visual information . Such multi-modal hallucinations could
cause unexpected behaviors when applying LVLMs to real-world scenarios, which therefore had to
be further investigated and mitigated.

Li et al.  and Lovenia et al.  took the first step towards evaluating the object hallucina-
tions in the LVLMs. Evaluations and experiments reveal that current LVLMs are prone to generate
inconsistent responses with respect to the associated image, including non-existent objects, wrong
object types, and attributes, incorrect semantic relationships, etc. . Furthermore, Liu et al.
, Zong et al.  and Liu et al.  show that LVLMs can be easily fooled and experience
a severe performance drop due to their over-reliance on the strong language prior, as well as its
inferior ability to defend against inappropriate user inputs . Jiang et al. , Wang et al.
 and Jing et al.  took a step forward to holistically evaluate multi-modal hallucination.
What’s more, when presented with multiple images, LVLMs sometimes mix or miss parts of the
visual context, as well as fail to understand temporal or logical connections between them, which
might hinder their usage in many scenarios, yet properly identifying the reason for such disorders
and tackling them still requires continued efforts. Despite the witnessed perception errors, LVLMs
can generate flawed logical reasoning results even when correctly recognizing all visual elements,
which remains further investigation.

Efforts have been made towards building a more robust large vision-language model. Gunjal et al.
, Lu et al. , Wang et al. , and Liu et al.  proposed to further finetune the model
for producing more truthful and helpful responses. Another line of work chooses to post-hoc rectify
the generated inconsistent content, such as , and , which introduced expert models. To
free from the external tools, Leng et al. , Huang et al. , and Zhao et al.  tried to fully
utilize the LVLM itself to alleviate hallucinations. Though proved to be effective, those methods
usually require additional data annotations, visual experts, training phases, and computational
costs, which prevent LVLMs from effectively scaling and generalizing to various fields. Thus, more
universal approaches are expected to build a more reliable system, such as faithful and large-scale
visual-text pre-training and alignment methods.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:34

Huang, et al.

7.2 Understanding Knowledge Boundary in LLMs
Despite the impressive capacity to capture factual knowledge from extensive data, LLMs still face
challenges in recognizing their own knowledge boundaries. This shortfall leads to the occurrence
of hallucinations, where LLMs confidently produce falsehoods without an awareness of their own
knowledge limits . Numerous studies delve into probing knowledge boundaries of
LLMs, utilizing strategies such as evaluating the probability of a correct response in a multiple-
choice setting , or quantifying the model’s output uncertainty by evaluating the similarity
among sets of sentences with uncertain meanings.

Furthermore, a line of work  has revealed that LLMs contain latent structures
within their activation space that relate to beliefs about truthfulness. Recent research  also
found substantial evidence for LLMs’ ability to encode the unanswerability of questions, despite
the fact that these models exhibit overconfidence and produce hallucinations when presented with
unanswerable questions. Nonetheless, Levinstein and Herrmann  have employed empirical
and conceptual tools to probe whether or not LLMs have beliefs. Their empirical results suggest that
current lie-detector methods for LLMs are not yet fully reliable, and the probing methods proposed
by Burns et al.  and Azaria and Mitchell  do not adequately generalize. Consequently,
whether we can effectively probe LLMs’ internal beliefs is ongoing, requiring further research.

8 CONCLUSION
In this comprehensive survey, we have undertaken an in-depth examination of hallucinations
within large language models, delving into the intricacies of their underlying causes, pioneering
detection methodologies as well as related benchmarks, and effective mitigation strategies. Although
significant strides have been taken, the conundrum of hallucination in LLMs remains a compelling
and ongoing concern that demands continuous investigation. Moreover, we envision this survey
as a guiding beacon for researchers dedicated to advancing robust information retrieval systems
and trustworthy artificial intelligence. By navigating the complex landscape of hallucinations, we
hope to empower these dedicated individuals with invaluable insights that drive the evolution of
AI technologies toward greater reliability and safety.

REFERENCES

 Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. 2023. SemDeDup: Data-efficient
learning at web-scale through semantic deduplication. ArXiv preprint abs/2303.09540 (2023). https://arxiv.org/abs/
2303.09540

 Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2023. Evaluating
correctness and faithfulness of instruction-following models for question answering. ArXiv preprint abs/2307.16877
(2023). https://arxiv.org/abs/2307.16877

 Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language Models Know When They’re Halluci-

nating References? ArXiv preprint abs/2305.18248 (2023). https://arxiv.org/abs/2305.18248

 Perplexity AI. 2023. Perplexity AI. https://www.perplexity.ai/
 Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yun-Hsuan Sung. 2023. Characterizing
Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models. ArXiv preprint abs/2302.05578
(2023). https://arxiv.org/abs/2302.05578

 Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad. 2022. A Review on
Language Models as Knowledge Bases. CoRR abs/2204.06031 (2022). https://doi.org/10.48550/ARXIV.2204.06031
arXiv:2204.06031

 Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,
Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian
Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James
Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong
Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George
Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White,

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:35

Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub
Sygnowski, and et al. 2023. Gemini: A Family of Highly Capable Multimodal Models. CoRR abs/2312.11805 (2023).
https://doi.org/10.48550/ARXIV.2312.11805 arXiv:2312.11805

 Anthropic. 2023. Claude. https://claude.ai/
 Antropic. 2024. Claude 3 haiku: our fastest model yet. 2024. https://www.anthropic.com/news/claude-3-haiku
 ArXiv. 2023. arxiv dataset. https://www.kaggle.com/datasets/Cornell-University/arxiv/versions/134
 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to Retrieve,
Generate, and Critique through Self-Reflection. CoRR abs/2310.11511 (2023). https://doi.org/10.48550/ARXIV.2310.
11511 arXiv:2310.11511

 Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau
Yih. 2024. Reliable, Adaptable, and Attributable Language Models with Retrieval. CoRR abs/2403.03187 (2024).
https://doi.org/10.48550/ARXIV.2403.03187 arXiv:2403.03187

 Amos Azaria and Tom M. Mitchell. 2023. The Internal State of an LLM Knows When its Lying. ArXiv preprint

abs/2304.13734 (2023). https://arxiv.org/abs/2304.13734

 Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented Language Model Prompting for Zero-
Shot Knowledge Graph Question Answering. ArXiv preprint abs/2306.04136 (2023). https://arxiv.org/abs/2306.04136
 Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng
Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of
ChatGPT on Reasoning, Hallucination, and Interactivity. ArXiv preprint abs/2302.04023 (2023). https://arxiv.org/abs/
2302.04023

 Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven
Failure Points When Engineering a Retrieval Augmented Generation System. CoRR abs/2401.05856 (2024). https:
//doi.org/10.48550/ARXIV.2401.05856 arXiv:2401.05856

 Mario Barrantes, Benedikt Herudek, and Richard Wang. 2020. Adversarial nli for factual correctness in text summari-

sation models. ArXiv preprint abs/2005.11739 (2020). https://arxiv.org/abs/2005.11739

 Pierre Basso. 1993. Conditional Causal Logic: A Formal Theory of the Meaning Generating Processes in a Cognitive
System. In Proceedings of the 13th International Joint Conference on Artificial Intelligence. Chambéry, France, August 28 -
September 3, 1993, Ruzena Bajcsy (Ed.). Morgan Kaufmann, 845–851. http://ijcai.org/Proceedings/93-2/Papers/002.pdf
 Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. ArXiv

preprint abs/2004.05150 (2020). https://arxiv.org/abs/2004.05150

 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of
Stochastic Parrots: Can Language Models Be Too Big?. In FAccT ’21: 2021 ACM Conference on Fairness, Accountability,
and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, Madeleine Clare Elish, William Isaac, and
Richard S. Zemel (Eds.). ACM, 610–623. https://doi.org/10.1145/3442188.3445922

 Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled Sampling for Sequence Prediction
with Recurrent Neural Networks. In Advances in Neural Information Processing Systems 28: Annual Conference on
Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, Corinna Cortes, Neil D.
Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (Eds.). 1171–1179. https://proceedings.neurips.cc/
paper/2015/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html

 Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans.
2023. The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A". ArXiv preprint abs/2309.12288 (2023).
https://arxiv.org/abs/2309.12288

 Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,
Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,
Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR
abs/2204.06745 (2022). https://doi.org/10.48550/ARXIV.2204.06745 arXiv:2204.06745

 Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den
Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick,
Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela
Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent
Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research,
Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR,
2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html

 Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda
Askell, Andy Jones, Anna Chen, et al. 2022. Measuring progress on scalable oversight for large language models.
ArXiv preprint abs/2211.03540 (2022). https://arxiv.org/abs/2211.03540

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:36

Huang, et al.

 Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. The method of paired

comparisons. Biometrika 39, 3/4 (1952), 324–345. https://www.jstor.org/stable/2334029

 Ruben Branco, António Branco, João António Rodrigues, and João Ricardo Silva. 2021. Shortcutted Commonsense:
Data Spuriousness in Deep Learning of Commonsense Reasoning. In Proceedings of the 2021 Conference on Empirical

