# Abstract

We present Stable Video Diffusion — a latent video diffu-
sion model for high-resolution, state-of-the-art text-to-video
and image-to-video generation. Recently, latent diffusion
models trained for 2D image synthesis have been turned
into generative video models by inserting temporal layers
and finetuning them on small, high-quality video datasets.
However, training methods in the literature vary widely,
and the field has yet to agree on a unified strategy for cu-

* Equal contributions.

In this paper, we identify and evalu-
rating video data.
ate three different stages for successful training of video
LDMs: text-to-image pretraining, video pretraining, and
high-quality video finetuning. Furthermore, we demon-
strate the necessity of a well-curated pretraining dataset
for generating high-quality videos and present a system-
atic curation process to train a strong base model, includ-
ing captioning and filtering strategies. We then explore
the impact of finetuning our base model on high-quality
data and train a text-to-video model that is competitive with
closed-source video generation. We also show that our base

1

model provides a powerful motion representation for down-
stream tasks such as image-to-video generation and adapt-
ability to camera motion-specific LoRA modules. Finally,
we demonstrate that our model provides a strong multi-view
3D-prior and can serve as a base to finetune a multi-view
diffusion model that jointly generates multiple views of ob-
jects in a feedforward fashion, outperforming image-based
methods at a fraction of their compute budget. We release
code and model weights at https://github.com/
Stability-AI/generative-models.

# Introduction

Driven by advances in generative image modeling with
diffusion models , there has been signifi-
cant recent progress on generative video models both in re-
search  and real-world applications 
Broadly, these models are either trained from scratch 
or finetuned (partially or fully) from pretrained image mod-
els with additional temporal layers inserted .
Training is often carried out on a mix of image and video
datasets .

While research around improvements in video modeling
has primarily focused on the exact arrangement of the spa-
tial and temporal layers , none of the afore-
mentioned works investigate the influence of data selection.
This is surprising, especially since the significant impact of
the training data distribution on generative models is undis-
puted . Moreover, for generative image modeling,
it is known that pretraining on a large and diverse dataset
and finetuning on a smaller but higher quality dataset sig-
nificantly improves the performance . Since many
previous approaches to video modeling have successfully
drawn on techniques from the image domain , it
is noteworthy that the effect of data and training strategies,
i.e., the separation of video pretraining at lower resolutions
and high-quality finetuning, has yet to be studied. This work
directly addresses these previously uncharted territories.

We believe that the significant contribution of data selec-
tion is heavily underrepresented in today’s video research
landscape despite being well-recognized among practition-
ers when training video models at scale. Thus, in contrast
to previous works, we draw on simple latent video diffu-
sion baselines  for which we fix architecture and training
scheme and assess the effect of data curation. To this end,
we first identify three different video training stages that
we find crucial for good performance: text-to-image pre-
training, video pretraining on a large dataset at low resolu-
tion, and high-resolution video finetuning on a much smaller
dataset with higher-quality videos. Borrowing from large-
scale image model training , we introduce a sys-
tematic approach to curate video data at scale and present an
empirical study on the effect of data curation during video

pretraining. Our main findings imply that pretraining on
well-curated datasets leads to significant performance im-
provements that persist after high-quality finetuning.

A general motion and multi-view prior Drawing on
these findings, we apply our proposed curation scheme to
a large video dataset comprising roughly 600 million sam-
ples and train a strong pretrained text-to-video base model,
which provides a general motion representation. We exploit
this and finetune the base model on a smaller, high-quality
dataset for high-resolution downstream tasks such as text-
to-video (see Figure 1, top row) and image-to-video, where
we predict a sequence of frames from a single conditioning
image (see Figure 1, mid rows). Human preference studies
reveal that the resulting model outperforms state-of-the-art
image-to-video models.

Furthermore, we also demonstrate that our model pro-
vides a strong multi-view prior and can serve as a base to
finetune a multi-view diffusion model that generates mul-
tiple consistent views of an object in a feedforward man-
ner and outperforms specialized novel view synthesis meth-
ods such as Zero123XL  and SyncDreamer .
Finally, we demonstrate that our model allows for ex-
plicit motion control by specifically prompting the tempo-
ral layers with motion cues and also via training LoRA-
modules  on datasets resembling specific motions
only, which can be efficiently plugged into the model.
To summarize, our core contributions are threefold: (i) We
present a systematic data curation workflow to turn a large
uncurated video collection into a quality dataset for gener-
ative video modeling. Using this workflow, we (ii) train
state-of-the-art text-to-video and image-to-video models,
outperforming all prior models. Finally, we (iii) probe the
strong prior of motion and 3D understanding in our models
by conducting domain-specific experiments. Specifically,
we provide evidence that pretrained video diffusion models
can be turned into strong multi-view generators, which may
help overcome the data scarcity typically observed in the
3D domain .

# Method

FVD (↓)

CogVideo (ZH)  751.34
CogVideo (EN)  701.59
Make-A-Video  367.23
550.61
Video LDM 
655.00
MagicVideo 
355.20
PYOCO 

SVD (ours)

242.02

Our 25 frame Image-
to-Video model is preferred by
human voters over GEN-2 
and PikaLabs .

4.1. Pretrained Base Model

As discussed in Section 3.2, our video model is based on
Stable Diffusion 2.1  (SD 2.1). Recent works  show
that it is crucial to adopt the noise schedule when training
image diffusion models, shifting towards more noise for
higher-resolution images. As a first step, we finetune the
fixed discrete noise schedule from our image model towards
continuous noise  using the network preconditioning
proposed in Karras et al.  for images of size 256 × 384.
After inserting temporal layers, we then train the model on
LVD-F on 14 frames at resolution 256 × 384. We use the

6

standard EDM noise schedule  for 150k iterations and
batch size 1536. Next, we finetune the model to generate 14
320 × 576 frames for 100k iterations using batch size 768.
We find that it is important to shift the noise schedule to-
wards more noise for this training stage, confirming results
by Hoogeboom et al.  for image models. For further
training details, see App. D. We refer to this model as our
base model which can be easily finetuned for a variety of
tasks as we show in the following sections. The base model
has learned a powerful motion representation, for example,
it significantly outperforms all baselines for zero-shot text-
to-video generation on UCF-101  (Tab. 2). Evaluation
details can be found in App. E.

4.2. High-Resolution Text-to-Video Model

We finetune the base text-to-video model on a high-quality
video dataset of ∼ 1M samples. Samples in the dataset gen-
erally contain lots of object motion, steady camera motion,
and well-aligned captions, and are of high visual quality al-
together. We finetune our base model for 50k iterations at
resolution 576 × 1024 (again shifting the noise schedule
towards more noise) using batch size 768. Samples in Fig-
ure 5, more can be found in App. E.

Ours vs PikaOurs vs Gen20.00.10.20.30.40.50.60.7User PreferenceoursbaselineApplying three camera motion LoRAs (horizontal,
zooming, static) to the same conditioning frame (on the left).

4.3. High Resolution Image-to-Video Model

Besides text-to-video, we finetune our base model for
image-to-video generation, where the video model receives
a still input image as a conditioning. Accordingly, we re-
place text embeddings that are fed into the base model
with the CLIP image embedding of the conditioning. Ad-
ditionally, we concatenate a noise-augmented  version
of the conditioning frame channel-wise to the input of the
UNet . We do not use any masking techniques and
simply copy the frame across the time axis. We finetune
two models, one predicting 14 frames and another one pre-
dicting 25 frames; implementation and training details can
be found in App. D. We occasionally found that standard
vanilla classifier-free guidance  can lead to artifacts:
too little guidance may result in inconsistency with the
conditioning frame while too much guidance can result in
oversaturation. Instead of using a constant guidance scale,
we found it helpful to linearly increase the guidance scale
across the frame axis (from small to high). Details can be
found in App. D. Samples in Figure 5, more can be found
in App. E.

In Section 4.5 we compare our model with state-of-the-
art, closed-source video generative models, in particular
GEN-2  and PikaLabs , and show that our model
is preferred in terms of visual quality by human voters. De-
tails on the experiment, as well as many more image-to-
video samples, can be found in App. E.

4.3.1 Camera Motion LoRA

To facilitate controlled camera motion in image-to-video
generation, we train a variety of camera motion LoRAs
within the temporal attention blocks of our model ;
see App. D for exact implementation details. We train these
additional parameters on a small dataset with rich camera-
motion metadata. In particular, we use three subsets of the
data for which the camera motion is categorized as “hori-

Generated multi-view frames of a GSO test object us-
ing our SVD-MV model (i.e. SVD finetuned for Multi-View gen-
eration), SD2.1-MV , Scratch-MV, SyncDreamer , and
Zero123XL .

zontally moving”, “zooming”, and “static”. In Figure 7 we
show samples of the three models for identical conditioning
frames; more samples can be found in App. E.

4.4. Frame Interpolation

To obtain smooth videos at high frame rates, we finetune our
high-resolution text-to-video model into a frame interpola-
tion model. We follow Blattmann et al.  and concatenate
the left and right frames to the input of the UNet via mask-
ing. The model learns to predict three frames within the two
conditioning frames, effectively increasing the frame rate
by four. Surprisingly, we found that a very small number of
iterations (≈ 10k) suffices to get a good model. Details and
samples can be found in App. D and App. E, respectively.

4.5. Multi-View Generation

To obtain multiple novel views of an object simultaneously,
we finetune our image-to-video SVD model on multi-view
datasets .
Datasets. We finetuned our SVD model on two datasets,
where the SVD model takes a single image and outputs
a sequence of multi-view images: (i) A subset of Obja-
verse  consisting of 150K curated and CC-licensed syn-
thetic 3D objects from the original dataset . For each
object, we rendered 360◦ orbital videos of 21 frames with

7

randomly sampled HDRI environment map and elevation
angles between [−5◦, 30◦]. We evaluate the resulting mod-
els on an unseen test dataset consisting of 50 sampled ob-
jects from Google Scanned Objects (GSO) dataset . and
(ii) MVImgNet  consisting of casually captured multi-
view videos of general household objects. We split the
videos into ∼200K train and 900 test videos. We rotate the
frames captured in portrait mode to landscape orientation.

The Objaverse-trained model is additionally conditioned
on the elevation angle of the input image, and outputs or-
bital videos at that elevation angle. The MVImgNet-trained
models are not conditioned on pose and can choose an ar-
bitrary camera path in their generations. For details on the
pose conditioning mechanism, see App. E.

Models. We refer to our finetuned Multi-View model as
SVD-MV. We perform an ablation study on the impor-
tance of the video prior of SVD for multi-view genera-
tion. To this effect, we compare the results from SVD-
from a video prior to those finetuned from an
MV i.e.
the text-to-image model SD2.1 (SD2.1-
image prior i.e.
MV), and that trained without a prior i.e.
from random
initialization (Scratch-MV). In addition, we compare with
the current state-of-the-art multiview generation models of
Zero123 , Zero123XL , and SyncDreamer .

Metrics. We use the standard metrics of Peak Signal-to-
Noise Ratio (PSNR), LPIPS , and CLIP  Simi-
larity scores (CLIP-S) between the corresponding pairs of
ground truth and generated frames on 50 GSO test objects.

Training. We train all our models for 12k steps (∼16 hours)
with 8 80GB A100 GPUs using a total batch size of 16, with
a learning rate of 1e-5.

# Conclusion

We present Stable Video Diffusion (SVD), a latent video
diffusion model for high-resolution, state-of-the-art text-to-
video and image-to-video synthesis. To construct its pre-
training dataset, we conduct a systematic data selection and
scaling study, and propose a method to curate vast amounts
of video data and turn large and noisy video collection into
suitable datasets for generative video models. Furthermore,
we introduce three distinct stages of video model training
which we separately analyze to assess their impact on the
final model performance. Stable Video Diffusion provides
a powerful video representation from which we finetune
video models for state-of-the-art image-to-video synthesis
and other highly relevant applications such as LoRAs for
camera control. Finally we provide a pioneering study on
multi-view finetuning of video diffusion models and show
that SVD constitutes a strong 3D prior, which obtains state-
of-the-art results in multi-view synthesis while using only a

8

fraction of the compute of previous methods.

We hope these findings will be broadly useful in the
generative video modeling literature. A discussion on
our work’s broader impact and limitations can be found
in App. A.

Acknowledgements

Special thanks to Emad Mostaque for his excellent support
on this project. Many thanks go to our colleagues Jonas
M¨uller, Axel Sauer, Dustin Podell and Rahim Entezari for
fruitful discussions and comments. Finally, we thank Harry
Saini and the one and only Richard Vencu for maintain-
ing and optimizing our data and computing infrastructure.

