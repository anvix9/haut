# Abstract

High throughput serving of large language models (LLMs)
requires batching sufficiently many requests at a time. How-
ever, existing systems struggle because the key-value cache
(KV cache) memory for each request is huge and grows
and shrinks dynamically. When managed inefficiently, this
memory can be significantly wasted by fragmentation and
redundant duplication, limiting the batch size. To address
this problem, we propose PagedAttention, an attention al-
gorithm inspired by the classical virtual memory and pag-
ing techniques in operating systems. On top of it, we build
vLLM, an LLM serving system that achieves (1) near-zero
waste in KV cache memory and (2) flexible sharing of KV
cache within and across requests to further reduce mem-
ory usage. Our evaluations show that vLLM improves the
throughput of popular LLMs by 2-4× with the same level
of latency compared to the state-of-the-art systems, such
as FasterTransformer and Orca. The improvement is more
pronounced with longer sequences, larger models, and more
complex decoding algorithms. vLLM’s source code is publicly
available at https://github.com/vllm-project/vllm.

# Introduction

The emergence of large language models (LLMs) like GPT  and PaLM  have enabled new applications such as pro-
gramming assistants  and universal chatbots 
that are starting to profoundly impact our work and daily
routines. Many cloud companies  are racing to pro-
vide these applications as hosted services. However, running
these applications is very expensive, requiring a large num-
ber of hardware accelerators such as GPUs. According to
recent estimates, processing an LLM request can be 10× more
expensive than a traditional keyword query . Given these
high costs, increasing the throughput—and hence reducing

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s).
SOSP ’23, October 23–26, 2023, Koblenz, Germany
© 2023 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0229-7/23/10.
https://doi.org/10.1145/3600006.3613165

Left: Memory layout when serving an LLM with
13B parameters on NVIDIA A100. The parameters (gray)
persist in GPU memory throughout serving. The memory
for the KV cache (red) is (de)allocated per serving request.
A small amount of memory (yellow) is used ephemerally
for activation. Right: vLLM smooths out the rapid growth
curve of KV cache memory seen in existing systems ,
leading to a notable boost in serving throughput.

the cost per request—of LLM serving systems is becoming
more important.

At the core of LLMs lies an autoregressive Transformer
model . This model generates words (tokens), one at a
time, based on the input (prompt) and the previous sequence
of the output’s tokens it has generated so far. For each re-
quest, this expensive process is repeated until the model out-
puts a termination token. This sequential generation process
makes the workload memory-bound, underutilizing the com-
putation power of GPUs and limiting the serving throughput.
Improving the throughput is possible by batching multi-
ple requests together. However, to process many requests
in a batch, the memory space for each request should be
efficiently managed. For example, Fig. 1 (left) illustrates the
memory distribution for a 13B-parameter LLM on an NVIDIA
A100 GPU with 40GB RAM. Approximately 65% of the mem-
ory is allocated for the model weights, which remain static
during serving. Close to 30% of the memory is used to store
the dynamic states of the requests. For Transformers, these
states consist of the key and value tensors associated with the
attention mechanism, commonly referred to as KV cache ,
which represent the context from earlier tokens to gener-
ate new output tokens in sequence. The remaining small

∗Equal contribution.

1

NVIDIA A100 40GBParameters (26GB, 65%)KV Cache(>30%)Others203040Memory usage (GB)Parameter sizeExisting systemsvLLM010203040Batch size (# requests)00.4k0.8k1.2kThroughput (token/s)

decoding algorithms, such as parallel sampling and beam
search, that generate multiple outputs per request. In these
scenarios, the request consists of multiple sequences that can
partially share their KV cache. However, memory sharing is
not possible in the existing systems because the KV cache of
the sequences is stored in separate contiguous spaces.

To address the above limitations, we propose PagedAt-
tention, an attention algorithm inspired by the operating
system’s (OS) solution to memory fragmentation and shar-
ing: virtual memory with paging. PagedAttention divides the
request’s KV cache into blocks, each of which can contain
the attention keys and values of a fixed number of tokens. In
PagedAttention, the blocks for the KV cache are not neces-
sarily stored in contiguous space. Therefore, we can manage
the KV cache in a more flexible way as in OS’s virtual mem-
ory: one can think of blocks as pages, tokens as bytes, and
requests as processes. This design alleviates internal frag-
mentation by using relatively small blocks and allocating
them on demand. Moreover, it eliminates external fragmen-
tation as all blocks have the same size. Finally, it enables
memory sharing at the granularity of a block, across the
different sequences associated with the same request or even
across the different requests.

In this work, we build vLLM, a high-throughput distributed
LLM serving engine on top of PagedAttention that achieves
near-zero waste in KV cache memory. vLLM uses block-level
memory management and preemptive request scheduling
that are co-designed with PagedAttention. vLLM supports
popular LLMs such as GPT , OPT , and LLaMA 
with varying sizes, including the ones exceeding the memory
capacity of a single GPU. Our evaluations on various models
and workloads show that vLLM improves the LLM serving
throughput by 2-4× compared to the state-of-the-art sys-
tems , without affecting the model accuracy at all. The
improvements are more pronounced with longer sequences,
larger models, and more complex decoding algorithms (§4.3).
In summary, we make the following contributions:
• We identify the challenges in memory allocation in serving
LLMs and quantify their impact on serving performance.
• We propose PagedAttention, an attention algorithm that
operates on KV cache stored in non-contiguous paged
memory, which is inspired by the virtual memory and
paging in OS.

• We design and implement vLLM, a distributed LLM serving

engine built on top of PagedAttention.

• We evaluate vLLM on various scenarios and demonstrate
that it substantially outperforms the previous state-of-the-
art solutions such as FasterTransformer  and Orca .

