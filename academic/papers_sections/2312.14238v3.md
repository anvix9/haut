# Abstract



# Introduction

The exponential growth of

large language models
(LLMs) has opened up numerous possibilities for multi-
modal AGI systems. However, the progress in vision and
vision-language foundation models, which are also critical
elements of multi-modal AGI, has not kept pace with LLMs.
In this work, we design a large-scale vision-language foun-
dation model (InternVL), which scales up the vision foun-
dation model to 6 billion parameters and progressively
aligns it with the LLM, using web-scale image-text data
from various sources. This model can be broadly applied
to and achieve state-of-the-art performance on 32 generic
visual-linguistic benchmarks including visual perception
tasks such as image-level or pixel-level recognition, vision-
language tasks such as zero-shot image/video classification,
zero-shot image/video-text retrieval, and link with LLMs to
create multi-modal dialogue systems. It has powerful visual
capabilities and can be a good alternative to the ViT-22B.
We hope that our research could contribute to the develop-
ment of multi-modal large models.

† This work is done when they are interns at Shanghai AI Laboratory;

(cid:66) corresponding author (daijifeng@tsinghua.edu.cn)

Large language models (LLMs) largely promote the devel-
opment of artificial general intelligence (AGI) systems with
their impressive capabilities in open-world language tasks,
and their model scale and performance are still increasing
at a fast pace. Vision large language models (VLLMs)
, which leverage
LLMs, have also achieved significant breakthroughs, en-
abling sophisticated vision-language dialogues and interac-
tions. However, the progress of vision and vision-language
foundation models, which are also crucial for VLLMs, has
lagged behind the rapid growth of LLMs.

To bridge vision models with LLMs, existing VLLMs
 commonly employ lightweight “glue”
layers, such as QFormer  or linear projection , to
align features of vision and language models. Such align-
ment contains several limitations: (1) Disparity in param-
eter scales. The large LLMs  now boosts up to 1000
billion parameters, while the widely-used vision encoders
of VLLMs are still around one billion. This gap may lead
to the under-use of LLM’s capacity. (2) Inconsistent rep-
resentation. Vision models, trained on pure-vision data or

1

contras(veimagetextvisionencodertextencodercontras(veimagetextgenerativescalingupvisionencoderto6B#paramslargelanguagemodellargelanguagemodelpromptsharedweightsimageclassesvisionencoder(a)Supervisedpre-training(b)Contrastivepre-training(c)InternVL: Scaling up vision encoder and aligning with LLM(ours)

Comparison results on various generic visual-linguistic tasks, including image classification, video classification, image-text
retrieval, image captioning, and multi-modal dialogue. The proposed InternVL achieves the best performance on all these tasks. Note that
only the models trained on public data are included. “IN” is an abbreviation for ImageNet .

aligned with the BERT series , often exhibit
(3) Inefficient
representation inconsistencies with LLMs.
connection. The “glue” layers are usually lightweight and
randomly initialized, which may not capture the rich cross-
modal interactions and dependencies that are crucial for
multi-modal understanding and generation.

These limitations reveal a large gap in both parameter
scale and feature representation ability between the vision
encoder and the LLM. To bridge this gap, our inspiration
lies in elevating the vision encoder to align with the param-
eter scale of the LLM and subsequently harmonizing their
representations. However, the training of such large-scale
models necessitates a vast amount of image-text data ob-
tained from the Internet. The significant heterogeneity and
quality variations within this data pose considerable chal-
lenges to the training process. To enhance the efficacy of
the training, generative supervision is considered as a com-
plementary approach to contrastive learning, as depicted in
This strategy aims to provide additional guidance
to the model during training. Yet, the suitability of low-
quality data for generative training remains a concern. Be-
sides, how to effectively represent the users’ commands and
align the representations between the vision encoder and
LLM is another open question.

To address these issues, we formulate the InternVL, a
large-scale vision-language foundation model, which aligns
the representation of the scaled-up vision encoder with the
LLM and achieves state-of-the-art performance on various
visual and vision-language tasks. As shown in Figure 1 (c),
InternVL has three key designs: (1) Parameter-balanced vi-
sion and language components: It includes a vision encoder
scaled up to 6 billion parameters and an LLM middleware
with 8 billion parameters, where the middleware functions
as a substantial “glue” layer to reorganize visual features
based on user commands. Unlike prior vision-only (Fig-
ure 1 (a)) or dual-tower (Figure 1 (b)) structures, our vi-
sion encoder and middleware offer flexible combinations
(2) Consistent
for both contrastive and generative tasks.
representations: To maintain the consistency of represen-

tations between the vision encoder and LLM, we employ a
pre-trained multilingual LLaMA , to initialize the mid-
dleware and align the vision encoder with it. (3) Progressive
image-text alignment: We leverage image-text data from di-
verse sources, ensuring training stability through a progres-
sive alignment strategy. This strategy initiates contrastive
learning on large-scale noisy image-text data and subse-
quently transitions to generative learning on fine-grained
data. This approach ensures a consistent enhancement of
model performance and task scope.

These designs endow our model with several advantages:
(1) Versatile. It functions as a standalone vision encoder for
perception tasks, or collaborates with the language middle-
ware for vision-language tasks and multi-modal dialogue
systems. The language middleware bridges the gap be-
tween the vision encoder and the LLM decoder. (2) Strong.
By leveraging the training strategy, large-scale parameters,
and web-scale data, our model has a powerful represen-
tation that helps to achieve state-of-the-art results on var-
ious vision and vision-language tasks, as shown in Fig-
ure 2. (3) LLM-friendly. Due to the aligned feature space
with LLMs, our model can smoothly integrate with exist-
ing LLMs, such as LLaMA series , Vicuna ,
and InternLM . These features distinguish our model
from the previous approaches and establish a leading vision-
language foundation model for various applications.
In summary, our contribution has three folds:
(1) We present a large-scale vision-language foundation
model—InternVL, which aligns the large-scale vision en-
coder with LLMs for the first time. The model demonstrates
strong performance on a wide range of generic visual-
linguistic tasks, including visual perception tasks, vision-
language tasks, and multi-modal dialogue.

(2) We introduce a progressive image-text alignment
strategy for the efficient
training of large-scale vision-
language foundation models. This strategy maximizes the
utilization of web-scale noisy image-text data for con-
trastive learning and fine-grained, high-quality data for gen-
erative learning.

2

87.889.678.475.987.866.482.082.194.575.771.679.659.654.637.056.869.869.363.493.980.468.851.488.974.570.069.294.6117.71531.385.9322.588.290.479.977.589.869.183.283.895.577.373.980.664.561.544.965.776.175.567.595.785.074.958.692.977.771.473.896.6128.21586.487.6327.635557595115135155IN-1KIN-ReaLIN-V2IN-AIN-RIN-SketchIN-1KIN-AIN-RIN-V2IN-SketchObjectNetIN-1K (ZH)IN-1K (JP)IN-1K (AR)IN-1K (IT)Kinetics-400Kinetics-600Kinetics-700Flickr30K I2TFlickr30K T2ICOCO I2TCOCO T2IFlickr30K-CN I2TFlickr30K-CN T2ICOCO-CN I2TCOCO-CN T2IXTD R@10 I2TZS COCO CaptionMMEPOPETiny LVLMPervious SOTAOur PerformanceDialogueLinear-Probe Image ClassificationZero-Shot Image & Video ClassificationZero-Shot Image-Text Retrieval(3) We extensively compare the proposed model with
the current state-of-the-art vision foundation models and
VLLMs.
The results indicate that InternVL achieves
leading performance on a broad range of generic visual-
linguistic tasks, including image classification (ImageNet),
semantic segmentation (ADE20K), video classification (Ki-
netics), image-text retrieval (Flickr30K & COCO), video-
text retrieval (MSR-VTT), and image captioning (COCO &
Flickr30K & NoCaps). Meanwhile, it is also effective for
multi-modal dialogue (MME & POPE & Tiny LVLM).

