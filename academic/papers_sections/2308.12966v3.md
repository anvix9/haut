# Abstract

In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models
(LVLMs) designed to perceive and understand both texts and images. Starting from the
Qwen-LM as a foundation, we endow it with visual capacity by the meticulously de-
signed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and
(iv) multilingual multimodal cleaned corpus. Beyond the conventional image descrip-
tion and question-answering, we implement the grounding and text-reading ability of
Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-
VL and Qwen-VL-Chat, set new records for generalist models under similar model scales
on a broad range of visual-centric benchmarks (e.g., image captioning, question answer-
ing, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on
real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates
superiority compared to existing vision-language chatbots. All models are public to
facilitate future research.

3
2
0
2

t
c
O
3
1

]

V
C
.
s
c
[

3
v
6
6
9
2
1
.
8
0
3
2
:
v
i
X
r
a

Qwen-VL achieves state-of-the-art performance on a broad range of tasks compared with other
generalist models.

∗Equal contribution, †Corresponding author

1

Some qualitative examples generated by our Qwen-VL-Chat. Qwen-VL-Chat supports multiple
image inputs, multi-round dialogue, multilingual conversation, text-reading, localization, fine-grained
recognition and understanding ability.

# Introduction

Recently, Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023; Gao et al.,
2023; Qwen, 2023) have attracted wide attention due to their powerful capabilities in text generation and
comprehension. These models can be further aligned with user intent through fine-tuning instructions,
showcasing strong interactive capabilities and the potential to enhance productivity as intelligent assistants.
However, native large language models only live in the pure-text world, lacking the ability to handle other
common modalities (such as images, speech, and videos), resulting in great restrictions on their application
scope. Motivated by this, a group of Large Vision Language Models (LVLMs) (Alayrac et al., 2022; Chen
et al., 2022; Li et al., 2023c; Dai et al., 2023; Huang et al., 2023; Peng et al., 2023; Zhu et al., 2023; Liu et al.,
2023; Ye et al., 2023b,a; Chen et al., 2023a; Li et al., 2023a; Zhang et al., 2023; Sun et al., 2023; OpenAI, 2023)
have been developed to enhance large language models with the ability to perceive and understand visual
signals. These large-scale vision-language models demonstrate promising potential in solving real-world
vision-central problems.

Nevertheless, despite that lots of works have been conducted to explore the limitation and potency of LVLMs,
current open-source LVLMs always suffer from inadequate training and optimization, thus lag far behind
the proprietary models (Chen et al., 2022, 2023b; OpenAI, 2023), which hinders further exploration and
application of LVLMs in open-source community. What’s more, as real-world visual scenarios are quite
complicated, fine-grained visual understanding plays a crucial role for LVLMs to assist people effectively
and precisely. But only a few attempts had been made toward this direction (Peng et al., 2023; Chen et al.,
2023a), the majority of open-source LVLMs remain perceiving the image in a coarse-grained approach and
lacking the ability to execute fine-grained perception such as object grounding or text reading.

2

In this paper, we explore a way out and present the newest members of the open-sourced Qwen families:
Qwen-VL series. Qwen-VLs are a series of highly performant and versatile vision-language foundation
models based on Qwen-7B (Qwen, 2023) language model. We empower the LLM basement with visual
capacity by introducing a new visual receptor including a language-aligned visual encoder and a position-
aware adapter. The overall model architecture as well as the input-output interface are quite concise and
we elaboratedly design a 3-stage training pipeline to optimize the whole model upon a vast collection of
image-text corpus.

Our pre-trained checkpoint, termed Qwen-VL, is capable of perceiving and understanding visual inputs,
generating desired responses according to given prompts, and accomplishing various vision-language tasks
such as image captioning, question answering, text-oriented question answering, and visual grounding.
Qwen-VL-Chat is the instruction-tuned vision-language chatbot based on Qwen-VL. As shown in Fig. 2,
Qwen-VL-Chat is able to interact with users and perceive the input images following the intention of users.

Specifically, the features of the Qwen-VL series models include:

• Leading performance: Qwen-VLs achieve top-tier accuracy on a vast of vision-centric understanding
benchmarks compared to counterparts with similar scales. Besides, Qwen-VL’s stuning performance
covers not only the conventional benchmarks e.g., captioning, question-answering, grounding), but
also some recently introduced dialogue benchmarks.

• Multi-lingual: Similar to Qwen-LM, Qwen-VLs are trained upon multilingual image-text data with a
considerable amount of corpus being in English and Chinese. In this way, Qwen-VLs naturally support
English, Chinese, and multilingual instructions.

• Multi-image: In the training phase, we allow arbitrary interleaved image-text data as Qwen-VL’s inputs.
This feature allows our Qwen-Chat-VL to compare, understand, and analyze the context when multiple
images are given.

• Fine-grained visual understanding: Thanks to the higher-resolution input size and fine-grained corpus
we used in training, Qwen-VLs exhibit highly competitive fine-grained visual understanding ability.
Compared to existing vision-language generalists, our Qwen-VLs possess much better grounding,
text-reading, text-oriented question answering, and fine-grained dialog performance.

