# Abstract

Large multimodal models (LMM) have recently shown
encouraging progress with visual instruction tuning. In this
paper, we present the first systematic study to investigate
the design choices of LMMs in a controlled setting under
the LLaVA framework. We show that the fully-connected
vision-language connector in LLaVA is surprisingly power-
ful and data-efficient. With simple modifications to LLaVA,
namely, using CLIP-ViT-L-336px with an MLP projection
and adding academic-task-oriented VQA data with response
formatting prompts, we establish stronger baselines that
achieve state-of-the-art across 11 benchmarks. Our final
13B checkpoint uses merely 1.2M publicly available data,
and finishes full training in ∼1 day on a single 8-A100 node.
Furthermore, we present some early exploration of open
problems in LMMs, including scaling to higher resolution
inputs, compositional capabilities, and model hallucination,
etc. We hope this makes state-of-the-art LMM research more
accessible. Code and model will be publicly available.

# Introduction

Large multimodal models (LMMs) have become increas-
ingly popular in the research community, as they are
the key building blocks towards general-purpose assis-
tants . Recent studies on LMMs are converg-
ing on a central concept known as visual instruction tun-
ing . The results are promising, e.g. LLaVA  and
MiniGPT-4  demonstrate impressive results on natural
instruction-following and visual reasoning capabilities. To
better understand the capability of LMMs, multiple bench-
marks  have been proposed. Recent
works further demonstrate improved performance by scal-
ing up the pretraining data , instruction-following
data , visual encoders , or language mod-
els , respectively. The LLaVA architecture is also lever-
aged in different downstream tasks and domains, including
region-level  and pixel-level  understanding,
biomedical assistants , image generation , adversarial
studies .

LLaVA-1.5 achieves SoTA on a broad range of 11 tasks
(Top), with high training sample efficiency (Left) and simple mod-
ifications to LLaVA (Right): an MLP connector and including
academic-task-oriented data with response formatting prompts.

However, despite many benchmarks and developments, it
still remains unclear what the best recipe is to train LMMs
towards the goal of general-purpose assistants. For exam-
ple, LLaVA  excels in conversational-style visual rea-
soning and even outperforms later approaches like Instruct-
BLIP  on such benchmarks , while InstructBLIP
excels in traditional VQA benchmarks that demands single-
word or short answers. Given the significant differences
in the model architecture and training data between them,
the root cause of the disparity in their capabilities remains
elusive, despite conjectures : the amount of training
data, the usage of resamplers like Qformer , etc. To this

VQAv2GQAVizWizSQA-IMGTextVQAPOPEMMEMMBenchMMBench-CNSEED-BenchLLaVA-BenchMM-Vet85.31293.849.533.463.150.778.953.458.225.678.257.538.968.261.51487.560.656.758.280.063.353.671.661.385.91531.367.763.661.672.536.1BLIP-2InstructBLIPQwen-VL-ChatLLaVA-1.5101103# Training Samples (M)InstructBLIPQwen-VL-ChatLLaVA-1.51291.21400500.560.67Pre-trainingInstruction Tuning

end, we present the first systematic study to investigate the
design choices of LMMs in a controlled setting. Our study
originates from LLaVA and builds a road map by carefully
making effective contributions from the perspectives of the
input, model, and data.

First, we unveil that the fully-connected vision-language
connector in LLaVA is surprisingly powerful and data-
efficient, and we establish stronger and more feasible base-
lines built upon the LLaVA framework. We report that two
simple improvements, namely, an MLP cross-modal con-
nector and incorporating academic task related data such
as VQA, are orthogonal to the framework of LLaVA, and
when used with LLaVA, lead to better multimodal under-
standing capabilities. In contrast to InstructBLIP  or
Qwen-VL , which trains specially designed visual resam-
plers on hundreds of millions or even billions of image-text
paired data, LLaVA uses one of the simplest architecture
design for LMMs and requires only training a simple fully-
connected projection layer on merely 600K image-text pairs.
Our final model can finish training in ∼1 day on a single
8-A100 machine and achieves state-of-the-art results on a
wide range of benchmarks. Moreover, unlike Qwen-VL 
that includes in-house data in training, LLaVA utilizes only
publicly available data.

Next, we delve into an early exploration of other open
problems of large multimodal models. Our findings include:
(1) Scaling to high-resolution image inputs. We show that
LLaVA’s architecture is versatile in scaling to higher res-
olutions by simply dividing images into grids and main-
tains its data efficiency; with the increased resolution, it
improves the model’s detailed perception capabilities and
reduces hallucination. (2) Compositional capabilities. We
find that large multimodal models are capable of general-
izing to compositional capabilities. For example, training
on long-form language reasoning together with shorter vi-
sual reasoning can improve the model’s writing capability
for multimodal questions. (3) Data efficiency. We show
that randomly downsampling LLaVA’s training data mixture
by up to 75% does not significantly decrease the model’s
performance, suggesting that the possibility of a more so-
phisticated dataset compression strategy can further improve
LLaVA’s already efficient training pipeline. (4) Data scaling.
We provide empirical evidence for the scaling of data granu-
larity in conjunction with the model’s capability is crucial
for an improved capability without introducing artifacts like
hallucination.

In sum, we perform a systematic study on the training
of large multimodal models, and introduce a simple yet
effective approach to balance the multitask learning and ef-
fective scaling for large multimodal models. Our improved
baselines, LLaVA-1.5, uses only public data, achieves the
state-of-the-art on a broad range of 11 tasks, and is signif-
icantly more data-efficient than previous approaches. By

rethinking the conventional approaches and exploring the
open problems in visual instruction tuning, we pave the way
for more robust and capable systems for LMMs. We hope
these improved and easily-reproducible baselines will pro-
vide a reference for future research in open-source LMMs.

