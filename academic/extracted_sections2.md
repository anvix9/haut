# Abstract

Large language models (LLMs) have emerged as powerful tools for generating human-quality text,
raising concerns about their potential for misuse in academic settings. This paper investigates the
use of DistilBERT, a distilled version of BERT, for detecting LLM-generated text. We evaluate its
performance on two publicly available datasets, LLM-Detect AI Generated Text and DAIGT-V3 Train
Dataset, achieving an average accuracy of around 94%. Our ﬁndings suggest that DistilBERT is a
promising tool for safeguarding academic integrity in the era of LLMs.

Keywords: DistilBert, BERT, LLM, Transformer, LLM-generated text, ChatGPT

# Method

3.1 Models

speciﬁcally

research
The primary model used in this
dis-
the
is
DistilBERT,
variant. DistilBERT
til bert base uncased
is a distilled version of BERT (Bidirectional
Encoder Representations from Transformers), a
transformer-based machine learning technique for
natural language processing tasks.

BERT is a powerful

language model that
has signiﬁcantly improved the state-of-the-art on
many Natural Language Processing tasks. How-
ever, it is also quite large, with models containing
billions of parameters  and being trained on
massive datasets. As a result, using BERT for pro-
duction applications can be challenging due to the
high computational requirements for training and
inference.

Fig. 1 Architecture of DistilBERT

3

The architecture of DistilBERT is shown in
DistilBERT addresses these issues by
creating a smaller, faster, and cheaper version of
BERT. It achieves this by using a process known
as distillation, where a larger model (the teacher)
transfers its knowledge to a smaller model (the
student). The student model is trained to mimic
the output of the teacher model, thus retaining
most of its performance while reducing its size and
computational requirements.

The distil bert base uncased [] variant of
DistilBERT is pre-trained on the same corpus as
the BERT base model in a self-supervised manner.
This means it was trained on raw texts without
any human labeling, using an automatic process
to generate inputs and labels from those texts
using the BERT base model. It was trained with
three objectives: distillation loss, masked language
modeling (MLM) , and cosine embedding loss.

Fig. 2 Flow diagram of DistilBERT

having 40% fewer parameters. In terms of infer-
ence time, DistilBERT is more than 60% faster
and requires 40% less memory than BERT. These
advantages make DistilBERT a highly eﬀective
choice for many NLP tasks, especially in scenarios
where computational resources are limited.

The Figure 2 depicts the DistilBERT classi-
ﬁcation process for identifying whether a given
text was written by a human or generated by
a machine. First, the input text is divided into
individual tokens (words or sub-words) and then
converted into numerical representations called
embeddings. These embeddings are then fed into
multiple attention layers that analyze the relation-
ships between the tokens. Finally, a classiﬁcation
layer determines the ﬁnal outcome, classifying
the text as either human-written or machine-
generated.

3.2 Metrics of Evaluation

The performance of the models was evaluated
using four key metrics: accuracy, precision, recall,
and the F1 score. These metrics were calculated
based on the True Positive (TP), True Negative
(TN), False Positive (FP), and False Negative
(FN) values derived from the model’s predictions.
Accuracy was computed as the ratio of cor-
rect predictions (both true positives and true
negatives) to the total number of instances. This
gives us an overall measure of how often the model
is correct in its predictions.

Accuracy =

T P + T N
T P + T N + F P + F N

(1)

Precision was deﬁned as the proportion of
true positive predictions out of all positive predic-
tions. It provides a measure of the model’s ability
to correctly identify positive instances.

Precision =

T P
T P + F P

(2)

Recall, also known as sensitivity, measures
the proportion of actual positive instances that
were correctly identiﬁed. It provides a measure of
the model’s ability to correctly identify positive
instances.

Compared to BERT, DistilBERT retains more
than 95% of the performance of BERT while

Recall =

T P
T P + F N

(3)

4

The F1 score is the harmonic mean of pre-
cision and recall, giving equal weight to both
metrics. It ranges from 0 to 1, with 1 indicat-
ing perfect precision and recall, and 0 indicating
poor performance. The F1 score is especially use-
ful when dealing with imbalanced datasets, as it
takes both false positives and false negatives into
account.

F1 Score =

2 ∗ (Precision ∗ Recall)
(Precision + Recall)

(4)

These metrics provide a comprehensive evalu-
ation of the model’s performance. By considering
both precision and recall, the F1 score oﬀers a
balance between these two metrics, making it a
better choice than accuracy when dealing with
imbalanced datasets.

# Conclusion

This research investigated the performance of Dis-
tilBERT in detecting LLM-generated text. We
conducted experiments on two publicly available
datasets, the LLM-Detect AI Generated Text
dataset and the DAIGT-V3 Train Dataset, and
evaluated the model’s performance using four key
metrics: accuracy, precision, recall, and the F1
score.

The results of the experiments showed that
DistilBERT achieved high performance in detect-
ing LLM-generated text. The model achieved
an average accuracy of around 94% on the
LLM-Detect AI Generated Text dataset and
the DAIGT-V3 Train Dataset. Additionally, the
model achieved high precision, recall, and F1
scores on both datasets.

These results suggest that DistilBERT is a
promising tool for detecting LLM-generated text.
The model’s high performance and eﬃciency make
it a viable option for a variety of applications,
such as academic integrity assessment and con-
tent moderation. We also explored the impact of
diﬀerent hyperparameter settings on the model’s
performance. It is found that the model was most
sensitive to the number of layers and the dropout
rate. However, even with a small number of lay-
ers and a moderate dropout rate, the model still
achieved high performance.

Overall, the research demonstrates that Distil-
BERT is a powerful and eﬀective tool for detecting

LLM-generated text. The model’s high perfor-
mance and eﬃciency make it a promising option
for a variety of applications.

5.1 Future Work

Future research directions for evaluating Distil-
Bert’s performance in detecting Large Language
Model (LLM) generated text could include the
following:

Exploring Alternative Models: While Dis-
tilBert has proven eﬀective, comparing its per-
formance against other prominent models like
RoBERTa , ALBERT , or T5  would
oﬀer a wider perspective on architectural suit-
ability for this task. This broader understand-
ing would guide us towards the most eﬀective
models for distinguishing human-written and AI-
generated text.

Expanding Linguistic Reach: The current
focus on English language evaluation presents an
opportunity for expansion. Future research can
assess DistilBert’s performance across diverse lan-
guages, gauging its ability to identify AI-generated
text in multilingual settings. This would signiﬁ-
cantly enhance its real-world applicability.

Tackling Longer Sequences: As the length
of analyzed text sequences increases, DistilBert’s
performance might decline. Investigating its han-
dling of longer sequences and devising methods
to improve its performance in such scenarios is
crucial for ensuring its eﬀectiveness in diverse use
cases.

Bridging the Gap to Real-World Appli-
cations: Existing research often leans towards
theoretical exploration. Future studies can delve
deeper into real-world applications such as plagia-
rism detection or fake news identiﬁcation, demon-
impact of these models
strating the practical
and guiding their development towards tangible
solutions.

Enhancing Model

Interpretability:
Understanding the rationale behind a model’s
classiﬁcation decisions can be invaluable. Future
research on model interpretability can not only
provide insights into the decision-making pro-
cess but also potentially lead to performance
improvements.

Combining Techniques: Exploring the com-
bination of DistilBert with other techniques like
rule-based systems or other machine learning

models holds promise for further performance
gains. This synergistic approach could leverage the
strengths of diﬀerent methods to create a more
robust and accurate system for LLM detection.

By pursuing these diverse research directions,
we can reﬁne and expand the capabilities of Distil-
Bert and other models, ultimately unlocking their
full potential
for distinguishing human-written
and AI-generated text across various languages
and real-world applications.

Data Availability Statement

The datasets analyzed during the current study
are available in the following repositories:
• LLM - Detect AI Generated
https://www.kaggle.com/competitions/
llm-detect-ai-generated-text/data

• DAIGT-V3 Train Dataset: https://www.kaggle.
com/datasets/thedrcat/daigt-v3-train-dataset

Text:

Access to these datasets may be subject to
speciﬁc terms and conditions. Please refer to the
respective repository pages for more details and
instructions on how to obtain access.

