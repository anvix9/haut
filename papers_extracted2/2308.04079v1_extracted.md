## 1 INTRODUCTION

Meshes and points are the most common 3D scene representations because they are explicit and are a good /fit for fast GPU/CUDA-based rasterization. In contrast, recent Neural Radiance Field (NeRF) methods build on continuous scene representations, typically optimizing a Multi-Layer Perceptron (MLP) using volumetric ray-marching for novel-view synthesis of captured scenes. Similarly, the most e/fficient radiance /field solutions to date build on continuous representations by interpolating values stored in, e.g., voxel [Fridovich-Keil and Yu et al. 2022] or hash [M√ºller et al. 2022] grids or points [Xu et al. 2022]. While the continuous nature of these methods helps optimization, the stochastic sampling required for rendering is costly and can result in noise. We introduce a new approach that combines the best of both worlds: our 3D Gaussian representation allows optimization with state-of-the-art (SOTA) visual quality and competitive training times, while our tile-based splatting solution ensures real-time rendering at SOTA quality for 1080p resolution on several previously published datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch et al. 2017] (see Fig. 1).

Our goal is to allow real-time rendering for scenes captured with multiple photos, and create the representations with optimization times as fast as the most e/fficient previous methods for typical real scenes. Recent methods achieve fast training [Fridovich-Keil

and Yu et al. 2022; M√ºller et al. 2022], but struggle to achieve the visual quality obtained by the current SOTA NeRF methods, i.e., Mip-NeRF360 [Barron et al. 2022], which requires up to 48 hours of training time. The fast - but lower-quality - radiance /field methods can achieve interactive rendering times depending on the scene (10-15 frames per second), but fall short of real-time rendering at high resolution.

Our solution builds on three main components. We /first introduce 3D Gaussians as a /flexible and expressive scene representation. We start with the same input as previous NeRF-like methods, i.e., cameras calibrated with Structure-from-Motion (SfM) [Snavely et al. 2006] and initialize the set of 3D Gaussians with the sparse point cloud produced for free as part of the SfM process. In contrast to most point-based solutions that require Multi-View Stereo (MVS) data [Aliev et al. 2020; Kopanas et al. 2021; R√ºckert et al. 2022], we achieve high-quality results with only SfM points as input. Note that for the NeRF-synthetic dataset, our method achieves high quality even with random initialization. We show that 3D Gaussians are an excellent choice, since they are a di/fferentiable volumetric representation, but they can also be rasterized very e/fficiently by projecting them to 2D, and applying standard ùõº -blending, using an equivalent image formation model as NeRF. The second component of our method is optimization of the properties of the 3D Gaussians - 3D position, opacity ùõº , anisotropic covariance, and spherical harmonic (SH) coe/fficients - interleaved with adaptive density control steps, where we add and occasionally remove 3D Gaussians during optimization. The optimization procedure produces a reasonably compact, unstructured, and precise representation of the scene (1-5 million Gaussians for all scenes tested). The third and /final element of our method is our real-time rendering solution that uses fast GPU sorting algorithms and is inspired by tile-based rasterization, following recent work [Lassner and Zollhofer 2021]. However, thanks to our 3D Gaussian representation, we can perform anisotropic splatting that respects visibility ordering - thanks to sorting and ùõº -blending - and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required.

To summarize, we provide the following contributions:

- ¬∑ Theintroduction of anisotropic 3D Gaussians as a high-quality, unstructured representation of radiance /fields.
- ¬∑ An optimization method of 3D Gaussian properties, interleaved with adaptive density control that creates high-quality representations for captured scenes.
- ¬∑ A fast, di/fferentiable rendering approach for the GPU, which is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis.

Our results on previously published datasets show that we can optimize our 3D Gaussians from multi-view captures and achieve equal or better quality than the best quality previous implicit radiance /field approaches. We also can achieve training speeds and quality similar to the fastest methods and importantly provide the /first real-time rendering with high quality for novel-view synthesis.

## 3 OVERVIEW

The input to our method is a set of images of a static scene, together with the corresponding cameras calibrated by SfM [Sch√∂nberger and Frahm 2016] which produces a sparse point cloud as a sidee/ffect. From these points we create a set of 3D Gaussians (Sec. 4), de/fined by a position (mean), covariance matrix and opacity ùõº , that allows a very /flexible optimization regime. This results in a reasonably compact representation of the 3D scene, in part because highly anisotropic volumetric splats can be used to represent /fine structures compactly. The directional appearance component (color) of the radiance /field is represented via spherical harmonics (SH), following standard practice [Fridovich-Keil and Yu et al. 2022; M√ºller et al. 2022]. Our algorithm proceeds to create the radiance /field representation (Sec. 5) via a sequence of optimization steps of 3D Gaussian parameters, i.e., position, covariance, ùõº and SH coe/fficients interleaved with operations for adaptive control of the Gaussian density. The key to the e/fficiency of our method is our tile-based rasterizer (Sec. 6) that allows ùõº -blending of anisotropic splats, respecting visibility order thanks to fast sorting. Out fast rasterizer also includes a fast backward pass by tracking accumulated ùõº values, without a limit on the number of Gaussians that can receive gradients. The overview of our method is illustrated in Fig. 2.

## 8 DISCUSSION AND CONCLUSIONS

We have presented the /first approach that truly allows real-time, high-quality radiance /field rendering, in a wide variety of scenes and capture styles, while requiring training times competitive with the fastest previous methods.

Our choice of a 3D Gaussian primitive preserves properties of volumetric rendering for optimization while directly allowing fast splat-based rasterization. Our work demonstrates that - contrary to widely accepted opinion - a continuous representation is not strictly necessary to allow fast and high-quality radiance /field training.

The majority ( ‚àº 80%) of our training time is spent in Python code, since we built our solution in PyTorch to allow our method to be easily used by others. Only the rasterization routine is implemented as optimized CUDA kernels. We expect that porting the remaining optimization entirely to CUDA, as e.g., done in InstantNGP [M√ºller et al. 2022], could enable signi/ficant further speedup for applications where performance is essential.

We also demonstrated the importance of building on real-time rendering principles, exploiting the power of the GPU and speed of software rasterization pipeline architecture. These design choices are the key to performance both for training and real-time rendering, providing a competitive edge in performance over previous volumetric ray-marching.

It would be interesting to see if our Gaussians can be used to perform mesh reconstructions of the captured scene. Aside from practical implications given the widespread use of meshes, this would allow us to better understand where our method stands exactly in the continuum between volumetric and surface representations.

In conclusion, we have presented the /first real-time rendering solution for radiance /fields, with rendering quality that matches the best expensive previous methods, with training times competitive with the fastest existing solutions.

