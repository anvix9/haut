## Abstract

The year 1948 witnessed the historic moment of the birth of classic information theory (CIT). Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function H ( U ) , channel capacity C = max p ( x ) I ( X ; Y ) and rate-distortion function R ( D ) = min p (ˆ x | x ): E d ( x, ˆ x ) ≤ D I ( X ; ˆ X ) . Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework of semantic information theory (SIT). We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information. Stemming from this core concept, synonymous mapping f , we introduce the measures of semantic information, such as semantic entropy H s ( ˜ U ) , up/down semantic mutual information I s ( ˜ X ; ˜ Y ) ( I s ( ˜ X ; ˜ Y )) , semantic capacity C s = max f xy max p ( x ) I s ( ˜ X ; ˜ Y ) , and semantic rate-distortion function R s ( D ) = min { f x ,f ˆ x } min p (ˆ x | x ): E d s (˜ x, ˆ ˜ x ) ≤ D I s ( ˜ X ; ˆ ˜ X ) . Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is, H s ( ˜ U ) ≤ H ( U ) , C s ≥ C and R s ( D ) ≤ R ( D ) . All these works composite the basis of semantic information theory. In addition, we discuss the semantic information measures in the continuous case. Especially, for the band-limited Gaussian channel, we obtain a new channel capacity formula, C s = B log [ S 4 ( 1 + P N 0 B )] , where the average synonymous length S indicates the identification ability of information. In summary, the theoretic framework of SIT proposed in this paper is a natural extension of CIT and may reveal great performance potential for future communication.

This work is supported by the National Natural Science Foundation of China (No. 62293481, 62071058). K. Niu and P. Zhang are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China (e-mail: { niukai, pzhang } @bupt.edu.cn).

## I. INTRODUCTION

Classic information theory (CIT), established by C. E. Shannon [1] in 1948, was a great achievement in the modern information and communication field. As shown in Fig. 1, the classic communication system includes source, encoder, channel with noise, decoder and destination. This theory is concerned with the uncertainty of information and introduces four critical measures, such as entropy, mutual information, channel capacity, and rate-distortion function to evaluate the performance of information processing and transmission. Especially, three famous coding theorems, such as, lossless/lossy source coding theorem and channel coding theorem, reveal the fundamental limitation of data compression and information transmission. Over the past 70 years or so, people developed many advanced techniques to approach these theoretical limits. For the lossless source coding, Huffman coding and arithmetic coding are the representative optimal coding methods can achieve the source entropy. Similarly, for the channel coding, polar code, as a great breakthrough [23], is the first constructive capacity-achieving coding scheme. Correspondingly, for the lossy source coding, some modern coding schemes, such as BPG (Better Portable Graphics) standard and H. 265/266 standard, can approach the rate-distortion lower bounds of image and video sources. It follows that information and communication technologies guided by CIT have approached the theoretical limitation and the performance improvement of modern communication systems encounters a lot of bottlenecks.

Essentially, Weaver [3], just one year after Shannon published the seminal paper on information theory, pointed out that communication involves problems at three levels as follows:

' LEVEL A . How accurately can the symbols of communication be transmitted? (The technical problem.)

LEVEL B . How precisely do the transmitted symbols convey the desired meaning? (The semantic problem.)

Fig. 1. The block diagram of classic communication system.

<!-- image -->

## C. Semantic Source Coding Method

Similar to the classic source coding, the variable length coding is desired for semantic source coding. Thus we also obtain the semantic version of Kraft inequality as following.

Theorem 16. (Semantic Kraft Inequality): Given a discrete random variable U ∈ U = { u i } N i =1 , the corresponding semantic variable ˜ U ∈ ˜ U = { ˜ u i s } ˜ N i s =1 , and the synonymous mapping f : ˜ U → U . For any prefix code over an alphabet of size F exists if and only if the codeword length l 1 , l 2 , · · · , l ˜ N satisfies

˜ N ∑ i s =1 F -l is ≤ 1 . (99)

The proof is similar to that of classic Kraft inequality and omitted. It should be noted that the semantic Kraft inequality has the same form as that of classic counterpart. However, since the semantic prefix code is performed over a synonymous set rather than a single syntactic symbol, the number of codewords is less than classic prefix code, that is, ˜ N ≤ N .

PROBABILITY DISTRIBUTION AND HUFFMAN CODES OF SYNTACTIC SOURCE U .

TABLE VII

| Syntactic symbol   | u 1   | u 2   | u 3   | u 4   |
|--------------------|-------|-------|-------|-------|
| Probability        | 1 2   | 1 4   | 1 8   | 1 8   |
| Syn. HC            | 0     | 10    | 110   | 111   |

Furthermore, we can obtain the average code length of optimal semantic source code as follows.

Theorem 17. Given the syntactic source distribution p ( u ) and the synonymous mapping f : ˜ U → U , let l ∗ 1 , l ∗ 2 , · · · , l ∗ ˜ N denote the optimal code lengths with an F -ary alphabet, then the expected length ¯ L ∗ of the optimal semantic code satisfies

H s ( ˜ U ) log F ≤ ¯ L ∗ < H s ( ˜ U ) log F +1 . (100)

Proof: Assign the code length as l i s = ⌈ -log F ∑ i ∈U is p ( u i ) ⌉ . Similar to the classic version, by semantic Kraft inequality, we have

H s ( ˜ U ) log F ≤ ¯ L ∗ ≤ ˜ N ∑ i s =1 p (˜ u i s ) l i s < H s ( ˜ U ) log F +1 . (101)

So we prove the theorem.

□

Example 4. (Semantic Huffman Coding): Now we describe an example of semantic Huffman coding. For a syntactic source U with four symbols u 1 , u 2 , u 3 , u 4 , the probability distribution is listed in Table VII. The information entropy is H ( U ) = 1 . 75 bits. By using Huffman coding, the codewords are shown in Table VII and the average code length is ¯ L = 1 . 75 bits = H ( U ) .

If we give a synonymous mapping f , that is, ˜ u 1 → { u 1 } , ˜ u 2 → { u 2 } , ˜ u 3 → { u 3 , u 4 } , the probability distribution of semantic source ˜ U is listed in Table VIII. So the semantic entropy is calculated as H s ( ˜ U ) = 1 . 5 sebits. Correspondingly, by the semantic Huffman codewords listed in Table VIII, the average code length is ¯ L s = 1 . 5 sebits = H s ( ˜ U ) . Distinctly, due to synonymous mapping, the average code length of semantic Huffman code is smaller than that of traditional Huffman code.

Given a syntactic sequence u = ( u 1 u 1 u 3 u 4 u 2 u 3 u 2 ) , by Table VII, the syntactic Huffman coding is x = (001101111011010) . On the other hand, by Table VIII, the semantic Huffman coding is

PROBABILITY DISTRIBUTION AND HUFFMAN CODES OF SEMANTIC SOURCE U .

TABLE VIII ˜

| Semantic symbol   | ˜ u 1 →{ u 1 }   | ˜ u 2 →{ u 2 }   | ˜ u 3 →{ u 3 , u 4 }   |
|-------------------|------------------|------------------|------------------------|
| Probability       | 1 2              | 1 4              | 1 4                    |
| Sem. HC           | 0                | 10               | 11                     |

x s = (001111101110) . Hence, the length of syntactic coding is L ( x ) = 15 bits and the length of semantic coding is L ( x s ) = 12 sebits so that the latter is smaller than the former, i.e., L ( x s ) < L ( x ) . Certainly, since the decoder can select arbitrary symbol from the set { u 3 , u 4 } when decoding ˜ u 3 , the result may be ˆ u = ( u 1 u 1 u 3 u 3 u 2 u 4 u 2 ) . Although such decoding sequence is different from the original one u = ( u 1 u 1 u 3 u 4 u 2 u 3 u 2 ) in the syntactic sense, the semantic information of the decoding results still keeps the same since u 3 and u 4 have the same meaning.

Remark 8. For the method of semantic source coding, we have two kinds of design thought. The first kind thought is to modify the traditional source coding, such as Huffman, arithmetic or universal coding. By using an elaborate synonymous mapping, these coding methods can be devised to further improve the compression efficiency. For the second thought, based on the deep learning method, we can construct a neural network model to perform semantic source coding. In this model, the synonymous mapping and semantic coding can be integrated and optimized to approach the theoretic limitation.

## C. Semantic Channel Coding Method

We now investigate the semantic channel coding method. Given a ( 2 n ( R + R s ) , n ) channel code with length n and semantic rate R , the codebook C can be divided into 2 nR synonymous codeword groups C s ( i s ) , i s ∈ { 1 , 2 , · · · , 2 nR } and each group has 2 nR s synonymous codewords. Consider the synonymous mapping, we propose a new method, named as maximum likelihood group (MLG) decoding algorithm to decode this semantic code. The basic idea is to calculate the likelihood probability of the received signal on a synonymous group and compare all the group likelihood probabilities so as to select a group with the maximum probability as the final decoding result.

Definition 23. Assume one codeword x n ∈ C s ( i s ) is transmitted on the discrete memoryless channel with the transition probability p ( y n | x n ) , the group likelihood probability is defined as

P ( y n |C s ( i s )) ≜ 2 nRs ∏ l =1 p ( y n | x n ( i s , l )) . (145)

So the maximum likelihood group decoding rule is written as

ˆ i s = arg max i s P ( y n |C s ( i s )) = arg max i s 2 nRs ∏ l =1 p ( y n | x n ( i s , l )) . (146)

Equivalently, this rule can also presented as a logarithmic version,

ˆ i s = arg max i s 2 nRs ∑ l =1 ln p ( y n | x n ( i s , l )) . (147)

Hence, we can calculate all the group likelihood probabilities and select one group with the maximum probability as the final decision. The index ˆ i s indicates the estimation of semantic information ˆ ˜ x n .

s

s

Next, we discuss the MLG decoding in the additive white Gaussian noise (AWGN) channel. When a signal is transmitted over the AWGN channel, the received signal can be represented by an equivalent low-pass signal sampled at time k :

y k = s k + z k (148)

where s k = { ± √ E s } is the binary phase shifted key (BPSK) signal, z k is a sample of a zeromean complex Gaussian noise process with variance σ 2 = N 0 / 2 . Let E s be the symbol energy and N 0 denote the single-sided power spectral density of the additive white noise. So the symbol signal-to-noise ratio (SNR) is defined as E s N 0 .

Assume one codeword x n ( i s , l ) is mapped into a transmitted signal vector s n ( i s , l ) = √ E s (1 n -2 x n ( i s , l )) where 1 n is the all-one vector with the length of n , by using MLG rule, we can write

ˆ i s = arg max i s 2 nRs ∑ l =1 ln p ( y n | x n ( i s , l )) = arg max i s 2 nRs ∑ l =1 ln [ 1 √ 2 πσ 2 e -∥ y n -s n ( is,l ) ∥ 2 2 σ 2 ] = arg min i s 2 nRs ∑ l =1 ∥ y n -s n ( i s , l ) ∥ 2 (149)

=

arg

min

i

s

where d 2 E ( y n , C s ( i s )) = ∑ 2 nRs l =1 ∥ y n -s n ( i s , l ) ∥ 2 is the squared Euclidian distance between the receive vector y n and the code group C s ( i s ) . Thus the MLG rule in AWGN channel is transformed into the minimum distance grouping decoding rule.

Now we investigate the group-wise error probability (GEP) of semantic channel code.

Theorem 22. Given a semantic channel code C with equipartition code groups C s , the GEP between C s ( i s ) and C s ( j s ) is upper bounded by

P ( C s ( i s ) →C s ( j s )) ≤ exp { -d GH ( C s ( i s ) , C s ( j s )) E s N 0 } , (150)

where d GH denotes the group Hamming distance which is defined as following

d GH ( C s ( i s ) , C s ( j s )) =

̸

min m [ ∑ 2 nRs l =1 d H ( x n ( i s , m ) , x n ( j s , l )) -∑ 2 nRs l =1 ,l = m d H ( x n ( i s , m ) , x n ( i s , l )) ] 2 ∥ ∑ 2 nRs l =1 ( x n ( j s , l ) -x n ( i s , l )) ∥ 2 . (151)

2

E

d

(

y

n

,

C

(

i

))

,

Proof: Assume one codeword x n (1 , 1) in the code group C s (1) is transmitted, the received signal vector can be represented as follows

y n = s n (1 , 1) + z n , (152)

where z n ∼ N ( 0 , σ 2 I ) is the Gaussian noise vector.

̸

Suppose a codeword x n ( j s , l ) ∈ C s ( j s ) , j s = 1 is mapped into the signal vector s n ( j s , l ) . By using the MLG rule, if a group-wise error occurs, the Euclidian distance between the received vector and the transmitted signal vector group satisfy the inequality

d 2 E ( y n , C s ( i s )) > d 2 E ( y n , C s ( j s )) . (153)

Substituting (149) and (152) into this inequality, we have

2 nRs ∑ l =1 ∥ y n -s n (1 , l ) ∥ 2 > 2 nRs ∑ l =1 ∥ y n -s n ( j s , l ) ∥ 2 ⇒ ∥ z n ∥ 2 + 2 nRs ∑ l =2 ∥ s n (1 , 1) -s n (1 , l ) + z n ∥ 2 > 2 nRs ∑ l =1 ∥ s n (1 , 1) -s n ( j s , l ) + z n ∥ 2 . (154)

After some manipulations, the error decision region can be written as

H =    z n :   2 nRs ∑ l =1 ( s n ( j s , l ) -s n (1 , l ))   ( z n ) T > 1 2   2 nRs ∑ l =1 ∥ s n (1 , 1) -s n ( j s , l ) ∥ 2 -2 nRs ∑ l =2 ∥ s n (1 , 1) -s n (1 , l ) ∥ 2      . (155)

Let d 2 E ( s n (1 , 1) , C s ( j s )) = ∑ 2 nRs l =1 ∥ s n (1 , 1) -s n ( j s , l ) ∥ 2 denote the distance between the transmit vector s n (1 , 1) and the code group C s ( j s ) and d 2 E ( s n (1 , 1) , C s (1)) = ∑ 2 nRs l =2 ∥ s n (1 , 1) -s n (1 , l ) ∥ 2 denote the inner distance of code group C s (1) .

So the codeword-to-group error probability can be derived as

P ( x n (1 , 1) →C s ( j s )) = Q    √ √ √ √ ( d 2 E ( s n (1 , 1) , C s ( j s )) -d 2 E ( s n (1 , 1) , C s (1))) 2 ∥ ∥ ∥ ∑ 2 nRs l =1 ( s n ( j s , l ) -s n (1 , l )) ∥ ∥ ∥ 2 2 N 0    , (156)

where Q ( x ) = 1 √ 2 π ∫ ∞ x e -t 2 / 2 dt is the tail distribution function of the standard normal distribution.

Furthermore, due to ( s n (1 , 1) -s n ( j s , l )) = 2 √ E s ( x n ( j s , l ) -x n (1 , 1)) , then we have ∥ s n (1 , 1) -s n ( j s , l ) ∥ 2 = 4 E s d H ( x n (1 , 1) , x n ( j s , l )) . Additionally, we can derive that ∥ ∥ ∥ ∑ 2 nRs l =1 ( s n ( j s , l ) -s n (1 , l )) ∥ ∥ ∥ 2 = 4 E s ∥ ∥ ∥ ∑ 2 nRs l =1 ( x n ( j s , l ) -x n (1 , l )) ∥ ∥ ∥ 2 = 4 E s ∥ ∆( C s ( j s ) , C s (1)) ∥ 2 . Thus the error probability can be

further written as

P ( x n (1 , 1) →C s ( j s )) = Q [ √ d GH ( x n (1 , 1) , C s ( j s )) 2 E s N 0 ] , (157)

where d GH ( x n (1 , 1) , C s ( j s )) =[ ∑ 2 nRs l =1 d H ( x n (1 , 1) , x n ( j s , l )) -∑ 2 nRs l =2 d H ( x n (1 , 1) , x n (1 , l ))] 2 / ∥ ∆( C s ( j s ) , C s (1)) ∥ 2 denotes the codeword-to-group Hamming distance.

Furthermore, using the inequality Q ( x ) ≤ e -x 2 2 , the codeword-to-group error probability can be upper bounded by

P ( x n (1 , 1) →C s ( j s )) ≤ e -d GH ( x n (1 , 1) , C s ( j s )) Es N 0 . (158)

Averaging over all the codewords of the group C s (1) , we obtain the upper bound of GEP as follows

P ( C s (1) →C s ( j s )) ≤ 2 nRs ∑ l =1 1 2 nR s e -d GH ( x n (1 ,l ) , C s ( j s )) Es N 0 ≤ exp { -d GH ( C s (1) , C s ( j s )) E s N 0 } . (159)

So we complete the proof.

□

In the ML decoding, the minimum Hamming distance d H,min determines the error performance of one linear channel code. Similarly, in the MLG decoding, the minimum group Hamming distance d GH,min = min d GH ( C s ( i s ) , C s ( j s )) dominates the performance of semantic channel code.

Example 5. We now give an example of semantic code constructed based on (7,4) Hamming code with synonymous mapping and MLG decoding. The codebook is shown in Table IX. All

the sixteen codewords are divided into eight code groups and each group has two synonymous codewords. For an instance, C s (1) has two codewords (0000000) and (1101000) and its semantic sequence is (000) . So this code can be regarded as a (7,3) semantic Hamming code with code rate R = 3 7 and R s = 1 7 .

By using ML decoding, the union bound of the error probability is

P e ≤ n ∑ d = d H,min A d Q ( √ 2 d E s N 0 ) ≤ n ∑ d = d H,min A d e -d Es N 0 . (160)

Since the minimum Hamming distance of this code is d H,min = 3 and distance spectrum is { A 3 = 8 , A 4 = 6 , A 7 = 1 } , the error probability of ML decoding is upper bounded by

P ML e ≤ 8 e -3 Es N 0 +6 e -4 Es N 0 + e -7 Es N 0 . (161)

Let { A d 1 ,d 2 } denote the group distance spectrum and d 1 and d 2 mean the codeword-to-group Hamming distance. By using MLG decoding, the union bound of the error probability is

P e ≤ n ∑ d 1 ,d 2 = d GH,min A d 1 ,d 2 2 [ Q ( √ 2 d 1 E s N 0 ) + Q ( √ 2 d 2 E s N 0 )] ≤ n ∑ d 1 ,d 2 = d GH,min A d 1 ,d 2 2 ( e -d 1 Es N 0 + e -d 2 Es N 0 ) . (162)

So the minimum group Hamming distance of this code is d GH,min = 2 and group distance spectrum is { A 2 , 2 = 6 , A 4 , 4 = 1 } . The corresponding upper bound of the MLG decoding is

P MLG e ≤ 6 e -2 Es N 0 + e -4 Es N 0 . (163)

Compare with (161) and (163), we find that the minimum distance of semantic Hamming code is decreased. However, for a long code length and well-designed synonymous mapping, the error performance of MLG decoding will be better than that using ML decoding.

Remark 10. From the viewpoint of practical application, semantic channel codes are a new kind of channel codes. Synonymous mapping provides a valuable idea for the construction and decoding of semantic channel codes. Unlike the traditional channel codes, semantic channel codes should optimize the minimum group Hamming distance. How to design an optimal synonymous mapping to cleverly partition the code group is significant for the design of semantic codes. Non-equipartition mapping may be more flexible than the equipartition mapping. On the other hand, the optimal decoding of semantic codes is the MLG rule rather ML rule. However,

TABLE IX THE CODEBOOK OF (7,3) SEMANTIC HAMMING CODE WITH SYNONYMOUS MAPPING.

|   Index i s |   Semantic sequence | Hamming code group C s ( i s )   |
|-------------|---------------------|----------------------------------|
|           1 |                 000 | { 0000000, 1101000 }             |
|           2 |                 001 | { 0110100, 1011100 }             |
|           3 |                 010 | { 1110010, 0011010 }             |
|           4 |                 011 | { 1000110, 0101110 }             |
|           5 |                 100 | { 1010001, 0111001 }             |
|           6 |                 101 | { 1100101, 0001101 }             |
|           7 |                 110 | { 0100011, 1001011 }             |
|           8 |                 111 | { 0010111, 1111111 }             |

due to the exponent complexity of MLG decoding algorithm, it is not practical for application. So we should pursuit lower complexity decoding algorithms for the semantic channel codes in the future.

## XI. CONCLUSIONS

In this paper, we develop an information-theoretic framework of semantic communication. We start from the synonym, a fundamental property of semantic information, to build the semantic information measures including semantic entropy, up/down semantic mutual information,

semantic channel capacity, and semantic rate distortion function. Then we extend the asymptotic equipartition property to the semantic sense and introduce the synonymous typical set to prove three significant coding theorems, that is, semantic source coding theorem, semantic channel coding theorem, and semantic rate distortion coding theorem. Additionally, we investigate the semantic information measures in the continuous case and derive the semantic capacity of Gaussian channel and semantic rate distortion of Gaussian source. All these works uncover the critical features of semantic communication and constitute the theoretic basis of semantic information theory.

For the theoretic analysis, the semantic information theory needs further development. In this paper, we only consider the semantic information measure and the fundamental limitation in the discrete or continuous memoryless case. In the future, we can further investigate the measure and limitation of semantic information in various memory source or channel cases, such as stationary and ergodic process (e.g. Markov process) or non-stationary non-ergodic process. Strong asymptotic equipartition property and strong typicality in the semantic sense should be further explored. On the other hand, the analysis of semantic capacity or semantic rate distortion with finite block length may also be an interesting research topic. In addition, in various multiuser communication scenarios, such as multiple access, broadcasting, relay etc., we can further analyze and derive the corresponding measure and performance limit of semantic information.

Guided by the classic information theory, in the past seventy years, the source coding and channel coding techniques have approached the theoretic limitation. On the contrary, the semantic information theory paves a new way for the coding techniques. From the viewpoint of semantic processing, with the help of synonymous mapping, the lossless source coding has much space to improve and the existing coding methods can be further modified and polished. The construction of semantic channel codes may be centered on the group Hamming distance and the optimization of decoding algorithms will be concentrated on the group decoding so that the information transmission techniques will usher in a new era that surpasses the classic limitation and approaches the semantic capacity. By the optimization of synonymous mapping, the classic lossy source coding techniques, such as vector quantization, prediction coding, and transform coding, will demonstrate new advantages to further improve the compression efficiency. Briefly, the performance bottleneck of classic communication will be broken and the traditional

communication will naturally evolve to the semantic communication.

For the new coding techniques based on deep learning (DL), the semantic information theory will lift its mystery veil and provide a systematic design and optimization tool. The synonymous mapping will provide a reasonable explanation for the semantic information extracted by the deep neural network. The basic structures of mainstream DL models, such as convolutional neural networks, transformer model, variational auto-encoder and so on, may be analyzed and optimized based on the semantic information measures. Furthermore, the system architecture of semantic communication based on deep learning can be simplified or optimized guided by the semantic information theory.

In summary, the theoretic framework proposed in this paper may help understanding the essential features of semantic information and shed light on some ambiguity problems in semantic communication. We believe that the semantic information theory will uncover a new chapter of information theory and have a profound impact on many fields such as communication, signal detection and estimation, deep learning and machine learning, and integrated sensing and communication etc.

