## Abstract

We release Code Llama , a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models ( Code Llama ), Python specializations ( Code Llama - Python ), and instruction-following models ( Code Llama - Instruct ) with 7B, 13B, 34B, and 70B parameters each. These models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. The 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use. 1

## 1 Introduction

Large language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency in natural language that allows them to be commanded and prompted to perform a variety of tasks (OpenAI, 2023; Touvron et al., 2023b). By utilizing large, in-domain datasets, their efficacy can be greatly improved for applications that require a combination of both natural and domain-specific language and understanding of specialized terminology. By training on domain-specific datasets, they have proved effective more broadly on applications that require advanced natural language understanding. A prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation (for a survey, see Xu & Zhu, 2022, also see Section 5). In this work, we present Code Llama , a family of LLMs for code generation and infilling derived from Llama 2 (Touvron et al., 2023b) and released under the same custom permissive license. We provide inference code for both completion and infilling models in the accompanying repository. 1 Our approach is based on gradually specializing and increasing the capabilities of Llama 2 models by applying a cascade of training and fine-tuning steps (Figure 2):

- · Code-training from foundation models. While most LLMs for code generation such as AlphaCode (Li et al., 2022), InCoder (Fried et al., 2023) or StarCoder (Li et al., 2023) are trained on code only, Codex (Chen et al., 2021) was fine-tuned from a general language model. We also start from a foundation model ( Llama 2 , Touvron et al., 2023b) pretrained on general-purpose text and code data. Our comparison (Section 3.4.1) shows that initializing our model with Llama 2 outperforms the same architecture trained on code only for a given budget.

Prompt: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month

Response: You can use the 'find' command in Bash to list all text files in the current directory (excluding subdirectories) that have been modified in the last month. The command would be:

find . -maxdepth 1 -type f -mtime -30 -name "*.txt"

Here's a breakdown of the command:

- * '.' specifies the current directory.
- * '-maxdepth 1' tells 'find' to only search the current directory and not any subdirectories.
- * '-type f' tells 'find' to only search for files (not directories or other types of files).
- * '-mtime -30' tells 'find' to only search for files that have been modified in the last 30 days.
- * '-name "*.txt"' tells 'find' to only search for files with the '.txt' extension.

Note that the '-mtime' option uses the number of days since the file was last modified, so '-mtime -30' means "modified in the last 30 days". If you want to search for files modified in the last month, you can use the '-mtime -30' option.

Figure 1: Example of response of Code Llama - Instruct (34B) when queried for a specific shell command.

- · Infilling. Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does not provide the capability to fill a missing portion of text while taking the full surrounding context into account. Our code-training for 7B, 13B and 70B Code Llama models features a multitask objective (Fried et al., 2023) consisting of both autoregressive and causal infilling prediction, enabling applications such as real-time completion in source code editors or docstring generation.
- · Long input contexts. Unlocking repository-level reasoning for completion or synthesis - as opposed to function-level or file-level - requires prompting the model with much longer context than the 4,096 tokens supported by Llama 2 . We propose an additional fine-tuning stage that extends the maximum context length from 4,096 tokens to 100,000 tokens by modifying the parameters of the RoPE positional embeddings (Su et al., 2021) used in Llama 2 . Our experiments show Code Llama operating on very large contexts with a moderate impact on performances on standard coding benchmarks (Section 3.3).
- · Instruction fine-tuning. For end-users, the utility of LLMs is significantly improved by instruction fine-tuning (Ouyang et al., 2022; Wei et al., 2022; OpenAI, 2023; Touvron et al., 2023b), which also helps preventing unsafe, toxic or biased generations. Code Llama - Instruct variants are further fine-tuned on a mix of proprietary instruction data for improved safety and helpfulness, and a new machine-generated self-instruct dataset created by prompting Llama 2 for coding problems and Code Llama to generate associated unit tests and solutions. Our results show that Code Llama - Instruct significantly improves performance on various truthfulness, toxicity and bias benchmarks at moderate cost in terms of code generation performance (Section 4).

Different combinations of these approaches lead to a family of code-specialized Llama 2 models with three main variants that we release in four sizes (7B, 13B, 34B and 70B parameters):

- · Code Llama : a foundational model for code generation tasks,
- · Code Llama - Python : specialized for Python,
- · Code Llama - Instruct : fine-tuned with human instructions and self-instruct code synthesis data.

An example of using Code Llama - Instruct is given in Figure 1. It show-cases that the model interprets natural language to determine suitable options for a command-line program and provides an explanation of the solution. We provide further qualitative examples in Appendix L. We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023), where our best models establish a new state of the art amongst open-source LLMs. The technical details of our training and fine-tuning procedures are provided in Section 2, followed by in-depth experiments and ablation studies, details of the safety/helpfulness evaluations and a discussion of related work.

<latexit sha1\_base64="xxjQ0qU69VzePnVfp3QwO6CZats=">AAAB/HicbVDLSsNAFJ3UV42vaJduBovgqiRS1GXRjcsK9gFtKJPppB06mYSZGyWU+ituXCji1g9x5984abPQ1gMDh3Pu4d45QSK4Btf9tkpr6xubW+Vte2d3b//AOTxq6zhVlLVoLGLVDYhmgkvWAg6CdRPFSBQI1gkmN7nfeWBK81jeQ5YwPyIjyUNOCRhp4FTsvuKjscmFQJSKH7VtD5yqW3PnwKvEK0gVFWgOnK/+MKZpxCRQQbTueW4C/pQo4FSwmd1PNUsInZAR6xkqScS0P50fP8OnRhniMFbmScBz9XdiSiKtsygwkxGBsV72cvE/r5dCeOVPuUxSYJIuFoWpwBDjvAk85IpREJkhhCpubsV0TBShYPrKS/CWv7xK2uc176JWv6tXG9dFHWV0jE7QGfLQJWqgW9RELURRhp7RK3qznqwX6936WIyWrCJTQX9gff4A8lSUUw==</latexit>

Figure 2: The Code Llama specialization pipeline . The different stages of fine-tuning annotated with the number of tokens seen during training. Infilling-capable models are marked with the ⇄ symbol.

<!-- image -->

## 3.4 Ablation studies



## 6 Discussion

We release a family of code-specialized Llama 2 models called Code Llama , with three main variants that we release with four sizes (7B, 13B, 34B, and 70B parameters): Code Llama , Code Llama - Python , Code Llama - Instruct . With real-world applications in mind, we trained our 7B, 13B, and 70B models to support infilling, and all our models to leverage large contexts. We tested their stability in inference up to 100K tokens (Figure 4a). Large context fine-tuning and infilling come at a cost on standard benchmarks left-to-right code generation benchmarks (Table 10), that are all based on short sequences (i.e. function level). Still, our 70B model is state-of-the-art among public models on standard python completion benchmarks, and our other models are competitive compared to models with similar numbers of parameters. On multilingual benchmarks, even our smallest model ( Code Llama 7B) outperforms every other public model.

The Code Llama - Instruct models are trained to provide zero-shot instruction ability to Code Llama . In this further fine-tuning, where we somewhat distillate Llama 2 -Chat, we focused not only on being more directly helpful (Figure 5c) but also sought to provide a safer model to use and deploy (Section 4). Following instruction and being overly safe can cost some points on evaluations (e.g. on HumanEval for the 34B model in Table 2), as exemplified in Figure 15. Further work is needed for LLMs to understand context and nuance in their instructions.

## G.1 Further Discussion

For illustrating the effect of increasing the base period of rotary position embeddings, we plot expectations for attention scores when varying the distance between key and query vectors in Figure 9a. Compared to the default base period of 10,000, θ = 1 , 000 , 000 reduces the decay in attention scores, which helps far-away tokens contribute to the current prediction. Notably, this change in rotation frequencies can be applied to pretrained models, with loss curves stabilizing within a few gradient steps at a low learning rate. While the uniform frequency scaling proposed by Chen et al. (2023b) is motivated by maintaining the overall range of rotations when increasing the context from the sequence length used for pretraining, our modification explicitly addresses the problem of performing attention over long distances.

