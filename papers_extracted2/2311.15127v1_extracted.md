## Abstract

We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for cu-

rating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base

<!-- image -->

<!-- image -->

model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/ Stability-AI/generative-models .

## 1. Introduction

Driven by advances in generative image modeling with diffusion models [38, 68, 71, 76], there has been significant recent progress on generative video models both in research [9, 42, 82, 95] and real-world applications [54, 74] Broadly, these models are either trained from scratch [41] or finetuned (partially or fully) from pretrained image models with additional temporal layers inserted [9, 32, 43, 82]. Training is often carried out on a mix of image and video datasets [41].

While research around improvements in video modeling has primarily focused on the exact arrangement of the spatial and temporal layers [9, 41, 43, 82], none of the aforementioned works investigate the influence of data selection. This is surprising, especially since the significant impact of the training data distribution on generative models is undisputed [13, 105]. Moreover, for generative image modeling, it is known that pretraining on a large and diverse dataset and finetuning on a smaller but higher quality dataset significantly improves the performance [13, 71]. Since many previous approaches to video modeling have successfully drawn on techniques from the image domain [9, 42, 43], it is noteworthy that the effect of data and training strategies, i.e., the separation of video pretraining at lower resolutions and high-quality finetuning, has yet to be studied. This work directly addresses these previously uncharted territories.

Webelieve that the significant contribution of data selection is heavily underrepresented in today's video research landscape despite being well-recognized among practitioners when training video models at scale. Thus, in contrast to previous works, we draw on simple latent video diffusion baselines [9] for which we fix architecture and training scheme and assess the effect of data curation . To this end, we first identify three different video training stages that we find crucial for good performance: text-to-image pretraining, video pretraining on a large dataset at low resolution, and high-resolution video finetuning on a much smaller dataset with higher-quality videos. Borrowing from largescale image model training [13, 64, 66], we introduce a systematic approach to curate video data at scale and present an empirical study on the effect of data curation during video

pretraining. Our main findings imply that pretraining on well-curated datasets leads to significant performance improvements that persist after high-quality finetuning.

A general motion and multi-view prior Drawing on these findings, we apply our proposed curation scheme to a large video dataset comprising roughly 600 million samples and train a strong pretrained text-to-video base model, which provides a general motion representation. We exploit this and finetune the base model on a smaller, high-quality dataset for high-resolution downstream tasks such as textto-video (see Figure 1, top row) and image-to-video, where we predict a sequence of frames from a single conditioning image (see Figure 1, mid rows). Human preference studies reveal that the resulting model outperforms state-of-the-art image-to-video models.

Furthermore, we also demonstrate that our model provides a strong multi-view prior and can serve as a base to finetune a multi-view diffusion model that generates multiple consistent views of an object in a feedforward manner and outperforms specialized novel view synthesis methods such as Zero123XL [14, 57] and SyncDreamer [58]. Finally, we demonstrate that our model allows for explicit motion control by specifically prompting the temporal layers with motion cues and also via training LoRAmodules [32, 45] on datasets resembling specific motions only, which can be efficiently plugged into the model.

To summarize, our core contributions are threefold: (i) We present a systematic data curation workflow to turn a large uncurated video collection into a quality dataset for generative video modeling. Using this workflow, we (ii) train state-of-the-art text-to-video and image-to-video models, outperforming all prior models. Finally, we (iii) probe the strong prior of motion and 3D understanding in our models by conducting domain-specific experiments. Specifically, we provide evidence that pretrained video diffusion models can be turned into strong multi-view generators, which may help overcome the data scarcity typically observed in the 3D domain [14].

## 2. Background

Most recent works on video generation rely on diffusion models [38, 84, 87] to jointly synthesize multiple consistent frames from text- or image-conditioning. Diffusion models implement an iterative refinement process by learning to gradually denoise a sample from a normal distribution and have been successfully applied to highresolution text-to-image [13, 64, 68, 71, 75] and video synthesis [9, 29, 41, 82, 95].

In this work, we follow this paradigm and train a latent [71, 92] video diffusion model [9, 23] on our video dataset. We provide a brief overview of related works which utilize latent video diffusion models (Video-LDMs)

in the following paragraph; a full discussion that includes approaches using GANs [10, 30] and autoregressive models [43] can be found in App. B. An introduction to diffusion models can be found in App. D.

## 5. Conclusion

We present Stable Video Diffusion (SVD), a latent video diffusion model for high-resolution, state-of-the-art text-tovideo and image-to-video synthesis. To construct its pretraining dataset, we conduct a systematic data selection and scaling study, and propose a method to curate vast amounts of video data and turn large and noisy video collection into suitable datasets for generative video models. Furthermore, we introduce three distinct stages of video model training which we separately analyze to assess their impact on the final model performance. Stable Video Diffusion provides a powerful video representation from which we finetune video models for state-of-the-art image-to-video synthesis and other highly relevant applications such as LoRAs for camera control. Finally we provide a pioneering study on multi-view finetuning of video diffusion models and show that SVD constitutes a strong 3D prior, which obtains stateof-the-art results in multi-view synthesis while using only a

fraction of the compute of previous methods.

We hope these findings will be broadly useful in the generative video modeling literature. A discussion on our work's broader impact and limitations can be found in App. A.

## D.2. Base Model Training and Architecture

As discussed in , we start the publicly available Stable Diffusion 2.1 [71] (SD 2.1) model. In the EDM-framework [51], SD 2.1 has the following preconditioning functions:

c SD2 . 1 skip ( σ ) = 1 , (5)

c SD2 . 1 out ( σ ) = -σ , (6)

c SD2 . 1 in ( σ ) = 1 √ σ 2 +1 , (7)

c SD2 . 1 noise ( σ ) = arg min j ∈ [1000] ( σ -σ j ) , (8)

(9)

where σ j +1 > σ j . The distribution over noise levels p ( σ ) used for the original SD 2.1. training is a uniform distribution over the 1000 discrete noise levels { σ j } j ∈ [1000] . One issue with the training of SD 2.1 (and in particular its noise distribution p ( σ ) ) is that even for the maximum discrete noise level σ 1000 the signal-to-noise ratio [53] is still relatively high which results in issues when, for example, generating very dark images [34, 56]. Guttenberg and CrossLabs [34] proposed offset noise , a modification of the training objective in Eq. (2) by making p ( n | σ ) non-isotropic Gaussian. In this work, we instead opt to modify the preconditioning functions and distribution over training noise levels altogether.

Image model finetuning. We replace the above preconditioning functions with

c skip ( σ ) = ( σ 2 +1 ) -1 , (10)

c out ( σ ) = -σ √ σ 2 +1 , (11)

c in ( σ ) = 1 √ σ 2 +1 , (12)

c noise ( σ ) = 0 . 25 log σ, (13)

(14)

which can be recovered in the EDM framework [51] by setting σ data = 1 ); the preconditioning functions were originally proposed in [79]. We also use the noise distribution and weighting function proposed in Karras et al. [51], namely log σ ∼ N ( P mean , P 2 std ) and λ ( σ ) = (1+ σ 2 ) σ -2 , with P mean = -1 . 2 and P std = 1 . We then finetune the neural network backbone F θ of SD2.1 for 31k iterations using this setup. For the first 1k iterations, we freeze all parameters of F θ except for the time-embedding layer and train on SD2.1's original training resolution of 512 × 512 . This allows the model to adapt to the new preconditioning functions without unnecessarily modifying the internal representations of F θ too much. Afterward, we train all layers of F θ for another 30k iterations on images of size 256 × 384 , which is the resolution used in the initial stage of video pretraining.

Video pretraining. We use the resulting model as the image backbone of our video model. We then insert temporal convolution and attention layers. In particular, we follow the exact setup from [9], inserting a total of 656M new parameters into the UNet bumping its total size (spatial and temporal layers) to 1521M parameters. We then train the resulting UNet on 14 frames on resolution 256 × 384 for 150k iters using AdamW [59] with learning rate 10 -4 and a batch size of 1536. We train the model for classifier-free guidance [36] and drop out the text-conditioning 15% of the time. Afterward, we increase the spatial resolution to 320 × 576 and train for an additional 100k iterations, using the same settings as for the lower-resolution training except for a reduced batch size of 768 and a shift of the noise distribution towards more noise, in

particular, we increase P mean = 0 . During training, the base model and the high-resolution Text/Image-to-Video models are all conditioned on the input video's frame rate and motion score. This allows us to vary the amount of motion in a generated video at inference time.

