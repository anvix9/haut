## 1 Introduction

Foundation models are general models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks. They form the basis of many modern AI systems.

The development of modern foundation models consists of two main stages: (1) a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next-word prediction or captioning and (2) a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities (for example, coding and reasoning).

In this paper, we present a new set of foundation models for language, called Llama 3 . The Llama 3 Herd of models natively supports multilinguality, coding, reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. Each member of the herd is listed in Table 1. All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity.

We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process:

- · Data. Compared to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity and quality of the data we use for pre-training and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data and the development of more rigorous quality assurance and filtering approaches for post-training data. We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2.
- · Scale. We train a model at far larger scale than previous Llama models: our flagship language model was pre-trained using 3 . 8 × 10 25 FLOPs, almost 50 × more than the largest version of Llama 2. Specifically, we pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens. As expected per

Table 1 Overview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 models.

|                         | Finetuned   | Multilingual   | Long context   | Tool use   | Release    |
|-------------------------|-------------|----------------|----------------|------------|------------|
| Llama 3 8B              | ✗           | ✗ 1            | ✗              | ✗          | April 2024 |
| Llama 3 8B Instruct     | ✓           | ✗              | ✗              | ✗          | April 2024 |
| Llama 3 70B             | ✗           | ✗ 1            | ✗              | ✗          | April 2024 |
| Llama 3 70B Instruct    | ✓           | ✗              | ✗              | ✗          | April 2024 |
| Llama 3.1 8B            | ✗           | ✓              | ✓              | ✗          | July 2024  |
| Llama 3.1 8B Instruct   | ✓           | ✓              | ✓              | ✓          | July 2024  |
| Llama 3.1 70B           | ✗           | ✓              | ✓              | ✗          | July 2024  |
| Llama 3.1 70B Instruct  | ✓           | ✓              | ✓              | ✓          | July 2024  |
| Llama 3.1 405B          | ✗           | ✓              | ✓              | ✗          | July 2024  |
| Llama 3.1 405B Instruct | ✓           | ✓              | ✓              | ✓          | July 2024  |

scaling laws for foundation models, our flagship model outperforms smaller models trained using the same procedure. While our scaling laws suggest our flagship model is an approximately compute-optimal size for our training budget, we also train our smaller models for much longer than is compute-optimal. The resulting models perform better than compute-optimal models at the same inference budget. We use the flagship model to further improve the quality of those smaller models during post-training.

- · Managing complexity. We make design choices that seek to maximize our ability to scale the model development process. For example, we opt for a standard dense Transformer model architecture (Vaswani et al., 2017) with minor adaptations, rather than for a mixture-of-experts model (Shazeer et al., 2017) to maximize training stability. Similarly, we adopt a relatively simple post-training procedure based on supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO; Rafailov et al. (2023)) as opposed to more complex reinforcement learning algorithms (Ouyang et al., 2022; Schulman et al., 2017) that tend to be less stable and harder to scale.

The result of our work is Llama 3: a herd of three multilingual 1 language models with 8B, 70B, and 405B parameters. We evaluate the performance of Llama 3 on a plethora of benchmark datasets that span a wide range of language understanding tasks. In addition, we perform extensive human evaluations that compare Llama 3 with competing models. An overview of the performance of the flagship Llama 3 model on key benchmarks is presented in Table 2. Our experimental evaluation suggests that our flagship model performs on par with leading language models such as GPT-4 (OpenAI, 2023a) across a variety of tasks, and is close to matching the state-of-the-art. Our smaller models are best-in-class, outperforming alternative models with similar numbers of parameters (Bai et al., 2023; Jiang et al., 2023). Llama 3 also delivers a much better balance between helpfulness and harmlessness than its predecessor (Touvron et al., 2023b). We present a detailed analysis of the safety of Llama 3 in Section 5.4.

We are publicly releasing all three Llama 3 models under an updated version of the Llama 3 Community License; see https://llama.meta.com . This includes pre-trained and post-trained versions of our 405B parameter language model and a new version of our Llama Guard model (Inan et al., 2023) for input and output safety. We hope that the open release of a flagship model will spur a wave of innovation in the research community, and accelerate a responsible path towards the development of artificial general intelligence (AGI).

As part of the Llama 3 development process we also develop multimodal extensions to the models, enabling image recognition, video recognition, and speech understanding capabilities. These models are still under active development and not yet ready for release. In addition to our language modeling results, the paper presents results of our initial experiments with those multimodal models.

Table 2 Performance of finetuned Llama 3 models on key benchmark evaluations. The table compares the performance of the 8B, 70B, and 405B versions of Llama 3 with that of competing models. We boldface the best-performing model in each of three model-size equivalence classes. △ Results obtained using 5-shot prompting (no CoT). ◁ Results obtained without CoT. ♢ Results obtained using zero-shot prompting.

| Category     | Benchmark                        | Llama 3 8B   | Gemma 2 9B   | Mistral 7B   | Llama 3 70B   | Mixtral 8x22B   | GPT 3.5 T urbo   | Llama 3 405B   | Nemotron 4   | 340B GPT -4 (0125)   | GPT -4o   | Claude 3.5 Sonnet   |
|--------------|----------------------------------|--------------|--------------|--------------|---------------|-----------------|------------------|----------------|--------------|----------------------|-----------|---------------------|
|              | MMLU (5-shot)                    | 69.4         | 72.3         | 61.1         | 83.6          | 76.9            | 70.7             | 87.3           | 82.6         | 85.1                 | 89.1      | 89.9                |
| General      | MMLU (0-shot, CoT)               | 73.0         | 72.3 △       | 60.5         | 86.0          | 79.9            | 69.8             | 88.6           | 78.7 ◁       | 85.4                 | 88.7      | 88.3                |
| General      | MMLU-Pro (5-shot, CoT)           | 48.3         | -            | 36.9         | 66.4          | 56.3            | 49.2             | 73.3           | 62.7         | 64.8                 | 74.0      | 77.0                |
| General      | IFEval                           | 80.4         | 73.6         | 57.6         | 87.5          | 72.7            |                  | 88.6           | 85.1         |                      |           | 88.0                |
| General      |                                  |              |              |              | 80.5          | 75.6            | 69.9             |                |              | 84.3                 | 85.6      |                     |
| Code         | HumanEval (0-shot)               | 72.6         | 54.3         | 40.2         |               |                 | 68.0             | 89.0           | 73.2         | 86.6                 | 90.2      | 92.0                |
| Code         | MBPP EvalPlus (0-shot) GSM8K     | 72.8         | 71.7         | 49.5         | 86.0          | 78.6            | 82.0             | 88.6           | 72.8         | 83.6                 | 87.8      | 90.5                |
| Math         | (8-shot, CoT)                    | 84.5         | 76.7         | 53.2         | 95.1          | 88.2            | 81.6             | 96.8           | 92.3 ♢       | 94.2                 | 96.1      | 96.4 ♢              |
|              | MATH (0-shot, CoT) ARC Challenge | 51.9         | 44.3         | 13.0         | 68.0 94.8     | 54.1            | 43.1             | 73.8           | 41.1         | 64.5                 | 76.6      | 71.1                |
| Reasoning    | (0-shot)                         | 83.4         | 87.6         | 74.2 28.8    |               | 88.7 33.3       | 83.7 30.8        | 96.9 51.1      | 94.6 -       | 96.4 41.4            | 96.7 53.6 | 96.7                |
|              | GPQA (0-shot, CoT) BFCL          | 32.8 76.1    | - -          |              | 46.7          |                 | 85.9             | 88.5           | 86.5         | 88.3                 | 80.5      | 59.4 90.2           |
| Tool use     | Nexus                            | 38.5         |              | 60.4 24.7    | 84.8 56.7     | - 48.5          | 37.2             | 58.7           | -            | 50.3                 | 56.1      | 45.7                |
|              | ZeroSCROLLS/QuALITY              | 81.0         | 30.0 -       | -            | 90.5          |                 | -                | 95.2           |              | 95.2                 | 90.5      |                     |
| Long context | InfiniteBench/En.MC              | 65.1         | -            | -            | 78.2          | - -             | -                | 83.4           | -            |                      |           | 90.5                |
| Long context | NIH/Multi-needle                 |              | -            |              |               |                 |                  |                | -            | 72.1                 | 82.5      | -                   |
| Long context |                                  | 98.8         |              | -            | 97.5          | -               | -                | 98.1           | -            | 100.0                | 100.0     | 90.8                |
| Multilingual | MGSM (0-shot, CoT)               | 68.9         | 53.2         | 29.9         | 86.9          | 71.1            | 51.4             | 91.6           | -            | 85.9                 | 90.5      | 91.6                |

## 2 General Overview

The model architecture of Llama 3 is illustrated in Figure 1. The development of our Llama 3 language models comprises two main stages:

- · Language model pre-training. We start by converting a large, multilingual text corpus to discrete tokens and pre-training a large language model (LLM) on the resulting data to perform next-token prediction. In the language model pre-training stage, the model learns the structure of language and obtains large amounts of knowledge about the world from the text it is 'reading'. To do this effectively, pre-training is performed at massive scale: we pre-train a model with 405B parameters on 15.6T tokens using a context window of 8K tokens. This standard pre-training stage is followed by a continued pre-training stage that increases the supported context window to 128K tokens. See Section 3 for details.
- · Language model post-training. The pre-trained language model has a rich understanding of language but it does not yet follow instructions or behave in the way we would expect an assistant to. We align the model with human feedback in several rounds, each of which involves supervised finetuning (SFT) on instruction tuning data and Direct Preference Optimization (DPO; Rafailov et al., 2024). At this post-training 2 stage, we also integrate new capabilities, such as tool-use, and observe strong improvements in other areas, such as coding and reasoning. See Section 4 for details. Finally, safety mitigations are also incorporated into the model at the post-training stage, the details of which are described in Section 5.4.

The resulting models have a rich set of capabilities. They can answer questions in at least eight languages, write high-quality code, solve complex reasoning problems, and use tools out-of-the-box or in a zero-shot way.

We also perform experiments in which we add image, video, and speech capabilities to Llama 3 using a compositional approach. The approach we study comprises the three additional stages illustrated in Figure 28:

- · Multi-modal encoder pre-training. We train separate encoders for images and speech. We train our image encoder on large amounts of image-text pairs. This teaches the model the relation between visual content and the description of that content in natural language. Our speech encoder is trained using a

Figure 1 Illustration of the overall architecture and training of Llama 3. Llama 3 is a Transformer language model trained to predict the next token of a textual sequence. See text for details.

<!-- image -->

self-supervised approach that masks out parts of the speech inputs and tries to reconstruct the masked out parts via a discrete-token representation. As a result, the model learns the structure of speech signals. See Section 7 for details on the image encoder and Section 8 for details on the speech encoder.

- · Vision adapter training. We train an adapter that integrates the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed imageencoder representations into the language model. The adapter is trained on text-image pairs. This aligns the image representations with the language representations. During adapter training, we also update the parameters of the image encoder but we intentionally do not update the language-model parameters. We also train a video adapter on top of the image adapter on paired video-text data. This enables the model to aggregate information across frames. See Section 7 for details.
- · Speech adapter training. Finally, we integrate the speech encoder into the model via an adapter that converts speech encodings into token representations that can be fed directly into the finetuned language model. The parameters of the adapter and encoder are jointly updated in a supervised finetuning stage to enable high-quality speech understanding. We do not change the language model during speech adapter training. We also integrate a text-to-speech system. See Section 8 for details.

Our multimodal experiments lead to models that can recognize the content of images and videos, and support interaction via a speech interface. These models are still under development and not yet ready for release.

## 3.2 Model Architecture

Llama 3 uses a standard, dense Transformer architecture (Vaswani et al., 2017). It does not deviate significantly from Llama and Llama 2 (Touvron et al., 2023a,b) in terms of model architecture; our performance gains are primarily driven by improvements in data quality and diversity as well as by increased training scale.

We make a few small modifications compared to Llama 2:

- · We use grouped query attention (GQA; Ainslie et al. (2023)) with 8 key-value heads to improve inference speed and to reduce the size of key-value caches during decoding.
- · We use an attention mask that prevents self-attention between different documents within the same sequence. We find that this change had limited impact during in standard pre-training, but find it to be important in continued pre-training on very long sequences.

Table 3 Overview of the key hyperparameters of Llama 3. We display settings for 8B, 70B, and 405B language models.

|                                                        | 8B               | 70B                   | 405B       |
|--------------------------------------------------------|------------------|-----------------------|------------|
| Layers                                                 | 32               | 80                    | 126        |
| Model Dimension                                        | 4,096            | 8192                  | 16,384     |
| FFN Dimension                                          | 14,336           | 28,672                | 53,248     |
| Attention Heads                                        | 32               | 64                    | 128        |
| Key/Value Heads                                        | 8                | 8                     | 8          |
| Peak Learning Rate Activation Function Vocabulary Size | 3 × 10 - 4       | 1 . 5 × 10 - 4 SwiGLU | 8 × 10 - 5 |
| Positional Embeddings                                  | 128,000 RoPE ( ) | θ = 500 ,             | 000        |

- · We use a vocabulary with 128K tokens. Our token vocabulary combines 100K tokens from the tiktoken 3 tokenizer with 28K additional tokens to better support non-English languages. Compared to the Llama 2 tokenizer, our new tokenizer improves compression rates on a sample of English data from 3.17 to 3.94 characters per token. This enables the model to 'read' more text for the same amount of training compute. We also found that adding 28K tokens from select non-English languages improved both compression ratios and downstream performance, with no impact on English tokenization.
- · We increase the RoPE base frequency hyperparameter to 500,000. This enables us to better support longer contexts; Xiong et al. (2023) showed this value to be effective for context lengths up to 32,768.

Llama 3 405B uses an architecture with 126 layers, a token representation dimension of 16,384, and 128 attention heads; see Table 3 for details. This leads to a model size that is approximately compute-optimal according to scaling laws on our data for our training budget of 3 . 8 × 10 25 FLOPs.

## 7.2 Model Architecture

Our visual-recognition model consists of three main components: (1) an image encoder, (2) an image adapter, and (3) a video adapter.

Image encoder. Our image encoder is a standard vision transformer (ViT; Dosovitskiy et al. (2020)) that is trained to align images and text (Xu et al., 2023). We use the ViT-H/14 variant of the image encoder,

which has 630M parameters that were trained on 2.5B image-text pairs for five epochs. The image encoder is pre-trained on images with resolution 224 × 224 ; images were split up into 16 × 16 patches of equal size ( i.e. , a patch size of 14 x 14 pixels). As also demonstrated by prior work such as ViP-Llava (Cai et al., 2024), we observe that image encoders trained via a contrastive text alignment objective are unable to preserve fine-grained localization information. To alleviate this, we employ a multi-layer feature extraction, where features from the 4 th , 8 th , 16 th , 24 th and 31 st layers are also provided in addition to the final layer features. In addition, we further insert 8 gated self-attention layers (making a total of 40 transformer blocks) prior to pre-training of the cross-attention layers to learn alignment-specific features. The image encoder therefore eventually has a total 850 M parameters with the additional layers. With the multi-layer features, the image encoder produces a 7680 -dimensional representation for each of the resulting 16 × 16 = 256 patches. The parameters of the image encoder are not frozen during subsequent training stages as we found it to improve performance, especially in domains such as text recognition.

Image adapter. We introduce cross-attention layers between the visual token representations produced by the image encoder and the token representations produced by the language model (Alayrac et al., 2022). The cross-attention layers are applied after every fourth self-attention layer in the core language model. Like the language model itself, the cross-attention layers use generalized query attention (GQA) for increased efficiency. The cross-attention layers introduce substantial numbers of additional trainable parameters into the model: for Llama 3 405B, the cross-attention layers have ≈ 100B parameters. We pre-train our image adapter in two stages: (1) initial pre-training followed by (2) annealing:

- · Initial pre-training. We pre-train our image adapter on our dataset of ∼ 6B image-text pairs described above. For compute efficiency reasons, we resize all images to fit within at most four tiles of 336 × 336 pixels each, where we arrange the tiles to support different aspect ratios, e.g. , 672 × 672 , 672 × 336 , and 1344 × 336 .
- · Annealing. We continue training the image adapter on ∼ 500M images from the annealing dataset described above. During annealing, we increase the per-tile image resolution to improve performance on tasks that require higher-resolution images, for example, infographics understanding.

Video adapter. Our model takes as input up to 64 frames (uniformly sampled from a full video), each of which is processed by the image encoder. We model temporal structure in videos through two components: (i) encoded video frames are aggregated by a temporal aggregator which merges 32 consecutive frames into one, (ii) additional video cross attention layers are added before every fourth image cross attention layer. The temporal aggregator is implemented as a perceiver resampler (Jaegle et al., 2021; Alayrac et al., 2022). We pre-train using 16 frames per video (aggregated to 1 frame), but increase the number of input frames to 64 during supervised finetuning. The video aggregator and cross attention layers have 0.6B and 4.6B parameters for Llama 3 7B and 70B, respectively.

## 8.2 Model Architecture



## 10 Conclusion

In many ways, the development of high-quality foundation models is still in its infancy. Our experience in developing Llama 3 suggests that substantial further improvements of these models are on the horizon. Throughout the development of the Llama 3 model family, we found that a strong focus on high-quality data, scale, and simplicity consistently yielded the best results. In preliminary experiments, we explored more complex model architectures and training recipes but did not find the benefits of such approaches to outweigh the additional complexity they introduce in model development.

Developing a flagship foundation model such as Llama 3 involves overcoming a plethora of deep technical problems but also requires clever organizational decisions. For example, to ensure Llama 3 is not accidentally overfitted on commonly used benchmarks, our pre-training data was procured and processed by a separate team that was strongly incentivized to prevent contamination of that pre-training data with external benchmarks. As another example, we ensure that our human evaluations remain trustworthy by allowing only a small set of researchers who do not contribute to model development to perform and access these evaluations. While such organizational decisions are rarely discussed in technical papers, we found them to be pivotal to the successful development of the Llama 3 family of models.

We shared the details of our development process because we believe this will: (1) help the larger research community understand the key factors of foundation model development and (2) contribute to a more informed debate about the future of foundation models in the general public. We also shared preliminary experiments with integrating multimodal capabilities into Llama 3. While these models are still under active development and not yet ready for release, we hope sharing our results early will accelerate research in this direction.

Following the positive outcomes of the detailed safety analyses presented in this paper, we publicly release our Llama 3 language models in order to accelerate the development of AI systems for a plethora of societally relevant use cases and enable the research community to scrutinize our models and identify ways to make these models better and safer. We believe that the public release of foundation models plays a key role in the responsible development of such models, and we hope that the release of Llama 3 encourages the industry to embrace the open, responsible development of AGI.

