## Abstract

Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational ine/fficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of e/fficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpli/fied end-to-end neural network architecture without attention or even MLP blocks ( Mamba ). Mamba enjoys fast inference (5 × higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.

## 1 Introduction

Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an e/ffective paradigm in modern machine learning. The backbone of these FMs are often sequence models , operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The e/fficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a /finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more e/fficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it e/ffective. As of yet, none of these variants have been shown to be empirically e/ffective at scale across domains.

Recently, structured state space sequence models (SSMs) (Gu, Goel, and Ré 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very e/fficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range

Arena (Tay, Dehghani, Abnar, et al. 2021). Many /flavors of SSMs (Gu, Goel, and Ré 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less e/ffective at modeling discrete and information-dense data such as text.

We propose a new class of selective state space models , that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.

Selection Mechanism. First, we identify a key limitation of prior models: the ability to e/fficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to /filter out irrelevant information and remember relevant information inde/finitely.

Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally e/fficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between di/fferent levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3 × faster on A100 GPUs).

Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design ( Mamba ) incorporating selective state spaces.

Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and e/fficiency together yield performance improvements on real data up to sequence length 1M.

We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspeci/fic task performance, on several types of modalities and settings:

- · Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions inde/finitely long ( > 1M tokens).
- · Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences .
- · Language Modeling. Mamba is the /first linear-time sequence model that truly achieves Transformer-quality performance , both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5 × generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).

Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba .

## 3.4 A Simpli/fied SSM Architecture

As with structured SSMs, selective SSMs are standalone sequence transformations that can be /flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention.

This architecture involves expanding the model dimension 𝐷 by a controllable expansion factor 𝐸 . For each block, most of the parameters (3 𝐸𝐷 2 ) are in the linear projections (2 𝐸𝐷 2 for input projections, 𝐸𝐷 2 for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for Δ , 𝑩 , 𝑪 , and the matrix 𝑨 ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always /fix to 𝐸 = 2 in our experiments and use two stacks of the block to match the 12 𝐷 2 parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular 'SwiGLU' variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023). Finally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)), motivated by RetNet's usage of a normalization layer in a similar location (Y. Sun et al. 2023).

## 4.6.1 Architecture

- · Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.
- · Replacing the complex-valued S4 variant from previous work with a real-valued one does not a/ffect performance much, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware e/fficiency.
- · Replacing any of these with a selective SSM (S6) signi/ficantly improves performance, validating the motivation of Section 3.

Table 6: ( Ablations: Architecture and SSM layer .) The Mamba block performs similarly to H3 while being simpler. In the inner layer, there is little di/fference among di/fferent parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More speci/fically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.

| M/o.sc/d.sc/e.sc/l.sc   | A/r.sc/c.sc/h.sc.   | SSM L/a.sc/y.sc/e.sc/r.sc   | P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc   |
|-------------------------|---------------------|-----------------------------|--------------------------------------------------|
| Hyena                   | H3                  | Hyena                       | 10 . 24                                          |
| H3                      | H3                  | S4 (complex)                | 10 . 30                                          |
| -                       | H3                  | S4 (real)                   | 10 . 34                                          |
| -                       | H3                  | S6                          | 8 . 95                                           |

Table 7: ( Ablations: Selective parameters .) Δ is the most important parameter (Theorem 1), but using multiple selective parameters together synergizes.

| M/o.sc/d.sc/e.sc/l.sc   | A/r.sc/c.sc/h.sc.   | SSM L/a.sc/y.sc/e.sc/r.sc   | P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc   |
|-------------------------|---------------------|-----------------------------|--------------------------------------------------|
| -                       | Mamba               | Hyena                       | 10 . 75                                          |
| -                       | Mamba               | S4 (complex)                | 10 . 54                                          |
| -                       | Mamba               | S4 (real)                   | 10 . 56                                          |
| Mamba                   | Mamba               | S6                          | 8 . 69                                           |

Table 8: ( Ablations: Parameterization of 𝑨 .) The more standard initializations based on S4D-Lin (Gu, Gupta, et al. 2022) perform worse than S4D-Real or a random initialization, when the SSM is selective.

| S/e.sc/l.sc/e.sc/c.sc/t.sc/i.sc/v.sc/e.sc Δ   | S/e.sc/l.sc/e.sc/c.sc/t.sc/i.sc/v.sc/e.sc 𝑩   | S/e.sc/l.sc/e.sc/c.sc/t.sc/i.sc/v.sc/e.sc 𝑪   |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|-----------------------------------------------|-----------------------------------------------|-----------------------------------------------|--------------------------------------------------|
| ✗                                             | ✗                                             | ✗                                             |                                            10.93 |
| ✗                                             | ✓                                             | ✗                                             |                                            10.15 |
| ✗                                             | ✗                                             | ✓                                             |                                             9.98 |
| ✓                                             | ✗                                             | ✗                                             |                                             9.81 |
| ✓                                             | ✓                                             | ✓                                             |                                             8.71 |

Table 9 and Table 10 consider varying the dimension of the Δ and ( 𝑩 , 𝑪 ) projections respectively. Changing them from static to selective provides the most bene/fit, while increasing the dimensions further generally improves performance modestly with a small increase in parameter count.

| 𝑨 𝑛 I/n.sc/i.sc/t.sc/i.sc/a.sc/l.sc/i.sc/z.sc/a.sc/t.sc/i.sc/o.sc/n.sc   | F/i.sc/e.sc/l.sc/d.sc   |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|--------------------------------------------------------------------------|-------------------------|--------------------------------------------------|
| 𝑨 𝑛 = - 1 2 + 𝑛𝑖                                                         | Complex                 |                                             9.16 |
| 𝑨 𝑛 = - 1 / 2                                                            | Real                    |                                             8.85 |
| 𝑨 𝑛 = -( 𝑛 + 1 )                                                         | Real                    |                                             8.71 |
| 𝑨 𝑛 ∼ exp (N( 0 , 1 ))                                                   | Real                    |                                             8.71 |

- · The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selective layer).

We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid attention architecture) in Appendix E.2.2.

## 5 Discussion

We discuss related work, limitations, and some future directions.

Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models.

Table 10: ( Ablations: SSM state dimension .) ( Top ) Constant 𝑩 and 𝑪 ( Bottom ) Selective 𝑩 and 𝑪 . Increasing the SSM state dimension 𝑁 , which can be viewed as an expansion factor on the dimension of the recurrent state, can signi/ficantly improve performance for a negligible cost in parameters/FLOPs, but only when 𝑩 and 𝑪 are also selective. Size of Δ projection /fixed to 64.

| S/i.sc/z.sc/e.sc /o.sc/f.sc Δ /p.sc/r.sc/o.sc/j.sc.   |   P/a.sc/r.sc/a.sc/m.sc/s.sc (M) |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|-------------------------------------------------------|----------------------------------|--------------------------------------------------|
| -                                                     |                            358.9 |                                             9.12 |
| 1                                                     |                            359.1 |                                             8.97 |
| 2                                                     |                            359.3 |                                             8.97 |
| 4                                                     |                            359.7 |                                             8.91 |
| 8                                                     |                            360.5 |                                             8.83 |
| 16                                                    |                            362.1 |                                             8.84 |
| 32                                                    |                            365.2 |                                             8.8  |
| 64                                                    |                            371.5 |                                             8.71 |

Table 9: ( Ablations: Expressivity of Δ .) The selection mechanism of Δ constructs it with a projection of the input. Projecting it even to dim. 1 provides a large increase in performance; increasing it further provides further improvements at the cost of a modest increase in parameters. State size /fixed to 𝑁 = 16.

|   S/t.sc/a.sc/t.sc/e.sc /d.sc/i.sc/m.sc/e.sc/n.sc/s.sc/i.sc/o.sc/n.sc 𝑁 |   P/a.sc/r.sc/a.sc/m.sc/s.sc (M) |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|-------------------------------------------------------------------------|----------------------------------|--------------------------------------------------|
|                                                                       1 |                            367.1 |                                             9.88 |
|                                                                       2 |                            367.4 |                                             9.86 |
|                                                                       4 |                            368   |                                             9.82 |
|                                                                       8 |                            369.1 |                                             9.82 |
|                                                                      16 |                            371.5 |                                             9.81 |
|                                                                       1 |                            367.1 |                                             9.73 |
|                                                                       2 |                            367.4 |                                             9.4  |
|                                                                       4 |                            368   |                                             9.09 |
|                                                                       8 |                            369.1 |                                             8.84 |
|                                                                      16 |                            371.5 |                                             8.71 |

No Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally de/fined as discretizations of continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeo/ff in more detail.

Downstream A/ffordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as /fine-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and a/ffordances.

Scaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper.

## 6 Conclusion

We introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for di/fferent domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model backbone.

