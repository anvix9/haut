## Abstract

Designing computationally efficient network architectures persists as an ongoing necessity in computer vision. In this paper, we transplant Mamba, a state-space language model, into VMamba, a vision backbone that works in linear time complexity. At the core of VMamba lies a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D helps bridge the gap between the ordered nature of 1D selective scan and the nonsequential structure of 2D vision data, which facilitates the gathering of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments showcase VMamba's promising performance across diverse visual perception tasks, highlighting its advantages in input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba .

## 1 Introduction

Visual representation learning stands as a fundamental research area in computer vision, which has witnessed remarkable progress in the era of deep learning. To represent complex patterns in vision data, two primary categories of backbone networks, i.e. , Convolution Neural Networks (CNNs) [49, 28, 30, 54, 38] and Vision Transformers (ViTs) [13, 37, 58, 68], have been proposed and extensively utilized in a variety of visual tasks. Compared to CNNs, ViTs generally demonstrate superior learning capabilities on large-scale data due to the integration of the self-attention mechanism [59, 13]. However, the quadratic complexity of self-attention w.r.t. the number of tokens introduces substantial computational overhead in downstream tasks involving large spatial resolutions.

To tackle this challenge, considerable efforts have been made to enhance the efficiency of attention computation [55, 37, 12]. However, existing approaches either impose limitations on the size of the effective receptive field [37] or experience evident performance degradation across diverse

Preprint. Under review.

Figure 1: Comparison of correlation establishment between image patches via (a) self-attention and (b) the proposed 2D-Selective-Scan (SS2D). Red boxes indicate the query image patch, with patch opacity representing the degree of information loss.

<!-- image -->

tasks [31, 62]. This motivates us to develop a novel architecture for vision data, preserving the inherent advantages of the vanilla self-attention mechanism, i.e., global receptive fields and dynamic weighting parameters [23].

Recently, Mamba [17], a novel State Space Model (SSM) [17, 44, 61] in the field natural language processing (NLP), has emerged as a highly promising approach for long sequence modeling with linear complexity. Drawing inspiration from this advancement, we introduce VMamba, a vision backbone integrating SSM-based blocks to facilitate efficient visual representation learning. However, the core algorithm of Mamba, i.e. , the parallelized selective scan operation, is essentially designed for processing one-dimensional sequential data. This poses a challenge when attempting to adapt it for processing vision data, which inherently lacks a sequential arrangement of visual components. To address this issue, we propose 2D Selective Scan (SS2D), a four-way scanning mechanism tailored for spatial domain traversal. In contrast to the self-attention mechanism (Figure 1 (a)), SS2D ensures that each image patch gains contextual knowledge exclusively through a compressed hidden state computed along the corresponding scanning path (Figure 1 (b)), thereby reducing the computational complexity from quadratic to linear.

Upon the VSS blocks, we develop a family of VMamba architectures ( i.e. , VMamba-Tiny/Small/Base) and accelerate them through a series of architectural enhancements and implementation optimizations. Compared to benchmark vision models built on CNN (ConvNeXt [38]), ViT (Swin [37] and HiViT [68]), and SSM (S4ND [45] and Vim [71]), VMamba consistently achieves superior image classification accuracy on ImageNet-1K [9] across model scales. Specifically, VMamba-Base achieves a top-1 accuracy of 83 . 9% , surpassing Swin by +0 . 4% , with a throughput exceeding that of Swin by a substantial margin over 40% ( 646 vs. 458 ). The superiority of VMamba extends across various downstream tasks, with VMamba-Tiny/Small/Base achieving 47 . 3% / 48 . 7% / 49 . 2% mAP in object detection on COCO [34] ( 1 × training schedule). This outperforms Swin by 4 . 6% / 3 . 9% / 2 . 3% and ConvNeXt by 3 . 1% / 3 . 3% / 2 . 2% , respectively. As for single-scale semantic segmentation on ADE20K [70], VMamba-Tiny/Small/Base achieves 47 . 9% / 50 . 6% / 51 . 0% mIoU, which surpasses Swin by 3 . 4% / 3 . 0% / 2 . 9% and ConvNeXt by 1 . 9% / 1 . 9% / 1 . 9% , respectively. Furthermore, unlike ViT-based models, which experience quadratic growth in computational complexity with the number of input tokens, VMamba exhibits linear growth in FLOPs while maintaining comparable performance. This underscores its state-of-the-art input scalability.

The contributions of this study are summarized as follows:

- · We propose VMamba, an SSM-based vision backbone network for visual representation learning with linear time complexity. A series of improvements in architectural design and implementation details are adopted to improve the inference speed of VMamba.
- · We introduce 2D Selective Scan (SS2D) to bridge the gap between 1D array scanning and 2D plane traversal, facilitating the extension of selective SSM to process vision data.
- · Without bells and whistles, VMamba demonstrates promising performance across a range of visual tasks, including image classification, object detection, and semantic segmentation. It also exhibits remarkable adaptability w.r.t. the length of the input sequence, showcasing linear growth in computational complexity.

## 3 Preliminaries

Formulation of SSMs. Originating from the Kalman filter [33], SSMs can be regarded as linear time-invariant (LTI) systems that map the input stimulation u ( t ) ∈ R to response y ( t ) ∈ R through the hidden state h ( t ) ∈ R N . Concretely, continuous-time SSMs can be formulated as linear ordinary differential equations (ODEs) as follows,

h ' ( t ) = Ah ( t ) + B u ( t ) , y ( t ) = Ch ( t ) + Du ( t ) , (1)

where A ∈ R N × N , B ∈ R N × 1 , C ∈ R 1 × N , and D ∈ R 1 are the weighting parameters.

Discretization of SSM. To be integrated into deep models, continuous-time SSMs must undergo discretization in advance. Concretely, consider the time interval [ t a , t b ] , the analytic solution to the

hidden state variable h ( t ) | t = t b can be expressed as

h ( t b ) = e A ( t b -t a ) h ( t a ) + e A ( t b -t a ) ∫ t b t a B ( τ ) u ( τ ) e -A ( τ -t a ) dτ. (2)

By sampling with the time-scale parameter ∆ ( i.e. , dτ | t i +1 t i = ∆ i ), h ( t b ) can be discretized by

h b = e A (∆ a + ... +∆ b -1 ) ( h a + b -1 ∑ i = a B i u i e -A (∆ a + ... +∆ i ) ∆ i ) , (3)

where [ a, b ] is the corresponding discrete step interval. Notably, this formulation approximates the result obtained by the zero-order hold (ZOH) method, which is frequently utilized in the literature of SSM-based models (please refer to Appendix A for detailed proof).

Selective Scan Mechanism. To tackle the limitation of LTI SSMs (Eq. 1) in capturing the contextual information, Gu et al. [17] propose a novel parameterization method for SSMs that integrates an input-dependent selection mechanism (referred to as S6). However, in the case of selective SSMs, the time-varying weighting parameters present a challenge for efficient computation of hidden states, as convolutions do not accommodate dynamic weights and are consequently rendered inapplicable. Nevertheless, as the recurrence relation of h b in Eq. 3 can be derived, the response y b can still be efficiently computed using associative scan algorithms [2, 43, 51] with linear complexity (the detailed explanation is deferred to Appendix B).

## 4.1 Network Architecture

We develop VMamba at three scales: VMamba-Tiny, VMamba-Small, and VMamba-Base (referred to as VMamba-T, VMamba-S, and VMamba-B, respectively). An overview of the architecture of VMamba-T is illustrated in Figure 3 (a), and detailed configurations are provided in Appendix E. The input image I ∈ R H × W × 3 is first partitioned into patches by a stem module, resulting in a 2D feature map with the spatial dimension of H/ 4 × W/ 4 . Subsequently, multiple network stages are employed to create hierarchical representations with resolutions of H/ 8 × W/ 8 , H/ 16 × W/ 16 , and H/ 32 × W/ 32 . Each stage comprises a down-sampling layer (except for the first stage) followed by a stack of Visual State Space (VSS) blocks.

The VSS blocks serve as the visual counterpart to Mamba blocks [17] (Figure 3 (b)) for representation learning. The initial architecture of VSS blocks (referred to as the 'vanilla VSS Block' in Figure 3 (c)) is formulated by substituting the S6 module, which stands as the core of Mamba in concurrently achieving global receptive fields, dynamic weights ( i.e. , selectivity), and linear complexity, with the newly proposed 2D-Selective-Scan (SS2D) module (to be introduced in the following subsection). To further enhance computational efficiency, we eliminate the entire multiplicative branch (encircled by the red box in Figure 3 (c)), as the effect of the gating mechanism has been achieved by the selectivity of SS2D. Consequently, the resulting VSS block (depicted in Figure 3 (d)) consists of a single network branch with two residual modules, mimicking the architecture of a vanilla Transformer block [60]. Throughout this paper, all results are obtained using VMamba models built with VSS blocks in this architecture.

## 5.3 Analysis and Discussion

Relationship between SS2D and Self-Attention. To formulate the response Y within the time interval [ a, b ] of length T , we denote the corresponding SSM-related variables u i ⊙ ∆ i ∈ R 1 × D v , B i ∈ R 1 × D k , and C i ∈ R 1 × D k as V ∈ R T × D v , K ∈ R T × D k , and Q ∈ R T × D k , respectively. Therefore, the j -th dimension of Y , i.e. , Y ( j ) ∈ R T × 1 , can be expressed as

Y ( j ) = [ Q ⊙ w ( j ) ] h a ( j ) + [ ( Q ⊙ w ( j ) ) ( K w ( j ) ) ⊤ ⊙ M ] V ( j ) , (4)

where h a ∈ R D k is the hidden state at step a , M denotes the temporal mask matrix of size T × T with the lower triangular part set to 1 and elsewhere 0, and ⊙ denotes element-wise product. Detailed derivations are deferred to Appendix C.

In Eq. 4, the matrix multiplication process involving Q , K , and V closely resembles the mechanism of self-attention, despite the inclusion of w . Concretely, the formulation of each element in w := [ w 1 ; . . . ; w T ] ∈ R T × D k × D v , i.e. , w i ∈ R D k × D v , can be written as w i = ∏ i j =1 e A∆ ⊤ a -1+ j , representing the cumulative attention weight at step i computed along the scanning path.

Visualization of Activation Maps. To gain an intuitive and in-depth understanding of SS2D, we further visualize the attention values in QK ⊤ and ( Q ⊙ w ) ( K / w ) ⊤ corresponding to a specific

Figure 6: Comparison of Effective Receptive Fields (ERF) [41] between VMamba and other benchmark models. Pixels with higher intensity indicate larger responses regarding the central pixel.

<!-- image -->

query patch within foreground objects (referred to as the 'activation map'). As shown in Figure 5 (b), the activation map of QK ⊤ demonstrates the effectiveness of SS2D in capturing and retaining the traversed information, with all previously scanned tokens in the foreground region being activated. Furthermore, the inclusion of w leads to activation maps that are more focused on the neighborhood of query patches (Figure 5 (c)), which is consistent with the temporal weighting effect inherent to the formulation of w . Nevertheless, the selective scan mechanism enables VMamba to accumulate the history along the scanning path, thereby facilitating the establishment of long-term dependencies across image patches. This is evident in the sub-figure encircled with a red box (Figure 5 (d)), where patches of the sheep far to the left (scanned in early steps) remain activated. For more visualizations and further discussion, please refer to Appendix D.

Visualization of Effective Receptive Fields. The Effective Receptive Field (ERF) [41, 11] refers to the region in the input space that contributes to the activation of a certain output unit. We conduct a comparative analysis of the central pixel's ERF across various visual backbones, both before and after training. The results presented in Figure 6 illustrate that among the models examined, only DeiT, HiViT, and VMamba, demonstrate global ERFs, whereas the others exhibit local ERFs, despite their theoretical potential for global coverage. Moreover, the linear time complexity of VMamba makes it more computationally efficient compared to DeiT and HiViT, which have quadratic costs w.r.t. the number of input patches.

Diagnostic Study on Selective Scan Patterns. We compare the proposed scanning pattern ( i.e. Cross-Scan) to three benchmark patterns, including unidirectional scanning (Unidi-Scan), bidirectional scanning (Bidi-Scan), and cascade scanning (Cascade-Scan, scanning the data rowwise and column-wise successively). Feature dimensions are adjusted to maintain similar architectural parameters and FLOPs for fair comparison. As illustrated in Figure 7, Cross-Scan outperforms other scanning patterns in both computational efficiency and classification accuracy, highlighting its effectiveness in achieving 2D-Selective-Scan. Removing the DWConv layer, which has been observed to potentially aid the model in learning 2D spatial information, further enhances this advantage. This emphasizes the inherent strength of Cross-Scan in capturing 2D contextual information through its adoption of four-way scanning.

Figure 7: Performance comparison of different scanning patterns.

<!-- image -->

## 6 Conclusion

This paper presents VMamba, an efficient vision backbone network built with State Space Models (SSMs). VMamba integrates the benefits of selective SSMs from NLP tasks into visual data processing, bridging the gap between ordered 1D scanning and non-sequential 2D traversal through the novel SS2D module. Moreover, we have substantially improved the inference speed of VMamba through a series of architectural and implementation refinements. The effectiveness of the VMamba family has

been demonstrated through extensive experiments, and the linear time complexity of VMamba makes it advantageous for downstream tasks with large-resolution inputs.

Limitations. While VMamba demonstrates promising experimental results, there is still room for this study to be further improved. Previous research has validated the efficacy of unsupervised pre-training on large-scale datasets ( e.g. , ImageNet-21K). However, the compatibility of existing pretraining methods with SSM-based architectures like VMamba, and the identification of pre-training techniques specifically tailored to such models, remain unexplored. Investigating these aspects could serve as a promising avenue for future research in architectural design. Moreover, limited computational resources have prevented us from exploring VMamba's architecture at the Large scale, as well as conducting a fine-grained search of hyperparameters to further improve experimental performance.

## H Ablation Study



## H.2 Influence of the Initialization Approach

In our study, we adopted the initialization scheme initially proposed for the SS2D block in S4D [19]. Therefore, it is necessary to investigate the contribution of this initialization method to the effectiveness of VMamba. To delve deeper into this matter, we replaced the default initialization with two alternative methods: random initialization and zero initialization.

For both initialization methods, we set the parameter D in equation 1 to a vector of all ones, mimicking a basic skip connection (thus we have y = Ch + Du ). Additionally, the weights and biases associated with the transformation from low rank to the dimension D v (which matches the input size), are initialized as random vectors, in contrast to Mamba [17], which employs a more sophisticated approach for initialization.

The main distinction between random and zero initialization lies in the parameter A in equation 5, which is typically initialized as a HiPPO matrix in both Mamba [17, 19] and our implementation of VMamba. Given that we selected the hyper-parameter d\_state to be 1, the Mamba initialization for log ( A ) can be simplified to all zeros, which aligns with zero initialization. In contrast, random initialization assigns a random vector to log ( A ) . We choose to initialize log ( A ) rather than A directly to ensure that A remains close to the all-ones matrix when the network parameters are near zero, which empirically helps enhance the stability of the training process.

The experiment results in Table 11 indicate that, at least for image classification with SS2D blocks, the model's performance is not significantly affected by the initialization method. Therefore, within this context, the sophisticated initialization method employed in Mamba [17] can be substituted with a simpler, more straightforward approach. We also visualize the ERF maps of models trained with

Figure 14: The visualization of ERF of VMamba with different initialization.

<!-- image -->

different initialization methods (see Figure 14), which intuitively reflect the robustness of SS2D across various initialization schemes.

