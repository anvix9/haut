## Aligning Text-to-Image Models using Human Feedback

Kimin Lee 1 Hao Liu 2 Moonkyung Ryu 1 Olivia Watkins 2 Yuqing Du 2 Craig Boutilier 1 Pieter Abbeel 2 Mohammad Ghavamzadeh 1 Shixiang Shane Gu 1

## Abstract

Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the humanlabeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve imagetext alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models.

## 1. Introduction

Deep generative models have recently shown remarkable success in generating high-quality images from text prompts (Ramesh et al., 2021; 2022; Saharia et al., 2022; Yu et al., 2022b; Rombach et al., 2022). This success has been driven in part by the scaling of deep generative models to large-scale datasets from the web such as LAION (Schuhmann et al., 2021; 2022). However, major challenges remain in domains where large-scale text-to-image models fail to generate images that are well-aligned with text prompts (Feng et al., 2022; Liu et al., 2022a;b). For instance, current text-to-image models often fail to produce reliable visual text (Liu et al., 2022b) and struggle with compositional image generation (Feng et al., 2022).

In language modeling, learning from human feedback has emerged as a powerful solution for aligning model behavior with human intent (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021; Nakano et al., 2021; Ouyang et al., 2022; Bai et al., 2022a). Such methods first learn a reward function intended to reflect what humans care about in the task, using human feedback on model outputs. The language model is then optimized using the learned reward function by a reinforcement learning (RL) algorithm, such as proximal policy optimization (PPO; Schulman et al. 2017). This RL with human feedback (RLHF) framework has successfully aligned large-scale language models (e.g., GPT-3; Brown et al. 2020) with complex human quality assessments.

Motivated by the success of RLHF in language domains, we propose a fine-tuning method for aligning text-to-image models using human feedback. Our method consists of the following steps illustrated in Figure 1: (1) We first generate diverse images from a set of text prompts designed to test output alignment of a text-to-image model. Specifically, we examine prompts where pre-trained models are more prone to errors - generating objects with specific colors, counts, and backgrounds. We then collect binary human feedback assessing model outputs. (2) Using this humanlabeled dataset, we train a reward function to predict human feedback given the image and text prompt. We propose an auxiliary task-identifying the original text prompt within a set of perturbed text prompts-to more effectively exploit human feedback for reward learning. This technique improves the generalization of reward function to unseen images and text prompts. (3) We update the text-to-image model via reward-weighted likelihood maximization to better align it with human feedback. Unlike the prior work (Stiennon et al., 2020; Ouyang et al., 2022) that uses RL for optimization, we update the model using semi-supervised learning to measure model-output quality w.r.t. the learned reward function.

We fine-tune the stable diffusion model (Rombach et al., 2022) using 27K image-text pairs with human feedback. Our fine-tuned model shows improvement in generating objects with specified colors, counts, and backgrounds. Moreover, it improves compositional generation (i.e., can bet-

Figure 1. The steps in our fine-tuning method. (1) Multiple images sampled from the text-to-image model using the same text prompt, followed by collection of (binary) human feedback. (2) A reward function is learned from human assessments to predict image-text alignment. We also utilize an auxiliary objective called prompt classification, which identifies the original text prompt within a set of perturbed text prompts. (3) We update the text-to-image model via reward-weighted likelihood maximization.

<!-- image -->

ter generate unseen objects 1 given unseen combinations of color, count, and background prompts). We also observe that the learned reward function is better aligned with human assessments of alignment than CLIP score (Radford et al., 2021) on tested text prompts. We analyze several design choices, such as using an auxiliary loss for reward learning and the effect of using 'diverse' datasets for fine-tuning.

We can summarize our main contributions as follows:

- · We propose a simple yet efficient fine-tuning method for aligning a text-to-image model using human feedback.
- · We show that fine-tuning with human feedback significantly improves the image-text alignment of a text-toimage model. On human evaluation, our model achieves up to 47% improvement in image-text alignment at the expense of mildly degraded image fidelity.
- · We show that the learned reward function predicts human assessments of the quality more accurately than the CLIP score (Radford et al., 2021). In addition, we show that rejection sampling based on our learned reward function can also significantly improve the image-text alignment.
- · Naive fine-tuning with human feedback can significantly reduce the image fidelity, despite better alignment. We find that careful investigations on several design choices are important in balancing alignment-fidelity tradeoffs.

Even though our results do not address all the failure modes of the existing text-to-image models, we hope that this work highlights the potential of learning from human feedback for aligning these models.

## 2. Related Work

Text-to-image models. Various deep generative models, such as variational auto-encoders (Kingma & Welling, 2013), generative adversarial networks (Goodfellow et al., 2020), auto-regressive models (Van Den Oord et al., 2016), and diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have been proposed for image distributions. Combined with the large-scale language encoders (Radford et al., 2021; Raffel et al., 2020), these models have shown impressive results in text-to-image generation (Ramesh et al., 2022; Saharia et al., 2022; Yu et al., 2022b; Rombach et al., 2022).

However, text-to-image models frequently struggle to generate images that are well-aligned with text prompts (Feng et al., 2022; Liu et al., 2022a;b). Liu et al. (2022b) show that current models fail to produce reliable visual text and often perform poorly w.r.t. compositional generation (Feng et al., 2022; Liu et al., 2022a). Several techniques, such as character-aware text encoders (Xue et al., 2022; Liu et al., 2022b) and structured representations of language inputs (Feng et al., 2022) have been investigated to address these issues. We study learning from human feedback, which aligns text-to-image models directly using human feedback on model outputs.

Fine-tuning with few images (Ruiz et al., 2022; Kumari et al., 2022; Gal et al., 2022) for personalization of textto-image diffusion models is also related with our work. DreamBooth (Ruiz et al., 2022) showed that text-to-image models can generate diverse images in a personalized way by fine-tuning with few images, and Kumari et al. (2022) proposed a more memory and computationally efficient method. In this work, we demonstrate that it is possible to fine-tune text-to-image models using simple binary (good/bad) human feedback.

Learning with human feedback. Human feedback has been used to improve various AI systems, from translation (Bahdanau et al., 2016; Kreutzer et al., 2018), to web question-answering (Nakano et al., 2021), to story generation (Zhou & Xu, 2020), to training RL agents without the need for hand-designed rewards (MacGlashan et al., 2017; Christiano et al., 2017; Warnell et al., 2018; Ibarz et al., 2018; Lee et al., 2021, inter alia), and to more truthful and harmless instruction following and dialogue (Ouyang et al., 2022; Bai et al., 2022a; Ziegler et al., 2019; Wu et al., 2021; Stiennon et al., 2020; Liu et al., 2023; Scheurer et al., 2022; Bai et al., 2022b, inter alia). In relation to prior works that focus on improving language models and game agents with human feedback, our work explores using human feedback to align multi-modal text-to-image models with human preference. Many prior works on learning with human feedback consist of learning a reward function and maximizing reward weighted likelihood (often dubbed as supervised fine-tuning) (see e.g. Ouyang et al., 2022; Ziegler et al., 2019; Stiennon et al., 2020) Inspired by their successes, we propose a fine-tuning method with human feedback for improving text-to-image models.

Evaluating image-text alignment. To measure the image-text alignment, various evaluation protocols have been proposed (Madhyastha et al., 2019; Hessel et al., 2021; Saharia et al., 2022; Yu et al., 2022b). Most prior works (Ramesh et al., 2022; Saharia et al., 2022; Yu et al., 2022b) use the alignment score of image and text embeddings determined by pre-trained multi-modal models, such as CLIP (Radford et al., 2021) and CoCa (Yu et al., 2022a). However, since scores from pre-trained models are not calibrated with human intent, human evaluation has also been introduced (Saharia et al., 2022; Yu et al., 2022b). In this work, we train a reward function that is better aligned with human evaluations by exploiting pre-trained representations and (a small amount) of human feedback data.

## 3. Main Method

To improve the alignment of generated images with their text prompts, we fine-tune a pre-trained text-to-image model (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022) by repeating the following steps shown in

Figure 1. We first generate a set of diverse images from a collection of text prompts designed to test various capabilities of the text-to-image model. Human raters provide binary feedback on these images (Section 3.1). Next we train a reward model to predict human feedback given a text prompt and an image as inputs (Section 3.2). Finally, we fine-tune the text-to-image model using reward-weighted log likelihood to improve text-image alignment (Section 3.3).

## 3.1. Human Data Collection

Image-text dataset. To test specific capabilities of a given text-to-image model, we consider three categories of text prompts that generate objects with a specified count, color, or background. 2 For each category, we generate prompts by combining a word or phrase from that category with some object; e.g., combining green (or in a city ) with dog . We also consider combinations of the three categories (e.g., two green dogs in a city ). From each prompt, we generate up to 60 images using a pretrained text-to-image model-in this work, we use Stable Diffusion v1.5 (Rombach et al., 2022).

Human feedback. We collect simple binary feedback from multiple human labelers on the image-text dataset. Labelers are presented with three images generated from the same prompt and are asked to assess whether each image is well-aligned with the prompt ('good') or not ('bad'). 3 We use binary feedback given the simplicity of our promptsthe evaluation criterion are fairly clear. More informative human feedback, such as ranking (Stiennon et al., 2020; Ouyang et al., 2022), should prove useful when more complex or subjective text prompts are used (e.g., artistic or open-ended generation).

## 3.2. Reward Learning

To measure image-text alignment, we learn a reward function r φ ( x , z ) (parameterized by φ ) that maps the CLIP embeddings 4 (Radford et al., 2021) of an image x and a text prompt z to a scalar value. It is trained to predict human feedback y ∈ { 0 , 1 } (1 = good, 0 = bad).

Formally, given the human feedback dataset D human = { ( x , z , y ) } , the reward function r φ is trained by minimizing the mean-squared-error (MSE):

L MSE ( φ ) = E ( x , z ,y ) ∼D human [ ( y -r φ ( x , z )) 2 ] .

Prompt classification. Data augmentation can significantly improve the data-efficiency and performance of learning (Krizhevsky et al., 2017; Cubuk et al., 2019). To effectively exploit the feedback dataset, we design a simple data augmentation scheme and auxiliary loss for reward learning. For each image-text pair that has been labeled good , we generate N -1 text prompts with different semantics than the original text prompt. For example, we might generate { Blue dog , . . . , Green dog } given the original prompt Red dog . 5 This process generates a dataset D txt = { ( x , { z j } N j =1 , i ' ) } with N text prompts { z j } N j =1 , including the original, for each image x , and the index i ' of the original prompt.

We use the augmented prompts in an auxiliary task, namely, classifying the original prompt for reward learning. Our prompt classifier uses the reward function r φ as follows:

P φ ( i | x , { z j } N j =1 ) = exp( r φ ( x , z i ) /T ) ∑ j exp( r φ ( x , z j ) /T ) , ∀ i ∈ [ N ] ,

where T > 0 is the temperature. Our auxiliary loss is

L pc ( φ ) = E ( x , { z j } N j =1 ,i ' ) ∼D txt [ L CE ( P φ ( i | x , { z j } N j =1 ) , i ' )] , (1)

where L CE is the standard cross-entropy loss. This encourages r φ to produce low values for prompts with different semantics than the original. Our experiments show this auxiliary loss improves the generalization to unseen images and text prompts. Finally, we define the combined loss as

L reward ( φ ) = L MSE ( φ ) + λ L pc ( φ ) ,

where λ is the penalty parameter.

The pseudo code of reward learning is in Appendix E.

## 3.3. Updating the Text-to-Image Model

We use our learned r φ to update the text-to-image model p with parameters θ by minimizing the loss

L ( θ ) = E ( x , z ) ∼D model [ -r φ ( x , z ) log p θ ( x | z ) ] + β E ( x , z ) ∼D pre [ -log p θ ( x | z ) ] , (2)

where D model is the model-generated dataset (i.e., images generated by the text-to-image model on the tested text prompts), D pre is the pre-training dataset , and β is a penalty parameter. The first term in (2) minimizes the rewardweighted negative log-likelihood (NLL) on D model . 6 By

Table 1. Examples of text categories.

| Category    | Examples                                                    |
|-------------|-------------------------------------------------------------|
| Count       | One dog ; Two dogs ; Three dogs ; Four dogs ; Five dogs ;   |
| Color       | A green colored dog ; A red colored dog ;                   |
| Background  | A dog in the forest ; A dog on the moon ;                   |
| Combination | Two blue dogs in the forest ; Five white dogs in the city ; |

Table 2. Details of image-text datasets and human feedback.

| Category    | Total # of images   | Human feedback (%)   | Human feedback (%)   | Human feedback (%)   |
|-------------|---------------------|----------------------|----------------------|----------------------|
|             |                     | Good                 | Bad                  | Skip                 |
| Count       | 6480                | 34 . 4               | 61 . 0               | 4 . 6                |
| Color       | 3480                | 70 . 4               | 20 . 8               | 8 . 8                |
| Background  | 2400                | 66 . 9               | 33 . 1               | 0 . 0                |
| Combination | 15168               | 35 . 8               | 59 . 9               | 4 . 3                |
| Total       | 27528               | 46 . 5               | 48 . 5               | 5 . 0                |

evaluating the quality of the outputs using a reward function aligned with the text prompts, this term improves the image-text alignment of the model.

Typically, the diversity of the model-generated dataset is limited, which can result in overfitting. To mitigate this, similar to Ouyang et al. (2022), we also minimize the pretraining loss , the second term in (2). This reduces NLL on the pre-training dataset D pre . In our experiments, we observed regularization in the loss function L ( θ ) in (2) enables the model to generate more natural images.

Different objective functions and algorithms (e.g., PPO; Schulman et al. 2017) could be considered for updating the text-to-image model similar to RLHF finetuning (Ouyang et al., 2022). We believe RLHF fine-tuning may lead to better models because it uses online sample generation during updates and KL-regularization over the prior model. However, RL usually requires extensive hyperparameter tuning and engineering, thus, we defer the extension to RLHF fine-tuning to future work.

## 4. Experiments

Wedescribe a set of experiments designed to test the efficacy of our fine-tuning approach with human feedback.

## 4.1. Experimental Setup

Models. For our baseline generative model, we use stable diffusion v1.5 (Rombach et al., 2022), which has been

## Original model

Figure 2. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). (a) Our model generates high-quality seen object ( dog ) with specified color, count and background. on seen text prompts. (b) Our model generates an unseen object ( tiger ) with specified color, count, and background. (c) Our model still generates reasonable images from unseen text categories (artistic generation).

<!-- image -->

## Fine-tuned model (ours)

(a) Seen text prompt: Two green dogs on the table.

<!-- image -->

<!-- image -->

(b) Unseen text prompt (unseen object): Four tigers in the field.

<!-- image -->

<!-- image -->

(c) Unseen text prompt (artistic generation): Oil painting of sunflowers.

<!-- image -->

pre-trained on large image-text datasets (Schuhmann et al., 2021; 2022). 7 For fine-tuning, we freeze the CLIP language encoder (Radford et al., 2021) and fine-tune only the diffusion module. For the reward model, we use ViT-L/14 CLIP model (Radford et al., 2021) to extract image and text embeddings and train a MLP using these embeddings as input. More experimental details (e.g., model architectures and the final hyperparameters) are reported in Appendix D.

Datasets. From a set of 2700 English prompts (see Table 1 for examples), we generate 27K images using the stable diffusion model (see Appendix B for further details). Table 2 shows the feedback distribution provided by multiple human labelers, which has been class-balanced. We note that the stable diffusion model struggles to generate the number of objects specified by the prompt, but reliably generates

specified colors and backgrounds.

We use 23K samples for training, with the remaining samples used for validation. We also use 16K unlabeled samples for the reward-weighted loss and a 625K subset 8 of LAION5B (Schuhmann et al., 2022) filtered by an aesthetic score predictor 9 for the pre-training loss.

## 4.2. Text-Image Alignment Results

Human evaluation. We measure human ratings of image alignment with 120 text prompts (60 seen text prompts and 60 unseen 10 text prompts), testing the ability of the models to render different colors, number of objects, and

Figure 3. (a) Accuracy of CLIP score (Radford et al., 2021) and our reward functions on predicting the preferences of human labelers. For our method, we consider a variant of our reward function, which is not trained with prompt classification (PC) loss in (1). (b) Performance of reward functions with varying the size of the training dataset.

<!-- image -->

Figure 4. Human evaluation results on 120 text prompts (60 seen text prompts and 60 unseen text prompts). We generate two sets of images (one from our fine-tuned model and one from the original model) with same text prompt. Then, human raters indicate which one is better, or tie (i.e., two sets are similar) in terms of imagetext alignment and image fidelity. Each query is evaluated by 9 independent human raters and we report the percentage of queries based on the number of positive votes. We also highlight the percentage of queries with two-thirds vote (7 or more positive votes) in the black box.

<!-- image -->

backgrounds (see Appendix B for the full set of prompts). Given two (anonymized) sets of images, one from our finetuned model and one from the stable diffusion model, we ask human raters to assess which is better w.r.t. image-text alignment and fidelity (i.e., image quality). 11 Each query is evaluated by 9 independent human raters. We show the percentage of queries based on the number of positive votes.

As shown in Figure 4, our method significantly improves image-text alignment against the original model. Specifically, 50% of samples from our model receive at least two-thirds vote (7 or more positive votes) for image-text alignment. However, fine-tuning somewhat degrades image fidelity (15% compared to 10%). We expect that this is because (i) we asked the labelers to provide feedback mainly on alignment, (ii) the diversity of our human data is limited, and (iii) we used a small subset of pre-training dataset for fine-tuning. 12 This issue can presumably be mitigated with larger rater and pre-training datasets.

Qualitative comparison. Figure 2 shows image samples from the original model and our fine-tuned counterpart (see Appendix A for more image examples). While the original often generates images with missing details (e.g., color, background or count) (Figure 2(a)), our model generates objects that adhere to the prompt-specified colors, counts and backgrounds. Of special note, our model generates high-quality images on unseen text prompts that specify unseen objects (Figure 2(b)). Our model also generates reasonable images given unseen text categories, such as artistic generation (Figure 2(c)).

However, we also observe several issues of our fine-tuned models. First, for some specific text prompts, our finetuned model generates oversaturated and non-photorealistic images. Our model occasionally duplicates entities within the generated images or produces lower-diversity images for the same prompt. We expect that it would be possible to address these issues with larger (and diverse) human datasets and better optimization (e.g., RL).

Figure 3(a) compares the accuracy of r φ and the CLIP score on unseen images from both seen and unseen text prompts. Our reward (green) more accurately predicts human evaluation than the CLIP score (red), hence is better aligned with typical human intent. To show the benefit of our auxiliary loss (prompt classification) in (1), we also assess a variant of our reward function which ignores the auxiliary loss (blue). The auxiliary classification task improves reward performance on both seen and unseen text prompts. The gain from the auxiliary loss clearly shows the impor-

<!-- image -->

(a) Fine-tuned model only with human-labeled dataset.

<!-- image -->

<!-- image -->

(b) Fine-tuned model with human-labeled and unlabeled datasets.

<!-- image -->

<!-- image -->

(c) Fine-tuned model with human-labeled, unlabeled and pre-training datasets.

<!-- image -->

Figure 5. Samples from fine-tuned models trained with different datasets on unseen text prompts. (a) Fine-tuned model only with human dataset generates low-quality images due to overfitting. (b) Unlabeled samples improve the quality of generated images. (c) Fine-tuned model can generate high-fidelity images by utilizing pre-training dataset.

## 4.3. Results on Reward Learning

Predicting human preferences. We investigate the quality of our learned reward function by evaluating its prediction of with human ratings. Given two images from the same text prompt ( x 1 , x 2 , z ) , we check whether our reward r φ generates a higher score for the human-preferred image, i.e., r φ ( x 1 , z ) > r φ ( x 2 , z ) when rater prefers x 1 . As a baseline, we compare it with the CLIP score (Hessel et al., 2021), which measures image-text similarity in the CLIP embedding space (Radford et al., 2021).

tance of text diversity and our auxiliary loss in improving data efficiency. Although our reward function is more accurate than the CLIP score, its performance on unseen text prompts ( ∼ 80% ) suggests that it may be necessary to use more diverse and large human datasets.

Rejection sampling. Similar to Parti (Yu et al., 2022b) and DALL-E (Ramesh et al., 2021), we evaluate a rejection sampling technique, which selects the best output w.r.t. the learned reward function. 13 Specifically, we generate 16 images per text prompt from the original stable diffusion model and select the 4 with the highest reward scores. We compare these to 4 randomly sampled images in Figure 6(a). Rejection sampling significantly improves image-text alignment (46% with two-thirds preference vote by raters) without sacrificing image fidelity. This result illustrates the significant value of the reward function in improving text-to-image models without any fine-tuning .

(a) Original model versus Rejection sampling

<!-- image -->

(b) Fine-tuned model versus Rejection sampling

Figure 6. Human evaluation on 120 tested text prompts (60 seen text prompts and 60 unseen text prompts). We generate two sets of images with same text prompt. Then, human raters indicate which one is better, or tie (i.e., two sets are similar) in terms for image-text alignment and image fidelity. Each query is evaluated by 9 independent human raters and we report the percentage of queries based on the number of positive votes. We also highlight the percentage of queries with two-thirds vote (7 or more positive votes) in the black box. (a) For rejection sampling, we generate 16 images per text prompt and select best 4 images based on reward score, i.e., more inference-time compute. (b) Comparison between fine-tuned model and original model with rejection sampling.

We also compare our fine-tuned model to the original with rejection sampling in Figure 6(b). 14 Our fine-tuned model achieves a 10% gain in image-text alignment (20%-10% two-thirds vote) but sacrifices 17% in image fidelity (3%20% two-thirds vote). However, as discussed in Section 4.2, we expect degradation in fidelity to be mitigated with larger human datasets and better hyper-parameters. Note also that rejection sampling has several drawbacks, including increased inference-time computation and the inability to improve the model (since it is output post-processing).

## 4.4. Ablation Studies

Effects of human dataset size. To investigate how human data quality affects reward learning, we conduct an ablation study, reducing the number of images per text prompt by half before training the reward function. Figure 3(b) shows that model accuracy decreases on both seen and unseen prompts as data size decreases, clearly demonstrating the importance of diversity and the amount of rater data.

Effects of using diverse datasets. To verify the importance of data diversity, we incrementally include unlabeled and pre-training datasets during fine-tuning. We measure the reward score (image-text alignment) on 120 tested text prompts and FID score (Heusel et al., 2017)-the similarity between generated images and real images-on MS-CoCo validation data (Lin et al., 2014). Table 3 shows that FID score is significantly reduced when the model is fine-tuned using only human data, despite better image-text alignment.

Table 3. Comparison with the original Stable Diffusion. For evaluating image fidelity, we measure FID scores on the MS-CoCo. For evaluating the image-text alignment, we measure reward scores and CLIP scores on 120 tested text prompts. ↑ ( ↓ ) indicates that the higher (lower) number is the better.

|                                            | FID on MS-CoCo ( ↓ )   | Average rewards on tested prompts ( ↑ )   |
|--------------------------------------------|------------------------|-------------------------------------------|
| Original model                             | 13 . 97                | 0 . 43                                    |
| Fine-tuned model w.o unlabeled & pre-train | 26 . 59                | 0 . 69                                    |
| Fine-tuned model w.o pre-train             | 21 . 02                | 0 . 79                                    |
| Fine-tuned model                           | 16 . 76                | 0 . 79                                    |

However, by adding the unlabeled and pre-training datasets, FID score is improved without impacting image-text alignment. We provide image samples from unseen text prompts in Figure 5. We see that fine-tuned models indeed generate more natural images when exploiting more diverse datasets.

## 5. Discussion

In this work, we have demonstrated that fine-tuning with human feedback can effectively improve the image-text alignment in three domains: generating objects with a specified count, color, or backgrounds. We analyze several design choices (such as using an auxiliary loss and collecting diverse training data) and find that it is challenging to balance the alignment-fidelity tradeoffs without careful investigations on such design choices. Even though our results do not

address all the failure modes of the existing text-to-image models, we hope that our method can serve as a starting point to study learning from human feedback for improving text-to-image models.

Limitations and future directions . There are several limitations and interesting future directions in our work:

- · More nuanced human feedback . Some of the poor generations we observed, such as highly saturated image colors, are likely due to similar images being highly ranked in our training set. We believe that instructing raters to look for a more diverse set of failure modes (oversaturated colors, unrealistic animal anatomy, physics violations, etc.) will improve performance along these axes.
- · Diverse and large human dataset . For simplicity, we consider a limited class of text categories (count, color, background) and thus consider a simple form of human feedback (good or bad). Due to this, the diversity of our human data is bit limited. Extension to more subjective text categories (like artistic generation) and informative human feedback such as ranking would be an important direction for future research.
- · Different objectives and algorithms . For updating the text-to-image model, we use a reward-weighted likelihood maximization. However, similar to prior work in language domains (Ouyang et al., 2022), it would be an interesting direction to use RL algorithms (Schulman et al., 2017). We believe RLHF fine-tuning may lead to better models because (a) it uses online sample generation during updates and (b) KL-regularization over the prior model can mitigate overfitting to the reward function.

## Acknowledgements

We thank Peter Anderson, Douglas Eck, Junsu Kim, Changyeon Kim, Jongjin Park, Sjoerd van Steenkiste, Younggyo Seo, and Guy Tennenholtz for providing helpful comments and suggestions. Finally, we would like to thank Sehee Yang for providing valuable feedback on user interface and constructing the initial version of human data, without which this project would not have been possible.

## References

Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 , 2021.

Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y. An actor-

critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086 , 2016.

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022a.

Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022b.

Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems , 2017.

Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019.

Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A., Narayana, P., Basu, S., Wang, X. E., and Wang, W. Y. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032 , 2022.

Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618 , 2022.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. Communications of the ACM , 63(11):139-144, 2020.

Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi, Y. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718 , 2021.

Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems , 2017.

Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems , 2020.

Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. Reward learning from human preferences and demonstrations in atari. In Advances in Neural Information Processing Systems , 2018.

Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013.

Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. Can neural machine translation be improved with user feedback? arXiv preprint arXiv:1804.05958 , 2018.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Communications of the ACM , 60(6):84-90, 2017.

Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488 , 2022.

Lee, K., Smith, L., and Abbeel, P. Pebble: Feedbackefficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In International Conference on Machine Learning , 2021.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll'ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision , 2014.

Liu, H., Sferrazza, C., and Abbeel, P. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv: Arxiv-2302.02676 , 2023.

Liu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum, J. B. Compositional visual generation with composable diffusion models. arXiv preprint arXiv:2206.01714 , 2022a.

Liu, R., Garrette, D., Saharia, C., Chan, W., Roberts, A., Narang, S., Blok, I., Mical, R., Norouzi, M., and Constant, N. Character-aware models improve visual text rendering. arXiv preprint arXiv:2212.10562 , 2022b.

Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.

MacGlashan, J., Ho, M. K., Loftin, R., Peng, B., Roberts, D., Taylor, M. E., and Littman, M. L. Interactive learning from policy-dependent human feedback. In International Conference on Machine Learning , 2017.

Madhyastha, P., Wang, J., and Specia, L. Vifidel: Evaluating the visual fidelity of image descriptions. arXiv preprint arXiv:1907.09340 , 2019.

Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 , 2022.

Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning , 2021.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 21(140):1-67, 2020.

Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot textto-image generation. In International Conference on Machine Learning , 2021.

Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.

Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022.

Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242 , 2022.

Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems , 2022.

Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho, K., and Perez, E. Training language models with language feedback. arXiv preprint arXiv: Arxiv-2204.14146 , 2022.

Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clipfiltered 400 million image-text pairs. arXiv preprint arXiv:2111.02114 , 2021.

Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402 , 2022.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.

Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning , 2015.

Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. Learning to summarize from human feedback. arXiv preprint arXiv:2009.01325 , 2020.

- Van Den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks. In International conference on machine learning , pp. 1747-1756, 2016.

Warnell, G., Waytowich, N., Lawhern, V., and Stone, P. Deep tamer: Interactive agent shaping in highdimensional state spaces. In Conference on Artificial Intelligence , 2018.

Wu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862 , 2021.

Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics , 10:291-306, 2022.

Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are imagetext foundation models. arXiv preprint arXiv:2205.01917 , 2022a.

Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789 , 2022b.

Zhou, W. and Xu, K. Learning to compare for better training and evaluation of open domain natural language generation models. In Conference on Artificial Intelligence , 2020.

Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.

## A. Qualitative Comparison

Original model

<!-- image -->

(a) Text prompt: A red colored tiger.

<!-- image -->

Figure 7. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). The fine-tuned model can generate an unseen object ( tiger ) with specified colors.

<!-- image -->

(b) Text prompt: A green colored tiger.

<!-- image -->

<!-- image -->

(c) Text prompt: A pink colored tiger.

<!-- image -->

Fine-tuned model (ours)

## Original model

Fine-tuned model (ours)

<!-- image -->

(a) Text prompt: Three wolves in the forest.

<!-- image -->

Figure 8. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). The fine-tuned model can generate an unseen object ( wolf ) with specified counts. However, the counts are not always perfect, showing a room for improvement.

<!-- image -->

(b) Text prompt: Four wolves in the forest.

<!-- image -->

<!-- image -->

(c) Text prompt: Five wolves in the forest.

<!-- image -->

## Original model

Figure 9. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). The fine-tuned model can generate cake with specified backgrounds.

<!-- image -->

## Fine-tuned model (ours)

(a) Text prompt: A cake in the city.

<!-- image -->

<!-- image -->

(b) Text prompt: A cake in the sea.

<!-- image -->

<!-- image -->

(c) Text prompt: A cake on the moon.

<!-- image -->

## Fine-tuned model (ours)

Figure 10. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). The fine-tuned model still generate rabbit in specified styles.

<!-- image -->

(a) Text prompt: An oil painting of rabbit.

<!-- image -->

(b) Text prompt: A black and white sketch of rabbit.

<!-- image -->

(c) Text prompt: A 3D render of rabbit.

<!-- image -->

Figure 11. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). The fine-tuned model still maintains performance across a wide distribution of text prompts.

<!-- image -->

<!-- image -->

(a) Text prompt: A zombie in the style of Picasso.

(b) Text prompt: A watercolor painting of a chair that looks like an octopus.

<!-- image -->

(c) Text prompt: A painting of a squirrel eating a burger.

<!-- image -->

## B. Image-text Dataset

In this section, we describe our image-text dataset. We generate 2774 text prompts by combining a word or phrase from that category with some object. Specifically, we consider 9 colors ( red, yellow, green, blue, black, pink, purple, white, brown ), 6 numbers ( 1-6 ), 8 backgrounds ( forest, city, moon, field, sea, table, desert, San Franciso ) and 25 objects ( dog, cat, lion, orange, vase, cup, apple, chair, bird, cake, bicycle, tree, donut, box, plate, clock, backpack, car, airplane, bear, horse, tiger, rabbit, rose, wolf ). 15 For each text prompt, we generate 60 or 6 images according to the text category. In total, our image-text dataset consists of 27528 image-text pairs. Labeling for training is done by two human labelers.

For evaluation, we use 120 text prompts listed in Table 4. Given two (anonymized) sets of 4 images, we ask human raters to assess which is better w.r.t. image-text alignment and fidelity (i.e., image quality). Each query is rated by 9 independent human raters in Figure 4 and Figure 6.

Table 4. Examples of text prompts for evaluation.

| Category   | Examples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Seen       | A red colored dog.; A red colored donut.; A red colored cake.; A red colored vase.; A green colored dog.; A green colored donut.; A green colored cake.; A green colored vase.; A pink colored dog.; A pink colored donut.; A pink colored cake.; A pink colored vase.; A blue colored dog.; A blue colored donut.; A blue colored cake.; A blue colored vase.; A black colored apple.; A green colored apple.; A pink colored apple.; A blue colored apple.; A dog on the moon.; A donut on the moon.; A cake on the moon.; A vase on the moon.; An apple on the moon.;                                                                                                                 |
| Unseen     | A red colored bear.; A red colored wolf.; A red colored tiger.; A red colored rabbit.; A green colored bear.; A green colored wolf.; A green colored tiger.; A green colored rabbit.; A pink colored bear.; A pink colored wolf.; A pink colored tiger.; A pink colored rabbit.; A blue colored bear.; A blue colored wolf.; A blue colored tiger.; A blue colored rabbit.; A black colored rose.; A green colored rose.; A pink colored rose.; A blue colored rose.; A bear on the moon.; A wolf on the moon.; A tiger on the moon.; A rabbit on the moon.; A rose on the moon.; A bear in the sea.; A wolf in the sea.; A tiger in the sea.; A rabbit in the sea.; A rose in the sea.; |

## C. Additional Results

Table 5. Percentage of generated images from our fine-tuned model that are better than (win), tied with, or worse than (lose) the compared to original stable diffusion model in terms of image-text alignment and fidelity.

|                | Image-text alignment   | Image-text alignment   | Image-text alignment   | Fidelity    | Fidelity    | Fidelity    |
|----------------|------------------------|------------------------|------------------------|-------------|-------------|-------------|
| Seen prompts   | Win                    | Lose                   | Tie                    | Win         | Lose        | Tie         |
| Color          | 58.9 ± 19.5            | 10.0 ± 7.1             | 31.1 ± 26.2            | 53.9 ± 39.4 | 28.3 ± 24.0 | 17.8 ± 23.1 |
| Count          | 69.4 ± 6.8             | 11.7 ± 5.3             | 18.9 ± 10.7            | 42.2 ± 36.4 | 37.2 ± 23.6 | 20.6 ± 22.9 |
| Background     | 53.3 ± 13.3            | 14.4 ± 6.4             | 32.2 ± 18.0            | 43.9 ± 31.0 | 39.4 ± 17.1 | 16.7 ± 20.1 |
| Unseen prompts | Image-text alignment   | Image-text alignment   | Image-text alignment   | Fidelity    | Fidelity    | Fidelity    |
| Unseen prompts | Win                    | Lose                   | Tie                    | Win         | Lose        | Tie         |
| Color          | 57.2 ± 8.5             | 14.4 ± 9.6             | 28.3 ± 16.3            | 39.4 ± 24.7 | 38.9 ± 9.7  | 21.7 ± 25.1 |
| Count          | 69.4 ± 16.1            | 8.3 ± 2.4              | 22.2 ± 18.0            | 42.8 ± 31.8 | 36.7 ± 9.1  | 20.6 ± 23.3 |
| Background     | 55.6 ± 9.6             | 11.1 ± 9.9             | 33.3 ± 18.9            | 46.1 ± 28.5 | 35.6 ± 13.6 | 18.3 ± 20.7 |

Aligning Text-to-Image Models using Human FeedbackTable 6. Percentage of generated images from original stable diffusion model with rejection sampling that are better than (win), tied with, or worse than (lose) the compared to original stable diffusion model in terms of image-text alignment and fidelity.

|                | Image-text alignment   | Image-text alignment   | Image-text alignment   | Fidelity    | Fidelity    | Fidelity    |
|----------------|------------------------|------------------------|------------------------|-------------|-------------|-------------|
| Seen prompts   | Win                    | Lose                   | Tie                    | Win         | Lose        | Tie         |
| Color          | 51.7 ± 18.6            | 20.6 ± 13.2            | 27.8 ± 31.1            | 37.8 ± 16.0 | 30.6 ± 15.2 | 31.7 ± 29.7 |
| Count          | 67.8 ± 14.2            | 12.8 ± 7.1             | 19.4 ± 20.6            | 31.7 ± 18.4 | 31.7 ± 21.2 | 36.7 ± 38.1 |
| Background     | 47.8 ± 15.3            | 10.0 ± 6.7             | 42.2 ± 20.3            | 41.1 ± 21.3 | 27.8 ± 14.9 | 31.1 ± 34.8 |
| Unseen prompts | Image-text alignment   | Image-text alignment   | Image-text alignment   | Fidelity    | Fidelity    | Fidelity    |
| Unseen prompts | Win                    | Lose                   | Tie                    | Win         | Lose        | Tie         |
| Color          | 47.8 ± 7.5             | 20.0 ± 14.3            | 32.2 ± 20.8            | 31.7 ± 21.9 | 39.4 ± 13.2 | 28.9 ± 32.1 |
| Count          | 62.2 ± 23.8            | 8.3 ± 6.2              | 29.4 ± 29.4            | 40.0 ± 27.9 | 25.6 ± 12.6 | 34.4 ± 38.4 |
| Background     | 62.8 ± 11.3            | 7.2 ± 5.8              | 30.0 ± 16.8            | 51.1 ± 17.6 | 23.9 ± 12.0 | 25.0 ± 27.8 |

Table 7. Percentage of generated images from our fine-tuned model that are better than (win), tied with, or worse than (lose) the compared to original stable diffusion model with rejection sampling in terms of image-text alignment and fidelity.

|                | Image-text alignment   | Image-text alignment   | Image-text alignment   | Fidelity    | Fidelity    | Fidelity    |
|----------------|------------------------|------------------------|------------------------|-------------|-------------|-------------|
| Seen prompts   | Win                    | Lose                   | Tie                    | Win         | Lose        | Tie         |
| Color          | 34.4 ± 19.2            | 27.2 ± 18.9            | 38.3 ± 37.6            | 41.1 ± 33.7 | 27.8 ± 8.2  | 31.1 ± 33.1 |
| Count          | 32.8 ± 11.1            | 35.0 ± 15.8            | 32.2 ± 25.3            | 32.8 ± 28.4 | 41.7 ± 7.8  | 25.6 ± 29.9 |
| Background     | 31.7 ± 12.2            | 24.4 ± 16.1            | 43.9 ± 27.0            | 36.1 ± 24.0 | 36.1 ± 15.2 | 27.8 ± 35.5 |
| Unseen prompts | Image-text alignment   | Image-text alignment   | Image-text alignment   | Fidelity    | Fidelity    | Fidelity    |
| Unseen prompts | Win                    | Lose                   | Tie                    | Win         | Lose        | Tie         |
| Color          | 40.0 ± 19.7            | 23.3 ± 12.7            | 36.7 ± 32.0            | 39.4 ± 20.3 | 32.8 ± 14.6 | 27.8 ± 32.7 |
| Count          | 53.3 ± 15.5            | 27.2 ± 9.2             | 19.4 ± 23.5            | 26.1 ± 19.5 | 52.2 ± 7.5  | 21.7 ± 25.4 |
| Background     | 33.9 ± 23.1            | 21.7 ± 9.7             | 44.4 ± 32.2            | 33.3 ± 25.6 | 37.8 ± 10.6 | 28.9 ± 33.7 |

## D. Experimental Details

Model architecture . For our baseline generative model, we use stable diffusion v1.5 (Rombach et al., 2022), which has been pre-trained on large image-text datasets (Schuhmann et al., 2021; 2022). For the reward model, we use ViT-L/14 CLIP model (Radford et al., 2021) to extract image and text embeddings and train a MLP using these embeddings as input. Specifically, we use two-layer MLPs with 1024 hidden dimensions each. We use ReLUs for the activation function between layers, and we use the Sigmoid activation function for the output. For auxiliary task, we use temperature T = 2 and penalty parameter λ = 0 . 5 .

Training . Our fine-tuning pipeline is based on publicly released repository ( https://github.com/huggingface/ diffusers/tree/main/examples/text\_to\_image ). We update the model using AdamW (Loshchilov & Hutter, 2017) with β 1 = 0 . 9 , β 2 = 0 . 999 , glyph[epsilon1] = 1 e -8 and weight decay 1 e -2 . The model is trained in half-precision on 4 40GB NVIDIA A100 GPUs, with a per-GPU batch size of 8, resulting in a toal batch size of 512 (256 for pre-training data and 256 for model-generated data). 16 It is trained for a total of 10,000 updates.

FID measurement using MS-CoCo dataset . We measure FID scores to evaluate the fidelity of different models using MS-CoCo validation dataset (i.e., val2014 ). There are a few caption annotations for each MS-CoCo image. We randomly choose one caption for each image, which results in 40,504 caption and image pairs. MS-CoCo images have different resolutions and they are resized to 256 × 256 before computing FID scores. We use pytorch-fid Python implementation for the FID measurement ( https://github.com/mseitzer/pytorch-fid ).

## E. Pseudocode

```
Algorithm 1 Reward Learning Pseudocode # x, z, y: image, text prompt, human label # clip: pre-trained CLIP model # preprocess: Image transform # pred\_r: two-layers MLPs # Get\_perturbated\_prompts: function to generate perturbated text prompts # lambda: penalty parameter # T: temperature # N: # of perturbated text prompts # main model def RewardFunction(x, z): # training loop for (x, z, y) in dataloader: # dims: (batch\_size, dim)
```

```
# compute embeddings for tokens img\_embedding = clip.encode\_image(prepocess(x)) txt\_embedding = clip.encode\_text(clip.tokenize(z)) input\_embeds = concatenate(img\_embedding, txt\_embedding) # predict score return pred\_r(input\_embeds) # MSE loss r\_preds = RewardFunction(x, z) loss = MSELoss(r\_preds, y) # Prompt classification scores = [r\_preds] for z\_neg in Get\_perturbated\_prompts(z, N): scores.append(RewardFunction(x, z\_neg)) scores = scores / T labels = [0] * batch\_size # origin text is always class 0 loss += lambda * CrossEntropyLoss(scores, labels) # update reward function optimizer.zero\_grad(); loss.backward(); optimizer.step()
```

## Algorithm 2 Perturbated Text Prompts Generation Pseudocode

```
# z: image, text prompt, human label # N: # of perturbated text prompts def Get\_perturbated\_prompts(z, N): color\_list = [''red'', ''yellow'', ...] obj\_list = [''dog'', ''cat'', ...] count\_list = [''One'', ''Two'', ...] loc\_list = [''in the sea.'', ''in the sky.'', ...] output = [] count = 0 while (count < N): idx = random.randint(0, len(count\_list)-1) count = count\_list[idx] idx = random.randint(0, len(color\_list)-1) color = color\_list[idx] idx = random.randint(0, len(loc\_list)-1) loc = loc\_list[idx] idx = random.randint(0, len(obj\_list)-1) obj = obj\_list[idx] if count == ''One'': text = ''{} {} {} {}.''.format(count, color, obj, loc) else: text = ''{} {} {}s {}.''.format(count, color, obj, loc) if z != text: count += 1 output.append(text) return output
```