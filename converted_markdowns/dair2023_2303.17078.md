## Machine Learning for Partial Differential Equations

Steven L. Brunton 1 ∗ and J. Nathan Kutz 2

1 Department of Mechanical Engineering, University of Washington, Seattle, WA 98195, United States

- 2 Department of Applied Mathematics, University of Washington, Seattle, WA 98195, United States

## Abstract

Partial differential equations (PDEs) are among the most universal and parsimonious descriptions of natural physical laws, capturing a rich variety of phenomenology and multi-scale physics in a compact and symbolic representation. This review will examine several promising avenues of PDE research that are being advanced by machine learning, including: 1) the discovery of new governing PDEs and coarse-grained approximations for complex natural and engineered systems, 2) learning effective coordinate systems and reduced-order models to make PDEs more amenable to analysis, and 3) representing solution operators and improving traditional numerical algorithms. In each of these fields, we summarize key advances, ongoing challenges, and opportunities for further development.

## 1 Introduction

Partial differential equations (PDEs) have been a cornerstone of mathematical physics and engineering design for over 250 years, since the introduction of the one-dimensional wave equation by d'Alembert in 1752 [20]. PDEs provide a formal mathematical infrastructure for relating how quantities of interest change in several variables, typically space and time. As such, PDEs provide a foundational description of the governing equations of many canonical spatio-temporal physical systems, including electrodynamics, quantum mechanics, fluid mechanics, heat transfer, etc. Today, nearly every aspect of our engineered world is based in some way on the predictive capability of PDEs, from structural modeling of buildings and bridges, to the design of aircraft and other vehicles, to the thermal and electromagnetic management systems in modern portable electronics. In general, we will consider a PDE for u ( x, t )

u t + N ( u, u x , u xx , · · · , x, t ; µ ) = f ( x ) (1)

on a spatial domain x ∈ [0 , L ] ,for time t ∈ [0 , T ] , and where the parameter µ denotes a parametric dependency. The initial and boundary conditions are given by

IC: u ( x, 0) = u 0 ( x ) (2a)

BC: α 1 u (0 , t ) + β 1 u x (0 , t ) = g 1 ( t ) and α 2 u ( L, t ) + β 2 u x ( L, t ) = g 2 ( t ) . (2b)

This may be generalized to systems of several spatial variables, or a system with no time dependence.

In the past half-century, the advent of computing has produced two revolutions in our capability to analyze and solve PDEs. In the first, closed-form analytic solution techniques, which typically rely on linearity and superposition principles, have given way to diverse computational approximations based upon finite difference, finite element, and spectral techniques. These computational approaches significantly expand the complexity of behaviors and solutions that can be analyzed. Importantly, scientific computing has allowed for the study of nonlinear systems for which our analytic techniques typically fail. Additionally, complex boundary conditions, difficult geometries, and multiscale interactions can all be characterized within this framework. Thus, the combination of analytic and computational methods for solving PDEs has driven critical technological advancements in many industries since the 1960s. However, modern PDE systems are often nonlinear, complex, and high-dimensional, rendering analytic techniques ineffective and computational methods intractable. More recently, the ongoing machine learning revolution is providing an entirely new approach for solving PDEs, based on the increasing wealth of high-quality data generated from both simulations and experiments. Indeed, the emergence of machine learning methods in the last decade have allowed the community a significantly different approach to modeling the dynamics of PDEs, allowing for the learning and construction of proxy, reduced-order models which are faithful representations of the full, high-dimensional complex dynamics. Although machine learning has been applied to study PDEs for nearly three decades [47], several key advances in computational capabilities and algorithms are dramatically accelerating these efforts in the past decade.

In this review, we explore several avenues of PDE research that are being advanced by machine learning:

- · Governing equations and coarse-grained closures: Emerging techniques in symbolic regression and new high-fidelity measurements are making it possible to learn new PDEs for systems that are not amenable to first-principles and by-hand derivations. Systems in neuroscience and epidemiology, as well as systems from traditional physics, such as plasma dynamics, non-Newtonian fluids, and active matter, are all candidates for improved PDE descriptions. Moreover, there are many systems where we have accurate governing equations, but they are too computationally expensive to resolve at all scales of the physics. Thus, one must resort to coarse-grained PDEs. Machine learning is enabling tremendous progress in this traditionally challenging field, for example in turbulence modeling and the modeling of geophysical fluids. Several other fields stand to benefit, including material science and biology.
- · Coordinate systems and reduced representations: Solution techniques for PDEs are intimately tied to a coordinate system. For example, the Fourier transform is the coordinate system that diagonalized Laplace's equation. However, for nonlinear PDEs there is generically no coordinate system that simplify the equations. Advances in modern Koopman operator theory are providing a powerful new perspective for finding effective coordinates even for nonlinear systems. Similarly, reduced-order models provide a reduction of a PDE to a much simpler ODE system that is tailored to the specific configuration and parameters of interest. Machine learning has rapidly been adopted as a new technique for ROMs, as it shares a significant overlap and history with this field of applied mathematics. For many iterative design optimization and control applications, ROMs are critical, as there is a tradeoff between the accuracy of a solution and the cost of computing it.
- · Numerical solutions and operator learning: Another major avenue of research is focused on learning the solution operator of complex PDEs, trained from limited amounts of high-fidelity data. These approaches are quite flexible and offer many advantages, including the ability to re-mesh solutions flexibly. In related work, researchers are currently using machine learning to improve traditional scientific computing workflows, for example to improve pre-conditioning and to learn improved stencils for shocks and discontinuities.

Our goal with this review is to provide a brief summary and organization of the rapid progress in this field, along with a high-level perspective on the ongoing challenges and avenues of future opportunity. Although machine learning is changing how we learn, represent, and solve PDEs, many things haven't changed. We still seek interpretable and generalizable representations of the governing equations and their solutions. We still use techniques from scientific computing to integrate many of these models and propagate their uncertainty. And we still use the same iterative design optimization and control algorithms, now wrapped around machine learned models and solutions. The importance of embedding physics into machine learning has also become increasingly clear in recent years [17-19, 21, 23, 39, 42, 67, 82, 133, 134]. Incorporating physics into the learning process makes it possible to achieve more accurate solutions, with more compact architectures, and from less and noisier training data.

## 2 Learning Governing Equations and Coarse-Grained Closures

Despite the significant progress over the past half century in the computational solution to PDEs, the fundamental process of how we derive PDEs has remained largely unchanged since the 1700s: equations are typically derived from governing conservations laws and symmetries using control volume techniques, with the pill box derivation remaining the standard in university classrooms. The current availability of vast quantities of measurement data from both familiar and exotic new fields of science and engineering are providing entirely new opportunities to learn PDEs, and thus perhaps underlying laws, principles, invariances and symmetries. This is one of the most promising avenues of modern scientific discovery, as we are on the cusp of the automatic and data-driven discovery of entirely new physics for systems that have alluded researchers for decades, if not centuries. Moreover, learning PDEs from data has several advantages over alternative approaches of using deep learning to 'mimic' the behavior of a complex system. First, PDEs are inherently interpretable, in the sense that they can be tied directly to geometry, conservation laws, symmetries, constraints, etc. Second, PDEs are highly generalizable, in that by changing the boundary conditions and parameters, a wide variety of phenomena and bifurcations may emerge.

Machine learning is ideal for representing arbitrarily complex input-output relationships from data. However, many techniques result in opaque models that are not interpretable. In contrast, symbolic regression techniques [16, 22, 122] is a class of machine learning that results in highly interpretable symbolic models by design. Recently, the sparse identification of nonlinear dynamics (SINDy) algorithm [22] has been extended to learn partial differential equations from data [115, 117]. The resulting algorithm, called PDE-FIND, provides new opportunities for scientific discovery by enabling the learning of new PDEs for unknown physics as well as coarse-grained closure models.

Figure 1: Sparse regression procedure to discover PDEs from data, demonstrated on the Navier-Stokes equations. A. Data is collected as snapshots of a solution to a PDE. B. Numerical derivatives are taken and data is compiled into a large matrix Θ , incorporating candidate terms for the PDE. C. Sparse regression is used to identify active terms in the PDE. D. Active terms in ξ are synthesized into a PDE. Modified from Rudy et al. [115].

<!-- image -->

The basic idea behind SINDy and PDE-FIND is to approximate the time derivative of the state of a dynamical system as a sparse linear combination of candidate terms that can describe the dynamics:

u t ≈ Θ( u, u x , u xx , uu x , · · · ) ξ (3)

where Θ is the library of candidate terms and ξ is a sparse vector that selects the relevant terms needed to describe the dynamics. There are several algorithms to learn these sparse dynamics, typically based on sparse optimization. Figure 1 illustrates this approach to learn the Navier-Stokes equations for a fluid flow.

In a relatively short time, PDE-FIND has been used to rediscover several models from classical physics, as well as to discover entirely new models of modern interest. Rediscovery, or the process of recapitulating existing theories with modern techniques, is a reasonable first step when testing out a new method. If the algorithm doesn't work on problems where we know the answer, it is unreasonable to expect it to yield new insights for more challenging systems. In addition, testing an algorithm on a known system may provide considerable insights. In early papers, PDE-FIND was applied to rediscover several canonical PDEs from physics, including the Navier-Stokes equation for fluid flows, and the Schrodinger equation for quantum mechanics.

More recently, PDE-FIND is being applied to entirely new fields of physics and natural sciences with promising results. Many of these advances have come in the field of fluid dynamics [5, 11, 12, 60, 111, 112, 120, 127, 140], where there are many open problems related to constituative modeling and turbulence closure modeling [4, 23, 48, 80, 107]. PDE-FIND has become a powerful tool for closure modeling of fluids, enabling both Reynolds averaged Navier-Stokes (RANS) closures [11, 12, 120] and large eddy simulation (LES) closures [140]. Figure 2 shows an approach based on a sparse Bayesian formulation of the problem that discovers LES closure models for largescale atmospheric simulations that preserve underlying symmetries and structure [140]. In addition to closure modeling for known PDEs, such as the Navier-Stokes equations, there are also efforts to learn additional physics terms corresponding to currently unmodeled mechanisms, for example in plasma physics [5], viscoelastic flows, granular materials, and active matter [126]. And while the incompressible Navier-Stokes equations are a remarkably accurate model for incompressible fluid flows, the magnetohydrodynamics (MHD) equations are much more of an approximation to plasma physics. Researchers are currently leveraging PDE-FIND to learn additional correction and closure terms for the MHD equations to more accurately match high-fidelity particle in cell simulations [5].

To model new physical and biological processes, several methodological innovations have been introduced into the PDE discovery framework. Perhaps the most fundamental advance is the introduction of the weak form PDE-FIND optimization by Messenger and Bortz [92, 93]. This approach solves the sparse regression problem after integrating the data over random control volumes, providing a dramatic improvement to the noise robustness of the algorithm. Weak form optimization may be thought of as a generalization of the integral SINDy [118] to PDE-FIND. Further improvements to noise robustness and limited data may be obtained through ensembling techniques [51], which use robust bagging to learn inclusion probabilities for the sparse terms ξ in Eq. (3) from limited and noisy data. These methodological innovations, and more, have been assembled into the open-source PySINDy software library [65], reducing the barrier to entry when applying these methods to new problems.

In addition to PDE-FIND, additional techniques for learning PDEs from data include PDE-NET [84, 85] and the Bayesian PDE discovery from data [6]. An earlier approach by Schaeffer et al. [119] importantly recognized that many PDE solutions are sparse in a suitably transformed solution space, introducing one of the first notions of

Geophysical Research Letters

10.1029/2020GL088376

Figure 2: Illustration of sparse Bayesian PDE discovery applied to LES closure modeling in large-scale geophysical fluid simulations. Modified from Zanna and Bolton [140].

<!-- image -->

sparsity in the field of PDEs. Other symbolic learning techniques are also promising, including symbolic learning on graph neural networks [40, 41, 116].

Figure 1. (a) Illustration of the RVM procedure; (b) schematic of the convolutional neural network (FCNN); (c) of /uniFB02 ine validation of the subgrid simulations for three parameterizations, denoted as Ŝ -the physics /uni2010 driven (Equation 5), and the FCNN -against the diagnosed forcing from high /uni2010 (ms -2 ), middle row the standard deviation (ms -2 ), and the bottom row the of the eddy momentum forcing, Sx and Ŝ x (the meridional component is y /uni2010 axes are longitude and latitude, respectively; the extent is 3,840 km in Figure 1. (a) Illustration of the RVM procedure; (b) schematic of the architecture of the convolutional neural network (FCNN); (c) of /uniFB02 ine validation of the subgrid momentum forcing simulations for three parameterizations, denoted as Ŝ -the physics /uni2010 driven Ŝ AZ , Ŝ BT revealed by (Equation 5), and the FCNN -against the diagnosed forcing from high /uni2010 resolution data, S . Top (ms -2 ), middle row the standard deviation (ms -2 ), and the bottom row the Pearson correlation of of the eddy momentum forcing, Sx and Ŝ x (the meridional component is shown in Supporting y /uni2010 axes are longitude and latitude, respectively; the extent is 3,840 km in each direction. In addition to discovering new PDEs and closure models, recent work by Callaham et al. [28] has shown that it is possible to cluster spatiotemporal data by which subset of the terms in a PDE are active in what regions of space and time. The resulting algorithm uncovers different regimes where a subset of the physics is active and in a dominant balance, providing a data-driven clustering based on the active terms in the PDE. In related work, Bakarji et al. [8] leveraged sparse model learning to automate the Buckingham Pi procedure for learning nondimensional quantities that mediate the transition across these dominant balance regimes and control bifurcations in system behavior. For example, these methods were applied to study boundary layers, where the classic boundary layer regions and Blasius scaling laws were recapitulated.

ZANNA AND BOLTON ZANNA AND BOLTON There are many exciting open problems in physical, engineered, and biological systems where PDE discovery may play an important role. In addition to discovering new mechanistic models and closures for problems in fluids and plasmas, granular materials, non-Newtonian and active matter, there are significant opportunities to learn coarse-grained models of biological systems, such as collective dynamics of many biological agents, the dynamics of bacterial colonies, spatiotemporal models in neuroscience and organized biological matter such as muscles. New methodological innovations will likely be required for these systems, for example to model non-stationarity and non-local interactions, as well as spatial heterogeneity. However, there is a large and active community working on addressing these issues, and the quality and quantity of measurement data is increasing every day.

## 3 Learning Coordinates and Reduced Representations

(a) Illustration of the RVM procedure; (b) schematic of the architecture of the physics /uni2010 constrained fully convolutional neural network (FCNN); (c) of /uniFB02 ine validation of the subgrid momentum forcing from the barotropic AZ BT In the history of science, many breakthroughs in learning governing equations have been preceded by learning an effective coordinate system. In recent decades, there are two dominant perspectives on effective coordinates related to PDEs, which we will explore here. First, advances in Koopman operator theory [25, 26, 70, 72, 94-96, 114] are making it possible to learn coordinate systems in which nonlinear dynamics appear linear. Second, it is often possible to reduce the dimension of a high-dimensional spatiotemporal system through a coordinate transformation to obtain a reduced-order model, which balances accuracy and efficiency. Both of these fields are rapidly progressing with advances in machine learning.

simulations for three parameterizations, denoted as

Ŝ

-

the physics

/uni2010

driven

Ŝ

,

Ŝ

revealed by the RVM algorithm

(Equation 5), and the FCNN

-

against the diagnosed forcing from high

/uni2010

resolution data,

S

. Top row shows the mean

-

2

(ms

## ), middle row the standard deviation (ms -2 ), and the bottom row the Pearson correlation of the zonal component Linearizing coordinate transformations

of the eddy momentum forcing, Sx and Ŝ x (the meridional component is shown in Supporting Information S1). The x /uni2010 and y /uni2010 axes are longitude and latitude, respectively; the extent is 3,840 km in each direction. 4 of 13 By construction, the Koopman operator [70] is a linear , infinite-dimensional operator that acts on the Hilbert space H of all scalar measurement functions g . The Koopman operator acts on functions of the state space of the dynamical system, trading nonlinear finite-dimensional dynamics for linear infinite-dimensional dynamics. It can be further generalized to map infinite-dimensional nonlinear dynamics to infinite-dimensional linear dynamics by appropriate choice of observables. The advantage of the Koopman representation is obvious: the linear problem

can be solved using a standard spectral decomposition (12). Thus the inversion of the operator is achieved by construction of the Koopman operator and by projecting into its eigenfunction space. Cover's theorem [38] represents a corresponding theory for how the projection to infinite-dimensions allows for linear separability of data, and thus the underlying success of kernel methods and support vector machines.

The Koopman operator is defined for discrete-time dynamical systems. A continuous dynamical system will induce a discrete-time dynamical system given by the flow map F t : M → M , which maps the state x ( t 0 ) to a future time x ( t 0 + t ) :

F t ( x ( t 0 )) = x ( t 0 + t ) = x ( t 0 ) + ∫ t 0 + t t 0 f ( x ( τ )) d τ . (4)

This induces the discrete-time dynamical system

x k +1 = F t ( x k ) , (5)

where x k = x ( kt ) . The analogous discrete-time Koopman operator is given by K t such that K t g = g · F t . Thus, the Koopman operator sets up a discrete-time dynamical system on the observable function g :

K t g ( x k ) = g ( F t ( x k )) = g ( x k +1 ) . (6)

The Koopman operator can be constructed using deep learning methods in order to enforce the above constraint on observables. The result is a spectral decomposition capable of representing the dynamical solutions of interest. Specifically, the eigenfunctions and eigenvalues of the Koopman operator K give a complete characterization of the dynamics K ϕ k = λ k ϕ k . The functions ϕ k ( x ) are Koopman eigenfunctions, and they define a set of intrinsic measurement coordinates, on which it is possible to advance these measurements with a linear dynamical system. Areduced-order linear model can be constructed by retaining the dominant Koopman eigenfunctions ϕ k .

Such linearizing transforms for PDEs are not new. Indeed, the Cole-Hopf transformation [36, 63] for solving diffusively regularized Burgers' equation was the first successful demonstration of such a technique, and the Inverse Scattering Transform (IST) [1, 2] generalized this framework for a class of completely integrable PDEs. But recent data-driven modeling paradigms have given Koopman theory a modern interpretation in terms of dynamical systems theory [94, 96]. And even more recently, deep learning approaches have provided Koopman embeddings for dynamics using neural networks [75, 89, 90, 100, 131, 135, 138]. This is in addition to enriching the observables of the dynamic mode decomposition [68, 73, 98, 99, 101, 136, 137]. Importantly, Koopman theory attempts to approximate the dynamics with a linear operator while the work of Lu et al [86, 87] and Kovachki et al [71] directly construct a nonlinear operator using neural networks.

As an example deep learning architecture (see Fig. 3), consider a neural network f ( u ) that advances the state variable forward in time u k +1 = f ( u k ) , and can be expressed by the formula

f ( u ) = ϕ d ( K ( ϕ e ( u ))) . (7)

The input of the the network u k is the state vector at time t k and the output is the state vector at time t k +1 . The network consists of three parts: (i) the encoder ϕ e , (ii) the linear dynamics K , and (iii) the decoder ϕ d . In this example, both the encoder and decoder are split into two parts where the outer encoder is χ + I and the inner encoder ψ e where ϕ e ( u ) = ψ e (( χ + I )( u )) . The outer encoder performs a coordinate transformation into a space in which the dynamics are linear. The inner encoder either (i) diagonalizes the system, (ii) reduces the dimensionality, or (iii) both. The decoder consists of the inner decoder ψ d and the outer decoder ζ + I . The inner and outer decoder are approximate inverses of the inner and outer encoder, respectively. The loss function used to train the network accounts for the autoencoder loss, the outer autoencoder loss, the inner autoencoder loss, the prediction loss, the linearity loss and regularization of the weights [56]. The diversity of methods used to build a Koopman representation [56, 68, 73, 75, 89, 90, 98-101, 131, 135-138] highlights the significant efforts by the community to leverage the eigenfunction expansion of the Koopman operator in order to construct the inverse operator. Koopman theory has also been recently combined with recurrent neural networks to predict turbulence time series [49].

## Reduced-order models

There is a rich history of constructing reduced-order models for PDEs [13-15, 62, 88, 105, 106, 108, 128, 129] by representing the dynamics in a lower-dimensional subspace or submanifold. The bulk of these methods are based on classical dynamical systems theory [61] and symmetry reduction and manifold arguments [3, 91, 113]. The most common and classical approach to reduced-order modeling of fluids involves identifying a low-dimensional

2

20

<!-- image -->

x

xx

t

xx

u t + uu -/uni03BDu = 0 v -/uni03BCv = 0 u u t Figure 3: A schematic of a neural network architecture used to discover a Koopman linearizing coordinate transformation. In this example, the nonlinear Burgers' equation is transformed into the linear heat equation.

v t = Lv /uni03C6 /uni03C8 v v t 0.5 1 0.5 1 Nonlinear Linear subspace, typically through proper orthogonal decomposition (POD), and then projecting the governing equations onto this subspace through Galerkin projection. In this approach, the field variable u ( x, t ) is approximated in a finite Galerkin expansion

Linear Dynamics

-20 -10 0 10 20 0 -20 -10 0 10 20 0 Spatial variable, x u ( x, t ) ≈ ¯ u ( x ) + r ∑ k =1 a k ( t ) ϕ k ( x ) (8)

where ¯ u ( x ) is the average field, ϕ k ( x ) are spatial POD modes and a k ( t ) are the amplitudes of these modes in time. This space-time separation of variables is usually computed through a singular value decomposition (SVD) [21], and represents a data-driven generalization of the Fourier transform. Galerkin projection of the governing equations (1) onto the orthogonal basis in (8) then yields an ordinary differential equation in the POD mode amplitudes a ( t ) :

d dt a ( t ) = f ( a ( t )) . (9)

The resulting ODE is typically much more tractable to simulate rapidly for iterative design optimization and realtime feedback control, but it comes at the cost of only approximately capturing the dynamics of the system. There are a number of well-known issues, such as stability issues of the reduced ODE, and limitations of this approach when applied to turbulence or other multiscale phenomena.

There are several major advances to this reduced-order modeling pipeline with the advent of machine learning. First, it is possible to dramatically improve the dimensionality reduction in Eq. (8) by replacing the linear subspace approximation with a nonlinear manifold approximation using a deep neural network autoencoder [23]. This is the approach taken by Lee and Carlberg [74]. It is also possible to learn the dynamics in Eq. (9) by data-driven regression, instead of projection based methods, for example using dynamic mode decomposition (DMD) for linear models [72, 114, 121], SINDy for nonlinear models [29-31, 45, 46, 58, 64, 66, 81-83], or other techniques such as reservoir computing [103, 104]. When SINDy is used to learn the reduced-order model in Eq. eq:ROM, this is referred to as Galerkin regression [82], and this approach has been applied to a wide variety of problems in fluid dynamics [29-31, 45, 46, 81-83], electroconvection [58], and plasmas [64, 66]. It is also possible to combine SINDy for reduced-order modeling with autoencoders for dimensionality reduction, as in Champion et al. [33]. Further, implicit kernel learning can be used to identify interpretable models when the state dimension is larger [7]. Another important avenue of research, called lift and learn [105, 108], discovers lifting transformations so that more complex nonlinearities may be written as quadratic dynamics in a higher-dimensional coordinate system. Transformers have recently been used for operator learning in PDEs [32, 79].

## 4 Numerical Methods and Learning Solution Operators

The previous sections have focused on learning new PDEs from data or learning better coordinate systems in which to represent the PDEs. A third major avenue of research, discussed here, involves improving existing numerical methods for solving PDEs with machine learning, including learning the solution operator directly.

## Operator inversion: Green's functions and eigenfunction expansions

For PDEs that are linear in u ( x, t ) and its derivatives, i.e N → L , there are number of analytic techniques that have been historically developed in order to represent the solution. For nonlinear operators N ( · ) , one typically relies on numerical methods to approximate the solution. Many modern deep learning methods are inspired by

Re

analytic techniques developed for linear operators for learning nonlinear operators. To be more specific, consider the time-independent linear operator

L u = f ( x ) . (10)

This problem has a rich and significant history in mathematical physics as quantum mechanics, electrodynamics and elasticity, for instance, all rely on understanding linear operators L [110]. Indeed, Sturm-Liouville theory was a unifying and foundational theoretical advancement for understanding the underlying description of many physicsbased problems of mathematical physics described by special functions (Bessel, Leguerre, Legendre, Hermite, etc).

The mathematical foundations for solving linear PDEs is to determine the inverse of L . Two methods, which are intimately connected, have traditionally been developed to represent the inverse operator L -1 : Green's functions and eigenfunction expansions. For the Green's function, one considers the associated problem L † G ( x, ξ ) = δ ( ξ ) where L † is the adjoint of L , δ ( · ) is the Dirac delta function and ξ ∈ [0 , L ] . Taking the inner product of both sides of (10) with respect to G ( ξ, x ) results in the solution

u ( x, t ) = L -1 f = 〈 G | f 〉 = ∫ D G ( ξ, x ) f ( ξ ) dξ. (11)

Thus the inverse of the differential operator L is, not surprisingly, an integration over the Green's function which acts as kernel function. Alternatively, eigenfunction expansions are based upon the associated spectral (eigenvalue) problem L φ n = λ n φ n where φ n and λ n are the eigenfunctions and eigenvalues respectively. The success of the Sturm-Liouville theory is based upon a linear operator for which linear superposition holds, thus any solution can be represented by a sum of its eigenfunctions u ( x, t ) = ∑ b n φ n . Inserting this solution form into (10) and taking inner products yields the solution form

u ( x, t ) = L -1 f = ∑ 〈 φ n | f 〉 λ n φ n . (12)

Thus the solution is represented as a projection into the orthonormal coordinate space of the eigenfunctions.

The representations (11) and (12) can be shown to be equivalent [37]. As noted, the difference in their representations of the solution have been used in deep learning to motivate learning for nonlinear operators. Specifically, the Green's function representation has led to neural operators [71, 76-78], DeepGreen [57] and DeepOnet [86, 87] architectures, while the eigenfunction expansion solution is the basis of learning Koopman operators and their spectral representation [24, 94, 96]. Like the Green's function and eigenfunction representations, they are alternative approaches for learning the inverse of an operator, in this case the inversion of the nonlinear operator N -1 ( · ) . The significant difficulty encountered for this inversion is that linear superposition no longer holds, thus undermining the creation of an underlying theoretical construct and guarantee for representation of the solution. However, deep learning architectures can leverage training data to build accurate representations of the operator inversion. The universal approximation capabilities of neural networks is well known which make them ideal for approximating the continuous functions associated with solutions of the PDE.

## Operator learning and kernel methods

In the original paper advocating the construction of Neural Operators , the Green's function representation of the solution motivates the proposed mappings between function spaces, thus allowing for the approximation of operators N ( · ) which encode governing equations and physics [71, 76-78]. Thus neural operators leverage integral kernel representation in their approximation of the operator. For instance, neural operators can make explicit use of multi-pole [77] and Fourier [76] kernels in order to construct operator representations. Thus nonlocal representations of the solution are parametrized by the integral operator. Recall that learning a nonlinear operator G ( · ) will be equivalent to learning the inverse of the PDE evolution (1). Thus kernel operators are intuitively appealing for the construction of the nonlinear operator based upon their connection to the linear kernel inversion (11). The overall representation of the operator is a trained neural network G = f θ where individual layers of the neural network are constructed from a learned integral representations that are updated according to the following kernel-based representation

v k +1 ( x ) = σ k +1 ( W t v k + ∫ D k K ( k ) ( x, y ) v k ( y ) dν k ( y ) + b k ( x ) ) (13)

where ν k is a Lebesgue measure on R d t . The kernel K ( k ) ( x, y ) is typically chosen to leverage advantageous representations, such as the multi-pole or Fourier kernels. Thus each layer of the network is trained using a physics-inspired concept of an integral (inverse) representation of the PDE dynamics. Thus instead of constructing a Green's function kernel, which can technically only be done with a linear operator L , the kernel representation is used to train a

Figure 4: Zero-shot super-resolution: Vorticity field of the solution to the two-dimensional Navier-Stokes equation with viscosity 10 4 ( Re = O (200) ); Ground truth on top and prediction on bottom. The model is trained on data that is discretized on a uniform 64 × 64 spatial grid and on a 20-point uniform temporal grid. The model is evaluated with a different initial condition that is discretized on a uniform 256 × 256 spatial grid and a 80-point uniform temporal grid ( From Kovachki et al [71] ).

<!-- image -->

representation of the inverse operator N -1 ( · ) . Rigorous estimates of the convergence rates and computational costs for learning such linear operators can now be derived rigorously [43, 44, 97].

More broadly, neural operators generalize standard feed-forward neural networks to learn mappings between infinite-dimensional spaces of functions defined on bounded domains of R d . The non-local component of the architecture is instantiated through either a parameterized integral operator or through multiplication in the spectral domain (which is a specific form of the kernel in the integral operator). Once trained, neural operators have the property of being discretization invariant: sharing the same network parameters between different discretizations of the underlying functional data. Thus it is a mesh free method, as shown in Fig. 4 on the Navier-Stokes equation.

Onamorefoundational level, Chen and Chen [35] developed a proof that neural networks with a single hidden layer can approximate accurately any nonlinear continuous operator. Thus a nonlinear operator is learned mapping from functions to functions. In practice, this is a highly impactful theory as it provides guarantees on the construction of an operator which contains information about the physics and dynamics of the system. The theorem of Chen and Chen is the basis of the DeepOnet method of Lu et al [86, 87] ( DeepOnet ) as well as the neural operators of Kovachki et al [71]. The original work of Chen and Chen [35] construct a universal approximation proof. The theorem provides a theoretical bounds on the ability of a neural network to approximate the operator G ( · ) . It also highlights the construction of two neural networks so that it can be more compactly represented as

∣ ∣ ∣ G ( u )( y ) -f θ 1 ( u ) · ˜ f θ 2 ( y ) ∣ ∣ ∣ < glyph[epsilon1] (14)

when considering the discretized representation of u ( x ) → u and new measurement (function evaluation) locations y → y . The two simultaneously trained networks are the branch network f θ 1 ( u ) and the trunk network ˜ f θ 2 ( y ) .

Mathematically, the concept is quite simple. Given a number of measurement (sensor) locations x k (usually selected from a computational grid) which prescribes the input function u k = u ( x k ) , a vector of training input data can be constructed u . The input data has a corresponding output data G ( u ) . In addition, training data mapping selections of random measurement points y to the output G ( u ( y ) is required. Thus the input functions u are encoded in a separate network than the location variables y . These are merged at the end as shown in the universal approximation proof of Chen and Chen [35]. Figure 5 shows the results of training from the original DeepOnet paper of Lu et al [76, 86, 87] on reaction-diffusion system. DeepOnets also can achieve small generalization errors by employing inductive biases. Remarkably, exponential convergence is observed in the deep learning algorithm.

So although both neural operators and DeepOnets accomplish the same goal, they do so with significantly different architectures. Neural operators exploit the kernel structure of generic operators while DeepOnets train by separating the input function from the spatial locations. Both have achieved promising results, highlighting the fact that the learning of operators can potentially allow for mesh-free models of physics systems. Of course, in order for this to actually be viable in practice, exceptional training data that resolves all scales should be employed in training. Figure 4 highlights the results from Kovachki et al [71] where neural operators are used to model fluid flows.

Figure 5: Learning a reaction-diffusion with DeepOnet. (A) (left) An example of a random sample of the input function u ( x ) . (middle) The corresponding output function s ( x, t ) at P different ( x, t ) locations. (right) Pairing of inputs and outputs at the training data points. The total number of training data points is the product of P times the number of samples of u . (B) Training error (blue) and test error (red) for different values of the number of random points P when 100 random u samples are used. (C) Training error (blue) and test error (red) for different number of u samples when P = 100 . The shaded regions denote one-standard-derivation ( From Lu et al [86] ).

<!-- image -->

## Accelerating numerical solutions

Finally, there are several emerging approaches to directly accelerate the numerical computation of the solutions to PDEs. For example, the Navier-Stokes equations for fluid flows are notoriously challenging to simulate at all resolutions because of the large degree of scale-separation in space and time. Improving the comptutational scaling, accuracy, and efficiency of numerical schemes is an important topic of modern machine-learning-enabled scientific computing [132]. For example, Bar-Sinai et al. [9] developed a deep learning approach to improve the estimation of spatial derivatives on coarse grids, outperforming traditional finite-difference methods. Stevens and Colonius [124] developed a related approach that improved upon fifth-order finite-difference schemes for shock-capturing simulations. These approaches solve a similar task as the neural operator approaches discussed earlier [76-78], which seek to improve simulations on coarser meshes. More classically, data-driven methods based on the ROMs developed earlier make it possible to learn more effective collocation points, resulting in discrete empirical interpolation methods for PDEs [10, 34]. Other approaches, such as FiniteNet [125] leverage long-short term memory (LSTM) networks to improve the simulation efficiency of PDEs. Machine learning is also improving the conditioning of flow solvers and the computation of inflow boundary conditions, for example with transformers [139].

Recently, Kochkov et al. [69] developed a deep learning correction for the two-dimensional Kolmogorov flow, showing that it is possible to simulate on a much coarser grid than is traditionally possible (e.g., approximately 10 times coarser in each dimension). Figure 6 shows the performance and architecture. This approach is morally similar to super-resolution efforts, which have gained considerable attention in fluid dynamics applications [53-55]. Extending these various approaches to three dimensions and more complex flows is necessary for these methods to gain wide adoption, and this represents an important and active area of current research.

Downloaded from https://www.pnas.org by 99.196.129.20 on January 17, 2023 from IP address 99.196.129.20.

Overview of our approach and results. (

A

) Accuracy versus computational cost with our baseline (direct simulation) and ML-accelerated [learned

interpolation (LI)] solvers. The x axis corresponds to pointwise accuracy, showing how long the simulation is highly correlated with the ground truth, whereas the y axis shows the computational time needed to carry out one simulation time unit on a single Tensor Processing Unit (TPU) core. Each point is annotated by the size of the corresponding spatial grid; for details see SI Appendix .( B ) Illustrative training and validation examples, showing Figure 6: Machine learned interpolation from coarse-grained to high-resolution flow fields, reproduced from Kochkov et al. [69].

the strong generalization capabilities of our model. (

C

) Structure of a single time step for our LI model, with a convolutional neural net controlling

learned approximations inside the convection calculation of a standard numerical solver.

ψ

and

u

refer to advected and advecting velocity components.

d

2

replicates of the convective flux module, corresponding to the flux of each velocity component in each spatial

## For d spatial dimensions there are 5 Discussion

direction.

to fit closures to classical turbulence models based on agreement with high-resolution DNSs (21-24). While potentially more accurate than traditional turbulence models, these new models have not achieved reduced computational expense. Another major thrust uses 'pure' ML, aiming to replace the entire Naviermuch faster than pure numerical simulations due to the reduced grid size. In this work we design algorithms that accurately solve the equations on coarser grids by replacing the components most affected by the resolution loss with better-performing learned In this perspective, we have explored how emerging techniques in machine learning are enabling major advances in the field of partial differential equations. In particular, we have summarized efforts to 1) discover new PDEs and coarse-grained closure models from data, 2) to uncover new coordinate systems in which the PDE and its solution become simpler, and 3) to directly learn solution operators and other techniques to accelerate numerics. In every case, despite significant progress, there are several ongoing challenges and opportunities for development.

Stokes simulation with approximations based on deep neural networks (25-30). A pure ML approach can be extremely efficient, avoiding the severe time-step constraints required for stability with traditional approaches. Because these models do not include the underlying physics, they often cannot enforce hard constraints, such as conservation of momentum and incompressibility. While these models often perform well on data from the training distribution, they often struggle with generalization. For example, they perform worse when exposed to novel forcing terms. We believe 'hybrid' approaches that combine the best of MLand traditional numerical methods are more promising. For example, ML can replace (31) or accelerate (32) iterative solves used inside some simulation methods without reducing accuracy. alternatives. We use data-driven discretizations (36, 37) to interpolate differential operators onto a coarse mesh with high accuracy (Fig. 1 C ). We train the model inside a standard numerical method for solving the underlying PDEs as a differentiable program, with the neural networks and the numerical method written in a framework [JAX (38)] supporting reverse-mode automatic differentiation. This allows for end-to-end gradientbased optimization of the entire algorithm, similar to prior work on density functional theory (39), molecular dynamics (40), and fluids (33, 34). The methods we derive are equation-specific and require high-resolution ground-truth simulations for training data. Since the dynamics of a PDE are local, the high-resolution simulations can be carried out on a small domain. The models In the field of discovery and coarse-graining, there are several avenues of ongoing research. Preliminary results show that it is possible to learn new physical mechanisms and closure models, mainly in fluid systems. There is a tremendous opportunity to refine and leverage these new closure models to accelerate simulations of turbulent fluid systems to enable their use in a diverse range of applications and technologies. Moreover, there are many new fields where this approach might be applied: neuroscience, epidemiology, active matter, non-Newtonian fluids, among others. In addition, there is an opportunity to incorporate partial knowledge of the physics, including symmetries and invariances. The dual of this, is that given a new discovered PDE, it may be possible to relate this to a new conservation or invariance. In any of these situations, when a PDE is uncovered, it is possible to automatically cluster the dynamics in space and time by what terms in the PDE are in a dominant balance with eachother. Similarly, it may be possible to identify the controlling nondimensional parameters that determine the bifurcation structure of the system.

Here we focus on hybrid models that use ML to correct errors in cheap, underresolved simulations (33-35). These models borrow strength from the coarse-grained simulations and are potentially remain stable during long simulations and have robust and predictable generalization properties, with models trained on small domains producing accurate simulations on larger domains, with Even when a PDE is known, from first principles or from data-driven learning algorithms, the presence of nonlinearity makes it so that there are no generic solution techniques. We have seen that advances in Koopman operator theory are making it possible to learn new coordinate systems in which nonlinear systems become linear.

2 of 8

|

PNAS

https://doi.org/10.1073/pnas.2101784118

Kochkov et al.

Machine learning-accelerated computational fluid dynamics

Fig. 1.

<!-- image -->

<!-- image -->

Forced turbulence

Generalization tests

<!-- image -->

<!-- image -->

Larger domain

<!-- image -->

More turbulent

Decaying

<!-- image -->

For example, the Cole-Hopf transformation may be seen as a Koopman coordinate transformation in which case the nonlinear Burgers' equation maps into the linear heat equation. There are many opportunities to discovery similar coordinate transformations for more complex systems, such as the Navier-Stokes equations. In addition to learning linearizing transformations, it may be possible to relax this stringent constraint, and instead learn transformations into a coordinate system where the dynamics are simplified, with asymptotic or perturbative nonlinearities. This is related to normal form theory, where it may be possible to dramatically simplify the dynamics with a much less complex coordinate transfomrmation.

Finally, there are several efforts underway to accelerate numerics associated with solving PDEs, as well as to approximate the solution operators directly. The universal approximation capabilities of neural networks make them particularly useful for representing the solutions to PDEs, which may be arbitrarily complex. Understanding how these solution operators vary with system parameters is an important avenue of ongoing research [102]. Similarly, machine learning may be used to accelerate traditional scientific computing workflows, for example by flexible super-resolution or learning of improved solution stencils. However, here are several challenges with these approaches, foremost the fact that traditional numerical algorithms are extremely mature and scaleable, so that machine learning solutions are expected to compete with decades of progress.

In all of the cases explored in this perspective, progress will be accelerated by a diverse and robust set of benchmark problems with which to assess new solutions [130]. In addition, we must stress that these techniques are primarily tools to be used by human experts for scientific discovery. In the past, many advances have been driven in the field of fluid mechanics [23], and this is likely to continue. For example, understanding sensitivities with resolvent analysis [123], using physics informed neural networks (PINNs) [109] for RANS modeling [50], and using wall measurements to estimate turbulent flow fields [59] are all exciting avenues of research. Interestingly, there are also efforts to understand neural networks using techniques from PDEs [27].

Although there is a desire for automated machine learning algorithms, when applied to science and engineering applications, this is still primarily a human endeavor. However, progress in the field of PDEs, enabled by machine learning, is undeniable. Despite this progress, there is still much we don't know about PDEs. For example, it is unknown whether or not all solutions of the incompressible fluid flow equations even remain bounded in finite time, making it one of the 'Millennium Prize' problems. Our limitations in our understanding of PDEs is nicely summarized by Richard Feynman [52]:

'The next great era of awakening of human intellect may well produce a method of understanding the qualitative content of equations. Today we cannot. Today we cannot see that the water flow equations contain such things as the barber pole structure of turbulence that one sees between rotating cylinders. Today we cannot see whether Schrodinger's equation contains frogs, musical composers, or morality-or whether it does not. We cannot say whether something beyond it like God is needed, or not. And so we can all hold strong opinions either way.'

## Acknowledgements

The authors acknowledge support from the National Science Foundation AI Institute in Dynamic Systems (grant number 2112085). SLB acknowledges support from the Army Research Office (ARO W911NF-19-1-0045).

## References

- [1] M. J. Ablowitz and H. Segur. Solitons and the inverse scattering transform . SIAM, 1981.
- [2] M. J. Ablowitz, D. J. Kaup, A. C. Newell, and H. Segur. The inverse scattering transform-fourier analysis for nonlinear problems. Studies in applied mathematics , 53(4):249-315, 1974.
- [3] R. Abraham, J. E. Marsden, and T. Ratiu. Manifolds, Tensor Analysis, and Applications , volume 75 of Applied Mathematical Sciences . Springer-Verlag, 1988.
- [4] S. E. Ahmed, S. Pawar, O. San, A. Rasheed, T. Iliescu, and B. R. Noack. On closures for reduced order modelsa spectrum of first-principle to machine-learned avenues. Physics of Fluids , 33(9):091301, 2021.
- [5] E. P. Alves and F. Fiuza. Data-driven discovery of reduced plasma physics models from fully-kinetic simulations. arXiv preprint arXiv:2011.01927 , 2020.
- [6] S. Atkinson. Bayesian hidden physics models: Uncertainty quantification for discovery of nonlinear partial differential operators from data. arXiv preprint arXiv:2006.04228 , 2020.
- [7] P. J. Baddoo, B. Herrmann, B. J. McKeon, and S. L. Brunton. Kernel learning for robust dynamic mode decomposition: Linear and nonlinear disambiguation optimization (lando). Proceedings of the Royal Society A , 478(2260):20210830, 2022.

| [8] J. Bakarji and D. M. Tartakovsky. Data-driven discovery of coarse-grained equations. arXiv preprint arXiv:2002.00790 , 2020.                                                                                                                                                         |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [9] Y. Bar-Sinai, S. Hoyer, J. Hickey, and M. P. Brenner. Learning data-driven discretizations for partial differential equations. Proceedings of the National Academy of Sciences , 116(31):15344-15349, 2019.                                                                          |
| [10] M. Barrault, Y. Maday, N. C. Nguyen, and A. T. Patera. An 'empirical interpolation'method: application to efficient reduced-basis discretization of partial differential equations. Comptes Rendus Mathematique , 339(9): 667-672, 2004.                                            |
| [11] S. Beetham and J. Capecelatro. Formulating turbulence closures using sparse regression with embedded form invariance. Physical Review Fluids , 5(8):084611, 2020.                                                                                                                   |
| [12] S. Beetham, R. O. Fox, and J. Capecelatro. Sparse identification of multiphase turbulence closures for coupled fluid-particle flows. Journal of Fluid Mechanics , 914, 2021.                                                                                                        |
| [13] P. Benner, S. Gugercin, and K. Willcox. A survey of projection-based model reduction methods for parametric dynamical systems. SIAM review , 57(4):483-531, 2015.                                                                                                                   |
| [14] P. Benner, P. Goyal, B. Kramer, B. Peherstorfer, and K. Willcox. Operator inference for non-intrusive model reduction of systems with non-polynomial nonlinear terms. Computer Methods in Applied Mechanics and Engi- neering , 372:113433, 2020.                                   |
| [15] G. Berkooz, P. Holmes, and J. Lumley. The proper orthogonal decomposition in the analysis of turbulent                                                                                                                                                                              |
| flows. Ann. Rev. Fluid Mech. , 25:539-575, 1993.                                                                                                                                                                                                                                         |
| [16] J. Bongard and H. Lipson. Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences , 104(24):9943-9948, 2007. [17] J. Brandstetter, R. v. d. Berg, M. Welling, and J. K. Gupta. Clifford neural layers for pde modeling. arXiv |
| preprint arXiv:2209.04934 , 2022. [18] J. Brandstetter, M. Welling, and D. E. Worrall. Lie point symmetry data augmentation for neural pde solvers. In International Conference on Machine Learning , pages 2241-2256. PMLR, 2022.                                                       |
| [19] J. Brandstetter, D. Worrall, and M. Welling. Message passing neural pde solvers. arXiv preprint arXiv:2202.03376 , 2022.                                                                                                                                                            |
| [20] H. Brezis and F. Browder. Partial differential equations in the 20th century. Advances in mathematics , 135(1): 76-144, 1998.                                                                                                                                                       |
| [21] S. L. Brunton and J. N. Kutz. Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control . Cambridge University Press, 2nd edition, 2022.                                                                                                                |
| [22] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences , 113(15):3932-3937, 2016.                                                         |
| [23] S. L. Brunton, B. R. Noack, and P. Koumoutsakos. Machine learning for fluid mechanics. Annual Review of Fluid Mechanics , 52:477-508, 2020.                                                                                                                                         |
| [24] S. L. Brunton, M. Budiˇsi'c, E. Kaiser, and J. N. Kutz. Modern Koopman theory for dynamical systems. arXiv preprint arXiv:2102.12086 , 2021.                                                                                                                                        |
| [25] S. L. Brunton, M. Budiˇsi'c, E. Kaiser, and J. N. Kutz. Modern Koopman theory for dynamical systems. SIAM Review , 64(2):229-340, 2022.                                                                                                                                             |
| [26] M. Budiˇsi'c, R. Mohr, and I. Mezi'c. Applied Koopmanism a). Chaos: An Interdisciplinary Journal of Nonlinear Science , 22(4):047510, 2012.                                                                                                                                         |
| [27] M. Burger, L. RUTHOTTO, S. OSHER, et al. Connections between deep learning and partial differential equations. European Journal of Applied Mathematics , 32(3):395-396, 2021.                                                                                                       |
| [28] J. L. Callaham, J. V. Koch, B. W. Brunton, J. N. Kutz, and S. L. Brunton. Learning dominant physical processes with data-driven balance models. Nature communications , 12(1):1-10, 2021.                                                                                           |
| [29] J. L. Callaham, J.-C. Loiseau, G. Rigas, and S. L. Brunton. Nonlinear stochastic modelling with langevin regression. Proceedings of the Royal Society A , 477(2250):20210092, 2021.                                                                                                 |
| [30] J. L. Callaham, S. L. Brunton, and J.-C. Loiseau. On the role of nonlinear correlations in reduced-order mod- eling. Journal of Fluid Mechanics , 938(A1), 2022.                                                                                                                    |
| [31] J. L. Callaham, G. Rigas, J.-C. Loiseau, and S. L. Brunton. An empirical mean-field model of symmetry- breaking in a turbulent wake. Science Advances , 8(eabm4786), 2022.                                                                                                          |
| [32] S. Cao. Choose a transformer: Fourier or galerkin. Advances in neural information processing systems , 34:24924- 24940, 2021.                                                                                                                                                       |
| [33] K. Champion, B. Lusch, J. N. Kutz, and S. L. Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences , 116(45):22445-22451, 2019.                                                                                    |
| [34] S. Chaturantabut and D. C. Sorensen. Nonlinear model reduction via discrete empirical interpolation. SIAM Journal on Scientific Computing , 32(5):2737-2764, 2010.                                                                                                                  |
| [35] T. Chen and H. Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE Transactions on Neural Networks ,                                                                         |
| 6(4): 911-917, 1995.                                                                                                                                                                                                                                                                     |

| [36] J. D. Cole. On a quasi-linear parabolic equation occurring in aerodynamics. Quart. Appl. Math. , 9:225-236, 1951.                                                                                                                                       |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [37] R. Courant and D. Hilbert. Methods of mathematical physics: partial differential equations . John Wiley & Sons, 2008.                                                                                                                                   |
| [38] T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pat- tern recognition. IEEE transactions on electronic computers , (3):326-334, 1965.                                                        |
| [39] M. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel, and S. Ho. Lagrangian neural networks. arXiv preprint arXiv:2003.04630 , 2020.                                                                                                            |
| [40] M. Cranmer, A. Sanchez-Gonzalez, P. Battaglia, R. Xu, K. Cranmer, D. Spergel, and S. Ho. Discovering sym- bolic models from deep learning with inductive biases. arXiv preprint arXiv:2006.11287 , 2020.                                                |
| [41] M. D. Cranmer, R. Xu, P. Battaglia, and S. Ho. Learning symbolic physics with graph networks. arXiv preprint arXiv:1909.05862 , 2019.                                                                                                                   |
| [42] P. De Haan, M. Weiler, T. Cohen, and M. Welling. Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs. arXiv preprint arXiv:2003.05425 , 2020.                                                                                     |
| [43] M. De Hoop, D. Z. Huang, E. Qian, and A. M. Stuart. The cost-accuracy trade-off in operator learning with neural networks. arXiv preprint arXiv:2203.13181 , 2022.                                                                                      |
| [44] M. V. de Hoop, N. B. Kovachki, N. H. Nelsen, and A. M. Stuart. Convergence rates for learning linear opera- tors from noisy data. arXiv preprint arXiv:2108.12515 , 2021.                                                                               |
| [45] N. Deng, B. R. Noack, M. Morzynski, and L. R. Pastur. Low-order model for successive bifurcations of the fluidic pinball. Journal of fluid mechanics , 884(A37), 2020.                                                                                  |
| [46] N. Deng, B. R. Noack, M. Morzy'nski, and L. R. Pastur. Galerkin force model for transient and post-transient dynamics of the fluidic pinball. Journal of Fluid Mechanics , 918, 2021.                                                                   |
| [47] M. Dissanayake and N. Phan-Thien. Neural-network-based approximations for solving partial differential equations. communications in Numerical Methods in Engineering , 10(3):195-201, 1994.                                                             |
| [48] K. Duraisamy, G. Iaccarino, and H. Xiao. Turbulence modeling in the age of data. Annual Reviews of Fluid Mechanics , 51:357-377, 2019.                                                                                                                  |
| [49] H. Eivazi, L. Guastoni, P. Schlatter, H. Azizpour, and R. Vinuesa. Recurrent neural networks and koopman- based frameworks for temporal predictions in a low-order model of turbulence. International Journal of Heat and Fluid Flow , 90:108816, 2021. |
| [50] H. Eivazi, M. Tahani, P. Schlatter, and R. Vinuesa. Physics-informed neural networks for solving reynolds- averaged navier-stokes equations. Physics of Fluids , 34(7):075117, 2022.                                                                    |
| [51] U. Fasel, J. N. Kutz, B. W. Brunton, and S. L. Brunton. Ensemble-sindy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control. Proceedings of the Royal Society A , 478(2260): 20210904, 2022.              |
| [52] R. P. Feynman, R. B. Leighton, and M. Sands. The Feynman Lectures on Physics , volume 2. Basic Books, 2013.                                                                                                                                             |
| [53] K. Fukami and K. Taira. Robust machine learning of turbulence through generalized buckingham pi-inspired pre-processing of training data. In APS Division of Fluid Dynamics Meeting Abstracts , pages A31-004, 2021.                                    |
| [54] K. Fukami, K. Fukagata, and K. Taira. Super-resolution reconstruction of turbulent flows with machine learn- ing. Journal of Fluid Mechanics , 870:106-120, 2019.                                                                                       |
| [55] K. Fukami, K. Fukagata, and K. Taira. Machine-learning-based spatio-temporal super resolution reconstruc- tion of turbulent flows. Journal of Fluid Mechanics , 909, 2021.                                                                              |
| [56] C. Gin, B. Lusch, S. L. Brunton, and J. N. Kutz. Deep learning models for global coordinate transformations that linearise pdes. European Journal of Applied Mathematics , 32(3):515-539, 2021.                                                         |
| [57] C. R. Gin, D. E. Shea, S. L. Brunton, and J. N. Kutz. Deepgreen: Deep learning of green's functions for nonlinear boundary value problems. arXiv preprint arXiv:2101.07206 , 2020.                                                                      |
| [58] Y. Guan, S. L. Brunton, and I. Novosselov. Sparse nonlinear models of chaotic electroconvection. Royal Society Open Science , 8(8):202367, 2021.                                                                                                        |
| [59] A. Guemes, S. Discetti, A. Ianiro, B. Sirmacek, H. Azizpour, and R. Vinuesa. From coarse wall measurements to turbulent velocity fields through deep learning. Physics of Fluids , 33(7):075121, 2021.                                                  |
| [60] D. R. Gurevich, P. A. Reinbold, and R. O. Grigoriev. Robust and optimal sparse regression for nonlinear pde models. Chaos: An Interdisciplinary Journal of Nonlinear Science , 29(10):103113, 2019.                                                     |
| [61] P. Holmes and J. Guckenheimer. Nonlinear oscillations, dynamical systems, and bifurcations of vector fields , vol- ume 42 of Applied Mathematical Sciences . Springer-Verlag, Berlin, Heidelberg, 1983.                                                 |
| [62] P. Holmes, J. L. Lumley, G. Berkooz, and C. W. Rowley. Turbulence, Coherent Structures, Dynamical Systems and Symmetry . Cambridge University Press, Cambridge, 2nd paperback edition, 2012.                                                            |
| [63] E. Hopf. The partial differential equation u t + uu x = µu xx . Comm. Pure App. Math. , 3:201-230, 1950. [64] A. A. Kaptanoglu, J. L. Callaham, C. J. Hansen, A. Aravkin, and S. L. Brunton. Promoting global stability in                              |
| data-driven models of quadratic nonlinear dynamics. arXiv preprint arXiv:2105.01843 , 2021. [65] A. A. Kaptanoglu, B. M. de Silva, U. Fasel, K. Kaheman, J. L. Callaham, C. B. Delahunt, K. Champion, J.-C.                                                  |

| Loiseau, J. N. Kutz, and S. L. Brunton. Pysindy: A comprehensive python package for robust sparse system identification. arXiv preprint arXiv:2111.08481 , 2021.                                                                                                                                  |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [66] A. A. Kaptanoglu, K. D. Morgan, C. J. Hansen, and S. L. Brunton. Physics-constrained, low-dimensional models for mhd: First-principles and data-driven approaches. Physical Review E , 104(015206), 2021.                                                                                    |
| [67] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-informed machine learning. Nature Reviews Physics , 3(6):422-440, 2021.                                                                                                                             |
| [68] S. Klus, F. Nuske, P. Koltai, H. Wu, I. Kevrekidis, C. Schutte, and F. No'e. Data-driven model reduction and transfer operator approximation. Journal of Nonlinear Science , pages 1-26, 2018.                                                                                               |
| [69] D. Kochkov, J. A. Smith, A. Alieva, Q. Wang, M. P. Brenner, and S. Hoyer. Machine learning accelerated computational fluid dynamics. arXiv preprint arXiv:2102.01010 , 2021.                                                                                                                 |
| [70] B. O. Koopman. Hamiltonian systems and transformation in Hilbert space. Proceedings of the National Academy of Sciences , 17(5):315-318, 1931. URL http://www.pnas.org/content/17/5/315.short .                                                                                              |
| [71] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint arXiv:2108.08481 , 2021.                                                                                               |
| [72] J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor. Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems . SIAM, 2016.                                                                                                                                               |
| [73] J. N. Kutz, J. L. Proctor, and S. L. Brunton. Applied Koopman theory for partial differential equations and data-driven modeling of spatio-temporal systems. Complexity , 2018(6010634):1-16, 2018.                                                                                          |
| [74] K. Lee and K. T. Carlberg. Model reduction of dynamical systems on nonlinear manifolds using deep convo- lutional autoencoders. Journal of Computational Physics , 404:108973, 2020.                                                                                                         |
| [75] Q. Li, F. Dietrich, E. M. Bollt, and I. G. Kevrekidis. Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator. Chaos: An Interdisciplinary Journal of Nonlinear Science , 27(10):103111, 2017.                  |
| [76] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895 , 2020.                                                                                |
| [77] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Multipole graph neural operator for parametric partial differential equations. arXiv preprint arXiv:2006.09535 , 2020.                                                                        |
| [78] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485 , 2020.                                                                             |
| [79] Z. Li, K. Meidani, and A. B. Farimani. Transformer for partial differential equations' operator learning. arXiv preprint arXiv:2205.13671 , 2022.                                                                                                                                            |
| [80] J. Ling, A. Kurzawski, and J. Templeton. Reynolds averaged turbulence modelling using deep neural net- works with embedded invariance. Journal of Fluid Mechanics , 807:155-166, 2016.                                                                                                       |
| [81] J.-C. Loiseau. Data-driven modeling of the chaotic thermal convection in an annular thermosyphon. Theoret- ical and Computational Fluid Dynamics , 34, 2020.                                                                                                                                 |
| [82] J.-C. Loiseau and S. L. Brunton. Constrained sparse Galerkin regression. Journal of Fluid Mechanics , 838:42-67, 2018.                                                                                                                                                                       |
| [83] J.-C. Loiseau, B. R. Noack, and S. L. Brunton. Sparse reduced-order modeling: sensor-based dynamics to full-state estimation. Journal of Fluid Mechanics , 844:459-490, 2018.                                                                                                                |
| [84] Z. Long, Y. Lu, X. Ma, and B. Dong. Pde-net: Learning pdes from data. In International Conference on Machine                                                                                                                                                                                 |
| Learning , pages 3208-3216. PMLR, 2018. [85] Z. Long, Y. Lu, and B. Dong. Pde-net 2.0: Learning pdes from data with a numeric-symbolic hybrid deep                                                                                                                                                |
| network. Journal of Computational Physics , 399:108925, 2019. [86] L. Lu, P. Jin, and G. E. Karniadakis.                                                                                                                                                                                          |
| Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193 , 2019. [87] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via deeponet based on |
| [88] J. Lumley. Toward a turbulent constitutive relation. Journal of Fluid Mechanics , 41(02):413-434, 1970.                                                                                                                                                                                      |
| [89] B. Lusch, J. N. Kutz, and S. L. Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. Nature communications , 9(1):4950, 2018.                                                                                                                                       |
| [90] A. Mardt, L. Pasquali, H. Wu, and F. No'e. VAMPnets: Deep learning of molecular kinetics. Nature Communi- cations , 9(5), 2018.                                                                                                                                                              |
| [91] J. E. Marsden and T. S. Ratiu. Introduction to mechanics and symmetry . Springer-Verlag, 2nd edition, 1999. [92] D. A. Messenger and D. M. Bortz. Weak sindy: Galerkin-based data-driven model selection. Multiscale                                                                         |
| Modeling & Simulation , 19(3):1474-1497, 2021. [93] D. A. Messenger and D. M. Bortz. Weak sindy for partial differential equations. Journal of Computational Physics , page 110525, 2021.                                                                                                         |
| [94] I. Mezi'c. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dynamics ,                                                                                                                                                                                |
| 41(1-3):309-325, 2005.                                                                                                                                                                                                                                                                            |

| [95] I. Mezi'c. Analysis of fluid flows via spectral properties of the Koopman operator. Ann. Rev. Fluid Mech. , 45: 357-378, 2013.                                                                                                                                                                |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [96] I. Mezi'c and A. Banaszuk. Comparison of systems with complex behavior. Physica D: Nonlinear Phenomena , 197(1-2):101 - 133, 2004. ISSN 0167-2789. doi: http://dx.doi.org/10.1016/j.physd.2004.06.015. URL http: //www.sciencedirect.com/science/article/pii/S0167278904002507 .              |
| [97] M. Mollenhauer, N. Mucke, and T. Sullivan. Learning linear operators: Infinite-dimensional regression as a well-behaved non-compact inverse problem. arXiv preprint arXiv:2211.08875 , 2022.                                                                                                  |
| [98] F. No'e and F. Nuske. A variational approach to modeling slow processes in stochastic dynamical systems. Multiscale Modeling & Simulation , 11(2):635-655, 2013.                                                                                                                              |
| [99] F. Nuske, B. G. Keller, G. P'erez-Hern'andez, A. S. Mey, and F. No'e. Variational approach to molecular kinetics. Journal of chemical theory and computation , 10(4):1739-1752, 2014.                                                                                                         |
| [100] S. E. Otto and C. W. Rowley. Linearly-recurrent autoencoder networks for learning dynamics. arXiv preprint arXiv:1712.01378 , 2017.                                                                                                                                                          |
| [101] J. Page and R. R. Kerswell. Koopman analysis of burgers equation. Physical Review Fluids , 3(7):071901, 2018.                                                                                                                                                                                |
| [102] S. Pan, S. L. Brunton, and J. N. Kutz. Neural implicit flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data. Journal of Machine Learning Research , 24(41):1-60, 2023.                                                                                            |
| [103] J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott. Using machine learning to replicate chaotic attractors and calculate lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science , 27(12):121102, 2017.                                                      |
| [104] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott. Model-free prediction of large spatiotemporally chaotic systems from data: a reservoir computing approach. Physical review letters , 120(2):024102, 2018.                                                                                  |
| [105] B. Peherstorfer and K. Willcox. Data-driven operator inference for nonintrusive projection-based model re- duction. Computer Methods in Applied Mechanics and Engineering , 306:196-215, 2016.                                                                                               |
| [106] B. Peherstorfer, Z. Drmac, and S. Gugercin. Stability of discrete empirical interpolation and gappy proper orthogonal decomposition with randomized and deterministic sampling points. SIAM Journal on Scientific Computing , 42(5):A2837-A2864, 2020.                                       |
| [107] S. Pope. A more general effective-viscosity hypothesis. Journal of Fluid Mechanics , 72(2):331-340, 1975. [108] E. Qian, B. Kramer, B. Peherstorfer, and K. Willcox. Lift & learn: Physics-informed machine learning for                                                                     |
| large-scale nonlinear dynamical systems. Physica D: Nonlinear Phenomena , 406:132401, 2020. Physics-informed neural networks: A deep learning frame- work for solving forward and inverse problems involving nonlinear partial differential equations. Journal of                                  |
| [109] M. Raissi, P. Perdikaris, and G. Karniadakis. Computational Physics , 378:686-707, 2019.                                                                                                                                                                                                     |
| [110] M. Reed and B. Simon. Methods of Modern Mathematical Physics. I . Academic Press Inc. [Harcourt Brace Jovanovich Publishers], New York, second edition, 1980.                                                                                                                                |
| [111] P. A. Reinbold, D. R. Gurevich, and R. O. Grigoriev. Using noisy or incomplete data to discover models of spatiotemporal dynamics. Physical Review E , 101(1):010203, 2020. [112] P. A. Reinbold, L. M. Kageorge, M. F. Schatz, and R. O. Grigoriev. Robust learning from noisy, incomplete, |
| high-dimensional experimental data via physically constrained symbolic regression. Nature communications , 12(1):1-8, 2021. [113] C. W. Rowley and J. E. Marsden. Reconstruction equations and the Karhunen-Lo'eve expansion for systems                                                           |
| [114] C. W. Rowley, I. Mezi'c, S. Bagheri, P. Schlatter, and D. Henningson. Spectral analysis of nonlinear flows. J. Fluid Mech. , 645:115-127, 2009.                                                                                                                                              |
| [115] S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz. Data-driven discovery of partial differential equations. Science Advances , 3(e1602614), 2017.                                                                                                                                     |
| [116] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning , pages 8459-8468. PMLR, 2020.                                                                   |
| [117] H. Schaeffer. Learning partial differential equations via data discovery and sparse optimization. In Proc. R. , volume 473, page 20160446. The Royal Society, 2017.                                                                                                                          |
| Soc. A                                                                                                                                                                                                                                                                                             |
| [118] H. Schaeffer and S. G. McCalla. Sparse model selection via integral terms. Physical Review E , 96(2):023302, 2017.                                                                                                                                                                           |
| [119] H. Schaeffer, R. Caflisch, C. D. Hauck, and S. Osher. Sparse dynamics for partial differential equations. Pro- ceedings of the National Academy of Sciences USA , 110(17):6634-6639, 2013.                                                                                                   |
| [120] M. Schmelzer, R. P. Dwight, and P. Cinnella. Discovery of algebraic reynolds-stress models using sparse symbolic regression. Flow, Turbulence and Combustion , 104(2):579-603, 2020.                                                                                                         |
| [121] P. J. Schmid. Dynamic mode decomposition for numerical and experimental data. J. Fluid. Mech , 656:5-28, 2010.                                                                                                                                                                               |

2009.

| [123] A. Sharma and B. McKeon. On coherent structure in wall turbulence. Journal of Fluid Mechanics , 728:196-238, 2013.                                                                                                                                           |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [124] B. Stevens and T. Colonius. Enhancement of shock-capturing methods via machine learning. Theoretical and Computational Fluid Dynamics , 34:483-496, 2020.                                                                                                    |
| [125] B. Stevens and T. Colonius. Finitenet: A fully convolutional lstm network architecture for time-dependent partial differential equations. arXiv preprint arXiv:2002.03014 , 2020.                                                                            |
| [126] R. Supekar, B. Song, A. Hastewell, G. P. Choi, A. Mietke, and J. Dunkel. Learning hydrodynamic equations for active matter from particle simulations and experiments. Proceedings of the National Academy of Sciences , 120(7):e2206994120, 2023.            |
| [127] B. Suri, L. Kageorge, R. O. Grigoriev, and M. F. Schatz. Capturing turbulent dynamics and statistics in exper- iments with unstable periodic orbits. Physical Review Letters , 125(6):064501, 2020.                                                          |
| [128] K. Taira, S. L. Brunton, S. Dawson, C. W. Rowley, T. Colonius, B. J. McKeon, O. T. Schmidt, S. Gordeyev, V. Theofilis, and L. S. Ukeiley. Modal analysis of fluid flows: An overview. AIAA Journal , 55(12):4013-4041, 2017.                                 |
| [129] K. Taira, M. S. Hemati, S. L. Brunton, Y. Sun, K. Duraisamy, S. Bagheri, S. Dawson, and C.-A. Yeh. Modal analysis of fluid flows: Applications and outlook. AIAA Journal , 58(3):998-1022, 2020.                                                             |
| [130] M. Takamoto, T. Praditia, R. Leiteritz, D. MacKinlay, F. Alesiani, D. Pfluger, and M. Niepert. Pdebench: An extensive benchmark for scientific machine learning. arXiv preprint arXiv:2210.07182 , 2022.                                                     |
| [131] N. Takeishi, Y. Kawahara, and T. Yairi. Learning Koopman invariant subspaces for dynamic mode decompo- sition. In Advances in Neural Information Processing Systems , pages 1130-1140, 2017.                                                                 |
| [132] R. Vinuesa and S. L. Brunton. Enhancing computational fluid dynamics with machine learning. Nature Computational Science , 2(6):358-366, 2022.                                                                                                               |
| [133] R. Wang, K. Kashinath, M. Mustafa, A. Albert, and R. Yu. Towards physics-informed deep learning for tur- bulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &Data Mining , pages 1457-1466, 2020. |
| [134] R. Wang, R. Walters, and R. Yu. Incorporating symmetry into deep dynamics models for improved general- ization. arXiv preprint arXiv:2002.03061 , 2020.                                                                                                      |
| [135] C. Wehmeyer and F. No'e. Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics. The Journal of Chemical Physics , 148(241703), 2018.                                                                                   |
| [136] M. O. Williams, I. G. Kevrekidis, and C. W. Rowley. A data-driven approximation of the Koopman operator: extending dynamic mode decomposition. Journal of Nonlinear Science , 6:1307-1346, 2015.                                                             |
| [137] M. O. Williams, C. W. Rowley, and I. G. Kevrekidis. A kernel approach to data-driven Koopman spectral analysis. Journal of Computational Dynamics , 2:247, 2015.                                                                                             |
| [138] E. Yeung, S. Kundu, and N. Hodas. Learning deep neural network representations for Koopman operators of nonlinear dynamical systems. arXiv preprint arXiv:1708.06850 , 2017.                                                                                 |
| [139] M. Z. Yousif, M. Zhang, L. Yu, R. Vinuesa, and H. Lim. A transformer-based synthetic-inflow generator for spatially-developing turbulent boundary layers. arXiv preprint arXiv:2206.01618 , 2022.                                                            |
| [140] L. Zanna and T. Bolton. Data-driven equation discovery of ocean mesoscale closures. Geophysical Research Letters , 47(17):e2020GL088376, 2020.                                                                                                               |