## VMamba: Visual State Space Model

Yue Liu UCAS liuyue171@mails.ucas.ac.cn

Yunjie Tian UCAS tianyunjie19@mails.ucas.ac.cn

Yuzhong Zhao

UCAS

zhaoyuzhong20@mails.ucas.ac.cn

Hongtian Yu UCAS

yuhongtian17@mails.ucas.ac.cn

Lingxi Xie Huawei Inc. 198808xc@gmail.com

Yaowei Wang

Pengcheng Lab. wangyw@pcl.ac.cn

Qixiang Ye UCAS

qxye@ucas.ac.cn

## Yunfan Liu

UCAS

yunfan.liu@ucas.ac.cn

## Abstract

Designing computationally efficient network architectures persists as an ongoing necessity in computer vision. In this paper, we transplant Mamba, a state-space language model, into VMamba, a vision backbone that works in linear time complexity. At the core of VMamba lies a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D helps bridge the gap between the ordered nature of 1D selective scan and the nonsequential structure of 2D vision data, which facilitates the gathering of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments showcase VMamba's promising performance across diverse visual perception tasks, highlighting its advantages in input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba .

## 1 Introduction

Visual representation learning stands as a fundamental research area in computer vision, which has witnessed remarkable progress in the era of deep learning. To represent complex patterns in vision data, two primary categories of backbone networks, i.e. , Convolution Neural Networks (CNNs) [49, 28, 30, 54, 38] and Vision Transformers (ViTs) [13, 37, 58, 68], have been proposed and extensively utilized in a variety of visual tasks. Compared to CNNs, ViTs generally demonstrate superior learning capabilities on large-scale data due to the integration of the self-attention mechanism [59, 13]. However, the quadratic complexity of self-attention w.r.t. the number of tokens introduces substantial computational overhead in downstream tasks involving large spatial resolutions.

To tackle this challenge, considerable efforts have been made to enhance the efficiency of attention computation [55, 37, 12]. However, existing approaches either impose limitations on the size of the effective receptive field [37] or experience evident performance degradation across diverse

Preprint. Under review.

Figure 1: Comparison of correlation establishment between image patches via (a) self-attention and (b) the proposed 2D-Selective-Scan (SS2D). Red boxes indicate the query image patch, with patch opacity representing the degree of information loss.

<!-- image -->

tasks [31, 62]. This motivates us to develop a novel architecture for vision data, preserving the inherent advantages of the vanilla self-attention mechanism, i.e., global receptive fields and dynamic weighting parameters [23].

Recently, Mamba [17], a novel State Space Model (SSM) [17, 44, 61] in the field natural language processing (NLP), has emerged as a highly promising approach for long sequence modeling with linear complexity. Drawing inspiration from this advancement, we introduce VMamba, a vision backbone integrating SSM-based blocks to facilitate efficient visual representation learning. However, the core algorithm of Mamba, i.e. , the parallelized selective scan operation, is essentially designed for processing one-dimensional sequential data. This poses a challenge when attempting to adapt it for processing vision data, which inherently lacks a sequential arrangement of visual components. To address this issue, we propose 2D Selective Scan (SS2D), a four-way scanning mechanism tailored for spatial domain traversal. In contrast to the self-attention mechanism (Figure 1 (a)), SS2D ensures that each image patch gains contextual knowledge exclusively through a compressed hidden state computed along the corresponding scanning path (Figure 1 (b)), thereby reducing the computational complexity from quadratic to linear.

Upon the VSS blocks, we develop a family of VMamba architectures ( i.e. , VMamba-Tiny/Small/Base) and accelerate them through a series of architectural enhancements and implementation optimizations. Compared to benchmark vision models built on CNN (ConvNeXt [38]), ViT (Swin [37] and HiViT [68]), and SSM (S4ND [45] and Vim [71]), VMamba consistently achieves superior image classification accuracy on ImageNet-1K [9] across model scales. Specifically, VMamba-Base achieves a top-1 accuracy of 83 . 9% , surpassing Swin by +0 . 4% , with a throughput exceeding that of Swin by a substantial margin over 40% ( 646 vs. 458 ). The superiority of VMamba extends across various downstream tasks, with VMamba-Tiny/Small/Base achieving 47 . 3% / 48 . 7% / 49 . 2% mAP in object detection on COCO [34] ( 1 × training schedule). This outperforms Swin by 4 . 6% / 3 . 9% / 2 . 3% and ConvNeXt by 3 . 1% / 3 . 3% / 2 . 2% , respectively. As for single-scale semantic segmentation on ADE20K [70], VMamba-Tiny/Small/Base achieves 47 . 9% / 50 . 6% / 51 . 0% mIoU, which surpasses Swin by 3 . 4% / 3 . 0% / 2 . 9% and ConvNeXt by 1 . 9% / 1 . 9% / 1 . 9% , respectively. Furthermore, unlike ViT-based models, which experience quadratic growth in computational complexity with the number of input tokens, VMamba exhibits linear growth in FLOPs while maintaining comparable performance. This underscores its state-of-the-art input scalability.

The contributions of this study are summarized as follows:

- · We propose VMamba, an SSM-based vision backbone network for visual representation learning with linear time complexity. A series of improvements in architectural design and implementation details are adopted to improve the inference speed of VMamba.
- · We introduce 2D Selective Scan (SS2D) to bridge the gap between 1D array scanning and 2D plane traversal, facilitating the extension of selective SSM to process vision data.
- · Without bells and whistles, VMamba demonstrates promising performance across a range of visual tasks, including image classification, object detection, and semantic segmentation. It also exhibits remarkable adaptability w.r.t. the length of the input sequence, showcasing linear growth in computational complexity.

## 2 Related Work

Convolutional Neural Networks (CNNs). Starting from AlexNet [32], significant efforts have been dedicated to enhancing the modeling capability [49, 53, 28, 30] and computational efficiency [29, 54, 66, 47] of CNN-based models across a spectrum of visual tasks. More sophisticated operators, such as the depth-wise convolution [29] and deformable convolution [5, 72], have been proposed for increasing the flexibility and efficacy of CNNs. Recently, drawing inspiration from the success of Transformers [60], modern CNNs [38] have demonstrated promising performance by incorporating long-range dependencies [11, 48, 35] and dynamic weights [24] into their architectural design.

Vision Transformers (ViTs). As the most representative pioneering work, ViT [13] explores the effectiveness of vision models based on vanilla Transformer architecture and reveals the significance of large-scale pre-training in improving the performance in image classification. To mitigate ViT's reliance on extremely large datasets, DeiT [58] introduces a teacher-student distillation strategy to transfer knowledge encapsulated in CNN models to ViTs, showcasing the importance of the inductive bias in visual perception. Following this line of thought, subsequent studies propose hierarchical ViTs [37, 12, 63, 40, 68, 56, 6, 10, 69, 1].

Another line of research focuses specifically on enhancing the computational efficiency of the selfattention mechanism, which serves as the cornerstone of ViTs. By formulating self-attention as the linear dot-product of kernel feature maps, Linear Attention [31] leverages the associativity property of matrix products to reduce the computational complexity from quadratic to linear. GLA [67] further introduces a hardware-efficient variant of linear attention, which balances memory movement against parallelizability. RWKV [46] also leverages the linear attention mechanism to combine the parallelizable training of transformers with the efficient inference of recurrent neural networks (RNNs). RetNet [52] integrates an additional gating mechanism to enable a parallelizable computation path as an alternative to recurrence, and RMT [15] extends it for visual representation learning by incorporating the temporal decay mechanism into the spatial domain.

State Space Models (SSMs). Despite the widespread adoption of ViT architectures in vision tasks, the quadratic complexity of self-attention presents a significant challenge when dealing with long input sequences, ( e.g. , high-resolution images). Among the attempts to improve the scaling efficiency [8, 7, 46, 52, 42], SSMs have emerged as compelling alternatives to Transformers, attracting significant attention from the community. Gu et al. [21] demonstrated the potential of SSM-based models in handling the long-range dependency with the HiPPO initialization [18]. To improve the practical feasibility, S4 [20] proposed to normalize the parameter matrices into a diagonal structure. Subsequently, various structured SSM models have emerged, each introducing different architectural enhancements. These include models with complex-diagonal structures [22, 19], support for multiple-input multiple-output [50], decomposition of diagonal plus low-rank operations [25], and selection mechanisms [17]. These advancements have also been integrated into larger representation models [44, 42, 16], further demonstrating the versatility and scalability of structured state space models in various applications. While these models primarily focus on applying SSMs to long-range and sequential data like text and speech, there has been limited exploration of SSMs' application to vision data with two-dimensional structures.

## 3 Preliminaries

Formulation of SSMs. Originating from the Kalman filter [33], SSMs can be regarded as linear time-invariant (LTI) systems that map the input stimulation u ( t ) ∈ R to response y ( t ) ∈ R through the hidden state h ( t ) ∈ R N . Concretely, continuous-time SSMs can be formulated as linear ordinary differential equations (ODEs) as follows,

h ' ( t ) = Ah ( t ) + B u ( t ) , y ( t ) = Ch ( t ) + Du ( t ) , (1)

where A ∈ R N × N , B ∈ R N × 1 , C ∈ R 1 × N , and D ∈ R 1 are the weighting parameters.

Discretization of SSM. To be integrated into deep models, continuous-time SSMs must undergo discretization in advance. Concretely, consider the time interval [ t a , t b ] , the analytic solution to the

hidden state variable h ( t ) | t = t b can be expressed as

h ( t b ) = e A ( t b -t a ) h ( t a ) + e A ( t b -t a ) ∫ t b t a B ( τ ) u ( τ ) e -A ( τ -t a ) dτ. (2)

By sampling with the time-scale parameter ∆ ( i.e. , dτ | t i +1 t i = ∆ i ), h ( t b ) can be discretized by

h b = e A (∆ a + ... +∆ b -1 ) ( h a + b -1 ∑ i = a B i u i e -A (∆ a + ... +∆ i ) ∆ i ) , (3)

where [ a, b ] is the corresponding discrete step interval. Notably, this formulation approximates the result obtained by the zero-order hold (ZOH) method, which is frequently utilized in the literature of SSM-based models (please refer to Appendix A for detailed proof).

Selective Scan Mechanism. To tackle the limitation of LTI SSMs (Eq. 1) in capturing the contextual information, Gu et al. [17] propose a novel parameterization method for SSMs that integrates an input-dependent selection mechanism (referred to as S6). However, in the case of selective SSMs, the time-varying weighting parameters present a challenge for efficient computation of hidden states, as convolutions do not accommodate dynamic weights and are consequently rendered inapplicable. Nevertheless, as the recurrence relation of h b in Eq. 3 can be derived, the response y b can still be efficiently computed using associative scan algorithms [2, 43, 51] with linear complexity (the detailed explanation is deferred to Appendix B).

## 4 VMamba: Visual State Space Model

## 4.1 Network Architecture

We develop VMamba at three scales: VMamba-Tiny, VMamba-Small, and VMamba-Base (referred to as VMamba-T, VMamba-S, and VMamba-B, respectively). An overview of the architecture of VMamba-T is illustrated in Figure 3 (a), and detailed configurations are provided in Appendix E. The input image I ∈ R H × W × 3 is first partitioned into patches by a stem module, resulting in a 2D feature map with the spatial dimension of H/ 4 × W/ 4 . Subsequently, multiple network stages are employed to create hierarchical representations with resolutions of H/ 8 × W/ 8 , H/ 16 × W/ 16 , and H/ 32 × W/ 32 . Each stage comprises a down-sampling layer (except for the first stage) followed by a stack of Visual State Space (VSS) blocks.

The VSS blocks serve as the visual counterpart to Mamba blocks [17] (Figure 3 (b)) for representation learning. The initial architecture of VSS blocks (referred to as the 'vanilla VSS Block' in Figure 3 (c)) is formulated by substituting the S6 module, which stands as the core of Mamba in concurrently achieving global receptive fields, dynamic weights ( i.e. , selectivity), and linear complexity, with the newly proposed 2D-Selective-Scan (SS2D) module (to be introduced in the following subsection). To further enhance computational efficiency, we eliminate the entire multiplicative branch (encircled by the red box in Figure 3 (c)), as the effect of the gating mechanism has been achieved by the selectivity of SS2D. Consequently, the resulting VSS block (depicted in Figure 3 (d)) consists of a single network branch with two residual modules, mimicking the architecture of a vanilla Transformer block [60]. Throughout this paper, all results are obtained using VMamba models built with VSS blocks in this architecture.

## 4.2 2D-Selective-Scan for Vision Data (SS2D)

While the sequential nature of the scanning operation in S6 aligns well with NLP tasks involving temporal data, it poses a significant challenge when applied to vision data, which is inherently nonsequential and encompasses spatial information ( e.g. , local texture and global structure). To address this issue, S4ND [45] reformulates SSM with convolutional operations, directly extending the kernel from 1D to 2D through outer-product. However, such modification prevents the weights from being input-independent, resulting in a limited capacity for capturing contextual information. Therefore, we adhere to the selective scan approach [17] for input processing, and propose the 2D-Selective-Scan (SS2D) module to adapt S6 to vision data without compromising its advantages.

Figure 2: Illustration of 2D-Selective-Scan (SS2D). Input patches are traversed along four different scanning paths ( Cross-Scan ), and each sequence is independently processed by distinct S6 blocks. Subsequently, the results are merged to construct a 2D feature map as the final output ( Cross-Merge ).

<!-- image -->

Figure 3: Left: Illustration of (a) the overall architecture of VMamba, and (b) - (d) the structure of Mamba and VSS blocks. Right: Comparison of variants of VMamba and benchmark methods in classification accuracy and computational efficiency.

<!-- image -->

As illustrated in Figure 2, data forwarding in SS2D involves three steps: cross-scan, selective scanning with S6 blocks, and cross-merge. Given the input data, SS2D first unfolds input patches into sequences along four distinct traversal paths ( i.e. , Cross-Scan), processes each patch sequence using a separate S6 block in parallel, and subsequently reshapes and merges the resultant sequences to form the output map ( i.e. , Cross-Merge). By adopting complementary 1D traversal paths, SS2D enables each pixel in the image to effectively integrate information from all other pixels in different directions, thereby facilitating the establishment of global receptive fields in the 2D space.

## 4.3 Accelerating VMamba

As shown in Figure 3 (e), the VMamba-T model with vanilla VSS blocks (referred to as 'Vanilla VMamba') achieves a throughput of 426 images/s and consists of 22 . 9 M parameters with 5 . 6 G FLOPs. Despite the state-of-the-art 82 . 2% classification accuracy (in the tiny-level, outperforms Swin-T [37] by 0 . 9% ), the low throughput and high memory overhead pose significant challenges for the practical deployment of VMamba.

In this subsection, we outline our efforts to enhance its inference speed, primarily focusing on advancements in both implementation details and architectural design. We evaluate the models through image classification on ImageNet-1K. The impact of each progressive improvement is summarized as follows, where ( % , img/s) denote the gain of top-1 accuracy on ImageNet-1K and inference throughput, respectively. This will be further discussed in Appendix E.

Step (a) ( +0 . 0% , +41 img/s) by re-implementing Cross-Scan and Cross-Merge in Triton .

| Step (b) ( +0 . 0% , - 3 img/s) by adjusting the CUDA implementation of the selective scan to ac- commodate float16 input and float32 output. This remarkably enhances the training efficiency (throughput from 165 to 184 ), despite slight speed fluctuation at test time.                                   |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Step (c) ( +0 . 0% , +174 img/s) by substituting the relatively slow einsum in selective scan with a linear transformation ( i.e. , torch.nn.functional.linear ). We also adopt the tensor layout of (B, C, H, W) to eliminate unnecessary data permutations.                                                  |
| Step (d) ( - 0 . 6% , +175 img/s) by introducing MLP into VMamba due to its computational efficiency. We also discard the DWConv (depth-wise convolutional [24]) layers and change the layer configuration from [2,2,9,2] to [2,2,2,2] to lower FLOPs.                                                         |
| Step (e) ( +0 . 6% , +366 img/s) by discarding the entire multiplicative branch, adopting the VSS block illustrate in Figure 3 (d), and reducing the parameter ssm-ratio (the feature expansion factor) from 2 . 0 to 1 . 0 . This allows us to raise the layer numbers to [2,2,5,2] while reducing the FLOPs. |
| Step (f) ( +0 . 3% , +161 img/s) by reducing the parameter d\_state (the SSM state dimension) from 16 . 0 to 1 . 0 . This allows us to raise ssm-ratio back to 2 . 0 and introduce the DWConv layers without increasing the FLOPs.                                                                              |
| Step (g) ( +0 . 1% , +346 img/s) by reducing the ssm-ratio to 1 . 0 while changing the layer configu- ration from [2,2,5,2] to [2,2,8,2] .                                                                                                                                                                     |

## 5 Experiments

In this section, we conduct a series of experiments to evaluate the performance of VMamba and compare it with popular benchmark models across various visual tasks. We also validate the effectiveness of the proposed 2D feature map traversing method by comparing it to alternative approaches. Additionally, we analyze the characteristics of VMamba by visualizing its effective receptive field (ERF) and activation map, and examining its scalability with increasingly long input sequences. We primarily followed the hyper-parameter settings and experimental configurations of Swin [37]. Please refer to Appendix E and F for detailed experiment settings, and Appendix H for more ablations. All experiments are conducted on a server with 8 × NVIDIA Tesla-A100 GPUs.

## 5.1 Image Classification

We evaluate VMamba's performance in image classification on ImageNet-1K [9], and the comparison results against benchmark methods are summarized in Table 1. With similar FLOPs, VMamba-T achieves a top-1 accuracy of 82 . 6% , outperforming DeiT-S by 2 . 8% and Swin-T by 1 . 3% . Notably, VMambamaintains its performance advantage at both Small and Base scale. For instance, VMamba-B achieves a top-1 accuracy of 83 . 9% , surpassing DeiT-B by 2 . 1% and Swin-B by 0 . 4% .

In terms of computational efficiency, VMamba-T achieves a throughput of 1,686 images/s, which is either better or comparable to state-of-the-art methods. This advantage persists across VMamba-S and VMamba-B, with a throughput of 877 images/s and 646 images/s, respectively. Compared to SSM-based models, the throughput of VMamba-T is 1 . 47 × higher than S4ND-Conv-T [45] and 1 . 08 × higher than Vim-S [71], while maintaining a performance lead over them by a clear margin of 0 . 4% and 2 . 1% , respectively.

## 5.2 Downstream Tasks

In this sub-section, we assess the performance of VMamba on downstream tasks, including object detection and instance segmentation on MSCOCO2017 [34], and semantic segmentation on ADE20K [70]. The training framework is developed based on the MMDetection [3] and MMSegmenation [4] library, and we follow [36] to use the Mask-RCNN [27] and UperNet [65] as the detection and segmentation network respectively.

Object Detection and Instance Segmentation. The results on MSCOCO are reported in Table 2. VMamba maintains superiority in both box and mask Average Precision (AP b and AP m ), irrespective of the training schedule employed. With the 12 -epoch fine-tuning schedule, VMamba-T/S/B achieves object detection mAPs of 47 . 3% / 48 . 7% / 49 . 2% , surpassing Swin-T/S/B by 4 . 6% / 3 . 9% / 2 . 3%

Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [64], following the protocol proposed in [37]. All images are of size 224 × 224 .

| Model             | (M)               | Params FLOPs (G)   | TP. (img/s)       | Top-1 (%)         |
|-------------------|-------------------|--------------------|-------------------|-------------------|
| Transformer-Based | Transformer-Based | Transformer-Based  | Transformer-Based | Transformer-Based |
| DeiT-S [58]       | 22M               | 4.6G               | 1761              | 79.8              |
| DeiT-B [58]       | 86M               | 17.5G              | 503               | 81.8              |
| HiViT-T [68]      | 19M               | 4.6G               | 1393              | 82.1              |
| HiViT-S [68]      | 38M               | 9.1G               | 712               | 83.5              |
| HiViT-B [68]      | 66M               | 15.9G              | 456               | 83.8              |
| Swin-T [37]       | 28M               | 4.5G               | 1244              | 81.3              |
| Swin-S [37]       | 50M               | 8.7G               | 718               | 83.0              |
| Swin-B [37]       | 88M               | 15.4G              | 458               | 83.5              |
| XCiT-S24 [1]      | 48M               | 9.2G               | 671               | 82.6              |
| XCiT-M24 [1]      | 84M               | 16.2G              | 423               | 82.7              |

Table 2: Left: Results of object detection and instance segmentation on MSCOCO. AP b and AP m denote box AP and mask AP, respectively. FLOPs are calculated with input size 1280 × 800 . The notation ' 1 × ' refers to models fine-tuned for 12 epochs, and ' 3 × MS' denotes multi-scale training for 36 epochs. Right: Results of semantic segmentation on ADE20K. FLOPs are calculated with an input size of 512 × 2048 . 'SS' and 'MS' denote single-scale and multi-scale testing, respectively.

| Model            | Params FLOPs (M)   | (G)           | TP. (img/s)   | Top-1 (%)     |
|------------------|--------------------|---------------|---------------|---------------|
| ConvNet-Based    | ConvNet-Based      | ConvNet-Based | ConvNet-Based | ConvNet-Based |
| ConvNeXt-T [38]  | 29M                | 4.5G          | 1198          | 82.1          |
| ConvNeXt-S [38]  | 50M                | 8.7G          | 684           | 83.1          |
| ConvNeXt-B [38]  |                    | 89M 15.4G     | 436           | 83.8          |
| SSM-Based        | SSM-Based          | SSM-Based     | SSM-Based     | SSM-Based     |
| S4ND-Conv-T [45] | 30M                | -             | 683           | 82.2          |
| S4ND-ViT-B [45]  | 89M                | -             | 397           | 80.4          |
| Vim-S [71]       | 26M                | -             | 811           | 80.5          |
| VMamba-T         | 30M                | 4.9G          | 1686          | 82.6          |
| VMamba-S         | 50M                | 8.7G          | 877           | 83.6          |
| VMamba-B         |                    | 89M 15.4G     | 646           | 83.9          |

| Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    |
|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|
| Backbone                   | AP b                       | AP m                       | Params                     | FLOPs                      |
| Swin-T                     | 42.7                       | 39.3                       | 48M                        | 267G                       |
| ConvNeXt-T                 | 44.2                       | 40.1                       | 48M                        | 262G                       |
| VMamba-T                   | 47.3                       | 42.7                       | 50M                        | 271G                       |
| Swin-S                     | 44.8                       | 40.9                       | 69M                        | 354G                       |
| ConvNeXt-S                 | 45.4                       | 41.8                       | 70M                        | 348G                       |
| VMamba-S                   | 48.7                       | 43.7                       | 70M                        | 349G                       |
| Swin-B                     | 46.9                       | 42.3                       | 107M                       | 496G                       |
| ConvNeXt-B                 | 47.0                       | 42.7                       | 108M                       | 486G                       |
| VMamba-B                   | 49.2                       | 44.1                       | 108M                       | 485G                       |
| Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule |
| Swin-T                     | 46.0                       | 41.6                       | 48M                        | 267G                       |
| ConvNeXt-T                 | 46.2                       | 41.7                       | 48M                        | 262G                       |
| NAT-T                      | 47.7                       | 42.6                       | 48M                        | 258G                       |
| VMamba-T                   | 48.8                       | 43.7                       | 50M                        | 271G                       |
| Swin-S                     | 48.2                       | 43.2                       | 69M                        | 354G                       |
| ConvNeXt-S                 | 47.9                       | 42.9                       | 70M                        | 348G                       |
| NAT-S                      | 48.4                       | 43.2                       | 70M                        | 330G                       |
| VMamba-S                   | 49.9                       | 44.2                       | 70M                        | 349G                       |

| ADE20K with crop size 512   | ADE20K with crop size 512   | ADE20K with crop size 512   | ADE20K with crop size 512   | ADE20K with crop size 512   |
|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|
| Backbone                    | mIOU (SS)                   | mIOU (MS)                   | Params                      | FLOPs                       |
| ResNet-50                   | 42.1                        | 42.8                        | 67M                         | 953G                        |
| DeiT-S + MLN                | 43.8                        | 45.1                        | 58M                         | 1217G                       |
| Swin-T                      | 44.5                        | 45.8                        | 60M                         | 945G                        |
| ConvNeXt-T                  | 46.0                        | 46.7                        | 60M                         | 939G                        |
| NAT-T                       | 47.1                        | 48.4                        | 58M                         | 934G                        |
| Vim-S                       | 44.9                        | -                           | 46M                         | -                           |
| VMamba-T                    | 47.9                        | 48.8                        | 62M                         | 949G                        |
| ResNet-101                  | 43.8                        | 44.9                        | 86M                         | 1030G                       |
| DeiT-B + MLN                | 45.5                        | 47.2                        | 144M                        | 2007G                       |
| Swin-S                      | 47.6                        | 49.5                        | 81M                         | 1039G                       |
| ConvNeXt-S                  | 48.7                        | 49.6                        | 82M                         | 1027G                       |
| NAT-S                       | 48.0                        | 49.5                        | 82M                         | 1010G                       |
| VMamba-S                    | 50.6                        | 51.2                        | 82M                         | 1028G                       |
| Swin-B                      | 48.1                        | 49.7                        | 121M                        | 1188G                       |
| ConvNeXt-B                  | 49.1                        | 49.9                        | 122M                        | 1170G                       |
| NAT-B                       | 48.5                        | 49.7                        | 123M                        | 1137G                       |
| RepLKNet-31B                | 49.9                        | 50.6                        | 112M                        | 1170G                       |
| VMamba-B                    | 51.0                        | 51.6                        | 122M                        | 1170G                       |

mAP, and ConvNeXt-T/S/B by 3 . 1% / 3 . 3% / 2 . 2% mAP, respectively. With the same configuration, the instance segmentation mAPs achieved by VMamba-T/S/B outperforms Swin-T/S/B by 3 . 4% / 2 . 8% / 1.8 % mAP, and ConvNeXt-T/S/B by 2 . 6% / 1 . 9% / 1.4 % mAP, respectively. Moreover, VMamba's advantages persist under the 36-epoch fine-tuning schedule with multi-scale training, showcasing its potential to achieve promising performance in downstream tasks with dense prediction.

Semantic Segmentation. Consistent with previous experiments, VMamba demonstrates superior performance in semantic segmentation on ADE20K with a comparable amount of parameters. As shown in Table 2, VMamba-T is 3 . 4% mIoU higher than Swin-T and 1 . 9% higher than ConvNeXt-T in the Single-Scale (SS) setting, and the advantage persists with Multi-Scale (MS) input. For models at the Small and Base level, VMamba-S/B outperforms NAT-S/B [26] by 2 . 6% / 2 . 5% mIoU under the SS setting, and 1 . 7% / 1 . 9% mIoU under the MS setting.

<!-- image -->

<!-- image -->

<!-- image -->

Figure 4: Illustration of the adaptability of VMamba to (a) downstream tasks, and (b) input image with progressively increased resolutions. Swin-T ∗ denotes Swin-T tested with scaled window sizes.

<!-- image -->

Figure 5: Illustration of the activation map for query patches indicated by red stars. Visualization results in (b) and (c) are obtained by combining the activation map of each scanning path in SS2D.

<!-- image -->

Discussion Experiment results in this subsection demonstrate VMamba's adaptability to object detection, instance segmentation, and semantic segmentation. In Figure 4 (a), we compare VMamba's performance with Swin and ConvNeXt, highlighting its advantages in addressing downstream tasks with comparable classification accuracy on ImageNet-1K. VMamba also exhibits greater tolerance to changes in input resolution, with linear growth in FLOPs (see Figure 4 (b)) and modest accuracy drop (further discussed in Appendix G), making it more effective and efficient compared to ViT-based methods when adapting to downstream tasks with inputs of larger spatial resolutions. This aligns with Mamba's advanced capability in efficient long sequence modeling [17].

## 5.3 Analysis and Discussion

Relationship between SS2D and Self-Attention. To formulate the response Y within the time interval [ a, b ] of length T , we denote the corresponding SSM-related variables u i ⊙ ∆ i ∈ R 1 × D v , B i ∈ R 1 × D k , and C i ∈ R 1 × D k as V ∈ R T × D v , K ∈ R T × D k , and Q ∈ R T × D k , respectively. Therefore, the j -th dimension of Y , i.e. , Y ( j ) ∈ R T × 1 , can be expressed as

Y ( j ) = [ Q ⊙ w ( j ) ] h a ( j ) + [ ( Q ⊙ w ( j ) ) ( K w ( j ) ) ⊤ ⊙ M ] V ( j ) , (4)

where h a ∈ R D k is the hidden state at step a , M denotes the temporal mask matrix of size T × T with the lower triangular part set to 1 and elsewhere 0, and ⊙ denotes element-wise product. Detailed derivations are deferred to Appendix C.

In Eq. 4, the matrix multiplication process involving Q , K , and V closely resembles the mechanism of self-attention, despite the inclusion of w . Concretely, the formulation of each element in w := [ w 1 ; . . . ; w T ] ∈ R T × D k × D v , i.e. , w i ∈ R D k × D v , can be written as w i = ∏ i j =1 e A∆ ⊤ a -1+ j , representing the cumulative attention weight at step i computed along the scanning path.

Visualization of Activation Maps. To gain an intuitive and in-depth understanding of SS2D, we further visualize the attention values in QK ⊤ and ( Q ⊙ w ) ( K / w ) ⊤ corresponding to a specific

Figure 6: Comparison of Effective Receptive Fields (ERF) [41] between VMamba and other benchmark models. Pixels with higher intensity indicate larger responses regarding the central pixel.

<!-- image -->

query patch within foreground objects (referred to as the 'activation map'). As shown in Figure 5 (b), the activation map of QK ⊤ demonstrates the effectiveness of SS2D in capturing and retaining the traversed information, with all previously scanned tokens in the foreground region being activated. Furthermore, the inclusion of w leads to activation maps that are more focused on the neighborhood of query patches (Figure 5 (c)), which is consistent with the temporal weighting effect inherent to the formulation of w . Nevertheless, the selective scan mechanism enables VMamba to accumulate the history along the scanning path, thereby facilitating the establishment of long-term dependencies across image patches. This is evident in the sub-figure encircled with a red box (Figure 5 (d)), where patches of the sheep far to the left (scanned in early steps) remain activated. For more visualizations and further discussion, please refer to Appendix D.

Visualization of Effective Receptive Fields. The Effective Receptive Field (ERF) [41, 11] refers to the region in the input space that contributes to the activation of a certain output unit. We conduct a comparative analysis of the central pixel's ERF across various visual backbones, both before and after training. The results presented in Figure 6 illustrate that among the models examined, only DeiT, HiViT, and VMamba, demonstrate global ERFs, whereas the others exhibit local ERFs, despite their theoretical potential for global coverage. Moreover, the linear time complexity of VMamba makes it more computationally efficient compared to DeiT and HiViT, which have quadratic costs w.r.t. the number of input patches.

Diagnostic Study on Selective Scan Patterns. We compare the proposed scanning pattern ( i.e. Cross-Scan) to three benchmark patterns, including unidirectional scanning (Unidi-Scan), bidirectional scanning (Bidi-Scan), and cascade scanning (Cascade-Scan, scanning the data rowwise and column-wise successively). Feature dimensions are adjusted to maintain similar architectural parameters and FLOPs for fair comparison. As illustrated in Figure 7, Cross-Scan outperforms other scanning patterns in both computational efficiency and classification accuracy, highlighting its effectiveness in achieving 2D-Selective-Scan. Removing the DWConv layer, which has been observed to potentially aid the model in learning 2D spatial information, further enhances this advantage. This emphasizes the inherent strength of Cross-Scan in capturing 2D contextual information through its adoption of four-way scanning.

Figure 7: Performance comparison of different scanning patterns.

<!-- image -->

## 6 Conclusion

This paper presents VMamba, an efficient vision backbone network built with State Space Models (SSMs). VMamba integrates the benefits of selective SSMs from NLP tasks into visual data processing, bridging the gap between ordered 1D scanning and non-sequential 2D traversal through the novel SS2D module. Moreover, we have substantially improved the inference speed of VMamba through a series of architectural and implementation refinements. The effectiveness of the VMamba family has

been demonstrated through extensive experiments, and the linear time complexity of VMamba makes it advantageous for downstream tasks with large-resolution inputs.

Limitations. While VMamba demonstrates promising experimental results, there is still room for this study to be further improved. Previous research has validated the efficacy of unsupervised pre-training on large-scale datasets ( e.g. , ImageNet-21K). However, the compatibility of existing pretraining methods with SSM-based architectures like VMamba, and the identification of pre-training techniques specifically tailored to such models, remain unexplored. Investigating these aspects could serve as a promising avenue for future research in architectural design. Moreover, limited computational resources have prevented us from exploring VMamba's architecture at the Large scale, as well as conducting a fine-grained search of hyperparameters to further improve experimental performance.

## References

| [1] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image trans- formers. NeurIPS , 34:20014-20027, 2021.                                                                                                                                                                      |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [2] Guy E Blelloch. Prefix sums and their applications. 1990.                                                                                                                                                                                                                                                                                                                                                              |
| [3] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 , 2019. |
| [4] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation , 2020.                                                                                                                                                                                                                                                               |
| [5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV , pages 764-773, 2017.                                                                                                                                                                                                                                                                    |
| [6] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. NeurIPS , 34:3965-3977, 2021.                                                                                                                                                                                                                                                                    |
| [7] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In ICLR , 2023.                                                                                                                                                                                                                                                                                                             |
| [8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory- efficient exact attention with io-awareness. NeurIPS , 35:16344-16359, 2022.                                                                                                                                                                                                                                          |
| [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR , pages 248-255, 2009.                                                                                                                                                                                                                                                             |
| [10] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual attention vision transformers. In ECCV , pages 74-92, 2022.                                                                                                                                                                                                                                                                    |
| [11] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In CVPR , pages 11963-11975, 2022.                                                                                                                                                                                                                                             |
| [12] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR , pages 12124-12134, 2022.                                                                                                                                                                                       |
| [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR , 2021.                                                                                                          |
| [14] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks , 107:3-11, 2018.                                                                                                                                                                                                                                     |
| [15] Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, and Ran He. Rmt: Retentive networks meet vision transformers. In CVPR , 2024.                                                                                                                                                                                                                                                                                    |
| [16] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In ICLR , 2022.                                                                                                                                                                                                                                     |

| [17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 , 2023.                                                                                                                           |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [18] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. NeurIPS , 33:1474-1487, 2020.                                                                                              |
| [19] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and initialization of diagonal state space models. NeurIPS , 35:35971-35983, 2022.                                                                                         |
| [20] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In ICLR , 2021.                                                                                                                               |
| [21] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. NeurIPS , 34:572-585, 2021.                                      |
| [22] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. NeurIPS , 35:22982-22994, 2022.                                                                                                            |
| [23] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. Demystifying local vision transformer: Sparse connectivity, weight sharing, and dynamic weight. arXiv preprint arXiv:2106.04263 , 2021.                               |
| [24] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. On the connection between local attention and dynamic depth-wise convolution. In ICLR , 2021.                                                                         |
| [25] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In ICLR , 2022.                                                                                                    |
| [26] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In CVPR , pages 6185-6194, 2023.                                                                                                                    |
| [27] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In ICCV , pages 2961-s2969, 2017.                                                                                                                                               |
| [28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR , pages 770-778, 2016.                                                                                                                        |
| [29] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.    |
| [30] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR , pages 4700-4708, 2017.                                                                                                         |
| [31] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML , pages 5156-5165, 2020.                                                                 |
| [32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS , pages 1106-1114, 2012.                                                                                                 |
| [33] Rudolf Emil Kálmán. A new approach to linear filtering and prediction problems. Journal of Basic Engineering , 82(1):35-45, 1960.                                                                                                                          |
| [34] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV , pages 740-755, 2014.                                                  |
| [35] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Kärkkäinen, Mykola Pechenizkiy, Decebal Constantin Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. In ICLR , 2023. |
| [36] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR , pages 12009-12019, 2022.                                              |
| [37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , pages 10012-10022, 2021.                                                    |
| [38] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A                                                                                                                                                           |

convnet for the 2020s. In

CVPR

, pages 11976-11986, 2022.

| [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.                                                                                                                                                                                      |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [40] Jiasen Lu, Roozbeh Mottaghi, Aniruddha Kembhavi, et al. Container: Context aggregation networks. NeurIPS , 34:19160-19171, 2021.                                                                                                                                                                      |
| [41] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. NeurIPS , 29:4898-4906, 2016.                                                                                                                             |
| [42] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In ICLR , 2022.                                                                                                                      |
| [43] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In ICLR , 2018.                                                                                                                                                                                         |
| [44] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In ICLR , 2023.                                                                                                                                                                  |
| [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christo- pher Ré. S4nd: Modeling images and videos as multidimensional signals with state spaces. NeurIPS , 35:2846-2861, 2022.                                                                            |
| [46] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. RWKV: reinventing rnns for the transformer era. In EMNLP , pages 14048-14077, 2023.                                                             |
| [47] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network design spaces. In CVPR , pages 10428-10436, 2020.                                                                                                                                             |
| [48] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. NeurIPS , 35:10353-10366, 2022.                                                                                                  |
| [49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni- tion. 2015.                                                                                                                                                                                      |
| [50] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In ICLR , 2022.                                                                                                                                                                          |
| [51] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In ICLR , 2022.                                                                                                                                                                          |
| [52] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621 , 2023.                                                                                  |
| [53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR , pages                                                                                                    |
| 1-9, 2015. [54] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks.                                                                                                                                                                                     |
| [55] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys , 55(6), 2022.                                                                                                                                                                      |
| [56] Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang, Qi Tian, and Qixiang Ye. Integrally pre-trained transformer pyramid networks. In CVPR , pages 18610- 18620, 2023.                                                                                      |
| [57] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems , 34:24261-24272, 2021. |
| [58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In ICML , pages 10347-10357, 2021.                                                                                |

Kaiser, and Illia Polosukhin. Attention is all you need.

, 30:5998-6008, 2017.

| [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS , 30:5998-6008, 2017.                                                 |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [61] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In CVPR , pages 6387-6397, 2023.                                         |
| [62] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.                                                                             |
| [63] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV , pages 568-578, 2021. |
| [64] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models , 2019.                                                                                                                                        |
| [65] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV , pages 418-434, 2018.                                                                                  |
| [66] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641 , 2021.                       |
| [67] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635 , 2023.                                                   |
| [68] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler and more efficient design of hierarchical vision transformer. In ICLR , 2023.                                                   |
| [69] Weixi Zhao, Weiqiang Wang, and Yunjie Tian. Graformer: Graph-oriented transformer for 3d pose estimation. In CVPR , pages 20438-20447, 2022.                                                                                          |
| [70] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR , pages 5122-5130, 2017.                                                                        |
| [71] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. In ICML , 2024.                                     |
| [72] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In CVPR , pages 9308-9316, 2019.                                                                                            |

## A Discretization of State Space Models (SSMs)

In this section, we explore the correlation between the discretized formulation of State Space Models (SSMs) obtained in Sec. 3 and those derived from the zero-order hold (ZOH) method [17], which is frequently used in studies related to SSM.

Recall the discretized formulation of SSMs derived in Sec. 3 as follows,

h b = e A (∆ a + ... +∆ b -1 ) ( h a + b -1 ∑ i = a B i u i e -A (∆ a + ... +∆ i ) ∆ i ) . (5)

Let b = a +1 , then the above equation can be re-written as

h a +1 = e A ∆ a h a + B a ∆ a u a , (6)

where A a := e A ∆ a is exactly the discretized form of the evolution matrix A obtained by ZOH, while B a := B a ∆ a is basically the first-order Taylor expansion of the discretized B acquired through ZOH.

## B Derivation of the Recurrence Relation of Selective SSMs

In this section, we derive the recurrence relation of the hidden state in selective SSMs. Given the expression of h b shown in Eq. 5, let us denote e A (∆ a + ... +∆ i -1 ) as p i A , a , then its recurrence relation can be directly written as

p i A , a = e A ∆ i -1 p i -1 A , a . (7)

For the second term of Eq. 5, we have

p b B , a = e A (∆ a + ... +∆ b -1 ) b -1 ∑ i = a B i u i e -A (∆ a + ... +∆ i ) ∆ i (8)

= e A ∆ b -1 p b -1 B , a + B b -1 u b -1 ∆ b -1 . (9)

Therefore, with the associations derived in Eq. 7 and Eq. 8, h b = p b A , a h a + p b B , a can be efficiently computed in parallel using associative scan algorithms [2, 43, 51], which are supported by numerous modern programming libraries. This approach effectively reduces the overall computational complexity to linear, and VMamba further accelerates the computation by adopting a hardware-aware implementation [17].

## C Details of the relationship between SS2D and Self-attention

In this section, we clarify the relationship between SS2D and the self-attention mechanism commonly employed in existing vision backbone models. Subsequently, visualization results are provided to substantiate our explanation.

Let T denote the length of the sequence with indices from a to b , we define the following variables

V := [ V 1 ; . . . ; V T ] ∈ R T × D v , where V i := u a + i -1 ⊙ ∆ a + i -1 ∈ R 1 × D v (10)

K := [ K 1 ; . . . ; K T ] ∈ R T × D k , where K i := B a + i -1 ∈ R 1 × D k (11)

Q := [ Q 1 ; . . . ; Q T ] ∈ R T × D k , where Q i := C a + i -1 ∈ R 1 × D k (12)

w := [ w 1 ; . . . ; w T ] ∈ R T × D k × D v , where w i := i ∏ j =1 e A∆ ⊤ a -1+ j ∈ R D k × D v (13)

H := [ h a ; . . . ; h b ] ∈ R T × D k × D v , where h i ∈ R D k × D v (14)

Y := [ y a ; . . . ; y b ] ∈ R T × D v , where y i ∈ R D v (15)

Note that in practice, the parameter A in Eq. 1 is simplified to R 1 × D k . Consequently, h ' ( t ) = Ah ( t ) + B u ( t ) is simplified to h ' ( t ) = A ⊙ h ( t ) + B u ( t ) , which is the reason why w i ∈ R D k × D v .

Based on these notations, the discretized solution of time-varying SSMs (Eq. 5) can be written as

h b = w T ⊙ h a + T ∑ i =1 w T w i ⊙ ( K i ⊤ V i ) , (16)

where ⊙ denotes the element-wise product between matrices, and the division is also elements-wise.

Based on the expression of the hidden state h b , the first term of the output of SSM, i.e. , y b , can be computed by

y b = Q T h b (17)

= Q T ( w T ⊙ h a ) + Q T T ∑ i =1 w T w i ⊙ ( K i ⊤ V i ) . (18)

Here we drop the skip connection between the input and the response for simplicity. Particularly, the j -th slice along dimension D v of y b , denoted as y b ( j ) ∈ R can be written as

y b ( j ) = ( Q T ⊙ w T ( j ) ) h a ( j ) + T ∑ i =1 ( Q T ⊙ w T ( j ) w i ( j ) K i ⊤ ) ⊙ V i ( j ) . (19)

Similarly, the j -th slice along dimension D v of the overall response Y , denoted as Y ( j ) ∈ R T × 1 , can be expressed as

Y ( j ) = ( Q ⊙ w ( j ) ) h a ( j ) + [ ( Q ⊙ w ( j ) ) ( K w ( j ) ) ⊤ ⊙ M ] V ( j ) , (20)

where M := tril ( T, T ) ∈ { 0 , 1 } T × T denotes the temporal mask matrix with the lower triangular portion of a T × T matrix set to 1 and elsewhere 0. It is evident that how matrices Q , K , and V are multiplied in Eq. 20 closely resembles the process in the self-attention module of Vision Transformers. Moreover, if w is in shape ( T, D k ) rather than ( T, D k , D v ) , then Eq. 17 and Eq. 20 reduces to the form of Gated Linear Attention (GLA) [67], which indicates that GLA is also a special case of Mamba.

## D Visualization of Attention and Activation Maps

In the preceding subsection, we illustrated how the computational process of selective SSMs shares similarities with self-attention mechanisms, allowing us to delve into the internal mechanism of SS2D through the visualization of its weight matrices.

Given the input image as in Figure 8 (a), illustrations of four scanning paths in SS2D are shown in Figure 8 (d), the visualization of corresponding attention maps by calculating QK ⊤ and ( Q ⊙ w ) ( K / w ) ⊤ are shown in Figure 8 (e) and Figure 8 (g) respectively. These results underscore the effectiveness of the proposed scanning approach ( i.e. , Cross-Scan) in capturing and retaining the traversed information, as each row in a single attention map corresponds to the attention between the current patch and all previously scanned foreground tokens impartially. Additionally, in Figure 8 (f), we showcase the transformed activation maps, where the pixel order corresponds to that of the first route, traversing the image row-wise from the upper-left to the bottom-right.

By rearranging the diagonal elements of the obtained attention map in the image space, we derive the visualization results illustrated in Figure 8 (b) and Figure 8 (c) corresponding to QK ⊤ and ( Q ⊙ w ) ( K / w ) ⊤ respectively. These maps illustrate the effectiveness of VMamba in accurately distinguishing between foreground and background pixels within an image.

Moreover, given a certain selected patch as the query, we visualize the corresponding 'activation map' by reshaping the associated row in the attention map (computed by QK ⊤ or ( Q ⊙ w ) ( K / w ) ⊤ ) This reflects the attention score between the query patch and all previously scanned patches. To get the complete visualization for a query patch, we collect and combine the activation maps from all four scanning paths in SS2D. Visualization results of the activation map for both QK ⊤ and ( Q ⊙ w ) ( K / w ) ⊤ are shown in Figure 9. Moreover, we also visualize the diagonal elements of attention maps computed by ( Q ⊙ w ) ( K / w ) ⊤ , where all foreground objects are effectively highlighted and separated from the background.

<!-- image -->

𝒘

Figure 8: Illustration of the attention maps obtained by SS2D.Figure 9: Illustration of activation maps for the query patch (marked with a red star).

<!-- image -->

Figure 10: Illustration of ERF maps throughout the training process of Vanilla-VMamba-T (with EMA).

<!-- image -->

## E Detailed Experiment Settings

Network Architecture. The architectural specifications of Vanilla-VMamba are outlined in Table 3, and detailed configurations of the VMamba series are provided in Table 4. The Vanilla-VMamba series are built with the vanilla VSS Block, which includes a multiplicative branch with no feedforward network (FFN) layer. In contrast, the VSS Block in the VMamba series eliminates the multiplicative branch and introduces FFN layers. Additionally, we provide alternative architectures for VMamba at Small and Base scales, referred to as VMamba-S[ s 1 l 20 ] and VMamba-B[ s 1 l 20 ], respectively. The notation ' sxly ' indicates that the ssm-ratio is set to x and the number of layers in stage 3 is set to y . Consequently, the versions presented in Table 1 can also be referred to as VMamba-S[ s 2 l 15 ] and VMamba-B[ s 2 l 15 ].

Experiment Setting. The hyper-parameters for training VMamba on ImageNet are inherited from Swin [37], except for the parameters related to drop\_path\_rate and the exponential moving average (EMA) technique. Specifically, VMamba-T/S/B models are trained from scratch for 300 epochs, with a 20-epoch warm-up period, using a batch size of 1024. The training process utilizes the AdamW optimizer [39] with betas set to (0 . 9 , 0 . 999) , a momentum of 0 . 9 , an initial learning rate of 1 × 10 -3 , a weight decay of 0 . 05 , and a cosine decay learning rate scheduler. Additional techniques such as label smoothing (0.1) and EMA (decay ratio of 0.9999) are also applied. The drop\_path\_ratio is set to 0.2 for Vanilla-VMamba-T, VMamba-T, 0.3 for Vanilla-VMamba-S, VMamba-S[ s 2 l 15 ], VMamba-S[ s 1 l 20 ], 0.6 for Vanilla-VMamba-B, VMamba-B[ s 2 l 15 ] and 0.5 for VMamba-B[ s 1 l 20 ]. No other training techniques are employed.

Throughput Evaluation. Detailed performance comparisons with various models are presented in Table 6. Throughput (referred to as TP. ) was assessed on an A100 GPU paired with an AMD EPYC 7542 CPU, utilizing the toolkit provided by [64]. Following the protocol outlined in [37], we set the batch size to 128. The training throughput (referred to as Train TP. ) is tested on the same device with mix-resolution , excluding the time consumption of optimizers. The batch size for measuring the training throughput is set to 128 as well.

Accelerating VMamba. Table 5 provides detailed configurations of intermediate variants in the acceleration process from Vanilla-VMamba-T to VMamba-T.

Evolution of ERF. We further generate the effective receptive field (ERF) maps throughout the training process for Vanilla-VMamba-T. These maps intuitively illustrate how VMamba's pattern of ERF evolves from being predominantly local to predominantly global, epoch by epoch.

## F Performance of the VMamba Family on Downstream Tasks

In this section, we present the experimental results of Vanilla-VMamba and VMamba on the MSCOCO and ADE20k datasets. The results are summarized in Table 7 and Table 8, respectively.

For object detection and instance segmentation, we adhere to the protocol outlined by Swin [37] and construct our models using the mmdetection framework [3]. Specifically, we utilize the AdamW optimizer [39] and fine-tune the classification models pre-trained on ImageNet-1K for both 12 and 36 epochs. The learning rate is initialized with 1 × 10 -4 and is decreased by a factor of 10 at the 9th and 11th epoch. We incorporate multi-scale training and random flipping with a batch size of 16, in line with established practices for object detection evaluations.

Table 3: Architectural overview of the Vanilla-VMamba series. Down-sampling is executed through patch merging [37] operations in stages 1, 2, and 3. The term Linear refers to a linear layer. The DWConv denotes a depth-wise convolution [24] operation. The proposed 2D-selective-scan is labeled as SS2D .

| layer name   | output size   | Vanilla-VMamba-T                                                                                                                                                        | Vanilla-VMamba-S                                                                                                                                                                      | Vanilla-VMamba-B                                                                                                                                                    | Vanilla-VMamba-B                 |
|--------------|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|
| stage 1      | 56 × 56       | vanilla VSSBLock        Linear 96 → 2 × 96 DWConv 3 × 3, 2 × 96 SS2D , dim 2 × 96 Linear 2 × 96 → 96 Multiplicative Linear 2 × 96 → 96        × 2         | vanilla VSSBLock        Linear 96 → 2 × 96 DWConv 3 × 3, 2 × 96 SS2D , dim 2 × 96 Linear 2 × 96 → 96 Multiplicative Linear 2 × 96 → 96        ×                         | vanilla VSSBLock        Linear 128 → 2 × 128 DWConv 3 × 3, 2 × 128 SS2D , dim 2 × 128 Linear 2 × 128 → 128 Multiplicative Linear 2 × 128 → 128        | × 2                              |
|              |               | patch merging → 192                                                                                                                                                     | patch merging → 192                                                                                                                                                                   | patch merging → 256                                                                                                                                                 | patch merging → 256              |
| stage 2      | 28 × 28       | vanilla VSSBLock        Linear 192 → 2 × 192 DWConv 3 × 3, 2 × 192 SS2D , dim 2 × 192 Linear 2 × 192 → 192 Multiplicative Linear 2 × 192 → 192                   | vanilla VSSBLock        × 2        Linear 192 → 2 × 192 DWConv 3 × 3, 2 × 192 SS2D , dim 2 × 192 Linear 2 × 192 → 192 Multiplicative Linear 2 × 192 → 192        | vanilla VSSBLock        Linear 256 → 2 × 256 DWConv 3 × 3, 2 × 256 SS2D , dim 2 × 256 Linear 2 × 256 → 256 Multiplicative Linear 2 × 256 → 256        | × 2                              |
|              |               | patch merging → 384                                                                                                                                                     | patch merging → 384 vanilla VSSBLock                                                                                                                                                  | patch merging → 512 vanilla VSSBLock                                                                                                                                |                                  |
| stage 3      | 14 × 14       | vanilla VSSBLock        Linear 384 → 2 × 384 DWConv 3 × 3, 2 × 384 SS2D , dim 2 × 384 Linear 2 × 384 → 384 Multiplicative Linear 2 × 384 → 384        × 9 |        Linear 384 → 2 × 384 DWConv 3 × 3, 2 × 384 SS2D , dim 2 × 384 Linear 2 × 384 → 384 Multiplicative Linear 2 × 384 → 384        ×                                  |        Linear 512 → 2 × 512 DWConv 3 × 3, 2 × 512 SS2D , dim 2 × 512 Linear 2 × 512 → 512 Multiplicative Linear 2 × 512 → 512                         | × 27                             |
| Param. (M)   | 1 × 1         | average pool, 1000-d fc, softmax                                                                                                                                        | average pool, 1000-d fc, softmax                                                                                                                                                      | 76.3                                                                                                                                                                | average pool, 1000-d fc, softmax |
|              |               | 22.9 44.4                                                                                                                                                               | 22.9 44.4                                                                                                                                                                             |                                                                                                                                                                     |                                  |
| FLOPs        | FLOPs         | × 10 9                                                                                                                                                                  | × 10 9                                                                                                                                                                                |                                                                                                                                                                     |                                  |

For semantic segmentation, we follow Swin [37] and construct a UperHead [65] network on top of the pre-trained model using the MMSegmentation library [4]. We employ the AdamW optimizer [39] and set the learning rate to 6 × 10 -5 . The fine-tuning process spans a total of 160 k iterations with a batch size of 16. The default input resolution is 512 × 512 .

## G Details of VMamba's Scale-Up Experiments

Given Mamba's exceptional ability in efficient long sequence modeling, we conduct experiments to assess whether VMamba inherits this characteristic. We evaluate the computational efficiency and classification accuracy of VMamba with progressively larger input spatial resolutions. Specifically, following the protocol in XCiT [1], we apply VMamba trained on 224 × 224 input to images with resolutions ranging from 288 × 288 to 768 × 768 . We measure the generalization performance in terms of the number of parameters, FLOPs, throughput during both training and inference, and the top-1 classification accuracy on ImageNet-1K. We also conduct experiments under the ' linear tuning ' setting, where only the header network, consisting of a single linear module, is fine-tuned from random initialization using features extracted by those backbone models.

Table 4: Architectural overview of the VMamba series.Table 5: Details of accelerating VMamba.

| layer name   | output size                            | VMamba-T                                                                                                                                                                                                                 | VMamba-S                                                                                                                                                                                                                                                                                                                                           | VMamba-B                                                                                                                                                                                             |
|--------------|----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| stem         | 112 × 112                              | conv 3 × 3 stride 2, LayerNorm, GeLU, conv 3 × 3 stride 2, LayerNorm                                                                                                                                                     | conv 3 × 3 stride 2, LayerNorm, GeLU, conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                                                                                                                               | conv 3 × 3 stride 2, LayerNorm, GeLU, conv 3 × 3 stride 2, LayerNorm                                                                                                                                 |
| stage 1      | 56 × 56                                | VSSBLock( ssm-ratio =1, mlp-ratio =4)      Linear 96 → ssm-ratio × 96 DWConv 3 × 3, ssm-ratio × 96 SS2D , dim ssm-ratio × 96 Linear ssm-ratio × 96 → 96 FFN mlp-ratio × 96      × 2                            | VSSBlock( ssm-ratio =2, mlp-ratio =4)      Linear 96 → ssm-ratio × 96 DWConv 3 × 3, ssm-ratio × 96 SS2D , dim ssm-ratio × 96 Linear ssm-ratio × 96 → 96 FFN mlp-ratio × 96      × 2                                                                                                                                                      | VSSBLock( ssm-ratio =2, mlp-ratio =4)      Linear 128 → ssm-ratio × 128 DWConv 3 × 3, ssm-ratio × 128 SS2D , dim ssm-ratio × 128 Linear ssm-ratio × 128 → 128 FFN mlp-ratio × 128      × 2 |
|              | conv 3 × 3 stride 2, LayerNorm         | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                                           | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                                                                                                                                                                     | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                       |
| stage 2      | 28 × 28                                | VSSBLock( ssm-ratio =1, mlp-ratio =4) VSSBlock(      Linear 192 → ssm-ratio × 192 DWConv 3 × 3, ssm-ratio × 192 SS2D , dim ssm-ratio × 192 Linear ssm-ratio × 192 → 192 FFN mlp-ratio × 192      × 2      | ssm-ratio =2, mlp-ratio =4) Linear 192 → ssm-ratio × 192 DWConv 3 × 3, ssm-ratio × 192 SS2D , dim ssm-ratio × 192 Linear ssm-ratio × 192 → 192 FFN mlp-ratio × 192      × 2      Linear 256 → DWConv 3 × 3, SS2D , dim Linear ssm-ratio × FFN mlp-ratio                                                                                  | VSSBLock( ssm-ratio =2, mlp-ratio =4) ssm-ratio × 256 ssm-ratio × 256 ssm-ratio × 256 256 → 256 × 256      × 2                                                                                  |
|              |                                        | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                                           | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                                                                                                                                                                     | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                       |
| stage 3      | 14 × 14                                | VSSBLock( ssm-ratio =1, mlp-ratio =4) VSSBlock(      Linear 384 → ssm-ratio × 384 DWConv 3 × 3, ssm-ratio × 384 SS2D , dim ssm-ratio × 384 Linear ssm-ratio × 384 → 384 FFN mlp-ratio × 384      × 8      | ssm-ratio =2, mlp-ratio =4) VSSBLock( ssm-ratio =2, Linear 384 → ssm-ratio × 384 DWConv 3 × 3, ssm-ratio × 384 SS2D , dim ssm-ratio × 384 Linear ssm-ratio × 384 → 384 FFN mlp-ratio × 384      × 15      Linear 512 → ssm-ratio × DWConv 3 × 3, ssm-ratio × 512 SS2D , dim ssm-ratio × 512 Linear ssm-ratio × 512 → FFN mlp-ratio × 512 | mlp-ratio =4) 512 512      × 15                                                                                                                                                                 |
|              | conv 3 × 3 stride 2, LayerNorm         | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                                           | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                                                                                                                                                                     | conv 3 × 3 stride 2, LayerNorm                                                                                                                                                                       |
| Param. (M)   |                                        | 30.2                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                    | 88.6                                                                                                                                                                                                 |
|              | 1 × 1 average pool, 1000-d fc, softmax | 1 × 1 average pool, 1000-d fc, softmax                                                                                                                                                                                   | 1 × 1 average pool, 1000-d fc, softmax                                                                                                                                                                                                                                                                                                             | 1 × 1 average pool, 1000-d fc, softmax                                                                                                                                                               |
|              |                                        |                                                                                                                                                                                                                          | 50.1                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                      |
| FLOPs        | FLOPs                                  | 4.91 × 10 9                                                                                                                                                                                                              | 8.72 × 10 9                                                                                                                                                                                                                                                                                                                                        | 15.36 × 10 9                                                                                                                                                                                         |

| Model            |    |   d\_state ssm-ratio DWConv |    | multiculative branch   | layers numbers   | FFN   | (M)   | Params FLOPs (G)   |   TP. (img/s) |   Train TP. (img/s) |   Top-1 (%) |
|------------------|----|----------------------------|----|------------------------|------------------|-------|-------|--------------------|---------------|---------------------|-------------|
| Vanilla-VMamba-T | 16 |                          2 | ✓  | ✓                      | [2,2,9,2]        |       | 22.9M | 5.63G              |           426 |                 138 |       82.17 |
| Step(a)          | 16 |                          2 | ✓  | ✓                      | [2,2,9,2]        |       | 22.9M | 5.63G              |           467 |                 165 |       82.17 |
| Step(b)          | 16 |                          2 | ✓  | ✓                      | [2,2,9,2]        |       | 22.9M | 5.63G              |           464 |                 184 |       82.17 |
| Step(c)          | 16 |                          2 | ✓  | ✓                      | [2,2,9,2]        |       | 22.9M | 5.63G              |           638 |                 195 |       82.17 |
| Step(d)          | 16 |                          2 |    | ✓                      | [2,2,2,2]        | ✓     | 29.0M | 5.63G              |           813 |                 248 |       81.65 |
| Step(e)          | 16 |                          1 |    |                        | [2,2,5,2]        | ✓     | 26.2M | 4.86G              |          1179 |                 360 |       82.17 |
| Step(f)          |  1 |                          2 | ✓  |                        | [2,2,5,2]        | ✓     | 30.7M | 4.86G              |          1340 |                 464 |       82.49 |
| Step(g)          |  1 |                          1 | ✓  |                        | [2,2,8,2]        | ✓     | 30.2M | 4.91G              |          1686 |                 571 |       82.6  |

According to the results summarized in Table 9, VMamba demonstrates the most stable performance across ( i.e. , modest performance drop) different input image sizes, achieving a top-1 classification accuracy of 74 . 7% without fine-tuning ( 79 . 2% with linear tuning ), while maintaining a relatively high throughput of 149 images per second at an input resolution of 768 × 768 . In comparison, Swin [37] achieves the second-highest performance with a top-1 accuracy of 73 . 1% without finetuning ( 77 . 5% under linear tuning ) at the same input size, when using scaled window sizes (set as the resolution divided by 32 ). However, its throughput significantly drops to 53 images per second. Furthermore, ConvNeXt [38] maintains a relatively high inference speed ( i.e. , a throughput of 103 images per second) with the largest input resolution. However, its classification accuracy drops to 69 . 5% when directly tested on images of size 768 × 768 , indicating its limited adaptability w.r.t. input images with large spatial resolutions. The performance of Deit-S also drops dramatically, mainly due to the use of interpolation in the absolute positional embedding.

The performance variations across different resolutions for various models are more intuitively illustrated in Figure 11. Notably, VMamba displays a linear increase in computational complexity (Figure 4 (b)), as measured by FLOPs, which is comparable to CNN-based architectures. This finding aligns with the theoretical conclusions drawn from selective SSMs [17].

Table 6: Performance comparison on ImageNet-1K with image size 224. † indicates that Vim is trained only in float32 in practical, in which case the train throughput is 232. [71].

| Model                | Size   | Image Params FLOPs (M)   | (G)   |   TP. (img/s) | Train TP. (img/s)   |   Top-1 (%) |
|----------------------|--------|--------------------------|-------|---------------|---------------------|-------------|
| DeiT-S [58]          | 224 2  | 22M                      | 4.6G  |          1761 | 2404                |        79.8 |
| DeiT-B [58]          | 224 2  | 86M                      | 17.5G |           503 | 1032                |        81.8 |
| ConvNeXt-T [38]      | 224 2  | 29M                      | 4.5G  |          1198 | 702                 |        82.1 |
| ConvNeXt-S [38]      | 224 2  | 50M                      | 8.7G  |           684 | 445                 |        83.1 |
| ConvNeXt-B [38]      | 224 2  | 89M                      | 15.4G |           436 | 334                 |        83.8 |
| HiViT-T [68]         | 224 2  | 19M                      | 4.6G  |          1393 | 1304                |        82.1 |
| HiViT-S [68]         | 224 2  | 38M                      | 9.1G  |           712 | 698                 |        83.5 |
| HiViT-B [68]         | 224 2  | 66M                      | 15.8G |           456 | 544                 |        83.8 |
| Swin-T [37]          | 224 2  | 28M                      | 4.5G  |          1244 | 987                 |        81.3 |
| Swin-S [37]          | 224 2  | 50M                      | 8.7G  |           718 | 642                 |        83   |
| Swin-B [37]          | 224 2  | 88M                      | 15.5G |           458 | 496                 |        83.5 |
| XCiT-S12/16          | 224 2  | 26M                      | 4.9G  |          1283 | 935                 |        82   |
| XCiT-S24/16          | 224 2  | 48M                      | 9.2G  |           671 | 509                 |        82.6 |
| XCiT-M24/16          | 224 2  | 84M                      | 16.2G |           423 | 385                 |        82.7 |
| S4ND-ConvNeXt-T [45] | 224 2  | 30M                      | -     |           683 | 369                 |        82.2 |
| S4ND-ViT-B [45]      | 224 2  | 89M                      | -     |           398 | 400                 |        80.4 |
| Vim-S [71]           | 224 2  | 26M                      | -     |           811 | 344 †               |        80.5 |
| Vanilla-VMamba-T     | 224 2  | 23M                      | 5.6G  |           638 | 195                 |        82.2 |
| Vanilla-VMamba-S     | 224 2  | 44M                      | 11.2G |           359 | 111                 |        83.5 |
| Vanilla-VMamba-B     | 224 2  | 76M                      | 18.0G |           268 | 84                  |        83.7 |
| VMamba-T             | 224 2  | 30M                      | 4.9G  |          1686 | 571                 |        82.6 |
| VMamba-S[ s 2 l 15 ] | 224 2  | 50M                      | 8.7G  |           877 | 314                 |        83.6 |
| VMamba-B[ s 2 l 15 ] | 224 2  | 89M                      | 15.4G |           646 | 247                 |        83.9 |
| VMamba-S[ s 1 l 20 ] | 224 2  | 49M                      | 8.6G  |          1106 | 390                 |        83.3 |
| VMamba-B[ s 1 l 20 ] | 224 2  | 87M                      | 15.2G |           827 | 313                 |        83.8 |

Figure 11: Throughtput and Top-1 accuracy change on ImageNet-1K w.r.t resolution rises. Swin-T ∗ denotes Swin-T tested with scaled window sizes.

<!-- image -->

## H Ablation Study

## H.1 Influence of the Scanning Pattern

In the main submission, we validate the effectiveness of the proposed scanning pattern (referred to as Cross-Scan) in SS2D by comparing to three alternative image traversal approaches, i.e. , Unidi-Scan, Bidi-Scan, and Cascade-Scan (Figure 12). Notably, as Unidi-Scan, Bidi-Scan, and Cross-Scan are all implemented in Triton , they exhibit little difference in throughput. Nevertheless, the findings in Table 10 indicate that Cross-Scan exhibits superior data modeling capacity, evident from its higher classification accuracy. This advantage is likely attributed to the two-dimensional prior introduced by the four-way scanning design. Nevertheless, the practical implementation of Cascade-Scan is notably constrained by its relatively slow computational pace. This limitation primarily stems from the inadequate compatibility between selective scanning and high-dimensional data, exacerbated by the multi-step scanning procedure.

Table 7: Object detection and instance segmentation results on COCO dataset. The FLOPs are calculated using inputs of size 1280 × 800 . Here, AP b and AP m denote box AP and mask AP, respectively. " 1 × " indicates models fine-tuned for 12 epochs, while " 3 × MS" signifies the utilization of multi-scale training for 36 epochs.

| Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    | Mask R-CNN 1 × schedule    |
|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|
| Backbone                   | AP b                       | AP b 50                    | AP b 75                    | AP m                       | AP m 50                    | AP m 75                    | Params                     | FLOPs                      |
| Swin-T                     | 42.7                       | 65.2                       | 46.8                       | 39.3                       | 62.2                       | 42.2                       | 48M                        | 267G                       |
| ConvNeXt-T                 | 44.2                       | 66.6                       | 48.3                       | 40.1                       | 63.3                       | 42.8                       | 48M                        | 262G                       |
| Vanilla-VMamba-T           | 46.5                       | 68.5                       | 50.7                       | 42.1                       | 65.5                       | 45.3                       | 42M                        | 286G                       |
| VMamba-T                   | 47.3                       | 69.3                       | 52.0                       | 42.7                       | 66.4                       | 45.9                       | 50M                        | 271G                       |
| Swin-S                     | 44.8                       | 66.6                       | 48.9                       | 40.9                       | 63.4                       | 44.2                       | 69M                        | 354G                       |
| ConvNeXt-S                 | 45.4                       | 67.9                       | 50.0                       | 41.8                       | 65.2                       | 45.1                       | 70M                        | 348G                       |
| Vanilla-VMamba-S           | 48.2                       | 69.7                       | 52.5                       | 43.0                       | 66.6                       | 46.4                       | 64M                        | 400G                       |
| VMamba-S                   | 48.7                       | 70.0                       | 53.4                       | 43.7                       | 67.3                       | 47.0                       | 70M                        | 349G                       |
| Swin-B                     | 46.9                       | -                          | -                          | 42.3                       | -                          | -                          | 107M                       | 496G                       |
| ConvNeXt-B                 | 47.0                       | 69.4                       | 51.7                       | 42.7                       | 66.3                       | 46.0                       | 108M                       | 486G                       |
| Vanilla-VMamba-B           | 48.6                       | 70.0                       | 53.1                       | 43.3                       | 67.1                       | 46.7                       | 96M                        | 540G                       |
| VMamba-B                   | 49.2                       | 71.4                       | 54.0                       | 44.1                       | 68.3                       | 47.7                       | 108M                       | 485G                       |
| Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule | Mask R-CNN 3 × MS schedule |
| Swin-T                     | 46.0                       | 68.1                       | 50.3                       | 41.6                       | 65.1                       | 44.9                       | 48M                        | 267G                       |
| ConvNeXt-T                 | 46.2                       | 67.9                       | 50.8                       | 41.7                       | 65.0                       | 44.9                       | 48M                        | 262G                       |
| Vanilla-VMamba-T           | 48.5                       | 70.0                       | 52.7                       | 43.2                       | 66.9                       | 46.4                       | 42M                        | 286G                       |
| VMamba-T                   | 48.8                       | 70.4                       | 53.5                       | 43.7                       | 67.4                       | 47.0                       | 50M                        | 271G                       |
| Swin-S                     | 48.2                       | 69.8                       | 52.8                       | 43.2                       | 67.0                       | 46.1                       | 69M                        | 354G                       |
| ConvNeXt-S                 | 47.9                       | 70.0                       | 52.7                       | 42.9                       | 66.9                       | 46.2                       | 70M                        | 348G                       |
| Vanilla-VMamba-S           | 49.7                       | 70.4                       | 54.2                       | 44.0                       | 67.6                       | 47.3                       | 64M                        | 400G                       |
| VMamba-S                   | 49.9                       | 70.9                       | 54.7                       | 44.2                       | 68.2                       | 47.7                       | 70M                        | 349G                       |

Figure 12: Illustration of different scanning patterns for selective scan.

<!-- image -->

Figure 13 indirectly demonstrates that among the analyzed scanning methods, only Bidi-Scan, Cascade-Scan, and Cross-Scan showcase global ERFs. Moreover, only Cross-Scan and Cascade-Scan exhibit two-dimensional (2D) priors. It is also noteworthy that DWConv [24] plays a significant role in establishing 2D priors, thereby contributing to the formation of global ERFs.

Table 8: Semantic segmentation results on ADE20K using UperNet [65]. We evaluate the performance of semantic segmentation on the ADE20K dataset with UperNet [65]. The FLOPs are calculated with input sizes of 512 × 2048 . "SS" and "MS" denote single-scale and multi-scale testing, respectively.

| method           | crop size   |   mIoU (SS) |   mIoU (MS) | Params   | FLOPs   |
|------------------|-------------|-------------|-------------|----------|---------|
| Swin-T           | 512 2       |        44.5 |        45.8 | 60M      | 945G    |
| ConvNeXt-T       | 512 2       |        46   |        46.7 | 60M      | 939G    |
| Vanilla-VMamba-T | 512 2       |        47.3 |        48.3 | 55M      | 964G    |
| VMamba-T         | 512 2       |        48   |        48.8 | 62M      | 949G    |
| Swin-S           | 512 2       |        47.6 |        49.5 | 81M      | 1039G   |
| ConvNeXt-S       | 512 2       |        48.7 |        49.6 | 82M      | 1027G   |
| Vanilla-VMamba-S | 512 2       |        49.5 |        50.5 | 76M      | 1081G   |
| VMamba-S         | 512 2       |        50.6 |        51.2 | 82M      | 1028G   |
| Swin-B           | 512 2       |        48.1 |        49.7 | 121M     | 1188G   |
| ConvNeXt-B       | 512 2       |        49.1 |        49.9 | 122M     | 1170G   |
| Vanilla-VMamba-B | 512 2       |        50   |        51.3 | 110M     | 1226G   |
| VMamba-B         | 512 2       |        51   |        51.6 | 122M     | 1170G   |

w/ DWConv

w/o DWConv

Figure 13: The visualization of ERF for models with different scanning patterns.

<!-- image -->

## H.2 Influence of the Initialization Approach

In our study, we adopted the initialization scheme initially proposed for the SS2D block in S4D [19]. Therefore, it is necessary to investigate the contribution of this initialization method to the effectiveness of VMamba. To delve deeper into this matter, we replaced the default initialization with two alternative methods: random initialization and zero initialization.

For both initialization methods, we set the parameter D in equation 1 to a vector of all ones, mimicking a basic skip connection (thus we have y = Ch + Du ). Additionally, the weights and biases associated with the transformation from low rank to the dimension D v (which matches the input size), are initialized as random vectors, in contrast to Mamba [17], which employs a more sophisticated approach for initialization.

The main distinction between random and zero initialization lies in the parameter A in equation 5, which is typically initialized as a HiPPO matrix in both Mamba [17, 19] and our implementation of VMamba. Given that we selected the hyper-parameter d\_state to be 1, the Mamba initialization for log ( A ) can be simplified to all zeros, which aligns with zero initialization. In contrast, random initialization assigns a random vector to log ( A ) . We choose to initialize log ( A ) rather than A directly to ensure that A remains close to the all-ones matrix when the network parameters are near zero, which empirically helps enhance the stability of the training process.

The experiment results in Table 11 indicate that, at least for image classification with SS2D blocks, the model's performance is not significantly affected by the initialization method. Therefore, within this context, the sophisticated initialization method employed in Mamba [17] can be substituted with a simpler, more straightforward approach. We also visualize the ERF maps of models trained with

Figure 14: The visualization of ERF of VMamba with different initialization.

<!-- image -->

different initialization methods (see Figure 14), which intuitively reflect the robustness of SS2D across various initialization schemes.

## H.3 Influence of the d\_state Parameter

Throughout this study, we primarily set the value of d\_state to 1 to optimize the computational speed of VMamba. To further explore the impact of d\_state on the model's performance, we conduct a series of experiments.

As shown in Table 12, with all other hyper-parameters frozen, we increase d\_state from 1 to 4. This leads to a minor improvement in performance but a substantial decrease in throughput, indicating a significant negative influence on the computational efficiency of VMamba. However, increasing d\_state to 8, while reducing ssm-ratio to maintain computational complexity, leads to improved accuracy. However, when d\_state is further increased to 16, with ssm-ratio set to 1, performance drops.

These findings suggest that modest increases in d\_state may not necessarily lead to improved performance. Instead, selecting the optimal combination of d\_state and ssm-ratio is crucial to achieving a good trade-off between inference speed and performance.

## H.4 Influence of ssm-ratio , mlp-ratio , and layer numbers

In this section, we investigate the trade-off between ssm-ratio , layer numbers , and mlp-ratio .

Experiment results shown in Table 13 suggest that reducing ssm-ratio leads to a notable decrease in performance but significantly enhances the inference speed. Conversely, increasing layer numbers enhances the performance while slowing down the model.

As the hyper-parameter ssm-ratio stands for the dimension employed by the SS2D module, the trade-off between ssm-ratio and layer numbers can be interpreted as a balance between channel-mixing and token-mixing [57]. Furthermore, we reduce mlp-ratio from 4.0 to 2.0 and progressively increase ssm-ratio to maintain constant FLOPs, as shown in Table 14. The results presented in Tables 13 and 14 underscore the importance of an optimal combination of ssm-ratio , mlp-ratio , and layer numbers for constructing a model that achieves a balance between effectiveness and efficiency.

## H.5 Influence of the Activation Function

In VMamba, the SiLU [14] activation function is utilized to build the SS2D block. However, experiment results in Table 15 reveal that VMamba maintains robustness across different activation functions. This implies that the selection of the activation function does not substantially affect the model's performance. Therefore, there is flexibility in choosing an appropriate activation function based on computational constraints or other preferences.

Table 9: Comparison of generalizability to inputs with increased spatial resolutions. The throughput and train throughput are measured with batch size of 32 using Pytorch 2.0, on an A100 GPU paired with an AMD EPYC 7542 CPU. Only model forward, loss forward and backward are included in calculating train throughput. we re-implemented the HiViT-T, as the checkpoint of HiViT-T has not been released. † denotes that the batch size ≤ 16 due to out-of-memory (OOM).

| Image Size   | Param. (M)   | FLOPs (G)     | TP. (img/s)   | Train TP. (img/s)   | Top-1 acc. (%)   | Top-1 acc. (%) (w/ linear tuning )   |
|--------------|--------------|---------------|---------------|---------------------|------------------|--------------------------------------|
| 224 2        | 30M          | 4.91G         | 1490          | 418                 | 82.60            | 82.64                                |
| 288 2        | 30M          | 8.11G         | 947           | 303                 | 82.95            | 83.03                                |
| 384 2        | 30M          | 14.41G        | 566           | 187                 | 82.41            | 82.77                                |
| 512 2        | 30M          | 25.63G        | 340           | 121                 | 80.92            | 81.88                                |
| 2            |              |               | 214           | 75                  | 78.60            | 80.62                                |
| 640          | 30M          | 40.04G        |               | 53                  | 74.66            |                                      |
| 768 2 2      | 30M          | 57.66G        | 149           | 399                 |                  | 79.22                                |
| 224 2        | 31M          | 4.86G         | 1227 761      | 255                 | 82.49            | 82.52                                |
| 288          | 31M          | 8.03G         |               |                     | 82.81            | 82.93                                |
| 384 2        | 31M          | 14.27G        | 452           | 155                 | 82.51            | 82.74                                |
| 512 640 2    | 31M 31M      | 25.38G 39.65G | 272 170       | 100 60              | 81.07 79.30      | 82.02 81.02                          |
| 768 2        | 31M          | 57.09G        | 117           | 42                  | 76.06            | 79.69                                |
| 224 2        | 23M          | 5.63G         | 628           | 189                 | 82.17            | 82.09                                |
| 288 2        | 23M          | 9.30G         | 390           | 117                 | 82.74            | 82.76                                |
| 2            |              | 16.53G        |               |                     |                  |                                      |
| 384          | 23M          |               | 212           | 65                  | 82.40            | 82.72                                |
| 512 2        | 23M          | 29.39G        | 138           | 53                  | 81.05            | 81.97                                |
| 640 2        | 23M          | 45.93G        | 87            | 27                  | 78.79            | 80.71                                |
| 768 2        | 23M          | 66.14G        | 52            | 18                  | 75.09            | 79.12                                |
| 224 2        | 28M          | 4.51G         | 1142          | 769                 | 81.19            | 81.18                                |
| 288 2        | 28M          | 7.60G         | 638           | 489                 | 81.46            | 81.62                                |
| 384 2        | 28M          | 14.05G        | 316           | 268                 | 80.67            | 81.12                                |
| 512 2        | 28M          | 26.65G        | 176           | 131                 | 78.97            | 80.21                                |
| 640 2        | 28M          | 45.00G        | 88            | 68                  | 76.55            | 78.89                                |
| 768 2        | 29M          | 70.72G        | 53            | 38                  | 73.06            | 77.54                                |
| 224 2        | 26M          | 4.87G         | 1127          | 505                 | 81.87            | 81.89                                |
| 288 2        | 26M          | 8.05G         | 724           | 462                 | 82.44            | 82.44                                |
| 384 2        | 26M          | 14.31G        | 425           | 308                 | 81.84            | 82.21                                |
| 512 2        | 26M          | 25.44G        | 244           | 185                 | 79.80            | 80.92                                |
| 640 2        | 26M          | 39.75G        | 158           | 122                 | 76.84            | 79.00                                |
| 768 2        | 26M          | 57.24G        |               | 87                  | 72.52            | 76.92                                |
| 224 2        | 19M          | 4.60G         | 111           | 1041                | 81.92            | 81.85                                |
| 288 2        | 19M          | 7.93G         | 1261 750      | 614                 | 82.45            | 82.42                                |
| 384 2        | 19M          | 15.21G        | 388           | 333                 | 81.51            | 81.91                                |
| 512 2        | 20M          | 30.56G        | 186           | 150                 | 79.30            | 80.49                                |
| 640 2        | 20M          |               |               |                     |                  | 78.58                                |
| 2            |              | 54.83G        | 93            | 71                  | 76.09            |                                      |
| 768          | 20M          | 91.41G        | 55            | 37 †                | 71.38            | 76.47                                |
| 224 2        | 22M          | 4.61G         | 1573          | 1306                | 80.69            | 80.40                                |
| 288 2        | 22M          | 7.99G         | 914           | 1124                | 80.80            | 80.63                                |
| 384 2        | 22M          | 15.52G        | 502           | 697                 | 78.87            | 79.54                                |
| 512 2        | 22M          | 31.80G        | 261           | 387                 | 74.21            | 76.91                                |
| 640 2        | 23M          | 58.17G        | 149           | 244                 | 68.04            | 73.31                                |
| 768 2        | 23M          | 98.70G        | 90            | 156                 | 60.98            | 69.62                                |
| 224 2        | 29M          | 4.47G         | 1107          | 614                 | 82.05            | 81.95                                |
| 288 2        | 29M          | 7.38G         | 696           | 403                 | 82.23            | 82.30                                |
| 384 2        | 29M          | 13.12G        | 402           | 240                 | 81.05            | 81.78                                |
| 512 2        | 29M          | 23.33G        | 226           | 140                 | 78.03            | 80.37                                |
| 640 2        | 29M          | 36.45G        | 147           | 90                  | 74.27            | 78.77                                |
| 768 2        | 29M          | 52.49G        | 103           | 63                  | 69.50            | 76.89                                |

Table 10: The performance of VMamba-T with different scanning patterns.Table 11: The performance of VMamba-T with different initialization.

| Model             | (M)               | Params FLOPs (G)   | TP. (img/s)       | Train TP. (img/s)   | Top-1 (%)         |
|-------------------|-------------------|--------------------|-------------------|---------------------|-------------------|
| VMamba w/ dwconv  | VMamba w/ dwconv  | VMamba w/ dwconv   | VMamba w/ dwconv  | VMamba w/ dwconv    | VMamba w/ dwconv  |
| Unidi-Scan        | 30.2M             | 4.91G              | 1682              | 571                 | 82.26             |
| Bidi-Scan         | 30.2M             | 4.91G              | 1687              | 572                 | 82.49             |
| Cascade-Scan      | 30.2M             | 4.91G              | 998               | 308                 | 82.42             |
| Cross-Scan        | 30.2M             | 4.91G              | 1686              | 571                 | 82.60             |
| VMamba w/o dwconv | VMamba w/o dwconv | VMamba w/o dwconv  | VMamba w/o dwconv | VMamba w/o dwconv   | VMamba w/o dwconv |
| Unidi-Scan        | 30.2M             | 4.89G              | 1716              | 578                 | 80.88             |
| Bidi-Scan         | 30.2M             | 4.89G              | 1719              | 578                 | 81.80             |
| Cascade-Scan      | 30.2M             | 4.90G              | 1007              | 309                 | 81.71             |
| Cross-Scan        | 30.2M             | 4.89G              | 1717              | 577                 | 82.25             |

Table 12: The performance of VMamba-T with different d\_state .

| initialization   |   (M) |   Params FLOPs (G) |   TP. (img/s) |   Train TP. (img/s) |   Top-1 acc. (%) |
|------------------|-------|--------------------|---------------|---------------------|------------------|
| mamba            |  30.2 |               4.91 |          1686 |                 571 |            82.6  |
| rand             |  30.2 |               4.91 |          1682 |                 570 |            82.58 |
| zero             |  30.2 |               4.91 |          1683 |                 570 |            82.67 |

Table 13: The performance of VMamba-T under different combination of ssm-ratio and layer numbers .

|    |   d\_state ssm-ratio |   (M) |   Params FLOPs (G) |   TP. (img/s) |   Train TP. (img/s) |   Top-1 acc. (%) |
|----|---------------------|-------|--------------------|---------------|---------------------|------------------|
|  1 |                 2   |  30.7 |               4.86 |          1340 |                 464 |            82.49 |
|  2 |                 2   |  30.8 |               4.98 |          1269 |                 432 |            82.5  |
|  4 |                 2   |  31   |               5.22 |          1147 |                 382 |            82.51 |
|  8 |                 1.5 |  28.6 |               5.04 |          1148 |                 365 |            82.69 |
| 16 |                 1   |  26.3 |               4.87 |          1164 |                 358 |            82.31 |

Table 14: The performance of VMamba under different combination of ssm-ratio and mlp-ratio .

|   ssm-ratio | layer numbers   |   (M) |   Params FLOPs (G) |   TP. (img/s) |   Train TP. (img/s) |   Top-1 acc. (%) |
|-------------|-----------------|-------|--------------------|---------------|---------------------|------------------|
|           2 | [2,2,5,2]       |  30.7 |               4.86 |          1340 |                 464 |            82.49 |
|           1 | [2,2,5,2]       |  25.6 |               3.98 |          1942 |                 647 |            81.87 |
|           1 | [2,2,8,2]       |  30.2 |               4.91 |          1686 |                 571 |            82.6  |

Table 15: The performance of VMamba-T with different activation functions in SS2D.

|    |   mlp-ratio ssm-ratio |   (M) |   Params FLOPs (G) |   TP. (img/s) |   Train TP. (img/s) |   Top-1 acc. (%) |
|----|-----------------------|-------|--------------------|---------------|---------------------|------------------|
|  4 |                   1   |  30.2 |               4.91 |          1686 |                 571 |            82.6  |
|  3 |                   1.5 |  28.5 |               4.65 |          1419 |                 473 |            82.75 |
|  2 |                   2.5 |  29.9 |               4.95 |          1075 |                 361 |            82.86 |

| activation   |   (M) |   Params FLOPs (G) |   TP. (img/s) |   Train TP. (img/s) |   Top-1 acc. (%) |
|--------------|-------|--------------------|---------------|---------------------|------------------|
| SiLU         |  30.2 |               4.91 |          1686 |                 571 |            82.6  |
| GELU         |  30.2 |               4.91 |          1680 |                 570 |            82.53 |
| ReLU         |  30.2 |               4.91 |          1684 |                 577 |            82.65 |