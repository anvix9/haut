## PANGUΣ : TOWARDS TRILLION PARAMETER LANGUAGE MODEL WITH SPARSE HETEROGENEOUS COMPUTING

TECHNICAL REPORT

Xiaozhe Ren 1 ∗

Pingyi Zhou 1 ∗

Xinfan Meng 1 ∗

Xinjing Huang 2 ∗

Yadao Wang 1 ∗

Weichao Wang 1

Pengfei Li 1

Xiaoda Zhang 2

Alexander Podolskiy 1

Grigory Arshinov 1

Andrey Bout 1

Irina Piontkovskaya 1

Jiansheng Wei 1

Xin Jiang 1

Teng Su 2

Qun Liu 1

Jun Yao 1

1

Noah's Ark Lab, Huawei Technologies 2 Distributed and Parallel Software Lab, Huawei Technologies

## ABSTRACT

The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors 2 and MindSpore framework 3 , and present the language model with 1.085T parameters named PanGuΣ . With parameter inherent from PanGuα [1], we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation (ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGuΣ provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.

K eywords Large Language Models · Distributed Training · Natural Language Processing

## 1 Introduction

Large Language Models (LLMs) [2, 3, 1, 4, 5, 6, 7, 8, 9, 10, etc.] have demonstrated unprecedented capabilities and potential in the areas of natural language understanding, generation and reasoning. By utilizing vast amount of textual data, the performance of language models scales up with compute budget and model parameters, demonstrating strong zero/few-shot learning abilities or even emergence abilities [4, 11]. Several large language models with hundreds of billion parameters have been published since GPT-3 [2], including but not limited to Megatron-Turing NLG [12], PanGuα [1], ERNIE 3.0 Titan [8], Gopher [5], PaLM [4], OPT [6], Bloom [10], and GLM-130B [9]. Researchers start to build even larger language models with more than one trillion parameters. Typically, this is accomplished by leveraging sparsely-activated models such as Mixture-of-Experts (MoE) [13]. Among the trillion-parameter models currently in existence, there are several noteworthy work such as Switch-C [14], GLaM [15], MoE-1.1T [16], Wu Dao 2.0 [17], and M6-10T [18]. However, only a select few have published comprehensive evaluation results over a wide range of tasks while simultaneously achieving anticipated performance. In our experience, the primary difficulty lies in the scaling efficiency.

Recent studies on the scaling laws of language models [19, 20, 21] demonstrate the necessity of training LLMs with sufficient amount of training data and corresponding compute budget to achieve optimal performance. Therefore, one of the main motivation for this work is to design a scalable model architecture and an efficient distributed training system that can consume the data with high training throughput.

- · Model Scaling . Model performance of LLMs is expected to scale up with larger model size. Comparing to the expensive computational cost for training dense Transformer model, sparse architectures such as Mixtureof-Experts (MoE) [13, 14, 15, 22] are considered to be an appealing choice to scale model size up without incuring linear increase in computational cost. However, MoE models suffer from the problems such as unbalanced workload and all-to-all communication latency. Moreover, how to extend existing dense model with MoE and how many experts to allocate in each layer remain open problems. Therefore, designing a trillion parameter sparse model with high performance and training efficiency is a significant yet challenging task.
- · System Scaling . Frameworks such as DeepSpeed 4 have been proposed to support training trillion parameter models. In practice, the main barrier often lies on limited compute budget, or more specifically the number of accelerating devices (e.g., GPU, NPU, TPU) that can be used. By utilizing techniques such as tensor parallelism [23], pipeline parallelism [24], zero redundancy optimizer [25] and rematerialization [26], practitioners can train trillion-parameter model with feasible batch sizes across thousands of accelerating devices. Alternatively, practitioners can reduce the amount of computation resources by utilizing heterogeneous computing techniques such as offloading some of the computation to host devices [27]. However, the current techniques inevitably hinder the training throughput due to slow bandwidth between the host and device as well as weak computing capabilities of CPUs compared to accelerating devices, which prevent feeding large language models with reasonably amount of data and achieving optimal performance. Therefore, how to efficiently scale the system performance with limited computation budget is critical to the performance of large language models.

In this work, we present PanGuΣ , a large language model with sparse architecture containing 1.085 trillion parameters. We develop PanGuΣ model under the framework of MindSpore 5 and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days. PanGuΣ inherent parameters from PanGuα [1] with Transformer decoder architecture and are extended via Random Routed Experts (RRE). Different from conventional MoE, RRE adopts two-level routing. At the first level experts are grouped by domain or task, and at the second level tokens are randomly and uniformly mapped to experts in each group without using any learnable gating function as in MoE. With the design of RRE, one can easily extract sub-models from the PanGuΣ for various downstream applications such as dialogue, translation, code generation or general nature language understanding. To make training system efficient and scalable, we propose Expert Computation and Storage Separation (ECSS) mechanism, which achieves 69905 tokens/s observed throughput in training 1.085 trillion PanGuΣ on cluster of 512 Ascend 910 accelerators, and reduces Host-to-Device and Device-to-Host communication as well as optimizer update computation by a large margin. As a whole, the training throughput is improved by 6.3x compared to the model of the same hyper-parameters but with MoE architecture. By consuming 329B tokens in more than 40 natural and programming languages, the sub-modal of PanGuΣ in Chinese domain significantly outperforms the previous SOTA models including PanGuα with 13B parameters and ERNIE 3.0 Titan [8] with 260B parameters over 16 downstream tasks in six categories in the zero-shot setting without any multitask finetuning or instruction tuning. We also test the performance of fine-tuned PanGuΣ on several applications domain such as dialogue, machine translation and code generation. PanGuΣ outperforms the SOTA models in the corresponding areas.

The rest of the technical report is organized as follows. Section 2 introduces the design philosophy and the architecture of PanGuΣ model. Section 3 introduces the collection and organization of the dataset. Section 4 describes system design and acceleration techniques. Section 5 presents the experimental results of PanGuΣ model.

## 2 Model

## 2.1 Design Principles

PanGuΣ aims to achieve the following goals.

- · Performance : state-of-the-art NLP performance across multiple domains and tasks.

- · Efficiency : training trillion parameters model with maximum system performance on a modest cluster.
- · Usability : extendable to various domains or tasks, without need of retraining the model from scratch.
- · Deployment : easily customizable and deployable in various real-world settings.

Achieving all the above goals at the same time is very challenging. Considering the first goal, a language model that can generalize and perform well across domains should have a very large number of parameters and be trained on large amount of data according to the scaling law [19, 20, 21]. However, training such a large model also means that a high-end cluster is mandatory, which somehow contradicts with the second goal. And the larger scale of the model also leads to increasing cost in deploying the trained model, which is related to the fourth goal.

Considering the high computational cost incurring during the training phase, we want the resulted model to be practically usable and efficient in many real applications. With this goal in mind, we propose to train the model in multiple domains and make it further extendable to any number of domains in a continuous learning paradigm, subject to the computation resource.

During training phase, the trillion parameters PanGuΣ model is fed with data from multiple domains. However, in the deployment phase, it is often unnecessary or even impossible to host the trillion parameters model for every application. Therefore, a model that allows for the grouping and separation of its parameters based on various training and deployment setups offers significant advantages.

## 2.2 PanGuΣ Architecture

## 2.2.1 Overview

Figure 1: PanGuΣ architecture. The architecture is mixed by dense transformer layers and sparse transformer layers. The lower M layers are dense layers shared across different domains. The upper N transformer layers' feed-forward part are sparsely activated via Random Routed Experts (RRE). Tokens from different domains have different embeddings.

<!-- image -->

PanGuΣ adopts an auto-regressive language modeling with stacked transformer decoder layers and a query layer on the top. The PanGuΣ architecture offers a flexible design. The bottom M layers are globally shared across all the domains, and the top N layers (including the query layer) are sparsely activated according to the domains of the input data. In each RRE layers, there are K experts in G groups in total, the number of experts in each group can be different. This flexible design offers three mode.

- · Mixed mode : when M > 0 , N > 0 and K > 0 , model contains both sparse RRE layers and dense layers.

- · Dense mode : when N = 0 or K = 1 , the architecture will reduce to a dense PanGuα model.
- · Sparse mode : when M = 0 and K > 1 , the architecture will be a sparse model.

In this trillion-parameters modeling practice, We use the mixed configuration by placing the shared parameters close to the input layer (bottom) and all the sparsely activated expert parameters close to the output layer (top). In the model designing stage, we benchmark various experts placement strategies on smaller scale models and the selected strategy obtains the lowest language modeling perplexity. Our hypothesis is that bottom layers tends to learn general knowledge, while the specific knowledge is in a higher level of abstraction and is more appropriate to be learned by the top layers. In the token embedding layer, we choose to use different embedding matrices for different domains.

## 2.2.2 Random Routed Experts

In the top N layers, we replace each feed-forward sub-layer with multiple conditionally activated feed-forward sub-layers (experts), following the Mixture of Experts (MoE) paradigm.

A key question in designing MoE architecture is how to route tokens to experts. For PanGuΣ , we propose a Random Routed Experts (RRE) mechanism, which is inspired by Hash Layers proposed in [29]. Specifically, RRE routes the tokens by IDs in a two-level manner. In the first level, the token is mapped to a group of candidate experts by domain, and then in the second level, one expert in this group is chosen according to a token-expert routing map to process the token. The routing map is randomly initialized and each layer has a independently initialized mapping for balancing the computation.

Figure 2: Random Routed Experts (RRE) in PanGuΣ . The token is first routed to a group of experts by domain, and then randomly routed to one of the experts in that domain. There is no learnable routers in the model.

<!-- image -->

RRE has several advantages over the commonly-used learnable routers.

- · During training, PanGuΣ allows for the addition, modification, or removal of domain-specific experts without any impact on the other experts. This attribute makes PanGuΣ highly flexible for alleviating the commonly encountered problem of catastrophic forgetting, which is crucial for life-long or continual learning.

- · In most real-world deployment setting, it is unnecessary or impractical to deploy a trillion-parameter model. PanGuΣ allows one to extract a sub-model for specific domains according to practical requirements and only deploy the sub-model. The sub-model may contain tens of billion parameters but still keep the predictive power of the original model on the target domains. Using this extract-and-deploy operation, we can easily deploy models for multiple industrial applications.
- · All the conventional MoE models rely on all-to-all communication collective operation to move data between experts residing on different devices. With our proposed two-level routing, experts from different domains don't exchange tokens, and all-to-all communication is constrained within each domain. As a result, the expensive global all-to-all operation is reduced to grouped all-to-all, saving much communication volume and reducing the end-to-end training latency.
- · Learnable router needs more computation, and can suffer from problem of unbalanced loads across the experts, which typically makes the training process less stable. RRE avoids all the above pitfalls since no additional parameters are introduced and randomly initialized routing table helps to balance the loads on experts.

RRE requires a routing map which is initialized before pretraining, Algorithm 1 describes how we construct the routing table.

## Algorithm 1: Routing table construction procedure in Random Routed Experts (RRE) mechanism.

input : number of domain d , number of layers l , number of experts per domain per layer e , size of vocabulary V . output : T , a tensor of shape ( d, l, V ) acting as the RRE routing table.

- 1 set random seed to 0 ;
- 2 initialize T ;
- 3 initialize glyph[vector]u , a vector [0 , ..., V -1] of size V ;
- 4 ˜ V = glyph[floorleft] V/e glyph[floorright] · e ;
- 5 glyph[vector]u [0 : ˜ V -1] = glyph[floorleft] glyph[vector]u [0 : ˜ V -1] /e glyph[floorright] ;
- 6 glyph[vector]u [ ˜ V : V -1] = [0 , ..., V -˜ V -1] ;
- 7 for j ← 0 to l -1 do
- 8 for i ← 0 to d -1 do
- 9 shuffle glyph[vector]v , a vector [0 , ..., V -1] of size V ;
- 10 T [ i ][ j ][ glyph[vector]v ] = glyph[vector]u + i ∗ e
- ;

## 3 Dataset

## 3.1 Collection

To better demonstrate the capability of PanGuΣ model to efficiently and independently learn from multiple domains, we collect datasets in 40 domains, with a large amount of data in four major domains: Chinese, English, Bilingual (Chinese and English) and code. The remaining domains with smaller portion consists of 26 other monolingual natural languages, 6 programming languages, and textual data from finance, health, law, and poetry domains, respectively.

For Chinese texts, we collect the WuDaoCorpora 2.0 [30] which contains 200GB and the CLUECorpus2020 [31] which contains 100GB. For English texts, the Pile dataset [32] which contains 800GB and C4 dataset [3] which contains 750GB were collected. For code, we use the Python code (147GB) which has been used in PanGu-Coder [33], as well as the Java code (161GB) from GHTorrent [34] , which are then filtered by file size ( < 1MB), average number of characters per line ( < 200), maximum number of characters per line ( < 1000) and their compilablity. Then, these collected English, Chinese and code texts data was sampled and distributed to the four major domains. Finally, we get more than 300B tokens for the four major domains. The detailed statistics of data distribution and data sources in four major domains are presented in Table 1.

For the remaining 36 domains, the data for 26 monolingual domains are mainly from CCAligned [35] and CCMatrix [36]. Similar to the code domain mentioned above, the data for 6 programming language domains are collected through GHTorrent [34] and filtered in the similar way. Finance domain data is filtered from the WuDaoCorpora 2.0 [30] using the tags. Health domain data is from Chinese MedDialog Dataset [37]. Law domain data is sampled from CAIL2018 [38]. Poetry domain dataset is from Werneror-Poetery 6 . Finally, we sampled more than 25B tokens for the 36 domains.

Table 1: Data distribution and data sources in four main domains

|   Domain ID | Domain                       | Tokens (Billion)                          | Data source                           |
|-------------|------------------------------|-------------------------------------------|---------------------------------------|
|           0 | Bilingual (Chinese, English) | 77.51 B Chinese (38.75) + English(38.76B) | CLUECorpus2020 , C4                   |
|           1 | Chinese                      | 75.47 B                                   | WuDaoCorpora 2.0                      |
|           2 | English                      | 75.90 B                                   | Pile , C4                             |
|           3 | Code (Python, Java)          | 75.24 B Python (50.24B) + Java (25B)      | Python (PanGu-Coder) Java (GHTorrent) |

## 3.2 Format

For the four major domains, each can be adapted to different downstream tasks. In order to better support domainspecific downstream tasks, this paper uses different data format for different domains. For Chinese and English domains, the <EOT> token which indicates the end of training text is inserted at the end of each training sample.

Figure 3: Data format of Chinese and English domains.

<!-- image -->

For Bilingual domain, the <EN> or <CN> token is inserted into the head of the training sample according to the source of the training sample (either from the Chinese dataset or the English dataset), and the <EOT> token is inserted at the end of each training sample.

Figure 4: Data format of Bilingual domain.

<!-- image -->

For the code domain, the <Python> or <Java> token is inserted into the head of the training sample based on the programming language type of the training sample, and the <EOT> token is inserted at the end of each training sample.

For the remaining 36 domains, the data formats of 26 monolingual domains, finance, health, law, and poetry domains are the same as the Chinese and English domains, and the data format of 6 programming language domains is the same as the code domain.

Figure 5: Data format of Code domain.

<!-- image -->

For a formatted data set D , suppose it contains n training samples D = { s 1 , s 2 , . . . , s n } . To make full use of the computing power of the Ascend 910 cluster and accelerate training in the pre-training phase, we concatenate all samples in the data set into a sequence, and then intercept training instances in the concatenated sequence according to the fixed length (1024), as shown in Figure 6. In the fine-tune phase, for each training sample in the formatted dataset, if the length is less than the fixed length, we pad the sample to the fixed length with a special token <Pad>. If the length is greater than the fixed length, the extra part is truncated. Figure 7 shows the process. Different to PanGuα model, each

training sample of PanGuΣ model contains two field: input sequence of token IDs which are training instance and their domain ID. The domain ID indicates which domain the training instance belongs to. The RRE layers of the PanGuΣ model decide which experts the training tokens is routed to by the domain ID.

Figure 7: Input format during model fine-tuning.

<!-- image -->

## 4 System

PanGuΣ is implemented with MindSpore 1.6 framework 7 and trained on 512 Ascend 910 accelerators (also know as Ascend 910 NPU).

Training a trillion parameters language model poses multiple challenges. First, it requires enormous amount of memory in training. Although the sparse architecture can effectively save computation, it doesn't reduce the memory consumption and we still need to store all the parameters and optimization states inside the accelerator memory. Assuming Adam optimizer [39] with mixed-precision training [40] is used, a 1T model typically consumes 16TB memory in total just for parameters, gradients and optimizer states. During training, the model needs extra memory for input data, network activations, communication buffers and temporary variables. We estimate that training a PanGuΣ model with 1 trillion parameters with a reasonably batch size needs more than 32TB memory and requires more than 1,000 Ascend 910 accelerators or NVIDIA V100 GPUs with 32GB High Bandwidth Memory (HBM).

Instead of pouring lots of hardware resources to scale-up the model, we aim to train PanGuΣ with a reasonably-sized cluster of 512 Ascend accelerators. To this end, we adopt the heterogeneous training and offload the optimizer states to CPU[27]. After enabling heterogeneous training, all optimizer states are moved from accelerator to the host with 750GB host memory and KunPeng 920 CPU 8 , and we can fit the entire training process into the cluster.

Second, the system throughput is unacceptable after enabling vanilla optimizer offloading. The root cause is again the sheer amount of parameters. Gradients and updated parameters need to be exchanged via the slow host-to-device and device-to-host communication, and CPUs need to iterate thorough all parameters and update them. To improve the training throughput, we leverage the sparse nature of PanGuΣ architecture. Since PanGuΣ use a sparse architecture and most of its parameters are conditionally activated, the optimizer only need to update part of experts in one iteration. So we propose Expert Computation and Storage Separation (ECSS) method as illustrated in Figure 8.

In Expert Computation and Storage Separation, we consider experts as knowledge database to store specific knowledge of different tasks or domains. In each iteration, experts are sparsely activated by different token IDs with specific domain. In MindSpore, we use lookup operator to select parts of activated experts, and sparsely update their parameters in the backward computation. In optimizer CPU offload computing, MindSpore copy FP16 parameters from host CPU to NPU, compute the gradients on NPU, move FP16 gradients from NPU to CPU, and compute optimizer states and update parameters in the host CPU. With a lower experts sparsity ratio such as 0 . 1 , the computation cost is only near 10 % of full model.

Figure 8: Expert Computation and Storage Separation (ECSS) in traning PanGuΣ . In each iteration, with sparsity ratio s ∈ (0 , 1] and expert amount K, only number of A = Ks experts activated by lookup operation, which reduce the communication cost between device and host, and cost of forward and backward computation in device as well as optimizer operations in host.

<!-- image -->

Besides ECSS with Ascend-KunPeng sparse heterogeneous computing, we also adopt other parallel training and accelerating techniques provided by MindSpore and CANN 9 . We use 8-ways model parallel for all the attention and feed-forward layers, 64-ways expert parallel without replica and 64-ways data parallel for non-expert parts. To further optimize memory footprint, rematerialization [26] and optimizer parallel [25] are also adopted to reduce the peak memory consumption. We also use FastGelu and fused LayerNorm to accelerate point-wise computation. By combining all the techniques together, we achieved 6.3 times throughput promotion compared to vanilla PanGuΣ heterogeneous training, as shown in Figure 9.

## 5 Experiments

## 5.1 Pretraining

## 5.1.1 Model Configuration

We use the following PanGuΣ configuration for this work. The configuration mostly follows the 13B version of PanGuα model. In this way, we can effective inherit the knowledge already learned by PanGuα .

Table 2: Model Configuration

|   #Layers (N+M) |   #Heads (N\_h) |   Hidden size (d) |   FFN size (d\_ff) |   #RRE layers (N) |   #Experts (K) |   # Groups (G) |
|-----------------|----------------|-------------------|-------------------|-------------------|----------------|----------------|
|              40 |             40 |              5120 |             20480 |                 8 |            640 |             40 |

Figure 9: Training throughput (token/s) of PanGuΣ wo/w Expert Computation and Storage Separation (ECSS). ECSS can achieve 6.3x increase of training throughput.

<!-- image -->

## 5.1.2 Pretraining settings

We use a cluster of 64 nodes, with each node equipped with 8 Ascend 910 accelerators and MindSpore framework. High performance collective communication library Huawei Collective Communication Library (HCCL) is used to facilitate high speed high bandwidth communication for distributed training.

There are two stages in PanGuΣ pretraining process. In the first stage, we activate four main domains' experts to consume data from all the four main domains including bilingual, Chinese, English and codes. In the second stage, we let all the experts to consume all domain's data. Figure 11 shows how 640 experts are assigned to 40 domain groups. We train PanGuΣ with global batch size of 512 with sequence length of 1024 for each sample. The pretraining lasts about 100 days. Figure 10 shows the loss curve of PanGuΣ pretraining.

Figure 10: Pretraining Loss

<!-- image -->

#tokens

Jell

Mixed-Precision training is enabled to speedup the training process. Apart from vocabulary embedding layer, loss function, Softmax operation, LayerNorm layers and Adam optimizer, all other operations adopt FP16 format.

Figure 11: Mapping between domains and experts in PanGuΣ .The data from particular domains are routed to a group of experts lying across different devices. The color of experts distinguish their corresponding domains. There are ten experts from different domain on each device.

<!-- image -->

Failure recovery is very important for long term large scale distributed training, especially for huge models like PanGuΣ . Therefore, a rigorous process of saving checkpoints and restarting from previous checkpoints is indispensable. For a trillion-parameter model, one set of checkpoint storing all parameters and optimizer states for a single iteration already has a jaw-dropping 10TB size. Uploading checkpoints of such size to our long term object store is a challenging task, since uploading all checkpoints at the same time quickly saturates the network bandwidth and inevitably lead to training failure. To solve this issue, we launch the upload process in a round-robin style and limit the number of the simultaneously running process. This solution proves to be effective and stable for our entire training process.

## 5.1.3 Hybrid Hyper-parameter ADAM Optimizer

We design a Hybrid Hyper-parameter ADAM Optimizer to provides further stability for PanGuΣ during the pretraining phase.

To better understand PanGuΣ training process, we inspected the statistics of training states and find out that the gradients of RRE layers are much smaller than a non-sparse model. To tackle such a problem, we first set a very small glyph[epsilon1] 1 for all model parameters, then we go one step further and set an even smaller glyph[epsilon1] 2 only for the RRE layers, since compared to the dense layers, sparse layers received smaller effective batch due to its conditionally-activated nature. Specifically, we set hybrid hyper-parameters for ADAM optimizer below:

Table 3: Hyper-parameters of PanGuΣ training.

|   β 1 |   β 2 |   glyph[epsilon1] 1 |   glyph[epsilon1] 2 |   end lr |   warmup steps |   decay steps |
|-------|-------|---------------------|---------------------|----------|----------------|---------------|
|   0.8 |  0.95 |               1e-08 |               1e-20 |    2e-05 |           5000 |        180000 |

## 5.2 Inheritance Learning

To improve the training efficiency, accelerate model convergence, and reduce carbon emissions during training, the PanGuΣ model inherits the capabilities of the existing model, and then continues to train in four domains simultaneously. In this paper, PanGuΣ inherits the PanGuα 13B version.

## 5.2.1 Extending vocabulary

Because PanGuα 's vocabulary is mainly designed to support Chinese texts, we extend its vocabulary to support both Chinese and English texts. PanGuΣ uses Byte-level BPE [41] instead of BPE adopted by PanGuα , the vocabulary is formulated by adding T5 [3] small vocabulary to PanGuα 's vocabulary, then remove repeated sub-words. Some special tokens are added to the vocab. These special tokens are classified into two types: control tokens (e,g., <python>, <Java>, <CN>, <EN>) and spaces tokens for representing whitespace runs of different lengths.

## 5.2.2 Inheriting and Extending model parameters

In order to inherit the capability of the existing model as much as possible, PanGuΣ 's word embedding and all experts in RRE layer are initialized with the corresponding embedding and feed-forward layers from PanGuα , and other parameters are initialized with corresponding parameters. For example, to initialize the word embedding parameters of PanGuΣ , we first create a word embeddings W s ∈ R v s × h , if a sub-word of PanGuΣ exists in PanGuα , its word embedding is initialized with those of PanGuα . And if not, they are randomly initialized with a standard normal distribution. For the experts parameters in the RRE layer of PanGuΣ , each expert is initialized with the FFN parameters of the corresponding layer in the PanGuα model.

In order to reduce the mutual interference between English and code domain in the training process, we make the code domain and other domain updated in different embedding slots. Therefore, we further extend the PanGuΣ word embedding W s ∈ R v s × h to W s ' ∈ R v s ' × h , ( v s ' = 2 × v s ) . The slots [ v s , 2 × v s ] of word embeddings W s ' belongs to code domain and the slots [0 , v s ] belongs other domain. Figure 12. shows how PanGuΣ inherits the PanGuα 's parameters and extends it.

Figure 12: The process of inheriting parameters of PanGuα and extending to PanGuΣ .

<!-- image -->

## 5.2.3 Extracting domain specific sub-model

It is expensive to deploy a trillion parameters model like PanGuΣ directly. In order to transfer abilities of PanGuΣ to various downstream tasks and reduce the consumption of serving resources, we propose a loss-free expert pruning method by leveraging the RRE design. Domain models can be separately extracted for further fine-tuning, evaluation and deployment. Figure 13 illustrates how to extract the the domain specific sub-model from PanGuΣ . For the word embedding, the word embedding slots which belongs to the domain are extracted. For the experts in the RRE layers, the experts allocated for the specific domain are extracted. Other parameters of PanGuΣ are copied seamlessly.

Figure 13: The process of extracting domain (e.g., Bilingual) model parameters.

<!-- image -->

## 5.3 Chinese Downstream Tasks Evaluation

## 5.3.1 Task Description

Following PanGuα , we evaluate PanGuΣ at zero-shot settings on 16 datasets of six tasks. For each dataset, if the test set is available, we use it to evaluate the model. Otherwise, we use the validation set. The following describes each task in turn.

Machine reading comprehension. This task contains four datasets: CMRC2018 [42], DRCD [43], DuReader [44], and C3 [45]. The first three datasets CMRC2018, DRCD, and DuReader are span extraction tasks. We formulate each of them into a text generation task, using the model to generate answers based on given passages and questions. And we use F1, exact match (EM), and ROUGE-1 as the evaluation metrics. In addition, for the DuReader dataset, which is aligned with PanGuα , only the Zhidao subset is selected to evaluate the model performance. The last dataset, C3, is a multi-choice reading comprehension task. Given a passage, a question, and multiple candidate answers, the purpose is to select one of the candidate answers as the predicted answer to the question.

Natural language inference. There are two datasets: OCNLI [46] and CMNLI [47]. Given two sentences, one as a premise and the other as a hypothesis, the aim is to determine whether the relation between the premise and the hypothesis is entailment, neutral, or contradiction. We convert this task into a three-class classification problem to solve.

Text classification. We use TNEWS and IFLYTEK [47] datasets. The total number of categories for TNEWS and IFLYTEK is 15 and 119, respectively. Following PanGuα , for each instance, we randomly sample three negative categories plus one ground-truth category to form a new set of candidate categories, then simplify this task into a four-class classification task for processing.

Semantic similarity. We use two datasets: AFQMC and CSL [47]. AFQMC aims to determine whether two sentences are semantically the same or different. Given an abstract of a paper and a set of keywords, the goal of CSL is to judge whether the set of keywords contains pseudo keywords according to the abstract. Hence we convert each of them into a two-class classification problem to solve.

Winograd schema challenge. This task contains only the CLUEWSC2020 [47] dataset. CLUEWSC2020 is a coreference resolution task. Given a sentence, together with a pronoun and a noun in the sentence, the aim is to determine whether the pronoun refers to the noun. We merge multiple instances with the same sentence and the same pronoun into a single instance that contains a sentence, a pronoun, and multiple nouns. Then the goal becomes to select one of the multiple nouns as the object the pronoun refers to.

Cloze and completion. There are five datasets: CHID [48], CMRC2019 [49], PD [50], CFT [50], and CMRC2017 [51]. Both CHID and CMRC2019 are multi-choice completion tasks. Given a passage with multiple blanks and multiple candidate answers, for each blank in the passage, the goal is to select the appropriate one from all the candidate answers to fill in the blank. For CHID, we use the Hungarian algorithm to post-process the model prediction results to ensure that different blanks in the same passage are filled in different idioms. On the CMRC2019 dataset, following ERNIE 3.0 Titan [8], for each blank, we randomly sample three negative candidate answers plus one ground-truth answer to form a new set of candidate answers, and moreover, beam search is also used in the model prediction process to find an optimal combination of answers for multiple blanks in a passage. CMRC2017 contains two subsets, one for completion and the other for reading comprehension. As with PanGuα , we also evaluate PanGuΣ only on the completion subset. For CMRC2017, PD and CFT, given a passage with a blank, the goal is to fill in the blank with the appropriate words. Aligned with ERNIE 3.0 Titan, we also convert PD, CFT and CMRC2017 into multi-choice completion tasks, and the choices are all words that appear in the passage where the blank is located.

## 5.3.2 Evaluation Details

Each dataset of all Chinese downstream tasks can be evaluated using either a generation-based method or a scoring-based method. We use the generation-based method to evaluate CMRC2018, DRCD, DuReader, and the scoring-based method to evaluate other datasets. For each instance, a text sequence is obtained by filling it into a manually designed template, and then the text sequence is fed into PanGuΣ for prediction to get the result. The templates we used for all datasets are shown in Table 4.

Table 4: The templates for all datasets. "/" indicates that the corresponding dataset does not contain answer choices.

| Dataset                       | Template                                                                                                                                                                                                                                                                                                                                                                | Answer Choices                                                                                                                               |
|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| CMRC2018 DRCD DuReader C3     | 阅 读 文 章 ： {Passage}\n 根 据 上 文 ， 回 答 如 下 问 题 ： {Question}\n 答 ： 阅 读 文 章 ： {Passage}\n 根 据 上 文 ， 回 答 如 下 问 题 ： {Question}\n 答 ： 阅 读 文 章 ： {Passage}\n 根 据 上 文 ， 回 答 如 下 问 题 ： {Question}\n 答 ： 问 ： {Question}\n 答 ： {Answer}\n 该 答 案 来 自 对 话 ： {Passage}                                             | / / / Answer ∈ All options                                                                                                                   |
| CMNLI OCNLI                   | {Premise} ？ {Answer} ， {Hypothesis} {Premise} ？ {Answer} ， {Hypothesis}                                                                                                                                                                                                                                                                                             | Answer ∈ { 对 , 错 , 或 许 } Answer ∈ { 对 , 错 , 或 许 }                                                                                    |
| TNEWS IFLYTEK                 | 这 是 关 于 {Answer} 的 文 章 ： {Passage} 这 是 关 于 {Answer} 的 应 用 程 序 ： {Passage}                                                                                                                                                                                                                                                                             | Answer ∈ All categories Answer ∈ All categories                                                                                              |
| AFQMC CSL                     | {Sentence1} 。 {Sentence2} 。 上 面 两个 句 子 的 语 义 是 {Answer} 摘 要 ： {Abstract} {Answer} 的 关 键 词 ： {Keywords}                                                                                                                                                                                                                                              | Answer ∈ { 不 同 的 , 相 同 的 } Answer ∈ { 含 有 错 误 , 全 部 正 确 }                                                                      |
| CLUEWSC2020                   | {Text to the left of the pronoun}{Answer}{Text to the right of the pronoun}                                                                                                                                                                                                                                                                                             | Answer ∈ All mentions                                                                                                                        |
| CHID PD CFT CMRC2017 CMRC2019 | {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} | Answer ∈ All candidates Answer ∈ All words in the text Answer ∈ All words in the text Answer ∈ All words in the text Answer ∈ All candidates |

## Generation-based evaluation method

For each instance to be predicted, it is filled into the corresponding template to obtain a text sequence. After that, the text sequence is used as the input to PanGuΣ to generate the answer. We use a greedy decoding strategy to generate the answer.

## Scoring-based evaluation method

Each instance to be predicted contains multiple candidate answers. For each candidate answer, a text sequence is obtained by filling the candidate answer together with the sample into the corresponding template, and the perplexity of the text sequence is calculated by PanGuΣ . Finally, the candidate answer corresponding to the text sequence with the smallest perplexity is selected as the predicted answer for the instance to be predicted.

## 5.3.3 Result

Wechoose PanGuα and ERNIE 3.0 Titan as the baseline for comparison. The performance of each Chinese downstream task is shown in Table 5. Compared to ERNIE 3.0 Titan with 260 billion parameters, PanGuΣ surpassed on 11 out of 16 datasets, with an average score of 3.96 points higher on all datasets.

Table 5: Zero-shot results of Chinese downstream tasks. Compared to ERNIE 3.0 Titan, PanGuΣ surpassed on 11 out of 16 datasets, with an average score of 3.96 points higher on all datasets.

| Task Type                  | Dataset                   | Split           | Metric                            | PanGu- α 13B                                   | ERNIE 3.0 Titan                                     | PanGu- Σ                                             |
|----------------------------|---------------------------|-----------------|-----------------------------------|------------------------------------------------|-----------------------------------------------------|------------------------------------------------------|
| Reading comprehension      | CMRC2018 DRCD DuReader C3 | dev dev dev dev | avg(EM/F1) avg(EM/F1) ROUGE-1 Acc | 10.37(1.46/19.28) 5.61(0.66/10.55) 24.46 54.47 | 30.41( 16.62 /44.20) 29.46(21.08/37.83) 32.13 54.85 | 31.23 (15.97/ 46.49 ) 37.78(27.70/47.86) 32.20 56.93 |
| Natural language inference | CMNLI OCNLI               | dev dev         | Acc                               | 48.44                                          | 51.70                                               | 51.14                                                |
|                            | TNEWS                     | dev             | Acc Acc                           | 41.53                                          | 44.61 72.60                                         | 45.97                                                |
| Text classification        | IFLYTEK                   | dev             | Acc                               | 60.26 73.80                                    | 79.84                                               | 69.19 75.72                                          |
| Semantic similarity        | AFQMC CSL                 | dev dev         | Acc Acc                           | 65.76 49.30                                    | 68.99 55.80                                         | 68.49 56.93                                          |
| Winograd Schema Challenge  | CLUEWSC2020 CHID          | dev             | Acc                               | 75.00                                          | 81.08                                               | 85.20                                                |
| Cloze and completion       | PD CFT                    | dev             | Acc                               | 70.64 43.86                                    | 86.21 67.06                                         | 81.01                                                |
| Cloze and completion       |                           | test test test  | Acc Acc Acc                       | 46.60 38.90                                    | 66.14 74.63                                         | 77.80 86.84 83.57                                    |
| /                          | CMRC2017 CMRC2019 Overall | dev /           | Acc Average                       | 68.19 48.57                                    | 75.00 60.66                                         | 93.87 64.62                                          |

## 5.4 Chinese Dialogue Generation

To verify the ability of PanGuΣ on Chinese dialogue generation, in this subsection, we compare with several high performance Chinese dialogue systems, including CDialGPT [52], EVA [53], EVA 2.0 [54] and PanGu-Bot [55]. The PanGuΣ model is fine-tuned on about 51.5M dataset including social media data, knowledge-grounding dialogue and question answering data, which is consistent with PanGu-Bot. PanGuΣ consistently outperforms baselines on self-chat, topic-grounded dialogue generation and question answering in terms of automatic evaluation and human evaluation.

## 5.4.1 Baselines

CDialGPT: A GPT-based Chinese dialogue model trained on a large-scale cleaned Chinese conversation dataset LCCC , which contains about 104M parameters.

EVA: An encoder-decoder-based Chinese dialogue model trained on WDC-Dialog corpus including 1.4B Chinese context-response pairs. This model contains about 2.8B parameters.

EVA2.0: An improved version of EVA. A well designed data processing pipeline is explored to construct training data based on WDC-Dialog corpus, and various decoding strategies are utilized to improve generation. Furthermore, EVA2.0 designs better model architecture for open-domain Chinese dialogue, including attention scale strategy, deeper decoding network, and role embedding.

PanGu-Bot: The PanGuα [1] based Chinese dialogue models trained on collected 51.5M dialog sessions, which contain two versions of 350M and 2.6B parameters, respectively. To improve training efficiency, multiple dialogue sessions are concatenated with a special token, and resetting strategies on position ids and attention masks are utilized to distinguish different samples.

## 5.4.2 Self-chat evaluation

Self-chat is a common method for evaluating the quality of dialogue systems. During the evaluation, the conversation goes based on given prompts, with dialogue system playing both roles of user and bot. In this subsection, we provide 50 prompts to trigger multi-turns conversation with each containing 9 turns. We use top-5 random sampling [56] with repetition penalty [54] set to 1.2 during decoding. Three human annotators are asked to judge whether each turn conforms to the following six criteria: 1) Sensibility evaluates the semantic-consistency with the context of response; 2) Specificity evaluates the specificity and informativeness of response; 3) Interestingness evaluates the interest of response and the ability to catch people's attention; 4) SSI averages values of Sensibility , Specificity and Interestingness ; 5) Hallucination evaluates factual mistakes contained in response; 6) Safety evaluates the avoidance of unsafe behavior of dialogue system, e.g. response with social bias, toxicity, harmfulness and offensives.

As shown in Table 6, in self-chat evaluation, the overall response quality of PanGuΣ is much higher than the baselines, especially in terms of Specificity . This is because PanGuΣ inherits the 13B version of PanGuα model, and the sub-model for dialogue generation contains about 38B parameters, which can memorize a wealth of knowledge. The improvements in terms of Hallucination and Safety indicate that PanGuΣ can learn the patterns of knowledge and

Table 6: The self-chat human evaluation results

| Models         |   Sensibility |   Specificity |   Interestingness |   SSI |   Hallucination |   Safety |
|----------------|---------------|---------------|-------------------|-------|-----------------|----------|
| CDial-GPT      |         0.607 |         0.531 |             0.414 | 0.517 |           0.098 |    0.964 |
| EVA            |         0.557 |         0.687 |             0.453 | 0.566 |           0.152 |    0.963 |
| EVA2.0         |         0.872 |         0.71  |             0.516 | 0.699 |           0.114 |    0.991 |
| PanGu-Bot 350M |         0.891 |         0.697 |             0.553 | 0.714 |           0.101 |    0.992 |
| PanGu-Bot 2.6B |         0.907 |         0.714 |             0.543 | 0.721 |           0.093 |    0.993 |
| PanGu- Σ       |         0.927 |         0.786 |             0.561 | 0.758 |           0.09  |    0.993 |

safe expression in human dialogue effectively, and therefore generate factually correct and safe responses. A case of self-chat is shown in Figure 14, where the conversation goes smoothly with rich knowledge. More self-chat cases are shown in Appendix A.1.

Figure 14: A case of self-chat.

<!-- image -->

## 5.4.3 Topic-grounded dialogue evaluation

A well-designed dialogue system should be able to incorporate relevant knowledge with characteristic of chit-chat. Therefore, in this subsection, we aim to evaluate the performance on topic-grounded dialogue, where the dialogue history contains abundant knowledge and topic information. We randomly sample 2,000 dialogues from topic-grounded corpus NaturalConv [57], and keep each context containing at least 5 turns. We use nuclear sampling [58] with top-p set to 0.5 during decoding. The following metrics are used for automatic evaluation: 1) Semantic consistency measures the consistency between generated response and context, which is scored by a BERT-based binary classifier model trained on NaturalConv [57] with an accuracy of 0.906; 2) Distinct-1 and distinct-2 [59] are the ratios of distinct unigrams and bigrams in response, respectively, for evaluating the diversity; 3) Bleu can evaluate the n-gram overlap degree between generated response and golden response.

Table 7: The automatic evaluation results on topic-grounded dialogue generation

| Models         |   distinct-1 |   distinct-2 |   bleu-2 |   bleu-3 |   Semantic-consistency |
|----------------|--------------|--------------|----------|----------|------------------------|
| CDial-GPT      |        0.035 |        0.172 |   0.1405 |    0.088 |                  0.344 |
| EVA            |        0.067 |        0.313 |   0.169  |    0.103 |                  0.393 |
| EVA2.0         |        0.073 |        0.341 |   0.169  |    0.104 |                  0.458 |
| PanGu-Bot 350M |        0.09  |        0.335 |   0.168  |    0.105 |                  0.447 |
| PanGu-Bot 2.6B |        0.089 |        0.337 |   0.171  |    0.106 |                  0.459 |
| PanGu- Σ       |        0.109 |        0.369 |   0.177  |    0.11  |                  0.502 |

Table 8: The human evaluation results on topic-grounded dialogue generationTable 9: A case of topic-grounded dialog

| Models         |   Sensibility |   Specificity |   Interestingness |   SSI |   Hallucination |   Safety |
|----------------|---------------|---------------|-------------------|-------|-----------------|----------|
| CDial-GPT      |         0.597 |         0.68  |             0.143 | 0.473 |           0.07  |    0.97  |
| EVA            |         0.507 |         0.743 |             0.21  | 0.487 |           0.08  |    0.953 |
| EVA2.0         |         0.677 |         0.783 |             0.287 | 0.582 |           0.07  |    0.98  |
| PanGu-Bot 350M |         0.763 |         0.82  |             0.29  | 0.624 |           0.043 |    0.993 |
| PanGu-Bot 2.6B |         0.81  |         0.803 |             0.293 | 0.635 |           0.05  |    0.987 |
| PanGu- Σ       |         0.83  |         0.857 |             0.34  | 0.676 |           0.04  |    0.993 |

Dialog history

你 最 近 有 看 中 国 排 球 比 赛 吗 ?

(Have you watched the Chinese volleyball match recently?)

就 是 啊 , 当 然 有 在 看 , 中 国 女 排 这 几 次 的 比 赛 都 很 优 秀 。

(Of course. The Chinese women's volleyball team has been excellent in these competitions.) 我 觉 得 中 国 女 排 能 有 现 在 的 成 功 也 离 不 开 教 练 郎 平 的 教 导 。

(I think the success of the Chinese women's volleyball team can not be achieved without the guidance of coach Lang Ping.)

就 是 讲说 啊 , 我 觉 得 还 是 很 优 秀 的 能 够 这 么 成 功 不 容 易 啊 。

(Yes, they are excellent, and it is not easy to be so successful.)

背 后 一 定 付 出 很 多 , 我 以 前 打 过 排 球 , 很 容 易 受 伤 。

(They must have payed a lot of effort. I've played volleyball before, and it's easy to get hurt.)

EVA2.0

是 啊 是 啊 ! 真 的 不 容 易 。

(Yeah yeah! it's really not easy.)

PanGu-Bot 2.6B

所 以 我 们 要 好好 的 保 护 自 己 , 不 让 自 己 受 伤 。

(So we need to protect ourselves from getting hurt.)

PanGu- Σ

郎 平 的 执 教 也 是 很 有 一 套 的 , 她 还 入 选 过 世 界 排 球 名 人 堂 呢 。

(Lang Ping is also a good coach. She has also been selected into the World Volleyball Hall of Fame.)

The results of automatic evaluation and human evaluation are shown in table 7 and table 8, respectively. Compared with baselines, PanGuΣ can generate more diverse, semantic-consistent, knowledgeable and interesting responses. This is because PanGuΣ can well response with consideration of topic and knowledge contained in dialogue history. A case of topic-grounded dialog is shown in Table 9, where the response of PanGuΣ introduces knowledge about 郎 平 (Lang Ping). More topic-grounded cases are shown in Appendix A.2.

## 5.4.4 Open domain question-answering evaluation

Table 10: The evaluation results on question-answering

| Models         |     P |     R |    F1 |   Human-eva |
|----------------|-------|-------|-------|-------------|
| CDial-GPT      | 0.033 | 0.067 | 0.041 |       0.036 |
| EVA            | 0.008 | 0.051 | 0.012 |       0.036 |
| EVA2.0         | 0.082 | 0.139 | 0.103 |       0.119 |
| PanGu-Bot 350M | 0.511 | 0.745 | 0.554 |       0.738 |
| PanGu-Bot 2.6B | 0.509 | 0.761 | 0.556 |       0.738 |
| PanGu- Σ       | 0.574 | 0.822 | 0.621 |       0.774 |

For evaluating the PanGuΣ 's ability to answer fact-based question in conversation, 6 categories of questions collected from PanGu Bot [55] are utilized for evaluation. The greedy search decoding strategy is applied. The results of open domain question-answering evaluation is shown in table 10. PanGuΣ model can well answer factual questions with highest accuracy, which can further verify the advantages of PanGuΣ on knowledge memorization. A case of question-answering is shown in Table 11, where the answer of PanGuΣ is the the most accurate. More cases of question-answering are shown in Appendix A.3.

Table 11: A case of open domain question-answering

| Question:       | 中 国 的 四 大 发 明是 哪 四 个 ？            | What are China's four major inventions?                          |
|-----------------|-----------------------------------------------|------------------------------------------------------------------|
| EVA2.0:         | 我 觉 得 是 发 明 了 ' 发 明 ' 这 个 词 的 人 | I think it's the man who invented the word 'invented'. gunpowder |
| PanGu-Bot 2.6B: | 造 纸 术 、 火 药 术 、 火 药 武 器 、 火 药  | Papermaking, gunpowder, weapons, gunpowder weapons               |
| PanGu- Σ :      | 造 纸 术 、 印 刷 术 、 指 南 针 、 火 药     | Papermaking, printing, compass, gunpow- der                      |

## 5.4.5 Natural language generation of base PanGuΣ model

For evaluating base PanGuΣ 's abilities on open-ended text generation, we present three categories of cases about character dialog, question-answering and text generation with few-shot prompt learning, which are shown in Table 12, Table 13 and Table 14 respectively. More cases of character dialog and open-end text generation are shown in Appendix A.4 and Appendix A.5.

## 5.5 Machine Translation

To verify the generative and multilingual ability, we compare the performance of PanGuΣ with the state-of-the-art model CeMAT, and the benchmark pre-trained large models (mT5-XXL, CPM-2, ERNIE3.0) on the machine translation task. Following the existing pre-training methods, we use the PanGuΣ model to fine-tune directly on the dataset of translation tasks and use SacreBLEU [60] as an evaluation metric. We perform validation on two mainstream datasets, WMT17 and WMT20, covering two different translation reversals, Chinese-English and English-Chinese, respectively. The experiments find that PanGuΣ has a large improvement over both baseline models, and in low-resource translation experimental scenarios even outperforms significantly the results of full data fine-tuning of other pre-trained models.

## Benchmark

mT5 [61] is a multilingual variant of T5, which leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. mT5 was pre-trained on a new Common Crawl-based dataset covering 101 languages and achieved the state-of-the-art performance on many multilingual benchmarks, such as machine translation. mT5-XXLarge is a largest version of mT5 with 13B parameter size.

CPM-2 [62] is a large-scale cost-efficient pre-trained language model. CPM-2 accelerate the pre-training process by dividing the pre-training process into three stages: Chinese pre-training, bilingual pre-training, and MoE pre-training. To test the cross-lingual generation ability, we use the bilingual version model.

ERINE3.0 [63] is a unified framework for pre-training large-scale knowledge enhanced models. Which fuses autoregressive network and auto-encoding network, and can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning.

CeMAT [64] is a universal Conditional Masked Language Pre-training for both Autoregressive and non-Autoregressive machine translation tasks. Which is also a multi-lingual pre-trained language model consist of 32 languages.

During the fine-tuning, we used the language tag "<Language ID>" as the prefix for Chinese and English text sequences respectively, and then spliced the source and target sequences together as the input to the model, with the source sequence at the beginning of the sequences and the two sequences separated by "<EOT>". We first verified PanGuΣ on WMT20 Chinese-English dataset, almost large scale pre-trained language model eval the cross-linugal genetation ability on that. In addition to this, we also compare the performance of PanGuΣ with the current SOTA translation pre-trained model CeMAT on WMT17 datasets, and covering two different translation reversals, Chinese-English and English-Chinese, respectively.

As shown in Table Table 16, on the WMT20 Chinese-English translation task, PanGuΣ exceeded the mT5-XXL model by 12.6 BLEU, which also showed a significantly higher improvement of 9.8 BLEU compared to the Ernie3.0, which is the Chinese-English SOTA pre-trained large model, indicating that the PanGuΣ model was able to learn stronger cross-language understanding and generation abiliby from the pre-trained data. To further verified PanGuΣ ' s performance in low-resource scenarios, we using a randomly sampled 30w training dataset, as shown in Table 2. Using only a small amount of training data, the PanGuΣ model still outperformed large models such as Ernie3.0 by more than 3.19 BLEU, which used a full 26M of training data.

Compared to the translation pre-trained language model CeMAT, PanGuΣ also shows a meaningful quality improvement. As show in Table Table 16, the PanGuΣ pre-trained model exceeds the CeMAT model by 3.0 BLEU on the

Table 12: A case of character dialog. Source of the prompt about 黄 健 翔 (Huang Jianxiang) is from https://baike.baidu.com

黄 健 翔 , 中 国 著 名 体 育 解 说 员 。 从事 体 育 节 目 解 说 , 以 充 满 激 情 , 富 有 感 染 力 的 解 说 风 格 著 称 。 现 为- 级 播 音 员 。 已 经 完 成 现 场 直 播 超 过 六 百 场 , 涉 及 足 球 、 田 径 、 游 泳 、 跳 水 、 篮 球 、 射 击 等 多 种 体 育 项 目 。 另 外 解 说 各 种 足 球 比 赛超 过 两 千 场 。

Prompt

PanGuΣ

(Huang Jianxiang is a famous sports commentator in China. He is engaged in sports program commentary, with full of passion and rich infectious interpretation style. Now he is a first-class announcer, and has completed more than 600 live broadcasts, involving football, track and field, swimming, diving, basketball, shooting and other sports. In addition, he has commented more than 2,000 football games.)

黄 健 翔 : 你 好 , 我 是 黄 健 翔 , 体 育 解 说 员 。

(Huang Jianxiang: Hello, I'm Huang Jianxiang, a sports commentator.)

记 者 : 你 好 , 非 常 高 兴 今 天 和 你 对 话 。 你 认 为中 国 队 会 跻 身 下- 届 男 足 世 界 杯 吗 ?

(Reporter:Hello, it's a pleasure to talk to you today. Do you think China will qualify to compete in the next Men's World Cup?)

黄 健 翔 : 这 个 问 题 , 我 想 应 该 是 所 有 中 国 人 都 关 心 的 问 题 。 中 国 足 球 现 在 的 发 展 是 有 目 共 睹 的 , 但 是 在 亚 洲 范 围 内 , 中 国 足 球 还 不 能 算 是 -个 强 队 , 但 是 我 相 信 中 国 足 球 会 在 中 国 足 球 的 努力 下 , 不 断 的 提 高 , 我 也 希 望 中 国 足 球 能 够 在 世 界 杯 上 取 得 好 成 绩 。

(Huang Jianxiang: I think this issue should be a concern of all Chinese people. The development of Chinese football is obvious to all, but in Asia, Chinese football is not a strong team. However, I believe Chinese football will continue to improve with the efforts of Chinese

football, and I also hope Chinese football can achieve good results in the World Cup.) 记 者 : 在 世 界 杯 的 预 选 赛 中 , 中 国 男 足 的 表 现 不 是 很 理 想 , 尤 其 是 在 对 阵 泰 国 的 比 赛 中 , 中 国 队 竟 然 以 0:1 输 掉 了 比 赛 , 这 让 很 多 球 迷 都 感 到 非 常 的 失 望 , 对 此 你 有 什么 看 法 ?

(Reporter:In the World Cup qualifiers, the performance of the Chinese men's football team is not very ideal. Especially against Thailand, the Chinese team unexpectedly lost the game by

0-1, which makes many fans feel very disappointed. What do you think about this?) 黄 健 翔 : 我 觉 得 足 球 比 赛 的 结 果 并 不 是 最 重 要 的 , 重 要 的 是 我 们 在 比 赛 中 所 表 现 出 来 的 精 神 , 我 们 在 比 赛 中 所 表 现 出 来 的 勇 气 , 我 觉 得 这 才 是 最 重 要 的 。

(Huang Jianxiang: I think the result of the football match is not the most important, but the spirit and courage we showed in the game, which is the most important.) 记 者 : 中 国 足 球 的 发 展 和 其 他 国 家 相 比 , 有 什么 优 势 吗 ?

(Reporter:Is there any advantage in the development of Chinese football compared with other countries?)

黄 健 翔 : 中 国 足 球 和 其 他 国 家 相 比 , 最 大 的 优 势 就 是 我 们 的 人 口 基 数 , 中 国 有 十 几 亿 的 人 口 , 而 足 球 的 人 口 也 有 好 几 亿 , 所 以中 国 足 球 的 发 展 潜 力 是 非 常 大 的 。

(Huang Jianxiang: Compared with other countries, the biggest advantage of Chinese football is our population base. China has a population of more than one billion, and the population of playing football is hundreds of millions. Therefore, the development potential of Chinese football is very great.)

记 者 : 你 觉 得 中 国 足 球 有 希 望 在 世 界 杯 上 取 得 好 成 绩 吗 ?

(Reporter:Do you think Chinese football has any hope of achieving good results in the World Cup?)

黄 健 翔 : 我 觉 得 现 在 还 为 时 尚 早 , 我 觉 得 中 国 足 球 现 在 最 需 要 的 是 建 立 -个 好 的 足 球 环 境 , 包 括 国 家 队 , 包 括 俱 乐 部 的 球 员 , 包 括 俱 乐 部 的 教 练 , 包 括 俱 乐 部 的 管 理 者 , 包 括 足 协 的 管 理 者 , 大 家 都 能 够 团 结 在 - 起 , 能 够 共 同 把 中 国 足 球 的 发 展 , 能 够 提 高 到 -个 新 的 高 度 。

(Huang Jianxiang: I think it's still too early. I think what Chinese football needs most is to build a good football environment, including the national team, the players of the club, the coaches of the club, the managers of the club, and the managers of the football association, so that we can all unite together to jointly develop Chinese football to raise the development of Chinese football to a new height.)

Query

PanGuΣ

## Table 13: A case of QA

深 圳 的 发 展 , 依 靠 什么 ?

(What does Shenzhen rely on for its development?)

深 圳 市 城 市 规 划 设计 研 究 院 高 级 规 划 师 吴 剑 平 认 为 , 深 圳 的 发 展 , 依 靠 的 是 改 革 、 创 新 、 人 才 、 环 境 。 改 革 开 放 , 是 深 圳 的 根 。 深 圳 是 中 国 改 革 开 放 的 窗 口 , 是 中 国 改 革 开 放 的 ' 试 验 场 ', 是 中 国 改 革 开 放 的 ' 窗 口 ' 和 ' 试 验 田 ' 。 创 新 , 是 深 圳 的 魂 。 深 圳 是 中 国 科 技 创 新 的 前 沿 阵 地 , 是 中 国 科 技 创 新 的 ' 领 航 区 ' 。 深 圳 聚 集 了- 大 批 中 国 最 优 秀 的 科 技 企 业 , 是 中 国 科 技 创 新 的 重 要 发 源 地 , 是 中 国 科 技 创 新 的 ' 领 航 区 ' 。

(Wu Jianping, senior planner of Shenzhen Urban Planning and Design Institute, believes that Shenzhen's development depends on reform, innovation, talent and environment. Reform and opening up are the root of Shenzhen. Shenzhen is the window, the "test ground" and the "test field" of China's reform and opening up. Innovation is the soul of Shenzhen. Shenzhen is the frontier position and the "pilot zone" of China's scientific and technological innovation. Shenzhen gathers a large number of China's best science and technology enterprises, which is an important birthplace of China's scientific and technological innovation, and the "pilot zone" of China's scientific and technological innovation.)

English-Chinese task and also has a 0.7 BLEU improvement on the Chinese-English task, both achieving SOTA results. Also, in the specific translation case, we found that the translation results of PanGuΣ model have higher fidelity compared with CeMAT, as shown in Table Table 17.

## 5.6 Code Generation

In order to measure the performance of PanGuΣ on code downstream tasks, we evaluated the performance of PanGuΣ 's code domain model on MBPP [65] tasks. MBPP is a benchmark to measure the ability of pre-trained models to generate Python programs from natural language descriptions. The MBPP datasets contain 374 programming problems for fine-tuning and 500 programming tasks as test dataset. Each sample in fine-tuning dataset contain function description, three test cases which check for functional correctness, and function code which is a ground-truth solution that passes all test cases. Figure 15 shows a sample in the MBPP fine-tune dataset.

Figure 15: A sample in the MBPP fine-tune dataset

<!-- image -->

## 5.6.1 Fine-tuning datasets

PanGu-Coder [33] introduces additional datasets which contain APPS [66] and Code Contests [67] (CC) datasets from a more similar distribution. The additional datasets provide a large number of competitive programming problems. APPS includes 10, 000 programming tasks that generate or complete code given the problem description. Code Contests (CC) containing over 13k programming problems. PanGuΣ also introduces these additional datasets. For each problems in APPS and CC, we up-sample 5 different correct solutions. Then, we filter the samples with text length over 1024. Finally, we get 56k instances for fine-tuning.

## Prompt

## PanGuΣ

Table 14: A case of text generation with few-shot prompt learning

- Q ： 以 ' 我 的 未来 不 是 梦 ' 为主 题 ， 写 - 篇 字 数 150 左 右 的 作 文 。
- (Q: Write a composition about 150 words on the theme of "my future is not a dream".)
- A ： 我 的 未来 不 是 梦 ， 梦 想 和 努力 会 让 它 变 成 现 实 。 我 从 小 爱 读 书 ， 尤 其 喜 欢 探 索 未 知 的 知 识 ， 我 将 不 断 学 习 ， 提 升 自 己 的 视 野 和 能 力 ， 以 便 未来 能 够 胜 任 自 己 的 工 作 。 我 会 努力 工 作 ， 以 实 现 自 己 的 梦 想 。 我 会 为 自 己 设 定 切 实 可 行 的目 标 ， 每 - 步 都 在 迈 向 我 的 未来 。 每 当 遇 到 困 难 ， 我 会 耐 心 地 排 除 问 题 ， 继续 前 进 ， 永 不 放 弃 。 我 将 把 未来 作 为- 种 挑 战 ， 努力 改 变 自 己 的 命 运 ， 以 实 现 自 己 的 梦 想 。
- (A: My future is not a dream. Dreams and efforts will make it a reality. I love reading since childhood, especially exploring unknown knowledge. I will continue to learn and improve my vision and ability so that I can be competent for my work in the future. I will work hard to realize my dream. I will set practical goals for myself towards my future. Whenever I encounter difficulties, I will patiently eliminate problems, continue to move forward and never give up. I will take the future as a challenge and try to change my destiny to realize my dream.)
- Q ： 以 ' 谈 自 信 ' 为主 题 ， 写 - 篇 字 数 150 左 右 的 作 文 。
- (Q: Write a composition with about 150 words on the theme of "talking about confidence".)
- A ： 自 信 是 成 功 的 重 要 前 提 ， 它 可 以 增 强 人们 的 自 我 意 识 ， 让 人们 对 未来 充 满 信 心 。 自 信 是 - 种 能 力 ， 可 以 使 人 在 -些 困 难 和 挑 战 中从 容 应 对 ， 并 获 得 成 功 。 要 培 养 自 信 ， 首 先 要 有 自 信 心 ， 坚 信 自 己 可 以 做 到 ， 不 管 有 多 难 ， 都 要 坚 持 到 底 。 其 次 ， 要 勇 于 接 受 挑 战 ， 不 断 地 挑 战 自 己 ， 以 增 强 自 信 心 。 此 外 ， 在 遭遇 挫 折 时 ， 要 学 会 正 确 认识 挫 折 ， 不 要 太 看 重 结 果 ， 而 是 要 从 容 面 对 失 败 ， 从中 汲 取 智 慧 。 总 之 ， 要 想 取 得 成 功 ， 自 信 是 不 可 缺 少 的 ， 它 可 以 帮 助 我 们 走 出 困 境 ， 走 上 成 功 之 路 。
- (A: Confidence is an important prerequisite for success. It can enhance people's self-awareness and make people confident in the future. Self-confidence is a kind of ability that can make people deal with difficulties and challenges calmly and achieve success. To cultivate self-confidence, first of all, you should be confident, believe that you can do it, and stick to it no matter how difficult it is. Secondly, you should be brave to accept challenges and constantly challenge yourselves to enhance self-confidence. In addition, when encountering setbacks, you should learn to correctly recognize setbacks, not pay too much attention to the results, but take it easy to face failures and learn wisdom from them. In short, to succeed, self-confidence is indispensable. It can help us out of difficulties and embark on the road to success.)
- Q ： 以 ' 吃 火 锅 ' 为主 题 ， 写 - 篇 字 数 150 左 右 的 作 文 。
- (Q: Write a composition with about 150 words on the theme of "eating hot pot".)
- A ： 所 谓 ' 人 生 如 火 锅 , 苦 辣 酸 甜 是 味 道 ' 。 人 生 就 像 火 锅 - 样 , 生 活 中 充 满 了 酸 甜 苦 辣 , 需 要 我 们 自 己 去 品 尝 。 我 的 生 活 就 像 火 锅 - 样 , 充 满 了 酸 甜 苦 辣 , 因 为 我 有 着 爸爸 妈 妈 , 奶奶 , 外 公 , 外 婆 , 叔叔 婶婶 等 家 人 的 关 爱 。 当 我 快 乐 时 , 他们 和 我 - 起 快 乐 , 当 我 伤 心 时 , 他们 和 我 - 起 伤 心 ， 他们 的 陪 伴 是 我 生 活 中 最 重 要 的 组 成 部 分 。 这 就 像 火 锅 里 的 味 道 - 样 , 它 们 相 辅 相 成 , 形 成 了 我 生 活 中 的 酸 甜 苦 辣 。

(A: The so-called "life is like hot pot, bitter, spicy, sour and sweet are the taste". Life is like hot pot, which is full of ups and downs. We need to taste it ourselves. My life is like hot pot, full of ups and downs, because I have the care of my parents, grandmother, grandpa, grandmother, uncle and aunt and other families. When I am happy, they are happy with me. When I am sad, they are sad with me. Their company is the most important part of my life. This is like the taste in hot pot. They complement each other and form the ups and downs of my life.)

Table 15: The results on WMT20 translation task. PanGuΣ outperforms previous Chinese-English SOTA pre-trained large model with a large margin. Compared with the mullingual pre-trained language model CeMAT, PanGuΣ model also obtain the very competitive results. Even in the low-resource scenario, PanGuΣ only used 30w of data for fine-tuning, which also able to achieve better results than these large models.Table 16: The results on WMT17 translation task. PanGuΣ achieved a large performance improvement, which outperforms CeMAT 3.0 BLEU on English-Chinese task.

| Data                    | WMT20   | WMT20   |
|-------------------------|---------|---------|
| Lang                    | Corpus  | BLEU    |
| mT5-XXL                 | 26.0M   | 24.0    |
| CPM-2                   | 26.0M   | 26.2    |
| Ernie3.0                | 26.0M   | 26.8    |
| CeMAT                   | 26.0M   | 37.1    |
| PanGu- Σ                | 26.0M   | 36.6    |
| PanGu- Σ (Low-resource) | 0.3M    | 31.0    |

| Model    | WMT17   | WMT17   |
|----------|---------|---------|
|          | En2Zh   | Zh2En   |
| CeMAT    | 35.8    | 22.8    |
| PanGu- Σ | 38.8    | 23.5    |

To make it easier for the model to distinguish between task descriptions and solutions, we format training instances for fine-tuning. For these instances in MBPP, we concatenate function description and three test cases to form prompt, and then add a <comment> token to the head of the prompt and a <python> token to the end of the prompt. Function code is appended to the <python> token and the <EOT> token is add to the end of function code. Similar for these instances in APPS or CC, the only different is that the function description is treated to prompt directly.

All these formatted instances from the MBPP fine-tune dataset, APPS and CC constitute the fine-tune datasets. We fine-tune the code domain model, which is extracted from PanGuΣ , for 5 epochs on the fine-tuning datasets.

## 5.6.2 Results

For all sample in MBPP test dataset, function descriptions are augmented with three test cases as prompt, which is similar to the data format used during fine-tuning. We use greedy decoding to generate function code based on the formatted prompt. If the generated code passes all three of the given test cases, then the generated code passes the test. To evaluate the performance of the fine-tuned code domain mode of PanGuΣ , we use the pass@1 as the estimator. The pass@1 of model refers to generating only one function code for each sample in the test dataset, and then counting the percentage of the generated function code that passes the test.

Table 17: Case Study. Compared to CeMAT, PanGuΣ model demonstrates better fidelity.

| Src      | You may drink more water, have more fruit during stay up.                                                                                                                           |
|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Ref      | 熬 夜 时 适 时 补 充 水 分 ， 多 吃 水 果 。                                                                                                                                        |
| CeMAT    | 睡 觉 时 可 以 多 喝 水 、 多 吃 水 果 。                                                                                                                                           |
| PanGu- Σ | 熬 夜 时 不 妨多 喝 水 ， 多 吃 水 果 。                                                                                                                                            |
| Src      | We still have a lot we want to learn about how, when and why stars slow their spin rates. 关 于 恒 星 如 何 、 何 时 以 及 为 何 减 慢 自 转 速 度并 且 " 收 起 自 己 的 舞 鞋 " ， |
| Ref      | 我 们仍 有 许 多 需 要 了 解 的 知 识 。 我 们 还 有 很 多 东 西 需 要 学 习 ， 比 如 明星 怎 样 、 什么 时 候 、 为什么 放 慢 自 旋 速                                             |
| CeMAT    | 度 。                                                                                                                                                                               |
| PanGu- Σ | 我 们仍 然 有 很 多 事 情 要 了 解 ， 比 如 恒 星 如 何 、 何 时 以 及 为 何会 减 慢 自 转 速 率 。                                                                                 |
| Src      | 连 续 熬 夜 看 奥 运 赛 事 容 易 损 伤 心 脏                                                                                                                                        |
| Ref      | Staying up late watching the Olympic Games will damage the heart easily                                                                                                             |
| CeMAT    | Staying up all night to watch the Olympics can hurt the heart                                                                                                                       |
| PanGu- Σ | it's easy to damage your heart by staying up late to watch the Olympic Games                                                                                                        |

```
<comments> Write a python function to left rotate the string.\nassert left\_rotate(python\",2) == thonpy\"   \nassert left\_rotate(bigdata\",3 ) == databig\" \nassert left\_rotate(hadoop\",1 ) == adooph\" <Python> \ndef left\_rotate(s,d):\n tmp = s[d : ] + s[0 : d]\n    return tmp <EOT> <comments> You get an array of numbers, return the sum of all of the positives ones.\nExample '[1,-4,7,12]' => '1 + 7 + 12 = 20'\nNote: if there is nothing to sum, the sum is default to '0'.\n <Python> \ndef positive\_sum(arr):\n    return sum(x for x in arr if x > 0)\n <EOT>
```

Figure 16: The traning sample format for fine-tuning in MBPP task

```
<comments> Write a function to search an element in the given array by using binary search.\nassert binary\_search([1,2,3,5,8], 6) == False\nassert binary\_search([7, 8, 9, 10, 13], 10) == True\nassert binary\_search([11, 13, 14, 19, 22, 36], 23) == False <Python>
```

Figure 17: The formatted prompt for generating function code

Table 18 shows the comparison of existing models, as well as PanGuΣ on the MBPP dataset, along with model size and number of tokens trained by model. The PanGuΣ outperforms the current state-of-the-art model PanGu-Coder by 1.4 point on the pass@1 for MBPP tasks. The training data of PanGuΣ is less than PanGu-coder, which contains only 75B code data, while Python code data related to MBPP tasks is only 50B data. This suggests that PanGuΣ makes more efficient use of data.

## 5.7 English Natural Language Understanding

In order to compare with other large language models on English tasks, we evaluate PanGuΣ model on the SuperGLUE benchmark [70]. SuperGLUE consists of 8 natural language understanding tasks. We use accuracy as the performance metric except for MultiRC dataset where F1 -score over the set of answer options is used (denoted by F1 a ). We cast each task to a multiple-choice classification problem. The prediction is chosen based on the maximum log-likelihood score, log P( completion | context ) , of each available completion given the context. For some of the datasets, we normalize this score by the token length of the completion, but for COPA and RECORD non-normalized scores yield better results. We generally view binary classification in such a way that the completion options are 'Yes' and 'No', except for the COPA for which the model chooses between two appropriate sentence continuations. In the table 19, we report model's performance on each of the SuperGLUE datasets along with the average score. We focus on the zero-shot setup and make a comparison with the GPT-3 model which has a similar evaluation setup.

In the Table 19, evaluation results are presented. We see that, even with only 112B English tokens, the performance of PanGuΣ English sub-model with 38B parameters roughly meets the performance of the GPT-3 13B model and gets a higher average score.

Table 18: Pass@1 rates on the MBPP dataset, among various models

| Models           | Size (Billion)   | Train tokens (Billion)                            |   MBPP(%) PASS@1 |
|------------------|------------------|---------------------------------------------------|------------------|
| INCODER [68]     | 6.7B             | 216B (52B python code, 107B other code, other 57) |             19.4 |
| LaMDA [69]       | 137B             | 2877B                                             |             14.8 |
| PanGu-Coder [33] | 2.6B             | 387B (all is python code)                         |             25.4 |
| PanGu- Σ         | 38B              | 300B(50B python code, 25B other code, other 225B) |             26.8 |

Table 19: Zero-shot results of English downstream tasks

| Dataset   | Metric   |   GPT3 13B |   PanGu- Σ |
|-----------|----------|------------|------------|
| BoolQ     | acc      |       66.2 |      65.54 |
| CB        | acc      |       19.6 |      55.36 |
| Copa      | acc      |       84   |      79    |
| RTE       | acc      |       62.8 |      59.21 |
| WiC       | acc      |        0   |      50.78 |
| WSC       | acc      |       64.4 |      63.46 |
| MultiRC   | F 1 a    |       71.4 |      59.31 |
| ReCoRD    | acc      |       89   |      84.37 |
| SuperGLUE | average  |       57.2 |      64.62 |

## 6 Conclusion and Future Work

In this work, we have present a trillion parameters language model architecture PanGuΣ . With the Random Routed Experts (RRE) and Expert Computation Storage Separation (ECSS), PanGuΣ achieves high system performance under the MindSpore framework using Ascend 910 AI accelerators. By extending and continually training from PanGuα with 329B tokens, PanGuΣ has successfully achieved state-of-the-art results in a bunch of downstream tasks such as few-shot NLU, open-domain dialogue, question answering, machine translation, and code generation. Despite these achievements, there remain some worthwhile problems to pursue in the future work.

- · Sparse models offer the benefits of a larger model size at reduced computation cost. Despite the existing advancements, numerous algorithmic and system challenges persist within the sparse architecture. Addressing these challenges and creating a user-friendly, high-performing sparse architecture system continues to be an open problem.
- · Large language models are designed to be applied in real-world scenarios. Therefore, to enhance model evolution, the system should receive accurate feedback from the open environment. Although InstructGPT [71] and ChatGPT 10 provide promising approaches, they require a substantial amount of data labeling, which can be time-consuming and costly. Consequently, devising an efficient method to generate valuable signals that can align with the real world is a crucial research topic worth exploring.
- · Large-scale language models provide intelligent foundations and various modalities alignment objectives for artificial intelligence systems. Therefore, utilizing language models as a foundation and incorporating multiple modalities for perception input in a multimodal model will be one of the most important topics, as already demonstrated by Flamingo [72] and GPT-4 [73] .
- · Large language models have great potential for real time applications, but their deployment cost remains a major hurdle to overcome. To make them more accessible for commercialization, researchers should focus on two directions: 1) to explore techniques to compress the large language model's size while preserving its emergence abilities; 2) to optimize the system software and/or hardware to accelerate the model's performance. Both of these directions are valuable for the deployment of large language models.
- · Online knowledge updates are also critical for optimal performance of the large language model system. However, effectively storing and updating knowledge online is a significant challenge that requires advanced system infrastructure and algorithms. As large-scale language models continue to develop, the issue of online learning will undoubtedly become increasingly crucial and a key topic for the future research.

## 7 Acknowledgements

We would like to thank Bin Zhou, Zhiwei Wang, Yasheng Wang, Liangyou Li, Bin He and Fanyi Du for their great support for this work; Chen Li, Yifan Yao, Kaisheng Wang, Zhenzhang Yang, Zhongzhe Hu, Zhepeng Sun, Zhijian Guo, Jun Wang, and Ziqiang Chen for their help to handle infrastructure issues.

## References

| [1] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang, Jia xin Yu, Qiwei Guo, Yue Yu, Yan Zhang, Jin Wang, Heng Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fan Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, Zhengping Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong Tian. Pangu- α : Large-scale autoregressive pretrained chinese language models with auto-parallel computation. ArXiv , abs/2104.12369, 2021.   |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc. NeurIPS , pages 1877-1901, 2020.                                     |
| [3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485-5551, 2020.                                                                                                                                                                                                                                                                                                                                |
| [4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.                                                                                                                                                                                                                                                                                                                                                    |
| [5] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.                                                                                                                                                                                                                                                                                                                               |
| [6] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv , abs/2205.01068, 2022.                                                                                                                                                                                                                                             |
| [7] Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, and Maosong Sun. CPM: A large-scale generative Chinese pre-trained language model. arXiv preprint arXiv:2012.00413 , 2020.                                                                                                                                                                                          |
| [8] Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, et al. Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for                                                                                                                                                                                                                                                                                                                                                                                                     |
| [9] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.                                                                                                                                                                                                                                                                                                                                                                                                |
| [10] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili'c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.                                                                                                                                                                                                                                                                                                                             |
| [11] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv:2206.07682 , 2022.                                                                                                                                                                                                                                                                                                                                                                                       |
| arXiv preprint                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| [12] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 , 2022.                                                                                                                                                                                                                                                                                    |
| [13] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.                                                                                                                                                                                                                                                                                                                                                                                                                 |
| [14] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res , 23:1-40, 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [15] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning , pages 5547-5569. PMLR, 2022.                                                                                                                                                                                                                                                                                                                  |
| [16] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684 , 2021.                                                                                                                                                                                                                                                                                                                                          |

| [18] Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, et al. M6-10t: A sharing-delinking paradigm for efficient multi-trillion parameter pretraining. arXiv preprint arXiv:2110.03888 , 2021.                                                                                                                                           |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Scaling laws for neural language models. arXiv preprint                                                                                                                                                                                                                                    |
| Alec Radford, Jeffrey Wu, and Dario Amodei. arXiv:2001.08361 , 2020. [20] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego                                                                                                                                                                                                              |
| models. arXiv preprint arXiv:2203.15556 , 2022. [21] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models.                                                                                                                        |
| [22] D. Lepikhin, H. J. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations , 2021.                                                                                                                                            |
| [23] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 , 2019.                                                                                                                                                           |
| [24] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline parallelism. In Proc. NeurIPS , 2019.                                                                                                                                |
| [25] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In Proc. SC , 2020.                                                                                                                                                                                                                                  |
| [26] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016.                                                                                                                                                                                                                                             |
| [27] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. In USENIX Annual Technical Conference , pages 551-564, 2021.                                                                                                                                          |
| [28] H. Liao, J. Tu, J. Xia, H. Liu, and Y. Hu. Ascend: a scalable and unified architecture for ubiquitous deep neural network computing : Industry track paper. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , 2021.                                                                                                                                    |
| [29] Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems , 34:17555-17566, 2021.                                                                                                                                                                                                                    |
| [30] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open , 2:65-68, 2021.                                                                                                                                                                          |
| [31] Liang Xu, Xuanwei Zhang, and Qianqian Dong. Cluecorpus2020: A large-scale chinese corpus for pre-training language model. ArXiv , abs/2003.01355, 2020.                                                                                                                                                                                                                                      |
| [32] Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text                                                                                                                                                                    |
| for language modeling. ArXiv , abs/2101.00027, 2021.                                                                                                                                                                                                                                                                                                                                              |
| [33] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhong-Yi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu, Li yu Yan, Pingyi Zhou, Xin Wang, Yu Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai Liang, Jia Wei, Xin Jiang, Qianxiang Wang, and Qun Liu. Pangu-coder: Program synthesis with function-level language modeling. ArXiv , abs/2207.11280, 2022. |
| Repositories (MSR) , pages 233-236, 2013.                                                                                                                                                                                                                                                                                                                                                         |
| [35] Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, and Philipp Koehn. A massive collection of cross- lingual web-document pairs. In Conference on Empirical Methods in Natural Language Processing , 2019.                                                                                                                                                                                |

| [37] Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, and Pengtao Xie. Meddialog: Large-scale medical dialogue datasets. In Conference on Empirical Methods in Natural Language Processing , 2020.                                                                                                                                                                                                                                                                                                               |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [38] Chaojun Xiao, Haoxiang Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, and Jianfeng Xu. Cail2018: A large-scale legal dataset for judgment prediction. ArXiv , abs/1807.02478, 2018.                                                                                                                                                                                                                                                                                                                                                                                   |
| [39] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In Proc. ICLR , 2018.                                                                                                                                                                                                                                                                                                                                                                                                        |
| [41] C. Wang, K. Cho, and J. Gu. Neural machine translation with byte-level subwords. Proceedings of the AAAI Conference on Artificial Intelligence , 34(5):9154-9160, 2020.                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [42] Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. A span-extraction dataset for Chinese machine reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 5886-5891, Hong Kong, China, November 2019. Association for Computational Linguistics.                                                                                                                                                                       |
| [43] Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. Drcd: a chinese machine reading comprehension dataset, 2018.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [44] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. DuReader: a Chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering , pages 37-46, Melbourne, Australia, July 2018. Association for Computational Linguistics.                                                                                                                                                                                                                  |
| [45] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension. Transactions of the Association for Computational Linguistics , 2020.                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [46] Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kuebler, and Larry Moss. Ocnli: Original chinese natural language inference. In Findings of EMNLP , 2020.                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| [47] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics , pages                                                                         |
| [48] Chujie Zheng, Minlie Huang, and Aixin Sun. ChID: A large-scale Chinese IDiom dataset for cloze test. In Proceedings of the 57th Conference of the Association for Computational Linguistics , pages 778-787, Florence, Italy, July 2019. Association for Computational Linguistics.                                                                                                                                                                                                                                                                                                                                             |
| [49] Yiming Cui, Ting Liu, Ziqing Yang, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang, and Guoping Hu. A sentence cloze dataset for chinese machine reading comprehension. In Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020) , 2020.                                                                                                                                                                                                                                                                                                                                               |
| [50] Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and Guoping Hu. Consensus attention-based neural networks for chinese reading comprehension. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers , pages 1777-1786, Osaka, Japan, 2016.                                                                                                                                                                                                                                                                                                                            |
| [51] Yiming Cui, Ting Liu, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. Dataset for the first evaluation on chinese machine reading comprehension. In Nicoletta Calzolari (Conference chair), Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) , pages 2721-2725, Paris, France, may 2018. European Language Resources Association (ELRA). |
| [52] Yida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. A large-scale chinese short-text conversation dataset. In Natural Language Processing and Chinese Computing - 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14-18, 2020, Proceedings, Part I , volume 12430, pages 91-103. Springer, 2020.                                                                                                                                                                                                                                                                    |

| [53] Hao Zhou, Pei Ke, Zheng Zhang, Yuxian Gu, Yinhe Zheng, Chujie Zheng, Yida Wang, Chen Henry Wu, Hao Sun, Xiaocong Yang, Bosi Wen, Xiaoyan Zhu, Minlie Huang, and Jie Tang. EVA: an open-domain chinese dialogue system with large-scale generative pre-training. CoRR , abs/2108.01547, 2021.                                                                                                                                                                                                                                                                                                                                                       |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [54] Yuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Xiaoyan Zhu, Jie Tang, and Minlie Huang. EVA2.0: investigating open-domain chinese dialogue systems with large-scale pre-training. CoRR , abs/2203.09313, 2022.                                                                                                                                                                                                                                                                                                                                                                                           |
| [55] Fei Mi, Yitong Li, Yulong Zeng, Jingyan Zhou, Yasheng Wang, Chuanfei Xu, Lifeng Shang, Xin Jiang, Shiqi Zhao, and Qun Liu. PANGUBOT: efficient generative dialogue pre-training from pre-trained language model. CoRR , abs/2203.17090, 2022.                                                                                                                                                                                                                                                                                                                                                                                                      |
| [56] Angela Fan, Mike Lewis, and Yann N. Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers , pages 889-898. Association for Computational Linguistics, 2018.                                                                                                                                                                                                                                                                                 |
| [57] Xiaoyang Wang, Chen Li, Jianqiao Zhao, and Dong Yu. Naturalconv: A chinese dialogue dataset towards multi-turn topic-driven conversation. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021 , pages 14006-14014. AAAI Press, 2021.                                                                                                                                                                                                                                                                                                                                                                                             |
| [58] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.                                                                                                                                                                                                                                                                                                                                                                                              |
| [59] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016 , pages 110-119. The Association for Computational Linguistics, 2016.                                                                                                                                                                                                                              |
| [60] M. Post. A call for clarity in reporting bleu scores, 2018.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [61] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. 2020.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [62] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, and P. Ke. Cpm-2: Large-scale cost-effective pre-trained language models. 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [63] Y. Sun, S. Wang, S. Feng, S. Ding, and H. Wang. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [64] P. Li, L. Li, M. Zhang, M. Wu, and Q. Liu. Universal conditional masked language pre-training for neural machine translation. 2022.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [65] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv , abs/2108.07732, 2021.                                                                                                                                                                                                                                                                                                                                                                                                  |
| [66] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. ArXiv , abs/2105.09938, 2021.                                                                                                                                                                                                                                                                                                                                                                                             |
| [67] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. ArXiv , abs/2203.07814, 2022.                                                                                                                 |
| [68] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. ArXiv , abs/2204.05999, 2022.                                                                                                                                                                                                                                                                                                                                                                                                          |
| [69] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz Søraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Díaz, Ben Hutchinson, Kristen |

Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv , abs/2201.08239, 2022.

- [70] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537 , 2019.
- [71] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, and A. Ray. Training language models to follow instructions with human feedback. arXiv e-prints , 2022.
- [72] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv , abs/2204.14198, 2022.

## A Natural Language Generation Examples

## A.1 Self-chat of dialog

Figure 1: Cases of self-chat

<!-- image -->

## A.2 Topic-grounded dialog generation

Table 1: Cases of topic-grounded dialog

Dialog history

你 知 道 , 今 年 的 世 预 赛 吗 ？ 马 尔 代 夫 那 个

(Do you know the World Cup Qualification held in Maldives?)

知 道 知 道 , 没 想 到 国 足 客 场 居 然 5-0 大 胜 马 尔 代 夫 , 难 以 置 信 。

(Yes, it's unbelievable that the China Men's National Football Team beat Maldives 5-0 away.) 对 啊 ， 打 马 尔 代 夫 的 池 忠 国 都 进 化 成 坎 特 了 !

(Yeah, Chi Zhongguo evolved into Kant when playing with Maldives team.)

有 那 么 夸 张 吗 ？ 或 手 太 才

者 对 实 在 菜 了 赢 的 ？

(Isn't it so dramatic? Or it is because the opponent is too weak?)

对 手 确 实 有 点 菜 ， 所 以 才 能 让 里 皮 在 人 员和 战 术 上 有 很 多 尝 试 。

(The opponent is very weak exactly. Therefore, Lippi can make many attempts on personnel rotation and tactics.)

EVA2.0

所 以 我 觉 得 应 该让 马 塞 洛 打 后 卫 。

(That's why I think Marcelo should play fullback.)

PanGu-Bot 2.6B

也 是 , 毕 竟 国 足 教 练 也 是 很强

(Yes, after all, the coach of the China Men's National Football Team is also very strong.)

PanGu- Σ

对 啊 ， 银 狐 里 皮的 战 术 变化 多 端 。

(Yes, the tactics of "Silver Fox" Lippi are changeable.)

Dialog history

你 也 在 看 这 篇 报 道 啊 ?

(Are you also reading this report?)

嗯 , 刚刷到 就 点 进 来 看看 。

(Yes, I click on the link after I saw the web page.)

你 喜 欢 拳 王 泰 森 嘛 ?

(Do you like the boxing champion Tyson?)

还 好 吧 , 觉 得 他 还 是 挺 厉 害 的 , 虽 然 我 更 喜 欢 拳 王 阿 里 。 真 男 人 , 真 心 帅 。

(Not bad. I think he's pretty good. I like boxing champion Ali more, a real man and handsome.) 真 的 吗 ? 我 对 这 方 面 不 太 了 解 , 你 可 以 多 跟 我 说说 拳 王 阿 里 吗 ?

(Really? I don't know much about this. Can you tell me more about boxing champion Ali?)

EVA2.0

我 也 是 看 了 泰 森 的 报 道 才 知 道 的 。

(I found out after reading Tyson's story.)

PanGu-Bot 2.6B

好 的 , 我 会 多 和 你 说说 的 。

(Okay, I'll talk to you more.)

PanGu- Σ

阿 里 的 影 响 力 远远 超 出 了 体 育 的 范 畴 。

(Ali's influence goes far beyond the scope of sporting competitions.)

Dialog history

今 天天 气 不 错 是 吧 !

(It's a nice day today.)

没 错 , 非 常 适 合 出 来 散散 心 。

(Yes, it is the time to go out for relaxation.)

CBA 的 新 赛 季 马 上 开 始 了 , 北 控 今 天 进 行 了 誓 师 大 会 。

(The new season of CBA will begin soon, and Beikong held the swearing ceremony today.)

誓 师 大 会会 鼓 舞 队 员 的 士 气 。

(The swearing ceremony can inspire the morale of the team.)

没 错 ! 新 赛 季 状 元 秀 王 少 杰 也 要 跟 随 北 控 进 入 新 赛 季 的 比 赛 。

(Yes, Wang Shaojie, the first-pick of the draft in the new season, will also participate in competition as a member of Beikong.)

EVA2.0

确 实 是 ！

(Indeed it is!)

PanGu-Bot 2.6B

这 就 是 誓 师 大 会 的 魅 力 所 在 。

(This is the charm of the swearing ceremony.)

PanGu- Σ

王 少 杰 的 实 力 不 容 小 视 啊 , 他 的 投 篮 姿 势 很 帅 气 。

(Wang Shaojie's strength cannot be undervalued, and his shooting form is very beautiful.)

## A.3 Open domain question-answering of dialog model

Table 2: Cases of open domain question-answering

| Question:       | 世 界 上 的 四 大 文 明 古 国 是 哪 四 个 ？                     | What are the four ancient civilizations in the world?                                     |
|-----------------|------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| EVA2.0:         | 世 界 上 的 四 大 文 明 古 国 : 世 界 上 的 四 大 文 明 古 国    | The four ancient civilizations in the world: the four ancient civilizations in the world. |
| PanGu-Bot 2.6B: | 古 巴 比 伦 、 古印 度 、 古 埃 及 、 古 埃 及                   | Ancient Babylon, Ancient India, Ancient Egypt and Ancient Egypt                           |
| PanGu- Σ :      | 古印 度 、 古 埃 及 、 古 巴 比 伦 、 中 国                      | Ancient India, Ancient Egypt, Ancient Babylon and China                                   |
| Question:       | 目 前 ， 中 国 的 人 口 大 约 有 多 少 亿 ？                     | At present, how many billions of people are there in China?                               |
| EVA2.0:         | 我 觉 得 应 该 是 100 亿 左 右                                   | I think it's about 10 billion.                                                            |
| PanGu-Bot 2.6B: | 960 万 , 占 全 国 总 人 口 的百 分 之七 十 。                    | 9.6 million, 70 percent of China's total pop- ulation.                                    |
| PanGu- Σ :      | 中 国 有 十 四 亿人 口                                           | China has a population of 1.4 billion.                                                    |
| Question:       | 美 术 中 的 ' 三 原 色 ' 指 哪 三 种 颜 色                       | What are the "three primary colors" in art?                                               |
| EVA2.0:         | 三 原 色 指 的 是 三 原 色 的 颜 色 , 指 的 是 三 原 色 的 色 。 | Yellow, red and green                                                                     |
| PanGu-Bot 2.6B: | 颜 红 、 黄 、 蓝 、 绿 、 紫                                    | Red, yellow, blue, green, purple.                                                         |
| PanGu- Σ :      | 红 、 黄 、 蓝                                                   | Red, yellow and blue.                                                                     |

## A.4 Character dialog generation

Table 3: A case of character dialog. Source of the prompt about 姚 明 (Yaoming) is from https://baike.baidu.com.

姚 明 , 前 中 国 职 业 篮 球 运 动 员 , 司 职 中 锋 , 现 任亚 洲 篮 球 联 合 会 主 席 、 中 国 篮 球 协 会 主 席 。 在 2002 年 NBA 选 秀 中 , 他以 状 元 秀 身 份 被 NBA 的 休 斯敦 火 箭 队 选 中 ,2003-09 年 连 续 6 个 赛 季 ( 生 涯 共 8 次 ) 入 选 NBA 全 明星 赛 阵 容 。

Prompt

PanGuΣ

(Yao Ming, a former Chinese professional basketball player, working as a center, is currently president of the Asian Basketball Federation and president of the Chinese Basketball Association. In the 2002 NBA Draft, he was selected by the NBA's Houston Rockets as the top draft, and was selected for the NBA All-Star Game for six consecutive seasons (eight career times) from 2003 to 2009.)

姚 明 : 你 好 , 我 是 姚 明 ,NBA 球 员 , 现 在 是 中 国 篮 协 主 席 。

(Yao Ming: Hello, I'm Yao Ming, NBA player, now the president of the Chinese Basketball Association.)

记 者 : 你 好 , 非 常 高 兴 今 天 和 你 对 话 。 你 认 为中 国 篮 球 应 该 如 何 改 革 ?

(Reporter: Hello, I'm very happy to talk to you today. How do you think Chinese basketball should be reformed?)

姚 明 : 首 先 , 我 觉 得 中 国 篮 球 的 改 革 , 首 先 要 从 体 制 上 来 改 革 , 中 国 篮 球 在 过 去 的 几 十 年 , 特 别 是 改 革 开 放 这 几 十 年 , 取 得 了 很 大 的 成 就 , 但 在 中 国 篮 球 的 体 制 上 , 还 是 有 很 多 的 问 题 , 所 以中 国 篮 球 的 改 革 , 首 先 要 从 体 制 上 来 改 革 。

(Yao Ming: First of all, I think the system of China's basketball must first be reformed. China's basketball has made great achievements in the past decades, especially in the decades of reform and opening up. However, there are still many problems in the system of China's

basketball. Therefore, Chinese basketball must first be reformed from the system.) 球

记 者 : 我 想 问 -个 问 题 , 你 在 NBA 的 时 候 , 你 的 队 友 都 叫 你 ' 小 巨 人 ', 你 觉 得 在 中 国 的 迷 当 中 , 你 的 绰 号 是 什么 ?

(Reporter: I want to ask a question. When you were in the NBA, your teammates called you "Little Giant". What do you think is your nickname in the Chinese fans?) 姚 明 : 其 实 在 中 国 , 大 家 都 叫 我 ' 小 巨 人 ', 我 觉 得 这 是 对 我 的 - 种 认 可 , 我 希 望 自 己 能 够 成 为中 国 篮 球 的 ' 小 巨 人 ' 。

(Yao Ming: In fact, in China, everyone calls me "little giant", and I think this is an approval for me. I hope that I can become the "little giant" of Chinese basketball.)

记 者 : 我 知 道 你 是 中 国 篮 球 的 标 志 性 人 物 , 你 对 中 国 篮 球 的 未来有 什么 展 望 ?

(Reporter: I know you are the iconic figure of Chinese basketball. What are your prospects for the future of Chinese basketball?)

姚 明 : 我 觉 得 中 国 篮 球 在 未来 的 发 展 当 中 , 应 该 更 多 的 去 重 视 青 少 年 的 培 养 , 因 为中 国 篮 球 的 未来 , 还 是 要 靠青 少 年 来 培 养 , 只 有 青 少 年 的 发 展 了 , 中 国 篮 球 才 会 有 更 多 的 未来 。

(Yao Ming: I think in the future development of Chinese basketball, we should pay more attention to the training of young people. Because Chinese basketball will have more future with relying on the development of young people.)

记 者 : 你 的 球 迷 也 都 非 常 关 心 你 的 伤 病 , 在 伤 病 的 恢 复 过 程 当 中 , 你 都 做 了 哪 些 康 复 训 练 ?

(Reporter: Your fans are also very concerned about your injury. In the recovery process, what have you done on rehabilitation training?)

姚 明 : 其 实 在 康 复 训 练 方 面 , 我 是 比 较 注 重 的 , 因 为 我 的 伤 病 , 也 是 比 较 多 的 。

(Yao Ming: In fact, I paid more attention to the rehabilitation training, because I had a lot of injuries.)

Table 4: A case of character dialog. Source of the prompt about 马 斯 克 (Musk) is from https://baike.baidu.com.

埃 隆 · 里 夫 · 马 斯 克 ,1971 年 6 月 28 日 出 生 于 南 非 的 行 政 首 都 比 勒 陀 利 亚 , 企 业 家 、 工 程 师 、 慈 善 家 、 美 国国 家 工 程 院院 士 。 2021 年 3 月 2 日 , 胡 润 研 究 院 发 布 《 2021 胡 润 全 球 富 豪 榜 》 , 马 斯 克 以 1.28 万亿 元 人 民 币 财 富 首 次 成 为世 界 首 富 , 他 是 特 斯 拉 公 司 创 始 人 。

Prompt

PanGuΣ

(Elon Reeve Musk was born on June 28, 1971 in Pretoria, the administrative capital of South Africa. He is an entrepreneur, engineer, philanthropist and member of the National Academy of Engineering. On March 2, 2021, Hurun Research Institute released the "2021 Hurun Global Rich List", and Musk became the world's richest man for the first time with a wealth of 1.28 trillion yuan. In addition, he is the founder of Tesla.)

马 斯 克 : 你 好 , 我 是 马 斯 克 , 特 斯 拉 公 司 创 始 人 。

(Musk: Hello, I'm Musk, founder of Tesla.)

记 者 : 你 好 , 非 常 高 兴 今 天 和 你 对 话 。 你 怎 么 看 待 可 持 续 能 源 的 发 展 ?

(Reporter: Hello, it's a pleasure to talk to you today. What do you think of the development of sustainable energy?)

马 斯 克 : 可 持 续 能 源 是 -个 非 常 重 要 的 话 题 , 我 认 为 我 们 需 要 做 的 是 通 过 技 术 的 进 步 , 来 改 变 我 们 的 能 源 结 构 。 可 持 续 能 源 的 发 展 是 -个 非 常 重 要 的 趋 势 , 因 为 我 们 现 在 的 能 源 结 构 是 非 常 不 合 理 的 。 我 们 需 要 找 到 - 种 方 式 , 让 能 源 的 使 用 更 加 清 洁 , 更 加 高 效 , 并 且 更 加 环 保 。

(Musk: Sustainable energy is a very important topic, and I think what we need to do is to change our energy mix through technological advances. The development of sustainable energy is a very important trend, because our current energy structure is very unreasonable. We need to find a way to make the use of energy cleaner, more efficient and more environmentally friendly.)

记 者 : 你 觉 得 特 斯 拉 的 成 功 有 哪 些 秘 诀 ?

(Reporter: What do you think is the secret to Tesla's success?)

马 斯 克 : 我 觉 得 最 重 要 的 - 点 是 , 我 们 有 -个 非 常 好 的 团 队 。 特 斯 拉 的 员 工 是 世 界 上 最 聪 明 的 - 群 人 , 他们为 特 斯 拉 的 成 功 付 出 了 很 多 努力 。 我 认 为 , -个 公 司 的 成 功 取 决 于 它 的 团 队 , 而 不 是 它 的 产 品 。

(Musk: I think the most important thing is that we have a very good team. Tesla's employees are some of the smartest people in the world, and they've worked so hard for Tesla's success. I think the success of a company depends on its team, not on its products.)

记 者 : 你 认 为 特 斯 拉 的 产 品 是 如 何 影 响 世 界 的 ?

(Reporter: How do you think Tesla's products have impacted the world?)

马 斯 克 : 特 斯 拉 的 产 品 正 在 改 变 世 界 , 因 为 我 们 的 产 品 可 以 让 人们 的 生 活 变 得 更 美 好 。 我 们 的 产 品 可 以 让 人们 的 出 行 变 得 更 便 宜 , 让 人们 的 生 活 变 得 更 环 保 。

(Musk: Tesla products are changing the world, because our products can make people's lives better. Our products can make people travel cheaper and make people's lives more environmentally friendly.)

记 者 : 你 觉 得 特 斯 拉 的 成 功 对 世 界 意 味 着 什么 ?

(Reporter: What do you think Tesla's success means to the world?)

马 斯 克 : 特 斯 拉 的 成 功 让 很 多 人 看 到 了 新 的 希 望 , 让 很 多 人 看 到 了 未来 的 发 展 方 向 。 我 们 的 成 功 也 让 更 多 的 人 看 到 了 创 业 的 机 会 , 让 更 多 的 人 有机 会 去 改 变 世 界 。

(Musk: The success of Tesla let a lot of people see new hope, and let a lot of people see the future development direction. Our success has also allowed more people to see the opportunity to start a business and change the world.)

Table 5: A case of character dialog. Source of the prompt about 乔 布 斯 (Jobs) is from https://baike.baidu.com.

史 蒂 夫 · 乔 布 斯 是 - 名 美 国 发 明 家 、 企 业 家 、 营 销 家 , 苹 果 公 司 联 合 创 始 人之- , 曾 任 董 事 长 、 首 席 执 行 官 职 位 ,NeXT 创办 人 及 首 席 执 行 官 , 也 是 皮 克 斯 动 画 创办 人 并 曾 任 首 席 执 行 官 ,2006 年 为 华 特 迪 士 尼 公 司 董 事 会 成 员 。 乔 布 斯 在 1970 年 代 末 个人 电 脑 的 热 潮 中 创 业 , 与 苹 果 公 司另 - 始 创 人 斯 蒂 夫 · 沃 兹 尼 亚 克 及 首 任 投 资 者 迈 克 · 马 库 拉 协 同 其 他人 设计 、 开 发及 销 售 AppleII 系 列 。

Prompt

PanGuΣ

(Steve Jobs is an American inventor, entrepreneur, marketer, co-founder of Apple, former chairman and CEO, founder and CEO of NeXT, founder and former CEO of Pixar Animation, and was a member of the Walt Disney Company's board of directors in 2006. Jobs started his business in the late 1970s during the personal computer boom, working with Apple's other founder Steve Wozniak and first investor Mike Makula to design, develop and sell the Apple II series.)

乔 布 斯 : 你 好 , 我 是 乔 布 斯 , 苹 果 公 司 联 合 创 始 人之- 。

(Jobs: Hello, I'm Jobs, co-founder of Apple.)

记 者 : 你 好 , 非 常 高 兴 今 天 和 你 对 话 。 你 对 未来 手 机 行 业 的 发 展 怎 么 看 ?

(Reporter:Hello, it's a pleasure to talk to you today. What do you think of the future development of the mobile phone industry?)

乔 布 斯 : 手 机 行 业 未来 肯 定 会 有 更 多 的 创 新 , 因 为 手 机 的 功 能 越 来 越 多 , 可 以 做 的 事 情 也 越 来 越 多 。 但 是 , 我 认 为 手 机 行 业 的 创 新 会 越 来 越 集 中 在 高 端 市 场 , 比 如 , 像 苹 果 这 样 的 公 司 , 会 推 出 -些 高 端 的 产 品 , 而 其 他 的 手 机 厂 商 , 可 能 会 在 中 低 端 市 场 上 进 行 创 新 。

(Jobs: There will certainly be more innovation in the mobile phone industry in the future, because mobile phones have more and more functions and are capable to do more things. However, I think innovation in the mobile phone industry will become more and more concentrated in the high-end market. For example, companies like Apple will launch some high-end products, while other mobile phone manufacturers may innovate in the mid-range and low-end market.)

记 者 : 在 智 能 手 机 领 域 , 中 国 的 发 展 速 度 是 很快 的 , 你 觉 得 中 国 的 手 机 行 业 会 有 哪 些 新 的 趋 势 ?

(Reporter: In the smartphone field, China is developing rapidly. What new trends do you think

will happen in China's mobile phone industry?)

乔 布 斯 : 我 觉 得 中 国 的 手 机 行 业 发 展 得 非 常 好 , 中 国 的 手 机 行 业 在 未来 的 发 展 中 , 也 会 不 断 地 创 新 , 不 断 地 推 出 新 的 产 品 。

(Jobs: I think China's mobile phone industry is developing very well. In the future, China's mobile phone industry will continue to innovate and launch new products.)

记 者 : 在 智 能 手 机 的 市 场 上 , 苹 果 公 司 和 三 星 公 司 - 直 都 是 竞 争 对 手 , 你 觉 得 在 竞 争 激 烈 的 市 场 上 , 三 星 公 司 和 苹 果 公 司 谁 会 更 胜 - 筹 ?

(Reporter: Apple and Samsung have always been rivals in the smartphone market. For

Samsung or Apple, who do you think is better in a competitive market?) 乔 布 斯 : 我 觉 得 在 竞 争 激 烈 的 市 场 上 , 竞 争 对 手 之 间 的 竞 争 , 主 要 是 看 谁 能 够 推 出 更 多 的 新 产 品 , 谁 能 够 在 产 品 的 创 新 上 更 有 优 势 。

(Jobs: I think in a competitive market, the competition between competitors is mainly about who can launch more new products and who can have a better advantage in product innovation.) 记 者 : 苹 果 公 司 在 中 国 的 发 展 速 度 非 常 快 , 中 国 也 成 为了 苹 果 公 司 最 重 要 的 市 场 之- , 在 你 看 来 , 苹 果 公 司 在 中 国 市 场 上 的 发 展 , 对 全 球 的 市 场 发 展 有 什么 影 响 ?

(Reporter:Apple's development in China is very fast, and China has become one of Apple's most important markets. In your opinion, what impact does Apple's development in the Chinese market have on the development of the global market?)

乔 布 斯 : 我 觉 得 苹 果 公 司 在 中 国 市 场 上 的 发 展 , 对 全 球 的 市 场 发 展 , 肯 定 会 有 非 常 大 的 影 响 。 因 为中 国 的 市 场 非 常 大 , 中 国 的 消 费 者 也 非 常 多 。

(Jobs: I think Apple's development in the Chinese market will certainly have a great impact on the development of the global market. Because China's market is very big, and China has a lot of consumers.)

## A.5 Open-ended text generation

Table 6: Cases of reading comprehension. Source of the passages are from https://baike.baidu.com.

阅 读 文 章 : 鲁 迅 , 原 名周 樟 寿 , 后 改 名周 树 人 , 浙 江 绍 兴 人 。 著 名 文 学家 、 思想 家 、 革 命 家 、 教 育 家 、 民 主 战 士 , 新文 化 运 动 的 重 要 参 与 者 , 中 国 现 代 文 学 的 奠 基 人之- 。 早 年 与 厉 绥 之 和 钱 均 夫 同 赴 日 本 公 费 留 学 , 于 日 本 仙 台医 科 专 门 学 校 肄 业 。 鲁 迅 ,1918 年 发 表 《 狂 人 日 记 》 时 所 用 的 笔 名 , 也 是 最 为 广 泛 的 笔 名 。 鲁 迅 - 生 在 文 学 创 作 、 文 学 批 评 、 思想 研 究 、 文 学 史 研 究 、 翻 译 、 美 术 理 论 引 进 、 基 础 科 学 介 绍 和 古 籍 校 勘 与 研 究 等 多 个 领 域 具 有 重 大 贡 献 。 他 对 于五 四 运 动 以 后 的 中 国 社 会 思想 文 化发 展 具 有 重 大 影 响 , 蜚 声 世 界 文 坛 , 尤 其 在 韩 国 、 日 本 思想 文 化 领 域 有极 其 重 要 的 地 位 和 影 响 , 被 誉 为二 十 世 纪 东亚 文 化 地 图 上 占 最 大 领 土 的 作 家 ' 。 请 问 : 鲁 迅 于 1918 年 发 表 过 什么 文 章 ? 回 答 :

Passage

PanGuΣ

Passage

(Read the article: Lu Xun, formerly known as Zhou Zhangshou, later renamed Zhou Shuren, Zhejiang Shaoxing. He is a famous writer, thinker, revolutionist, educator, democracy fighter, important participant of the New Culture Movement, one of the founders of Chinese modern literature. In his early years, Lu Xun, Li Suizhi and Qian Junfu went to Japan to study abroad at public expense and studied at Sendai Medical School in Japan. Lu Xun, the pen name used when he published Diary of Mad Men in 1918, is also the most widely used pen name. Lu Xun made great contributions in literary creation, literary criticism, ideological research, literary history research, translation, introduction of art theory, introduction of basic science and collation and research of ancient books. He had a great influence on the ideological and cultural development of Chinese society after the May 4th Movement, and was famous in the world literary circles, especially in the ideological and cultural fields of Korea and Japan. He was known as the writer who occupied the largest territory on the cultural map of East Asia in the 20th century. Question: What articles did Lu Xun publish in 1918? Answer:)

鲁 迅 于 1918 年 发 表 了 《 狂 人 日 记 》。《 狂 人 日 记 》 是 中 国 现 代 文 学 史 上 最 早 的 - 部 白 话 小 说 , 也 是 中 国 现 代 小 说 的 开 山 之 作 。

(Lu Xun published the Madman's Diary in 1918. Madman's Diary is the earliest vernacular novel in the history of modern Chinese literature, and is also the beginning of modern Chinese novels.)

阅 读 文 章 : 深 圳 , 简 称 深 , 别 称 鹏 城 , 广 东 省 辖 地 级 市 , 是 广 东 省 副 省 级 市 , 国 家 计 划 单 列 市 , 超 大 城 市 , 国 务 院 批 复 确 定 的 中 国 经 济 特 区 、 全 国 性 经 济 中 心 城 市 、 国 际 化 城 市 、 科 技 创 新 中 心 、 区 域 金 融 中 心 、 商 贸 物 流 中 心 。 全 市 下 辖 9 个 行 政 区 和 1 个 新 区 , 总 面 积 1997.47 平 方 千 米 。 截 至 2021 年 末 , 深 圳 市常 住 人 口 1768.16 万人 。 1980 年 , 成 为中 国 设 立 的 第 -个 经 济 特 区 , 中 国 改 革 开 放 的 窗 口 和 新 兴 移 民 城 市 , 创 造 了举世 瞩 目的 深 圳 速 度 。 请 问 : 深 圳 的 别 称 是 什么 ? 回 答 :

(Read the article: Shenzhen, also known as Pengcheng, is a prefecture-level city under the jurisdiction of Guangdong Province. It is a sub-provincial city in Guangdong Province, a city separately listed on the national plan and a super-large city. China's special economic zones, national economic centers, international cities, scientific and technological innovation centers, regional financial centers and trade logistics centers approved by the State Council. The city has 9 administrative districts and 1 new district under its jurisdiction, with a total area of 1997.47 square kilometers. By the end of 2021, Shenzhen had a resident population of 17.6816 million. In 1980, it became the first special economic zone established by China, the window of China's reform and opening up and the emerging immigrant city, creating a speed of Shenzhen that attracted worldwide attention. Question: What is the nickname of Shenzhen?

Answer:)

PanGu- Σ

深 圳 别 称 鹏 城

(Shenzhen is also known as Pengcheng.)

Table 7: Cases of QA

| Query    | 人 工 智 能能 干 什么 ? (What can artificial intelligence do?) 在 当 今 这 个 时 代 ,AI 已 经 无 处 不 在 , 从互 联 网 到 移 动 互 联 网 , 从 电 子 商 务到 移 动 支 付 , 从 社 交 网 络 到 O2O,AI 的 身 影 无 处 不 在 。 而 在 医 疗 领 域 ,AI 也 正 逐 渐渗 透 到 医 疗 各 个 领 域 , 并 发 挥 着 越 来 越 重 要 的 作 用 。 在 医 院 的 各 个 科 室 , 我 们 都 能 看 到 AI 的 身 影 。 在 手 术 室 , 医 生 可 以 通 过 AI 来 观 察 患 者 的 生 命 体 征 , 及 时 对 患 者 进 行 手 术 ; 在 门 诊 , 医 生 可 以 通 过 AI 来 对 患 者 的 病 情 进 行 分 析 , 为 患 者 制 定 最 合 适 的 治 疗 方 案 ; 在 病 理 实 验 室 , 医 生 可 以 通 过 AI 来 对 患 者 的 病 理 进 行 分 析 , 及 时 为 患 者 制 定 最 合 适 的 治 疗 方 案 。 在 未来 ,AI 的 应 越 来 越 , 而 这 些 应 的 背 后 , 都 离 不 数 据 的 支 持 。                                                                                                                                                                                                                                                                                                                                                                                                  |
|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| PanGu- Σ | (In today's era, AI is everywhere, from the Internet to mobile Internet, from e-commerce to mobile payment, from social networks to O2O. In the medical field, AI is gradually infiltrating into the medical field and playing an increasingly important role. In every department of the hospital, we can see AI. In the operating room, doctors can observe the patient's vital signs through AI and operate on the patient in time. In the outpatient department, doctors can use                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Query    | more AI applications will be developed, and these applications must be supported by data.) 请详 细 说 明 熬 夜 的 坏 处 。 Please elaborate on the disadvantages of staying up late. 熬 夜 是 现 在 很 多 人 都 有 的 - 种 习 惯 , 因 为 工 作 的 原 因 , 很 多 人 都 会 选 择 熬 夜 , 那 么 , 熬 夜 对 身 体 有 什么 危 害 呢 ? 1 、 导 致 皮 肤 受 损 。 熬 夜 对 皮 肤 的 伤 害 是 非 常 大 的 , 经 常 熬 夜 会 让 皮 肤 的 水 分 流 失 , 皮 肤 的 代 谢 能 力 也 会 下 降 , 导 致 皮 肤 出 现 各 种 问 题 , 比 如 长 痘 、 黑 眼 圈 、 脸 色 暗 沉 等 。 2 、 导 致 免 疫 力 下 降 。 经 常 熬 夜 会 导 致 人 体 的 免 疫 力 下 降 , 从 而 导 致 各 种 疾病 的 发 生 , 尤 其 是 对 于 女 性 来 说 , 经 常 熬 夜 会 导 致 内 分 泌 失 调 , 从 而 导 致 月 经 不 调 等 问 题 。 3 、 导 致 精 神 萎 靡 。 经 常 熬 夜 会 让 人 精 神 萎 靡 , 经 常 会 感 到 疲 劳 , 从 而 影 响 到 工 作 和 生 活 , 所 以 经 常 熬 夜 的 人 往往 会 感 到 精 神 不 佳 , 做 事 无 精 打 采 。 4 、 导 致 肠胃 不 适 。 经 常 熬 夜 会 导 致 人 的 肠胃 功 能 紊 乱 , 出 现 消 化 不 良 的 问 题 , 所 以 经 常 熬 夜 的 人 经 常 会 出 现 腹 胀 、 腹 泻 等 肠胃 不 适 的 症 状 。 导 经 常 会 导 人 的 质 下 经 常 的 人 会 现 的 |
| PanGu- Σ | choose to stay up late. So, what is the harm to the health of staying up late? 1. Causes skin damage. Staying up late is very harmful to the skin. Often staying up late will cause the skin's moisture loss, and the skin's metabolic capacity will also decline, which will lead to various skin problems, such as acne, dark circles under the eyes, dullness, etc.. 2. Causes reduced immunity. Often staying up late can lead to the decline of human immunity,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |

## Prompt

PanGuΣ

太太

## Table 8: A case of text generation with few-shot prompt learning

## Prompt

上 联 ： 马 上 催 征 催 上 马

(The first couplet: Expedite the expedition, and urge the horse immediately.)

下 联 ： 春 迎 快 乐 快 迎 春

(The second couplet: Welcome happiness in spring, and welcome spring immediately.)

上 联 ： 浮 生 若 梦 凭 心 写

(The first couplet: Life is like a dream, which should be written with heart.)

下

联

：

此 世 如 歌 任 意 弹

(The second couplet: Play freely the life which is like a song.)

上 联 ： 苍 官 青 士 左 右 树

(The first couplet: Old officials, young men, and trees on the left and right.)

PanGu- Σ

下 联 ： 海 内 名 花 满 门 香

(The second couplet: The famous flowers in the country are full of fragrance.)

## Table 9: A case of coreference resolution with few-shot prompt learning

- Q ： 苏 小 姐 听 了 ， 心 里 直 刺 痛 ， 回 答 孙 太太 同 时 安 慰 自 己 道 ： ' 那 绝 不 可 能 ! 鲍 小 姐 有未 婚 夫 ， 她 自 己 跟 我 讲 过 。 她 留 学 的 钱 还 是 她 未 婚 夫 出 的 。 ' ' 她 ' 指 代 的 内 容 是 孙 吗 ？
- (Q: When Miss Su heard this, her heart ached. She answered Mrs. Sun and comforted herself, "That's impossible! Miss Bao has a fiance, and she told me about it herself. Her money for studying abroad is still from her fiance."

Does "she" refer to Mrs. Sun?)

：

不

是

，

' 她 ' 指 代 的 是 鲍 小 姐

A

(A:No, "she" refers to Miss Bao.)

- Q ： 这 家 人也 有 -个 女 孩子 ， 年 纪 不 大 ， 远远 地 盯 着 我 ， 害 羞 而 好奇 。 我 - 对 上 她 的 目 光 ， 她 就 慌 不 迭 扭 头 而 去 。
- 上 面 这 句 话 中 的 ' 她 ' 指 代 的 是 什么 ？
- (Q: There was also a girl in this family. She was young and looked at me from a distance, shy and curious. As soon as I met her eyes, she turned away in a panic.

What does "she" refer to in the above sentence?)

A ： ' 她 ' 指 代 的 是 女 孩子

- (A: "She" refers to a girl.)
- Q ： 张 先 生 呵呵 大 笑 ， - 面 分 付 进 来 的 女 佣 说 ： ' 快 去 跟 太太 小 姐 说 ， 客 人 来 了 ， 请 她 们 出 来 。 '
- 上 面 这 句 话 中 的 ' 她 们 ' 指 代 的 是 什么 ？
- (Q: Mr.Zhang laughed and said to the maid, "Go and tell the ladies and girls that the guests are coming, and ask them out."

What does "they" refer to in the above sentence?)

- A ： ' 她 们 ' 指 代 的 是 太太 小 姐 , 因 为 太太 小 姐 在 屋 子 里

(A: "They" refer to ladies and girls, because they are in the room.)