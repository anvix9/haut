{
    "topics": [
        "Large Language Models",
        "Pretraining",
        "Distributed Training",
        "Transformers",
        "Scaling Law"
    ],
    "research": "Q1: What pretraining scale-up method has been developed using a combination of high-quality web data and hardware scalability?\n\nContribution: We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data.",
    "method": "Methodology: We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoderonly models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text-the largest openly documented pretraining run. \n\nWe employed the following key methodological approach:\n\n1. **Pretraining Data**: Utilized a diverse and high-quality corpus primarily sourced from web data to train the models.\n2. **Model Architecture**: Designed causal decoder-only models with varying parameters (7B, 40B, and 180B) within the Falcon series.\n3. **Training Paradigm**: Employed distributed training on up to 4,096 A100s on cloud AWS infrastructure, utilizing a custom distributed training codebase that enables efficient pretraining.\n4. **Pretraining and Fine-tuning**: Developed custom techniques for pretraining and fine-tuning the models, including optimizing popular recipes (e.g., RoBERTa) and inference methods.\n\nWe tested the following hypotheses:\n\n* Falcon-180B can outperform state-of-the-art language models like PaLM and GPT-4.\n* Our custom distributed training codebase enables efficient pretraining on large-scale datasets.\n* The use of causal decoder-only models leads to improved performance in certain NLP tasks.\n\nExperimental setups included evaluating the Falcon series against established benchmarks, such as the 1-shot performance of PaLM-2 Large, and comparing with other popular language models (GPT-3.5/4, LLaMA-1/2, Inflection-1).\n\nOverall, our methodology integrates cutting-edge techniques for efficient pretraining and fine-tuning of large language models, demonstrating competitive performance on various NLP tasks.",
    "results": "Results: The authors introduce the Falcon series of causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data, with the largest model, Falcon-180B, outperforming other models such as PaLM or Chinchilla and improving upon concurrently developed models like LLaMA 2 or Inflection-1. Falcon-180B significantly improves upon previously reported results, nearing the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it one of the top three language models in the world.\n\nThese results contribute to addressing the research question by showcasing the capabilities of the Falcon series in competitive benchmarking, demonstrating the potential for open-source and collaborative development of large language models. By releasing the models under permissive licenses and sharing their data pipeline and custom tooling, the authors aim to foster open-science and accelerate technological progress in this field."
}