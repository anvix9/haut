{
    "topics": [
        "Artificial Intelligence",
        "Auditing",
        "Governance",
        "Large Language Models",
        "Natural Language Processing"
    ],
    "research": "Q1: What novel blueprint does this article propose for auditing large language models (LLMs)?\n\nContribution: A three-layered approach to auditing LLMs, comprising governance audits of technology providers, model audits of LLMs after pre-training, and application audits of applications based on LLMs.",
    "method": "Methodology:..... Large language models (LLMs) require a three-layered approach to auditing, which complements and informs each other. The approach consists of governance audits, model audits, and application audits.\n\nGovernance audits assess the organisational procedures of technology providers that design and disseminate LLMs. Model audits, on the other hand, evaluate LLMs after pre-training but prior to their release. This layer aims to identify an LLM's limitations, inform continuous redesign efforts, communicate capabilities and limitations, and address potential risks like information hazards and misinformation.\n\nModel audits focus on four key characteristics: performance, robustness, information security, and truthfulness. To assess these characteristics, auditors employ various tools and methods:\n\n1.  Performance: Standardised benchmarks, such as GLUE or SuperGLUE, can evaluate an LLM's capabilities across multiple tasks.\n2.  Robustness: Evaluation toolkits like the Robustness Gym or open-source platforms like Dynabench can test a model's vulnerability to adversarial attacks in different domains.\n3.  Information security: Metrics such as exposure or tools like DisCo, SEAT, or CAT can interrogate biases in LLMs' pre-trained word embeddings.\n4.  Truthfulness: Benchmarks like TruthfulQA provide a percentage score indicating how truthful an LLM is.\n\nWhile model audits focus on these four characteristics, training datasets should also be reviewed for gaps or biases that may create representational and allocational harms. Auditors can use existing tools and methods to interrogate biases in LLMs' pre-trained word embeddings.\n\nA medium-level access level is required for auditors to assess LLMs accurately during model audits, which allows them to manipulate model parameters and review learning procedures and task objectives. Model audits should complement other approaches like application audits, as they may not be able to address all risks associated with LLMs.",
    "results": "Results: The authors propose a three-layered approach to auditing Large Language Models (LLMs), which involves governance audits, model audits, and application audits.\n\nThe main results of the study indicate that this approach has the potential to identify and manage some of the ethical and social risks associated with LLMs. However, there are also limitations to this approach, including conceptual problems related to constructing validity for model audits, institutional challenges in implementing independent third-party audits, and practical difficulties in addressing all types of risks through auditing alone.\n\nQuantitative metrics included performance evaluations, benchmarking, and impact assessments. Qualitative observations highlighted the importance of combining governance, model, and application audits to ensure robustness and effectiveness. Comparative analyses showed that existing AI auditing procedures are not well-equipped to assess the governance challenges posed by LLMs.\n\nThe study's findings contribute to addressing the research question by providing a novel blueprint for auditing LLMs. The authors argue that this approach can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks associated with LLMs, but it is not intended to replace existing governance mechanisms."
}