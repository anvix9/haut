{
    "topics": [
        "In-Context Learning",
        "Semantic Priors",
        "Input-Label Mappings",
        "Language Models",
        "Model Scale"
    ],
    "research": "Q1: How do large language models learn to override semantic priors when presented with flipped labels versus semantically-unrelated label ICL (SUL-ICL) settings?\n\nContribution: We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings.",
    "method": "Methodology:.... We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings.\n\nWe investigate two setups- ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). To address this research question, we employ a combination of data-driven methods and experimental designs. \n\nKey methodological approach: \n1. We use a range of NLP tasks (Sentiment Analysis, Question Classification, Textual Entailment Recognition, Financial Sentiment Analysis, Hate Speech Detection) to evaluate the performance of ICL in different contexts.\n2. Our experiments consist of two primary setups:\n   a. **ICL with flipped labels**: We randomly flip the labels presented in-context, thereby testing the models' ability to override semantic priors and learn input-label mappings.\n   b. **Semantically-unrelated label ICL (SUL-ICL)**: We present semantically-unrelated labels alongside their inputs, forcing language models to learn the input-label mappings shown in incontext exemplars.\n\nTo test these setups across various model families:\n1. We employ a set of five language model families, including GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM.\n2. For each setup and family, we experiment with three different sizes of models: 350M (small), 1.3B (medium), and 8B (large).\n3. We use pre-trained model weights where available and fine-tune models on top of them using our own custom datasets.\n\nWe also analyze the effects of instruction tuning on ICL performance, as some results suggest that strengthening the use of semantic priors has a more significant impact on model performance than learning input-label mappings.\n\nOverall, by combining multiple experimental designs, data-driven methods, and model comparisons, we aim to gain insights into how in-context learning in language models is influenced by semantic priors versus input-label mappings, particularly as model scale increases.",
    "results": "Results: \n\nKey findings from the paper include:\n\n* Large language models can override semantic priors when presented with enough flipped labels, showing that smaller models rely more heavily on prior knowledge.\n* The ability to learn semantically-unrelated label ICL (SUL-ICL) also emerges primarily with model scale, where large-enough language models can perform linear classification in a SUL-ICL setting.\n* Instruction-tuned models strengthen both the use of semantic priors and the capacity to learn input-label mappings, but more so the former.\n\nThese findings highlight how larger language models develop emergent abilities to map inputs to many types of labels, demonstrating true symbolic reasoning. The results contribute to addressing the research question by demonstrating how in-context learning behavior changes with model scale, ultimately improving performance on linear classification tasks and expanding our understanding of the capabilities of large language models."
}