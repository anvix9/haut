## 1 INTRODUCTION

Neural volumetric scene representations such as Neural Radiance Fields (NeRF) [Mildenhall et al. 2020] enable photorealistic novel view synthesis of scenes with complex geometry and appearance, but the compute required to query such neural representations during volumetric raymarching prohibits real-time rendering. Subsequent works have proposed discretized volumetric representations that can substantially increase rendering performance [Garbin et al. 2021; Hedman et al. 2021; Yu et al. 2021], but these approaches do not yet enable practical real-time rendering for large-scale scenes.

These representations struggle to scale to larger scenes primarily due to graphics hardware constraints. Volumetric data is necessarily larger than a 2D surface representation and occupies more space in memory. Similarly, while a camera ray intersects a hard surface

at most once, rendering a ray through a volume may require many samples. With state of the art neural or hybrid representations, each of these sample queries is very expensive to evaluate, either in terms of compute or memory bandwidth. As a result, methods that work for scenes with limited extent (single objects in space or forward-facing scenes) typically do not scale up to larger unbounded scenes.

All neural or hybrid volumetric methods must address two fundamental trade-o/ffs that arise from these constraints:

- Â· Volume vs. surface? Purely volumetric rendering models are most amenable to gradient-based optimization and produce excellent view synthesis results [Barron et al. 2022]. On the other hand, increasing sparsity and moving closer [Wang et al. 2021; Yariv et al. 2021] or completely [Chen et al. 2022a; Munkberg et al. 2022] to a surface-like representation degrades image quality but results in compact representations that are cheap to render.
- Â· Memory bound vs. compute bound? The most compact representations (such as the MLP network in Mildenhall et al. [2020] or the low-rank decomposition in Chen et al. [2022b]) require many FLOPS to query, and the fastest representations (such as the sparse 3D data structures used in Yu et al. [2021] and Hedman et al. [2021]) consume large amounts of graphics memory.

One approach to this trade-o/ff is to embrace a slower, more compact volumetric model for optimization and to subsequently 'bake' it into a larger but faster representation for rendering. However, baking often a/ffects the representation or rendering model which can lead to a large drop in image quality. Though this can partially be ameliorated by /fine-tuning the baked representation, /fine-tuning does not easily scale to larger scenes, as computing gradients for optimization requires signi/ficantly more memory than rendering.

The goal of our work is to /find a representation that is well suited for both optimization and fast rendering. Our solution is a single uni/fied radiance /field representation with two di/fferent underlying parameterizations . In both stages, our memory-e/fficient radiance /field (MERF) is de/fined by a combination of a voxel grid [Sun et al. 2022; Yu et al. 2022] and triplane data structure [Chan et al. 2022]. During optimization, we use the NGP hash grid structure [MÃ¼ller et al. 2022] to compress our parameterization, which allows for di/fferentiable sparsi/fication and provides an inductive bias that aids convergence. After optimization, we query the recovered NGP to explicitly bake out the MERF and create a binary occupancy grid to accelerate rendering. Critically, both the NGP-parameterized and baked MERF represent the same underlying radiance /field function . This means that the high quality achieved by the optimized MERF carries over to our real-time browser-based rendering engine.

## 3 PRELIMINARIES

We begin with a short review of relevant prior work on radiance /fields for unbounded scenes. A radiance /field maps every 3D position x âˆˆ R 3 and viewing direction d âˆˆ S 2 to the volumetric density ğœ âˆˆ R + at that location and the RGB color emitted from it along the view direction, c âˆˆ R 3 . The color of the ray emitted from point o in the direction d can then be computed using the radiance /field by sampling points along the ray, x ğ‘– = o + ğ‘¡ ğ‘– d , and compositing the corresponding densities { ğœ ğ‘– } and colors { c ğ‘– } according to the numerical quadrature approach of Max [1995]:

C = âˆ‘ï¸ ğ‘– ğ‘¤ ğ‘– c ğ‘– , ğ‘¤ ğ‘– = ğ›¼ ğ‘– ğ‘‡ ğ‘– , ğ‘‡ ğ‘– = ğ‘– -1 GLYPH<214> ğ‘— = 1 ( 1 -ğ›¼ ğ‘— ) , ğ›¼ ğ‘– = 1 -ğ‘’ -ğœ ğ‘– ğ›¿ ğ‘– , (1)

where ğ‘‡ ğ‘– and ğ›¼ ğ‘– denote transmittance and alpha values of sample ğ‘– , and ğ›¿ ğ‘– = ğ‘¡ ğ‘– + 1 -ğ‘¡ ğ‘– is the distance between adjacent samples.

The original NeRF work parameterized a radiance /field using a Multilayer Perceptron (MLP), which outputs the volume density and view-dependent color for any continuous 3D location. In order to reduce the number of MLP evaluations to one per ray, SNeRG uses a deferred shading model in which the radiance /field is decomposed into a 3D /field of densities ğœ , di/ffuse RGB colors c d , and feature vectors f [Hedman et al. 2021].

Fig. 2. Our scene representation. For a location x along a ray: (1) We query its eight neighbors on a low-resolution 3D grid; and we project it onto each of the three axis-aligned planes, and then query each projection's four neighbors on a high-resolution 2D grid. (2) The eight low-resolution 3D neighbors are evaluated and trilinearly interpolated while the three sets of four high-resolution 2D neighbors are evaluated and bilinearly interpolated, and the resulting features are summed into a single feature vector t . (3) The feature vector is split and nonlinearly mapped into three components: density ğœ , RGB color c ğ‘‘ , and a feature vector f encoding view dependence e/ffects.

<!-- image -->

SNeRG's deferred rendering model volumetrically accumulates the di/ffuse colors { c ğ‘‘,ğ‘– } and features { f ğ‘– } along the ray, similar to Equation 1:

C ğ‘‘ = âˆ‘ï¸ ğ‘– ğ‘¤ ğ‘– c ğ‘‘,ğ‘– , F = âˆ‘ï¸ ğ‘– ğ‘¤ ğ‘– f ğ‘– , (2)

and computes the ray's color as the sum of the accumulated di/ffuse color C ğ‘‘ and the view-dependent color computed using a small MLP â„ that takes as input C ğ‘‘ , F , and the viewing direction d :

C = C ğ‘‘ + â„ ( C ğ‘‘ , F , d ) . (3)

SNeRG uses a large MLP during training and bakes it after convergence into a block-sparse grid for real-time rendering.

In order for radiance /fields to render high quality unbounded scenes containing nearby objects as well as objects far from the camera, mip-NeRF 360 [Barron et al. 2022] uses a contraction function to warp the unbounded scene domain into a /finite sphere:

contract ( x ) = ( x if âˆ¥ x âˆ¥ 2 â‰¤ 1 GLYPH<16> 2 -1 âˆ¥ x âˆ¥ 2 GLYPH<17> x âˆ¥ x âˆ¥ 2 if âˆ¥ x âˆ¥ 2 > 1 (4)

## 7.4 Limitations

Since we use the view-dependence model introduced in SNeRG [Hedman et al. 2021], we also inherit its limitations: By evaluating viewdependent color once per ray, we are unable to faithfully model viewdependent appearance for rays that intersect with semi-transparent objects. Furthermore, since the tiny MLP has limited capacity, it may struggle to scale to much larger scenes or objects with complex re/flections.

Moreover, our method still performs volume rendering, which limits it to devices equipped with a su/fficiently powerful GPU such as laptops, tablets or workstations. Running our model on smaller, thermally limited devices such as mobile phones or headsets will require further reductions in memory and runtime.

## 8 CONCLUSION

We have presented MERF, a compressed volume representation for radiance /fields, which admits real-time rendering of large-scale scenes in a browser. By using novel hybrid volumetric parameterization, a novel contraction function that preserves straight lines, and a baking procedure that ensures that our real-time representation describes the same radiance /field as was used during optimization, MERF is able to achieve faster and more accurate real-time rendering of large and complicated real-world scenes than prior real-time NeRF-like models. Out of all real-time methods, ours produces the highest-quality renderings for any given memory budget. Not only does it achieve 31 . 6% (MSE) higher quality in the outdoor scenes compared to MobileNeRF, the previous state-of-the-art, it also requires less than half of the GPU memory.

Fig. 4. PSNR (higher is be/t\_ter) vs VRAM consumption (lower is be/t\_ter) for our model, our improved SNeRG baseline, MobileNeRF, and an ablation of our model without 3D grids. Each line besides MobileNeRF represents the same model with a varying resolution of its underlying spatial grid, indicated by marker size.

<!-- image -->

Table 2. We compare our final model a/f\_ter baking to the model before baking (a) demonstrating that our Proposal-MLP-aware baking pipeline is almost lossless. Omi/t\_ting quantization-aware training (b) leads to a drop in rendering quality. Our proposed contraction function performs on par with the original spherical contraction function (c), while enabling e/fficient ray-AABB intersection tests. Results are averaged over all outdoor scenes.

|                               |   PSNR â†‘ |   SSIM â†‘ |   LPIPS â†“ |
|-------------------------------|----------|----------|-----------|
| (a) Pre-baking                |    23.2  |    0.62  |     0.336 |
| (b) w/o quant.-aware training |    22.64 |    0.603 |     0.347 |
| (c) Spherical contraction     |    23.22 |    0.619 |     0.341 |
| Ours (Post-baking)            |    23.19 |    0.616 |     0.343 |

Table 3. Performance comparison on the 'outdoor' scenes.

|             | PSNR â†‘                       | SSIM â†‘                       | LPIPS â†“                      | VRAM                         | DISK â†“                       | FPS â†‘                        |
|-------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|
|             | MacBook M1 Pro, 1280 Ã— 720   | MacBook M1 Pro, 1280 Ã— 720   | MacBook M1 Pro, 1280 Ã— 720   | MacBook M1 Pro, 1280 Ã— 720   | MacBook M1 Pro, 1280 Ã— 720   | MacBook M1 Pro, 1280 Ã— 720   |
| Mobile-NeRF | 21.95                        | 0.470                        | 0.470                        | 1162                         | 345                          | 65.7                         |
| SNeRG++     | 23.64                        | 0.672                        | 0.285                        | 4571                         | 3785                         | 18.7                         |
| Ours        | 23.19                        | 0.616                        | 0.343                        | 524                          | 188                          | 28.3                         |
|             | NVIDIA RTX 3090, 1920 Ã— 1080 | NVIDIA RTX 3090, 1920 Ã— 1080 | NVIDIA RTX 3090, 1920 Ã— 1080 | NVIDIA RTX 3090, 1920 Ã— 1080 | NVIDIA RTX 3090, 1920 Ã— 1080 | NVIDIA RTX 3090, 1920 Ã— 1080 |
| Instant-NGP | 22.90                        | 0.566                        | 0.371                        | -                            | 107                          | 4                            |
| Ours        | 23.19                        | 0.616                        | 0.343                        | 524                          | 188                          | 119                          |

## A TRAINING DETAILS

All SNeRG++ and MERF models use the same training hyperparameters and architectures. We train for 25000 iterations with a batch size of 2 16 pixels. A training batch is created by sampling pixels from all training images. We use the Adam [Kingma and Ba 2015] optimizer with an exponentially decaying learning rate. The learning rate is warmed up during the /first 100 iterations where it is increased from 1e -4 to 1e -2. Then the learning rate is decayed to 1e -3. Adam's hyperparameters ğ›½ 1, ğ›½ 2 and ğœ– are set to 0 . 9, 0 . 99 and 1e -15, respectively. To regularize the hash grids we use a weight decay of 0 . 1 on its grid values.

## B ARCHITECTURE

For parameterizing all grids (i.e. 3D voxel grids and 2D planes) we use an MLP with a multi-resolution hash encoding [MÃ¼ller et al. 2022]. The hash encoding uses 20 levels and the individual hash tables have 2 21 entries. Following MÃ¼ller et al. [2022], hash collisions are resolved with a 2-layer MLP with 64 hidden units. This MLP outputs an 8-dimensional vector representing density, di/ffuse RGBand the 4-dimensional view-dependency feature vector. For our deferred view-dependency model we closely follow SNeRG [Hedman et al. 2021] and use a 3-layer MLP with 16 hidden units. As in SNeRG viewing directions are encoded with 4 frequencies. Following MipNeRF360 [Barron et al. 2022] we use hierarchical sampling with three levels and therefore require two Proposal-MLPs. The

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Â·

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

flowerbed

<!-- image -->

treehill

<!-- image -->

Ground truth

<!-- image -->

Ours

Mobile-NeRF

Instant NGP

mip-NeRF 360

Fig. 6. Visual comparison between MERF and other view synthesis methods. Mobile-NeRF [Chen et al. 2022a] is the only other real-time method (30fps or be/t\_ter). Instant NGP [MÃ¼ller et al. 2022] runs at interactive rates (around 5fps) and mip-NeRF 360 [Barron et al. 2022] is an extremely heavyweight o/ffline method (around 30 seconds to render a single frame), representing the current state-of-the-art view synthesis quality.

<!-- image -->

With low-resolution 3D grid

<!-- image -->

Without low-resolution 3D grid

Proposal-MLPs consist of 2 layers with 64 hidden units and use a hash encoding. Since a Proposoal-MLP merely needs to model coarse geometry, we use for the Proposal-MLPs' hash encodings only 10 levels, a maximum grid resolution of 512 and a hash table size of 2 16 .

