## Abstract

Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp a/ffordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of highlevel semantics, while contrastive learning approaches capture the opposite. We then introduce V oltron, a framework for languagedriven representation learning from human videos and associated captions. V oltron trades o/ff language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning /five distinct robot learning problems - a uni/fied platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all /five problems, we /find that V oltron's languagedriven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features. 1

## 1 Introduction

Good words are worth much, and cost little.

- G/e.sc/o.sc/r.sc/g.sc/e.sc H/e.sc/r.sc/b.sc/e.sc/r.sc/t.sc

Realizing a future of ubiquitous, broadly capable robots is predicated on systems capable of generalizable perception and interaction [Weiss et al. 1987; Chaumette and Hutchinson 2006; Levine et al. 2016]. Towards this goal, recent work in robotics present approaches for learning visual representations to bootstrap learning for visuomotor control [Parisi et al. 2022; Nair et al. 2022; Radosavovic et al. 2022]. Critically, these approaches show that we can learn such representations from real-world videos of human behavior - speci/fically, egocentric video datasets such as Something-Something-v2 and Ego4D [Goyal et al. 2017; Grauman et al. 2022] - instead of solely relying on in-domain robotics data that is scarce and expensive. While prior work has developed and evaluated representations for visuomotor control, robot learning is an expansive discipline,

spanning a diverse spectrum of problems : predicting grasp proposals from visual input [Saxena et al. 2008; Mahler et al. 2017], languageconditioned imitation learning [Tellex et al. 2011] and belief/intent tracking for human-robot interaction [Hauser 2012; Javdani et al. 2018], amongst others. Broadening our focus to problems beyond learning for control enables us to develop /flexible, generalizable representations that capture both low-level spatial reasoning and high-level semantic understanding - a /flexibility that is a key prerequisite to realizing a foundation model for robotics [Bommasani et al. 2021]. Thus, we ask: how can we learn visual representations that generalize across the diverse spectrum of problems in robot learning?

Recent approaches for learning visual representations for robotics use pretraining objectives that re/flect di/fferent inductive biases for what the learned representations should capture. Masked Visual Pretraining [MVP; Radosavovic et al. 2022] proposes using masked autoencoding [He et al. 2022] to prioritize visual reconstruction from heavily masked video frames, encoding representations that facilitate per-pixel reconstruction. Separately, Reusable Representations for Robotic Manipulation [R3M; Nair et al. 2022] eschews pixel reconstruction for two contrastive learning objectives: time contrastive learning [Sermanet et al. 2018] and video-language alignment. These approaches show strong performance on imitation learning in simulated and real-world settings, with sizeable improvements over strong alternatives such as ResNet or CLIP features [He et al. 2016; Radford et al. 2021]; however, they have not been evaluated beyond these settings. As a /first contribution, we evaluate these representations on problems beyond control and identify inconsistent evaluation performance , with huge penalties depending on the approach and speci/fic application. MVP performs well on problems such as grasp a/ffordance prediction, but struggles with higher-level problems such as language-conditioned imitation. R3M instead excels at the higher-level problems, but degrades completely on problems such as grasp a/ffordance prediction.

Motivated by this, we present V oltron , a framework for languagedriven visual representation learning for robotics that learns representations that capture both low-level and high-level features, empirically outperforming prior approaches over all applications. V oltron models take videos and associated language captions as input to a masked autoencoding pipeline, reconstructing one (or more) frames from a masked context. The novelty of our framework is in how we use language supervision . Depending on a tunable probability ùõº , we either condition on ( ùõº = 0 ) , or generate ( ùõº > 0 ) the associated caption. Explicitly conditioning on words in di/fferent contexts allows for low-level pattern recognition at the local, spatial level, while generating language from our learned visual

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Figure 1: V oltron Evaluation Suite. We introduce a suite of evaluation problems spanning /five applications within robotics , including grasp a/ffordance prediction, referring expression grounding, single-task visuomotor control (in simulation), language-conditioned imitation learning (on a real robot), and intent scoring.

<!-- image -->

encoding allow us to infer higher-level features around a/ffordances and intents. Furthermore, guided by the hypothesis that language is especially useful in describing change , we study dual-frame contexts consisting of the initial and current observation in multi-timestep tasks. Altogether, we examine three di/fferent V oltron variants : V - Cond ( Language Conditioning : single frame, ùõº = 0), V - Dual ( Adding Context : dual-frame conditioning, ùõº = 0), and V - Gen ( Adding Language Generation : dual-frame, ùõº = 0 . 5 - we /find that ùõº = 1 with no language-conditioning at all hurts performance).

To evaluate V oltron and other visual representation learning approaches, we assemble a new evaluation suite (depicted in Figure 1) spanning /five problem domains within robotics: 1) dense segmentation for grasp a/ffordance prediction [Zeng et al. 2017], 2) object detection from referring expressions (e.g., 'the blue co/ffee mug to the left of the plate') in cluttered scenes [Wang et al. 2021], 3) imitation learning for visuomotor control (in simulation) [Nair et al. 2022], 4) learning multi-task language-conditioned policies for real-world manipulation [Stepputtis et al. 2020] (on a real-world Franka Emika /fixed-arm manipulator), and 5) zero-shot intent scoring [Javdani et al. 2018; Chen et al. 2021]. We choose these tasks for their broad coverage; tasks such as grasp a/ffordance prediction and referring expression grounding require reasoning over low-level spatial features, while language-conditioned imitation and intent scoring require a deeper understanding of semantics.

Through experiments controlling for pretraining data and model capacity, we show that the simplest V oltron representations (from V - Cond ) strictly outperform both MVP and R3M representations across all evaluation domains. Furthermore, by adapting our models to learn from multiple frame contexts and that favor generation (e.g., with V - Dual and V - Gen ), we show that we can further boost performance on evaluations requiring higher-level features such as with language-conditioned policy learning (on a real robot) and intent scoring. Though language-conditioning o/ffers universal performance gains, there are tradeo/ffs between V oltron

models ; adding language generation hurts performance on some control tasks, even though its necessary for strong performance on intent scoring. Furthermore, V oltron with single-frame language conditioning performs well on non-episodic tasks (e.g., grasping), but underperforms multi-frame models on control tasks. There is not yet a silver bullet - a single representation strong on all tasks - but the ability to balance tradeo/ffs between encoding low and high-level features o/ffers a net win over restrictions of past work.

Contributions. 1) We present V oltron , a framework for languagedriven visual representation learning. Through controlled experiments and comprehensive ablations we demonstrate that V oltron's representations strictly outperform the prior art across 2) a new evaluation suite composed of /five distinct problem domains within robotics. Finally, 3) we analyze the tradeo/ffs between di/fferent V oltron models that balance di/fferent types of feature learning, outlining several directions for future work. We release all models, the evaluation suite, code (pretraining and adaptation), and preprocessed data (https://sites.google.com/view/voltron-robotics).

Limitations. We do not have access to the compute resources to train models of the same scale and data used in prior work [Radosavovic et al. 2022; Nair et al. 2022]. Instead, we carefully reproduce MVP and R3M - the current state-of-the-art approaches - by pretraining on the Something-Something-v2 dataset [Goyal et al. 2017], further controlling for batch ordering, model capacity, and other sources of randomness (full details are in ¬ß4). However, for full context we also include results from the o/fficial release artifacts from both these works, as well as other methods such as CLIP [Radford et al. 2021], though we note these results in gray or with dashed lines as to indicate they are not directly comparable.

## 6 Ablations, Extensions, & Further Analysis

The comparative results across the various evaluation problem domains paint V oltron's language-driven representations in a favorable light relative to MVP and R3M baselines. Yet, there remain key questions that we address in this section: is language supervision actually driving these results? Why generative language modeling over masked language modeling? Will V oltron scale?

Ablation: The Impact of Language Supervision. The second row of Table 4 shows a subset of evaluation results across three different problem domains when training a 'no-language' variant of

the V - Cond architecture - this variant is in essence an alternate version of a masked autoencoder that uses the small architecture modi/fications we added for training stability in ¬ß4. As such, it also serves as an architecture ablation when compared to the R-MVP results, enabling us to isolate the impact of the small stability modi/fications described in ¬ß4. Indeed, the results con/firm our hypotheses: /first, removing language results in a de/finitive drop in performance across all evaluation applications. Second, the respective results for each evaluation application are on par with the corresponding results for the R-MVP model, demonstrating that the performance of V oltron models does not stem from the architecture. We delve further into this ablation in ¬ßC.1.

Ablation: Generative vs. Masked Language Modeling. Looking at the V oltron objective, a natural question to ask is why we chose language generation over masked language modeling . Furthermore, recent and concurrent work propose learning multimodal masked autoencoders (M3AE) both within and outside of robotics [Geng et al. 2022; Liu et al. 2022], showing promising results in learning visual representations for image classi/fication tasks, amongst others. To assess the di/fferences, we choose to reproduce the M3AE model in a manner similar to our reproduction of MVP and R3M; we keep the same Something-Something-v2 pretraining data, adopting the exact procedure described in Geng et al. [2022], then evaluating the resulting representations on the same subset of evaluation domains as in the prior ablation (third row of Table 4). Surprisingly, we see drastic drops in performance across the board . Looking at the pretraining curves, we identify a possible reason for this failure: in optimizing M3AE on Sth-Sth, we see the language modeling loss go to zero almost immediately , leading to over/fitting. A possible explanation is that the masked language modeling conditioned on visual contexts in datasets annotated with short, predictable narrations leads to degenerate representations, while generative language modeling is not susceptible to the same types of collapse; looking at ways to mitigate this seems like a promising direction for future work. Explicit details around pretraining and evaluating R-M3AE, with an in-depth discussion are in ¬ßC.2.

Table 4: Ablation Experiments. We select a subset of evaluations from ¬ß5 - grasp a/ffordance prediction, referring expression grounding, and single-task visuomotor control.

|                 |   Grasp PR @ Top-1% |   Refer Total Accuracy | Imitate (n = 25)   |
|-----------------|---------------------|------------------------|--------------------|
| V + Lang [Ours] |               80.71 |                  89.38 | 38.2 ¬± 5.09        |
| No-Language ‚Üì   |               65.83 |                  53.44 | 33.1 ¬± 4.79        |
| R-M3AE ‚Üì‚Üì       |               52.79 |                  51.61 | 24.0 ¬± 4.21        |

Extension: Scaling Up. Prior approaches have shown gains in scaling model capacity; here, we present preliminary evidence that V oltron models behave similarly. For each evaluation in ¬ß5, we evaluate a ViT-Base variant of V - Cond (86M parameters vs. the 22M in the ViT-Small). We see universal improvement: Top5% precision for grasping (Table 2; middle row) increases by 15%, expression grounding accuracy improves (Table 3; middle row), as does performance on control.

Extension: Robustness to Real-World Distractors. Factors such as lighting conditions, time of day, and accidental environment perturbations (e.g., a colleague knocking over the camera) can have a profound impact on performance of robotic systems, especially if learned representations are not robust. We run a limited 'robustness' evaluation after training language-conditioned policies from the demonstrations described in ¬ß5.4. Success rates before and after introducing visual distractors for two of the 'meta-tasks' are in Figure 6 (bottom right). 3 We /find that V oltron and R-MVP models are robust to even the most extreme distractors - seemingly a bene/fit of per-patch masking coupled with MAP-based extraction.

## 7 Discussion & Conclusion

We propose V oltron, a framework for language-driven representation learning that balances conditioning and generation to shape the balance of low and high-level features captured. We introduce an evaluation suite spanning /five diverse problems within robotics for holistically evaluating visual representations. Through controlled experiments and ablations, we validate the strengths of our representations; across all evaluation tasks, V oltron models that balance language conditioning and generation strictly outperform prior approaches such as R3M and MVP, and in many cases show performance competitive with or exceeding that of approaches that use orders of magnitude more data or more expressive models.

Yet, while language is a pivotal source of supervision, there are still key questions to answer. Why is language-based pretraining helpful on tasks that have nothing to do with language? Why not try to learn one model that can encode both low-level and high-level features, without tradeo/ffs? While there is not a silver bullet yet we hope that future work takes a deep, grounded look at these questions, identifying what existing representations capture - and more importantly, what they miss. Our hope is that V oltron serves as a starting point; a /flexible, uni/fied framework for future improvements in visual representation learning for robotics.

## Overview

In the appendices below, we provide additional details around the implementation, pretraining, and adaptation procedures described in the main text, in addition to delving deeper into various discussions. Finally, we add additional results and visualizations that further complement the /findings from the main text.

We provide open-source code for loading and using pretraining models, hosted links for our preprocessing splits (including the actual batches seen during training), and a separate, standalone open-source code repository for our evaluation suite. Our hope is that the evaluation suite especially is general and easy to use for downstream work on evaluating learned representations. The full manifest of resources are as follows:

- ¬∑ Project Page (videos & additional links): https://sites.google.com/view/voltron-robotics
- ¬∑ Open-Source Modeling Repository (pretraining code for all approaches , loading models): https://github.com/siddk/voltron-robotics
- ¬∑ Open-Source Evaluation Suite (general API for evaluating on di/fferent problem domains): https://github.com/siddk/voltron-evaluation

All model and automated evaluation code is in PyTorch; however, the evaluation code can be easily overridden to suit your needs.

An overview of each appendix can be found below. We further indicate which parts of the appendices are best viewed here in the text or on the project page; for videos and visualizations, we highly recommend navigating to the latter.

## D.1 Additional Preprocessing Discussion

We described our preprocessing approach in ¬ß4: following the R3M paper, we sample /five frames from each video clip for each epoch of pretraining. Seeing multiple frames from the same visual context is minimally necessary for the R3M time-contrastive learning objective, but we posit in this discussion (following the questions in Appendix A) that repeatedly sampling from the same visual context - even with a reconstruction objective - allows for picking up on /finer-grained changes within a context. The best evidence we have for this is in looking at how prior work constructs their pretraining datasets.

The original MVP work [Xiao et al. 2022; Radosavovic et al. 2022] constructs static datasets of images by iterating through the various video clips in their pretraining datasets - Sth-Sth, Ego4D [Grauman et al. 2022], 100 Days of Hands [Shan et al. 2020] - at a /fixed rate, usually from 0.2 to 1 frames per second. Given video clip lengths of 2 seconds, this means that in aggregate these pretraining datasets comprise maybe 2-3 frames sampled from the same clip, if that. Contrast that with this work and R3M, sampling multiple frames from each video clip for every pretraining epoch (for 400 epochs). This not only means that we are seeing the same context repeatedly, but also that we are seeing di/fferent views of the same context; this can help tune reconstruction towards picking up on /finer-grained features (e.g., if a high-capacity model is able to memorize prior contexts given enough repetition).

This o/ffers a (again, speculative) explanation of why V oltron models outperform MVP (EgoSoup) models that are both higher-capacity and trained on orders of magnitude more data - but de/finitely requires further experiments to prove. In the meantime, it seems as though taking steps to use as much of the pretraining datasets we have access to as possible is in our best interest.

Figure 15: Default Feature Extraction in MAE Models. Prior work in masked autoencoding including MVP use the embedding corresponding to a dummy <CLS> token appended to the Transformer input for downstream adaptation. While this is motivated in the supervised learning setting, it is not clear what this embedding captures in the MAE setting, as it never receives explicit supervision. We /find that pooling the learned patch embeddings is strictly better.

<!-- image -->

