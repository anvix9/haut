## Abstract

The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of taskspecific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex questionand-answering tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt , to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving superior performance on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationships demonstrate the effectiveness of our method. 1

## 1 Introduction

Large language models (LLMs) (Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022a; Tay et al., 2022; Scao et al., 2022; Zeng et al., 2022; Smith et al., 2022) have achieved great success in recent years. A typical way of applying LLMs is in-context learning (Brown et al.,

2020) by providing a number of instructions and exemplars, which performs well on conventional language understanding and generation tasks but performs poorly on complex reasoning tasks (Rae et al., 2021; Liang et al., 2022; Wei et al., 2022a). Recent prompting studies (Wei et al., 2022b; Wang et al., 2022; Zhou et al., 2022) found that elaborating the reasoning steps in the exemplars endows LLMs with good reasoning abilities, namely chainof-thought (CoT) prompting. However, chain-ofthought prompting depends on human engineering: it requires humans to select a few informative questions and then annotate them with CoT and answers. The human-annotated exemplars (questions with annotated CoT and answers) are not necessarily the most effective for different tasks. For example, the original chain-of-thought prompting (Wei et al., 2022b) crafted exemplars for eight questions, which are either randomly selected from the training set or manually composed by humans. Due to there being a significant variance in the nature of reasoning tasks in terms of difficulty, scope, domain, and so on, we do not know what kind of question is the most worthy of annotating. It is also not clear whether a particular set of exemplars is the best to elicit the desired information. However, the good news is that annotating eight exemplars for different tasks is trivial. It costs little money and human effort. In light of this, we identify the key problem as how to determine which questions are the most important and helpful for annotation. We propose a solution to this problem by leveraging uncertainty and introducing a bit of human effort to annotate a small set of questions. The annotation budget is reasonable.

By borrowing ideas from the related problem of uncertainty-based active learning (Gentile et al., 2022), we introduce several metrics to characterize the uncertainty among the model's predictions on each question. Therefore, we propose a new uncertainty-based annotation strategy that chooses

(1) Uncertainty Estimation

Figure 1: Illustrations of our proposed approach. There are four stages. (1) Uncertainty Estimation : with or without a few human-written chain-of-thoughts, we query the large language model k ( k ' 5 in this illustration) times to generate possible answers with intermediate steps for a set of training questions. Then, we calculate the uncertainty u based on k answers via an uncertainty metric (we use disagreement in this illustration). (2) Selection : according to the uncertainty, we select the most uncertain questions for annotation. (3) Annotation : we involve humans to annotate the selected questions. (4) Inference : infer each question with the new annotated exemplars.

<!-- image -->

(2) Selection

a number of questions from the downstream dataset and involves humans annotating the rational chains, significantly improving the performance. Specifically, given a dataset D , we first ask the model to answer it k times. Then, we calculate the uncertainty u of this model based on k answers to each question. With u , we select the most uncertain n questions with the largest u and annotate these questions by the oracle to craft new exemplars E . Finally, we pre-pend E to each test question following the standard recipe of chain-of-thought prompting (Wei et al., 2022b). The schematics of our proposed approach are illustrated in Figure 1. There are several different ways for uncertainty estimation in the literature (Settles, 2009; Culotta and McCallum, 2005). In our main experiments, we characterize the uncertainty u by the disagreement and entropy of all predicted answers. In addition, we investigate other different uncertainty metrics, like variance and self-confidence. For selfconfidence, we re-organize the generated answer with the question using a new template and then ask the model's confidence for such generation. In this scenario, u is defined as a categorical variable from {very confident, confident, not confident, wrong answer}. It is observed that the disagreement, entropy, and variance perform similarly well, while self-confidence is not working because LLMs are prone to be over-confident.

We conduct our experiments on eight datasets, spanning arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Experimental results demonstrate the effectiveness of our proposed method by outperforming the competitive baseline models. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship display the benefits of each proposed module and reveal their effects. Our contributions are threefold: 1) We propose to judiciously select the most helpful and informative questions for annotation, reducing the human engineering workload. 2) We introduce an effective uncertainty-based question selection strategy with several different uncertainty metrics. 3) Our proposed method surpasses competitive baseline models by a large margin on multiple reasoning tasks. To the best of our knowledge, our work is the first to demonstrate the benefits of active question selection in chain-of-thought prompting for solving complex reasoning tasks.

## 2.3 Inference

With the new annotated exemplars E , we prompt each question with them in the inference stage. In addition, we apply self-consistency (Wang et al., 2022) to infer a question m times with a temperature T , and then select the most consistent answer.

## 5 Analysis

In this section, we further conduct several additional experiments to disclose the effects of fewshot prompts, active selection, different annotators, uncertainty metrics, pool size, and prompt engineering. Finally, we analyze the relationship between uncertainty and accuracy, hoping to provide more explanation about how our method works.

Table 2: Ablation study on three arithmetic reasoning tasks, CSQA, and Letter (4). Zero-Shot-Active-Prompt denotes the removal of the dependence of few-shot CoTs during uncertainty estimation. Anno. (A) and Anno. (B) are two different annotators. (D), (E), and (V) denote the disagreement, entropy, and variance, respectively. Bold represents the best among each dataset. The results of GSM8K, ASDiv, SingEq are obtained with code-davinci-002 while the results of CSQA and Letter (4) are obtained with text-davinci-002 .

| METHOD                  |      |      | GSM8K ASDiv SingleEq   |      | CSQA Letter (4)   |
|-------------------------|------|------|------------------------|------|-------------------|
| Auto-CoT                | 62.8 | -    | -                      | 74.4 | 59.7              |
| Manual-CoT              | 63.1 | 80.4 | 87.5                   | 73.5 | 56.6              |
| SC                      | 78   | 87.8 | 93.7                   | 72.9 | 57.6              |
| Random-CoT              | 78.6 | 87.1 | 94.0                   | 74.5 | 65.5              |
| Zero-Shot-Active-Prompt | 82.2 | 86.7 | 94.2                   | -    | -                 |
| Active-Prompt (D)       | 82.2 | 88.4 | 94.5                   | 76.6 | 67.7              |
| Active-Prompt (E)       | 83.4 | 89.3 | 95.5                   | 78.8 | 66.7              |
| Active-Prompt (V)       | 75.2 | 86.4 | 94.0                   | -    | -                 |
| Active-Prompt-Anno. (A) | 82.2 | 88.4 | 94.5                   | 76.0 | 58.3              |
| Active-Prompt-Anno. (B) | 84   | 89.6 | 94.7                   | 75.2 | 57.5              |

## 5.1 Ablation Study

In this section, we reveal the impact of various modules in our proposed model design. First, we reported the performance under the zero-shot setting by removing the dependency of a few exemplars, then explored the contributions of our proposed active example selection strategy. In addition, we explore the effects of different annotators, different uncertainty metrics, and pool sizes. To verify their contributions, we ablate them one by one and evaluate three downstream tasks: GSM8K, ASDiv, and SingleEq. The results are shown in Table 2.

Effects of Few-Shot Prompts In our main experiments, we start with 4-8 manually annotated exemplars to help infer answers during the uncertainty estimation stage and demonstrate the effectiveness of our method. These annotated exemplars are directly taken from Wei et al. (2022b). However, our method is independent of the exemplars provided. In this section, we conduct further experiments with the assumption that we do not have access to them. Inspired by the recent research of Zero-ShotCoT (Kojima et al., 2022), we found it is possible to bypass the manual effort of writing the initial exemplars. Instead of using 4-8 human-written exemplars to generate k predictions, we simply add 'Let's think step by step." and let LLMs generate the reasoning steps and the final answer. The results are shown in Table 2 Zero-Shot-Active-Prompt, which performs competitively to Active-Prompt, demonstrating that our method is not necessarily

dependent on the few-shot exemplars.

Effects of Active Selection Our main contribution is the proposal of an effective example selection strategy (namely active selection). We replace the active selection with random selection by randomly selecting the same number of questions for annotation. The annotation process is exactly the same as Active-Prompt with the same annotation process and annotator. This model is called Random-CoT. The results are shown in Table 2. It is observed that Active-Prompt outperforms Random-CoT by a significant margin. Random-CoT only performs comparably to another baseline model self-consistency, illustrating that our applied annotation process has no advantages, and it is the active selection strategy that leads to performance gains. For example, on the GSM8K dataset, Random-CoT (78.6) slightly outperforms SC (78.0) while significantly underperforming Active-Prompt (82.2) by 3.6%. The full results of Random-CoT on all datasets are reported in Table 1 with a consistent performance drop compared with Active-Prompt.

Effects of Annotators In our main experiments, we asked the annotator not to do trial and error with minimum human engineering because the focus of our method is the question selection, rather than the best possible annotation. However, different annotators can still cause variations in the performance. In this section, we discuss the effects of different annotators. In addition to the annotator (annotator A), we directly use the human-annotated rationales from the GSM8K dataset (annotator B). The results are reported in Table 2. The results of annotators A and B are consistently better than baseline models, demonstrating the robustness of our proposed selection method. Surprisingly, we found that directly applying the solutions provided by GSM8K outperforms our annotated rationales, suggesting that the existing annotation of GSM8K is of high quality. In addition, we note that human prompt engineering has two complementary components: question selection and prompt template engineering. The method proposed in this work provides a good solution to the first problem. It is also possible to combine this technique with human-optimized prompt templates to further improve performance.

Effects of Uncertainty Metrics In our main experiments, we adopt disagreement and entropy as the uncertainty metric. In addition to those, other

Figure 2: Comparison among the different numbers of predicted answers.

<!-- image -->

uncertainty metrics can be incorporated. In this section, we mainly discuss four uncertainty metrics: disagreement, entropy, variance, and selfconfidence. The definitions of the first three metrics are illustrated in Section 2.1 and the definition of self-confidence can be found in Appendix D. First, we found that disagreement is not applicable to datasets with limited search space. For example, the StrategyQA has only two labels (yes or no), and the predictions often tie in the maximum disagreement 2/2=1. Therefore, we adopt entropy for StrategyQA. Second, the self-confidence-based method performs badly, so we did not conduct more experiments with it. We displayed an example of its prediction in Table 8. We conjecture that it is because GPT-3 is prone to be over-confident, which is consistent with previous observations (Si et al., 2022). Introducing an external well-trained discriminator to evaluate confidence is a practical way, and we leave it to future work. Last, the comparison between disagreement-, entropy- and variance-based methods are shown in Table 2. The results illustrate that they perform competitively well on ASDiv and SingleEq, while disagreement and entropy outperform variance in GSM8K. Therefore, we simply choose disagreement and entropy as the primary metrics in our main experiments.

Effects of Pool Size In the first step for uncertainty estimation, we generate k answers for each input question to construct a pool of predictions. Here, k affects the performance of estimating the uncertainty, further affecting the downstream task's performance. To show the effect of the number of predicted answers, we plot the accuracy with respect to varying numbers of predicted answers (1, 5, 10, 15) in Figure 2 based on text-davinci-003 . The results show that with the increase in pool size, the performance continues to increase and will converge at k ' 10 . It is intuitive that a small k may confuse the selection process, leading to ties, while

Table 3: Analysis of the transferability of active exemplars. CD-002, TD-002, TD-003 denote code-davinci-002 , text-davinci-002 , and text-davinci-003 . TD-002 (CoT), TD-002 (SC), and TD-003 (CoT) are three baseline methods without Active-Prompt. TD-002 Ñ TD-002 (CoT) denotes selecting exemplars by text-davinci-002 and inference by text-davinci-002 . CD-002 Ñ TD-002 (SC) denotes selecting exemplars by code-davinci-002 and inference by text-davinci-002 . CD-002 Ñ TD-003 (CoT) denotes selecting exemplars by code-davinci-002 and inference by text-davinci-003 .

| METHOD                |      |      |   GSM8K CSQA Letter (4) |
|-----------------------|------|------|-------------------------|
| TD-002 (CoT)          | 46.9 | 73.5 |                    56.6 |
| TD-002 Ñ TD-002 (CoT) | 48.4 | 74.7 |                    57.7 |
| TD-002 (SC)           | 58.2 | 72.9 |                    57.6 |
| CD-002 Ñ TD-002 (SC)  | 73.2 | 76.6 |                    67.7 |
| TD-003 (CoT)          | 61.7 | 76.2 |                    70.2 |
| CD-002 Ñ TD-003 (CoT) | 65.6 | 78.9 |                    71.2 |
| TD-003 Ñ TD-003 (CoT) | 67.2 | 80.8 |                    73.7 |

a large k will lead to more accurate uncertainty estimation with better performance.

## 5.2 Uncertainty Analysis

The motivation of our proposed method is reducing the model's uncertainty to help elicit the reasoning ability of LLMs, further improving the few-shot prompting performance. In this section, we display the relationship between uncertainty and accuracy. In Appendix A Figure 3, we report the uncertainty quantity and accuracy on GSM8K, ASDiv, and SingleEq. We observe that there is a highly negative correlation between uncertainty and accuracy. With the decrease in uncertainty, the accuracy increases, demonstrating that reducing the model's uncertainty indeed helps improve the fewshot prompting-based predictions.

Table 4: Comparison with weaker models. Bold represents the best among each dataset.

| METHOD                          |   GSM8K |   AsDiv |   SVAMP |   SingleEq |
|---------------------------------|---------|---------|---------|------------|
| CoT (Llama2-70b-chat)           |    54.8 |    73.2 |    77.4 |       84.6 |
| Active-Prompt (Llama2-70b-chat) |    57.7 |    74.5 |    82.2 |       86.8 |
| CoT (gpt-3.5-turbo)             |    74.2 |    82.5 |    83.8 |       95   |
| Active-Prompt (gpt-3.5-turbo)   |    77.1 |    83.6 |    85.5 |       96   |

Table 5: Transferability between gpt-3.5-turbo and Llama models.

| METHOD                            |   GSM8K |   AsDiv |   SVAMP |   SingleEq |
|-----------------------------------|---------|---------|---------|------------|
| gpt-3.5-turbo                     |    74.2 |    82.5 |    83.8 |       95   |
| gpt-3.5-turbo Ñ gpt-3.5-turbo     |    77.1 |    83.6 |    85.5 |       96   |
| Llama2-70b-chat Ñ gpt-3.5-turbo   |    78.7 |    85.9 |    84.2 |       95.4 |
| Llama2-70b-chat                   |    54.8 |    73.2 |    77.4 |       84.6 |
| Llama2-70b-chat Ñ Llama2-70b-chat |    56.9 |    74.9 |    82.5 |       83.2 |
| gpt-3.5-turbo Ñ Llama2-70b-chat   |    58.9 |    74.7 |    81.2 |       86   |

## 7 Conclusion

In this paper, we proposed Active-Prompt to elicit reasoning abilities in large language models (LLMs). Inspired by the idea of annotating reasoning steps to obtain effective exemplars, we aim to select the most helpful questions for annotation judiciously instead of arbitrarily. For this purpose, we propose an uncertainty-based active selection strategy to determine which questions are the most

important and helpful to annotate from a pool of task-specific questions. We introduce four different strategies of uncertainty estimation for ActivePrompt: disagreement, entropy, variance, and selfconfidence. These four strategies characterize uncertainty from different perspectives, and we primarily apply disagreement and entropy. Empirically, Active-Prompt achieved a promising performance on eight widely used datasets for arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Further analyses of different uncertainty metrics, annotators, pool sizes, zero-shot learning, and an accuracy-uncertainty relationship demonstrate the effectiveness of our method.

## Limitations

We have shown that Active-Prompt demonstrates superior performance over previous chain-ofthought prompting methods. While exciting, there are several limitations to the current work with future opportunities.

Experiments with more models. In our experiments, we present the complete results of text-davinci-002 and code-davinci-002 since code-davinci-002 is free of charge in the initial limited beta period. However, due to the high cost of text-davinci-002 and text-davinci-003 , we were only able to carry out experiments with one of them. In addition, one promising direction is to experiment with more powerful models like GPT-4 (OpenAI, 2023). Unfortunately, conducting experiments with GPT-4 APIs is too costly. Furthermore, we did not conduct the experiments of self-consistency with gpt-3.5-turbo due to the cost. In the future, we plan to conduct experiments with GPT-4 and self-consistency experiments with gpt-3.5-turbo once we have more budgets.

Reproducibility In our experiments, we conduct most of the experiments with code-davinci-002 since it is free of charge in the initial limited beta period. The experiments of code-davinci-002 are finished before March 2023. However, OpenAI decided to shut off the access to code-davinci-002 , which makes it difficult for researchers to reproduce our experiments. However, one can access it via OpenAI's researcher access program 3 although the authors still do not have access to it.

## B Uncertainty Analysis

Figure 3 shows the relation between accuracy and uncertainty.

## C Variance Analysis

In our primary experiment, the step of uncertainty estimation necessitates querying each prompt in the training set k times to assess uncertainty. However, for datasets with a large number of instances - such as the GSM8K training set, which comprises 7,473 instances-to conserve resources we randomly sample 1,000 instances to estimate uncertainty. To expose the inherent randomness in this sampling process, we repeated the random sampling three times, aiming to examine its vari-

ance. The results, as illustrated in Table 7, reveal that our method demonstrates robustness against the randomness of sampling. Sampling 1,000 instances proved to be sufficient for achieving stable and satisfactory results.

Table 7: Experimental results on the GSM8K dataset with three seeds.

| DATASET   |   Seed 1 |   Seed 12 |   Seed 42 |
|-----------|----------|-----------|-----------|
| GSM8K     |     78.5 |      78.5 |      78.4 |

## Comparison with Diversity-based Methods

Table 10: Comparison with Auto-CoT. The results of Auto-CoT are taken directly from the original paper. For a fair comparison, none of the results apply the self-consistency method. Active-Prompt applies the rationales annotated by humans. Bold represents the best among each dataset. All the results are obtained with code-davinci-002 .

| METHOD        |   GSM8K |   MultiArith |   AddSub |
|---------------|---------|--------------|----------|
| Auto-CoT      |    62.8 |         93.2 |     91.9 |
| Active-Prompt |    67   |         95.5 |     93.2 |

Auto-CoT (Zhang et al., 2022b) proposes a diversity-based method for question selection, and ours proposes an uncertainty-based method for it. In this section, we compare our method with AutoCoT to demonstrate their effectiveness and differences. Owing to Auto-CoT only reporting the results on GSM8K, MultiArith, and AddSub on code-davinci-002 without self-consistency, we first compare our method with it on these three datasets in the same setting. The results are shown in Table 10. It is observed that Active-Prompt outperforms Auto-CoT by a large margin. We attribute the improvement to uncertainty-based selection and human annotation. Note that both diversity and

Table 11: Comparison with Complex-CoT. Bold represents the best among each dataset. All the results are obtained with gpt-3.5-turbo .

| METHOD            |   GSM8K AsDiv SVAMP SingleEq |      |      |      |
|-------------------|------------------------------|------|------|------|
| Complex-CoT       |                         76.3 | 82.4 | 79.9 | 93.3 |
| Active-Prompt (D) |                         77.1 | 83.6 | 85.5 | 96   |

uncertainty are useful for selecting the most informative questions, and they are complementary. We consider the combination of diversity and uncertainty as an important future direction.

## G Comparison with Complexity-based Methods

Complex-CoT (Fu et al., 2022) is a strong baseline which takes the complexity of prompts into consideration and proposes to select those complex prompts as exemplars. We find that ActivePrompt outperforms Complex-CoT, demonstrating the effectiveness of our proposed uncertainty-based methods. In addition, we can combine uncertainty and complexity to achieve better performance, and we leave this for future work.

