## Abstract

Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space ( i.e. , exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer , supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.

## 1. Introduction

'The infinite use of finite means.'

- Noam Chomsky (Chomsky, 1965)

Generative image models conditioned on text can now produce photorealistic and diverse images (Ramesh et al.,

2022; Saharia et al., 2022; Rombach et al., 2021; Yu et al., 2022; Chang et al., 2023). To further achieve customized generation, many recent works extend the textto-image models by introducing conditions such as segmentation maps (Rombach et al., 2021; Wang et al., 2022b; Couairon et al., 2022), scene graphs (Yang et al., 2022), sketches (Voynov et al., 2022), depthmaps (stability.ai, 2022), and inpainting masks (Xie et al., 2022; Wang et al., 2022a), or by finetuning the pretrained models on a few subject-specific data (Gal et al., 2022; Mokady et al., 2022; Ruiz et al., 2022). Nevertheless, these models still provide only a limited degree of controllability for designers when it comes to using them for practical applications. For example, generative models often struggle to accurately produce images with specifications for semantics, shape, style, and color all at once, which is common in real-world design projects.

We argue that the key to controllable image generation relies not only on conditioning, but even more significantly on compositionality (Lake et al., 2017). The latter can exponentially expand the control space by introducing an enormous number of potential combinations ( e.g., a hundred images with eight representations each yield about 100 8 combinations). Similar concepts are explored in the fields of language and scene understanding (Keysers et al., 2019; Johnson et al., 2016), where the compositionality is termed compositional generalization , the skill of recognizing or generating a potentially infinite number of novel combinations from a limited number of known components.

In this work, we build upon the above idea and present Composer, a realization of compositional generative models . By compositional generative models , we refer to generative models that are capable of seamlessly recombining visual components to produce new images (Figure 1). Specifically, we implement Composer as a multi-conditional diffusion model with a UNet backbone (Nichol et al., 2021). At every training iteration of Composer, there are two phases: in the decomposition phase, we break down images in a batch into individual representations using computer vision algorithms or pretrained models; whereas in the composition phase, we optimize Composer so that it can reconstruct these images from their representation subsets. Despite being trained

Figure 1. Concept of compositional image synthesis , which first decomposes an image to a set of basic components and then recomposes a new one with high creativity and controllability. To this end, the components in various formats serve as conditions in the generation process and allow flexible customization at the inference stage. Best viewed in large size.

<!-- image -->

with only a reconstruction objective, Composer is capable of decoding novel images from unseen combinations of representations that may come from different sources and potentially incompatible with one another.

While conceptually simple and easy to implement, Composer is surprisingly powerful, enabling encouraging performance on both traditional and previously unexplored image generation and manipulation tasks, including but not limited to: text-to-image generation, multi-modal conditional image generation, style transfer, pose transfer, image translation, virtual try-on, interpolation and image variation from various directions, image reconfiguration by modifying sketches, depth or segmentation maps, colorization based on optional palettes , and more. Moreover, by introducing an orthogonal representation of masking , Composer is able to restrict the editable region to a user-specified area for all the above operations, more flexible than the traditional inpainting operation, while also preventing modification of pixels outside this region. Despite being trained in a multitask manner, Composer achieves a zero-shot FID of 9.2 in text-to-image synthesis on the COCO dataset (Lin et al., 2014) when using only caption as the condition, indicating its ability to produce high-quality results.

## 2. Method

Our framework comprises the decomposition phase, where an image is divided into a set of independent components; and the composition phase, where the components are reassembled utilizing a conditional diffusion model. We first give a brief introduction to diffusion models and the guidance directions enabled by Composer. Subsequently, we explain the implementation of image decomposition and composition in details.

## 3.1. Training Details

We train a 2B parameter base model for conditional image generation at 64 × 64 resolution, a 1.1B parameter model for upscaling images to 256 × 256 resolution, and a 300M parameter model for further upscaling images to 1024 × 1024 resolution. Additionally, we trained a 1B parameter prior model for optionally projecting captions to image embeddings. We use batch sizes of 4096, 1024, 512, and 512 for the prior, base, and two upsampling models, respectively. We train on a combination of public datasets, including ImageNet21K (Russakovsky et al., 2014), WebVision (Li et al., 2017), and a filtered version of the LAION dataset (Schuhmann et al., 2022) with around 1B images. We eliminate duplicates, low resolution images, and images potentially contain harmful content from the LAION dataset. For the base model, we pretrain it with 1M

steps on the full dataset using only image embeddings as the condition, and then finetune the model on a subset of 60M examples (excluding LAION images with aesthetic scores below 7.0) from the original dataset for 200K steps with all conditions enabled. The prior and upsampling models are trained for 1M steps on the full dataset.

## 5. Conclusion and Discussion

Our decomposition-composition paradigm demonstrates that when conditions are composable rather than used independently, the control space of generative models can be vastly expanded. As a result, a broad range of traditional generative tasks can be reformulated using our Composer architecture, and previously unexplored generative abilities are unveiled, motivating further research into a variety of decomposition algorithms that can achieve increased controllability. In addition, we present multiple ways to utilize Composer for a range of image generation and manipulation tasks based on classifier-free and bidirectional guidance, giving useful references for future research.

Although we empirically find a simple, workable configuration for joint training of multiple conditions in Section 2.3, the strategy is not perfect, e.g., it may downweight the single-conditional generation performance. For example, without access to global embeddings, sketch- or depthbased generation usually produces relatively dark images. Another issue is that conflicts may exist when incompatible conditions occur. For instance, text embeddings are often downweighted in generated results when image and text embeddings with different semantics are jointly used.

Previous studies (Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022) highlight the potential risks associated with image generation models, such as deceptive and harmful content. Composer's improvements in controllability further raise this risk. We intend to thoroughly investigate how Composer can mitigate the risk of misuse and possibly creating a filtered version before making the work public.

## A. Architecture Details

Table 1. Hyperparameters for Composer. We use DPM-Solver++ (Lu et al., 2022b) as the sampling algorithm for all diffusion models.

|                          | Prior      | 64         | 64 → 256   | 256 → 1024   |
|--------------------------|------------|------------|------------|--------------|
| Diffusion steps          | 1000       | 1000       | 1000       | 1000         |
| Noise schedule           | cosine     | cosine     | cosine     | linear       |
| Sampling steps           | 100        | 50         | 20         | 10           |
| Sampling variance method | dpm-solver | dpm-solver | dpm-solver | dpm-solver   |
| Model size               | 1B         | 2B         | 1.1B       | 300M         |
| Channels                 | -          | 512        | 320        | 192          |
| Depth                    | -          | 3          | 3          | 2            |
| Channels multiple        | -          | 1,2,3,4    | 1,2,3,5    | 1,1,2,2,4,4  |
| Heads channels           | -          | 64         | 64         | -            |
| Attention resolution     | -          | 32,16,8    | 32,16      | -            |
| Dropout                  | -          | 0.1        | 0.1        | -            |
| Weight decay             | 6.0e-2     | -          | -          | -            |
| Batch size               | 4096       | 1024       | 512        | 512          |
| Iterations               | 1M         | 1M         | 1M         | 1M           |
| Learning rate            | 1.1e-4     | 1.2e-4     | 1.1e-4     | 1.0e-4       |
| Adam β 2                 | 0.96       | 0.999      | 0.999      | 0.999        |
| Adam glyph[epsilon1]     | 1.0e-6     | 1.0e-8     | 1.0e-8     | 1.0e-8       |
| EMA decay                | 0.9999     | 0.9999     | 0.9999     | 0.9999       |

