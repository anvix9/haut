## Abstract

Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼ 1 day on a single 8-A100 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available.

## 1. Introduction

Large multimodal models (LMMs) have become increasingly popular in the research community, as they are the key building blocks towards general-purpose assistants [2, 30, 43]. Recent studies on LMMs are converging on a central concept known as visual instruction tuning [36]. The results are promising, e.g . LLaVA [36] and MiniGPT-4 [62] demonstrate impressive results on natural instruction-following and visual reasoning capabilities. To better understand the capability of LMMs, multiple benchmarks [17, 27, 34, 37, 55] have been proposed. Recent works further demonstrate improved performance by scaling up the pretraining data [3, 14, 54], instruction-following data [14, 18, 29, 58], visual encoders [3], or language models [39], respectively. The LLaVA architecture is also leveraged in different downstream tasks and domains, including region-level [8, 56] and pixel-level [26, 50] understanding, biomedical assistants [31], image generation [5], adversarial studies [6, 59].

Figure 1. LLaVA-1.5 achieves SoTA on a broad range of 11 tasks (Top), with high training sample efficiency (Left) and simple modifications to LLaVA (Right): an MLP connector and including academic-task-oriented data with response formatting prompts.

<!-- image -->

However, despite many benchmarks and developments, it still remains unclear what the best recipe is to train LMMs towards the goal of general-purpose assistants. For example, LLaVA [36] excels in conversational-style visual reasoning and even outperforms later approaches like InstructBLIP [14] on such benchmarks [55], while InstructBLIP excels in traditional VQA benchmarks that demands singleword or short answers. Given the significant differences in the model architecture and training data between them, the root cause of the disparity in their capabilities remains elusive, despite conjectures [37, 55]: the amount of training data, the usage of resamplers like Qformer [32], etc . To this

end, we present the first systematic study to investigate the design choices of LMMs in a controlled setting. Our study originates from LLaVA and builds a road map by carefully making effective contributions from the perspectives of the input, model, and data.

First, we unveil that the fully-connected vision-language connector in LLaVA is surprisingly powerful and dataefficient, and we establish stronger and more feasible baselines built upon the LLaVA framework. We report that two simple improvements, namely, an MLP cross-modal connector and incorporating academic task related data such as VQA, are orthogonal to the framework of LLaVA, and when used with LLaVA, lead to better multimodal understanding capabilities. In contrast to InstructBLIP [14] or Qwen-VL [3], which trains specially designed visual resamplers on hundreds of millions or even billions of image-text paired data, LLaVA uses one of the simplest architecture design for LMMs and requires only training a simple fullyconnected projection layer on merely 600K image-text pairs. Our final model can finish training in ∼ 1 day on a single 8-A100 machine and achieves state-of-the-art results on a wide range of benchmarks. Moreover, unlike Qwen-VL [3] that includes in-house data in training, LLaVA utilizes only publicly available data.

Next, we delve into an early exploration of other open problems of large multimodal models. Our findings include: (1) Scaling to high-resolution image inputs. We show that LLaVA's architecture is versatile in scaling to higher resolutions by simply dividing images into grids and maintains its data efficiency; with the increased resolution, it improves the model's detailed perception capabilities and reduces hallucination. (2) Compositional capabilities. We find that large multimodal models are capable of generalizing to compositional capabilities. For example, training on long-form language reasoning together with shorter visual reasoning can improve the model's writing capability for multimodal questions. (3) Data efficiency. We show that randomly downsampling LLaVA's training data mixture by up to 75% does not significantly decrease the model's performance, suggesting that the possibility of a more sophisticated dataset compression strategy can further improve LLaVA's already efficient training pipeline. (4) Data scaling. We provide empirical evidence for the scaling of data granularity in conjunction with the model's capability is crucial for an improved capability without introducing artifacts like hallucination.

In sum, we perform a systematic study on the training of large multimodal models, and introduce a simple yet effective approach to balance the multitask learning and effective scaling for large multimodal models. Our improved baselines, LLaVA-1.5, uses only public data, achieves the state-of-the-art on a broad range of 11 tasks, and is significantly more data-efficient than previous approaches. By

rethinking the conventional approaches and exploring the open problems in visual instruction tuning, we pave the way for more robust and capable systems for LMMs. We hope these improved and easily-reproducible baselines will provide a reference for future research in open-source LMMs.

## 3. Approach



## 3.1. Preliminaries

As the seminal work of visual instruction tuning, LLaVA [36] showcases commendable proficiency in visual reasoning capabilities, surpassing even more recent models on diverse benchmarks [4, 55] for real-life visual instruction-following tasks. LLaVA uses a single linear layer to project the visual features to language space, and optimizes the whole LLM for visual instruction tuning. However, LLaVA falls short on academic benchmarks that typically require shortform answers ( e.g . single-word), and tends to answer yes for yes/no questions due to the lack of such data in the training distribution.

On the other hand, InstructBLIP [14] is the pioneer to incorporate academic-task-oriented datasets like VQA-v2 [19] along with LLaVA-Instruct [36], and demonstrates improved performance on VQA benchmarks. It pretrains Qformer [32] on 129M image-text pairs and only finetunes the instructionaware Qformer for visual instruction tuning. However, recent studies [7, 55] show that it does not perform as well as LLaVA on engaging in real-life visual conversation tasks. More specifically, as shown in Table 1a, it can overfit to VQA training sets with short-answers, even on requests that require detailed responses.

## 3.3. Scaling the Data and Model

MLP vision-language connector. Inspired by the improved performance in self-supervised learning by changing from a linear projection to an MLP [9, 10], we find that improving the vision-language connector's representation power with a two-layer MLP can improve LLaVA's multimodal capabilities, compared with the original linear projection.

Academic task oriented data. We further include additional academic-task-oriented VQA datasets for VQA, OCR, and region-level perception, to enhance the model's capabilities in various ways, as shown in Table 2. We first include

Figure 2. LLaVA-1.5-HD. Scaling LLaVA-1.5 to higher resolutions by splitting the image into grids and encoding them independently. This allows the model to scale to any resolution, without performing positional embedding interpolation for ViTs. We additionally concatenate the feature of a downsampled image to provide the LLM with a global context.

<!-- image -->

Table 2. Scaling results on ■ data, ■ model, and ■ resolution. We choose to conduct experiments on GQA [21], MME [17], and MM-Vet [55] to examine the representative capabilities of VQA with short answers, VQA with output formatting, and natural visual conversations, respectively. ∗ Training images of GQA were observed during training.

| Method                                            | LLM Res.                                          |                                                   |                                                   | GQA MME                                           | MM-Vet                                            |
|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|
| InstructBLIP                                      | 14B                                               | 224                                               | 49.5                                              | 1212.8                                            | 25.6                                              |
| Only using a subset of InstructBLIP training data | Only using a subset of InstructBLIP training data | Only using a subset of InstructBLIP training data | Only using a subset of InstructBLIP training data | Only using a subset of InstructBLIP training data | Only using a subset of InstructBLIP training data |
| 0 LLaVA                                           | 7B                                                | 224                                               | -                                                 | 809.6                                             | 25.5                                              |
| 1                                                 | +VQA-v2 7B                                        | 224                                               | 47.0                                              | 1197.0                                            | 27.7                                              |
| 2                                                 | +Format prompt 7B                                 | 224                                               | 46.8                                              | 1323.8                                            | 26.3                                              |
| 3                                                 | +MLP VL connector 7B                              | 224                                               | 47.3                                              | 1355.2                                            | 27.8                                              |
| 4                                                 | +OKVQA/OCR 7B                                     | 224                                               | 50.0                                              | 1377.6                                            | 29.6                                              |
| Additional scaling                                | Additional scaling                                | Additional scaling                                | Additional scaling                                | Additional scaling                                | Additional scaling                                |
| 5                                                 | +Region-level VQA 7B                              | 224                                               | 50.3                                              | 1426.5                                            | 30.8                                              |
| 6 +Scale up resolution                            | 7B                                                | 336                                               | 51.4                                              | 1450                                              | 30.3                                              |
| 7 +GQA                                            | 7B                                                | 336                                               | 62.0 ∗                                            | 1469.2                                            | 30.7                                              |
| 8 +ShareGPT                                       | 7B                                                | 336                                               | 62.0 ∗                                            | 1510.7                                            | 31.1                                              |
| 9 +Scale up LLM                                   | 13B                                               | 336                                               | 63.3 ∗                                            | 1531.3                                            | 36.1                                              |

four additional datasets that are used in InstructBLIP: openknowledge VQA (OKVQA [41], A-OKVQA [45]) and OCR (OCRVQA [42], TextCaps [47]). A-OKVQA is converted to multiple choice questions and a specific response formatting prompt is used: Answer with the option's letter from the given choices directly . With only a subset of the datasets InstructBLIP uses, LLaVA already surpasses it on all three tasks in Table 2, suggesting LLaVA's effective design. Furthermore, we find further adding region-level VQA datasets (Visual Genome [25], RefCOCO [24, 40]) improves the model's capability of localizing fine-grained visual details.

Additional scaling. We further scale up the input image resolution to 336 2 to allow the LLM to clearly 'see' the details of images, by swapping the vision encoder to CLIPViT-L-336px (the highest resolution available for CLIP). In addition, we add the GQA dataset as an additional visual knowledge source. We also incorporate ShareGPT [46] data and scale up the LLM to 13B as in [3, 8, 39]. Results on MM-

Vet shows the most significant improvement when scaling the LLM to 13B, suggesting the importance of the base LLM's capability for visual conversations.

LLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA [36].

Computational cost. For LLaVA-1.5, we use the same pretraining dataset, and keep the training iterations and batch size roughly the same for instruction tuning as LLaVA [36]. Due to the increased image input resolution to 336 2 , the training of LLaVA-1.5 is ∼ 2 × as long as LLaVA: ∼ 6 hours of pretraining and ∼ 20 hours of visual instruction tuning, using 8 × A100s.

## 6. Conclusion

In this paper, we take a step towards demystifying the design of large multimodal models, and propose a simple, effective, and data-efficient baseline, LLaVA-1.5, for large multimodal models. In addition, we explore the open problems in visual instruction tuning, scale LMMs to higher resolutions, and present some intriguing findings in terms of model hallucination and compositional capabilities for LMMs. We hope these improved and easily-reproducible baselines as well as the new findings will provide a reference for future research in open-source LMM.

Limitations. Despite the promising results demonstrated by LLaVA-1.5, it still has limitations including prolonged training for high-resolution images, lack of multiple-image understanding, limited problem solving capabilities in certain fields. It is not exempt from producing hallucinations, and should be used with caution in critical applications ( e.g . medical). See appendix for a detailed discussion.

Acknowledgements. This work was supported in part by NSF CAREER IIS2150012, and Institute of Information & communications Technology Planning & Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 20220-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training).

## A. Implementation Details



## Introduction:

I recently had the pleasure of visiting the beautiful Waikiki Beach in Hawaii, and I must say, it was an unforgettable experience. The crystal blue waters, the lively atmosphere, and the stunning cityscape in the background made it a perfect destination for a relaxing getaway. In this travel blog post, I'll share my experiences and must-see attractions during my trip to Waikiki Beach.

## Conclusion:

My trip to Waikiki Beach was an unforgettable experience filled with adventure, culture, and relaxation. From the breathtaking views of the ocean to the vibrant cityscape, this tropical paradise has something for everyone. I highly recommend visiting Waikiki Beach for a sunny escape and a chance to immerse yourself in the beautiful Hawaiian culture.

Compose an engaging travel blog post about a recent trip to this place , highlighting cultural experiences and must-see attractions .

In my recent trip to Honolulu, I had the opportunity to experience the vibrant local culture and explore the beautiful beaches that this city has to offer. One of the must-see attractions was Waikiki Beach, where I witnessed a lively atmosphere with people sunbathing, swimming, and playing beach games. The crystal blue waters of the Pacific Ocean were a stunning sight, and the white sandy beach provided the perfect spot for relaxation.

Aside from the beach, I also had the chance to immerse myself in the local culture by visiting the Pearl Harbor Memorial, which paid tribute to the events that led to the United States' involvement in World War II. The memorial was a somber reminder of the past, and it was fascinating to learn about the history surrounding the attack on Pearl Harbor.

Other cultural experiences included trying local Hawaiian cuisine, such as poke bowls and traditional Hawaiian dishes, which were delicious and offered a unique taste of the region. I also had the opportunity to explore the city's bustling streets, where I came across local shops, markets, and street performers, adding to the lively atmosphere.

Overall, my trip to Honolulu was an enriching experience, filled with cultural discoveries, breathtaking natural beauty, and unforgettable memories.

Figure 5. Compositional capability: multilingual visual conversation. LLaVA-1.5 generalizes to multilingual visual conversations, when training on visual instruction following data (English-only) together with the text-only ShareGPT data (multilingual). However, there can still be errors in some languages ( e.g . Korean, errors marked in red).

<!-- image -->

Query : What are the things I should be cautious about when I visit here?

Reference (English): When visiting the pier over the lake, there are a few things you should be cautious about. First, ensure that you have appropriate footwear with good traction, as the pier may be wet or slippery, especially if it has been raining or if there is algae on the surface. Second, be mindful of the weather conditions, as the image shows a cloudy sky, which could indicate potential rain or storms. It is essential to stay safe and avoid being caught in a sudden storm or lightning. Lastly, be aware of your surroundings and any potential hazards, such as the presence of boats or other watercraft in the area. Always follow any posted signs or guidelines to ensure a safe and enjoyable experience.

## C. Limitations

Despite the promising results demonstrated by LLaVA-1.5, several limitations must be acknowledged. First, LLaVA1.5 utilizes full image patches, potentially prolonging each training iteration. While visual resamplers [3, 14, 32] reduce the number of visual patches in LLMs, they currently cannot achieve convergence as efficiently as LLaVA with a comparable amount of training data, probably due to more trainable parameters in the resamplers. The development

of a sample-efficient visual resampler could pave the way for future scaling-up of instruction-following multimodal models. Second, LLaVA-1.5 is not yet capable of processing multiple images due to the lack of such instruction-following data, and the limit of the context length. Third, although LLaVA-1.5 exhibits proficiency in following complex instructions, its problem-solving capabilities can still be limited in certain domains, which could be improved with a more capable language model and with high-quality, targeted visual instruction tuning data. Finally, despite its significantly reduced propensity for hallucination, LLaVA-1.5 is not exempt from producing hallucinations and occasionally disseminating misinformation, and should be used with caution in critical applications ( e.g . medical).

