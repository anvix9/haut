## Abstract

Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $ 0.003-about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.

## 1 Introduction

Many NLP applications require high-quality labeled data, notably to train classifiers or evaluate the performance of unsupervised models. For example, researchers often aim to filter noisy social media data for relevance, assign texts to different topics or conceptual categories, or measure their sentiment or stance. Regardless of the specific approach used for these tasks (supervised, semi-supervised, or unsupervised), labeled data are needed to build a training set or a gold standard against which performance can be assessed. Such data may be available for high-level tasks such as semantic evaluation 1 and hate speech (ElSherief et al., 2021), as well as, sometimes, more specific tasks such as party ideology (Herrmann and Doring, 2023). More typically, however, researchers have to conduct original annotations to ensure that the labels match their conceptual categories (Benoit et al., 2016). Until recently, two main strategies were available. First, researchers can recruit and train coders, such as research assistants. Second, they can rely on crowd-workers on platforms such as Amazon Mechanical Turk (MTurk). Often, these two strategies are used in combination: trained annotators create a relatively small gold-standard dataset, and crowd-workers are employed to increase the volume of labeled data. Each strategy has its own advantages and disadvantages. Trained annotators tend to produce high-quality data, but involve significant costs. Crowd workers are a much cheaper and more flexible option, but the quality may be insufficient, particularly for complex tasks and languages other than English. Moreover, there have been concerns that MTurk data quality has decreased (Chmielewski and Kucker, 2020), while alternative platforms such as CrowdFlower and FigureEight are no longer practicable options for academic research since they were acquired by Appen, a company that is focused on a business market.

This paper explores the potential of large language models (LLMs) for text annotation tasks, with a focus on ChatGPT, which was released in November 2022. It demonstrates that zero-shot ChatGPT classifications (that is, without any additional training) outperform MTurk annotations, at a fraction of the cost. LLMs have been shown to perform very well for a wide range of purposes, including ideological scaling (Wu et al., 2023), the classification of legislative proposals (Nay, 2023), the resolution of cognitive psychology tasks, (Binz and Schulz, 2023), and the simulation of human samples for survey research (Argyle et al., 2023). While a few studies suggested that ChatGPT might perform text annotation tasks of the kinds we have described (Kuzman et al., 2023; Huang et al., 2023), to the best of our knowledge no systematic evaluation has been conducted yet. Our analysis relies on a sample of 2,382 tweets that we collected for a previous study

(Alizadeh et al., 2022). For that project, the tweets were labeled by trained annotators (research assistants) for five different tasks: relevance, stance, topics, and two kinds of frame detection. Using the same codebooks that we developed to instruct our research assistants, we submitted the tasks to ChatGPT as zero-shot classifications, as well as to crowd-workers on MTurk. We then evaluated the performance of ChatGPT against two benchmarks: (i) its accuracy, relative to that of crowd-workers, and (ii) its intercoder agreement, relative to that of crowd workers as well as of our trained annotators.

We find that for four out of five tasks, ChatGPT's zero-shot accuracy is higher than that of MTurk. For all tasks, ChatGPT's intercoder agreement exceeds that of both MTurk and trained annotators. Moreover, ChatGPT is significantly cheaper than MTurk: the five classification tasks cost about $ 68 on ChatGPT (25,264 annotations) and $ 657 on MTurk (12,632 annotations) (see Section 4 for details). ChatGPT's per-annotation cost is therefore about $ 0.003, or a third of a cent-about twenty times cheaper than MTurk, with higher quality. At this cost, it might potentially be possible to annotate entire samples, or to create large training sets for supervised learning. Based on our tests, 100,000 annotations would cost about $ 300.

While further research is needed to better understand how ChatGPT and other LLMs perform in a broader range of contexts, these results demonstrate their potential to transform how researchers conduct data annotations, and to disrupt parts of the business model of platforms such as MTurk.

## 3 Discussion

This paper demonstrates the potential of LLMs to transform text-annotation procedures for a variety of tasks common to many research projects. Despite the focus on a single dataset and the relatively limited number of tests, the evidence strongly suggests that LLMs may already be a superior approach compared to crowd-annotations on platforms such as MTurk. At the very least, the findings demonstrate the importance of studying the text-annotation properties and capabilities of LLMs more in depth. The following questions and steps seem particularly promising: (i) performance of ChatGPT across multiple languages; (ii) performance of ChatGPT across multiple types of text (social media, news media, legislation, speeches, etc.); (iii) implementation of few-shot learning on ChatGPT, compared with fine-tuned models such as BERT and RoBERTa; (iv) construction of semi-automated data labeling systems in which a model first learns by observing human annotations, and is then used to recommend or even automate the labeling (Desmond et al., 2021); (v) using chain of thought prompting and other strategies to increase the performance of zero-shot reasoning (Kojima et al., 2022); (vi) and of course

implementation of annotation tasks with GPT-4, as soon as availability permits.

## 4 Materials and Methods



