## 1 INTRODUCTION

Meshes and points are the most common 3D scene representations because they are explicit and are a good /fit for fast GPU/CUDA-based rasterization. In contrast, recent Neural Radiance Field (NeRF) methods build on continuous scene representations, typically optimizing a Multi-Layer Perceptron (MLP) using volumetric ray-marching for novel-view synthesis of captured scenes. Similarly, the most e/fficient radiance /field solutions to date build on continuous representations by interpolating values stored in, e.g., voxel [Fridovich-Keil and Yu et al. 2022] or hash [M√ºller et al. 2022] grids or points [Xu et al. 2022]. While the continuous nature of these methods helps optimization, the stochastic sampling required for rendering is costly and can result in noise. We introduce a new approach that combines the best of both worlds: our 3D Gaussian representation allows optimization with state-of-the-art (SOTA) visual quality and competitive training times, while our tile-based splatting solution ensures real-time rendering at SOTA quality for 1080p resolution on several previously published datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch et al. 2017] (see Fig. 1).

Our goal is to allow real-time rendering for scenes captured with multiple photos, and create the representations with optimization times as fast as the most e/fficient previous methods for typical real scenes. Recent methods achieve fast training [Fridovich-Keil

and Yu et al. 2022; M√ºller et al. 2022], but struggle to achieve the visual quality obtained by the current SOTA NeRF methods, i.e., Mip-NeRF360 [Barron et al. 2022], which requires up to 48 hours of training time. The fast - but lower-quality - radiance /field methods can achieve interactive rendering times depending on the scene (10-15 frames per second), but fall short of real-time rendering at high resolution.

Our solution builds on three main components. We /first introduce 3D Gaussians as a /flexible and expressive scene representation. We start with the same input as previous NeRF-like methods, i.e., cameras calibrated with Structure-from-Motion (SfM) [Snavely et al. 2006] and initialize the set of 3D Gaussians with the sparse point cloud produced for free as part of the SfM process. In contrast to most point-based solutions that require Multi-View Stereo (MVS) data [Aliev et al. 2020; Kopanas et al. 2021; R√ºckert et al. 2022], we achieve high-quality results with only SfM points as input. Note that for the NeRF-synthetic dataset, our method achieves high quality even with random initialization. We show that 3D Gaussians are an excellent choice, since they are a di/fferentiable volumetric representation, but they can also be rasterized very e/fficiently by projecting them to 2D, and applying standard ùõº -blending, using an equivalent image formation model as NeRF. The second component of our method is optimization of the properties of the 3D Gaussians - 3D position, opacity ùõº , anisotropic covariance, and spherical harmonic (SH) coe/fficients - interleaved with adaptive density control steps, where we add and occasionally remove 3D Gaussians during optimization. The optimization procedure produces a reasonably compact, unstructured, and precise representation of the scene (1-5 million Gaussians for all scenes tested). The third and /final element of our method is our real-time rendering solution that uses fast GPU sorting algorithms and is inspired by tile-based rasterization, following recent work [Lassner and Zollhofer 2021]. However, thanks to our 3D Gaussian representation, we can perform anisotropic splatting that respects visibility ordering - thanks to sorting and ùõº -blending - and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required.

To summarize, we provide the following contributions:

- ¬∑ Theintroduction of anisotropic 3D Gaussians as a high-quality, unstructured representation of radiance /fields.
- ¬∑ An optimization method of 3D Gaussian properties, interleaved with adaptive density control that creates high-quality representations for captured scenes.
- ¬∑ A fast, di/fferentiable rendering approach for the GPU, which is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis.

Our results on previously published datasets show that we can optimize our 3D Gaussians from multi-view captures and achieve equal or better quality than the best quality previous implicit radiance /field approaches. We also can achieve training speeds and quality similar to the fastest methods and importantly provide the /first real-time rendering with high quality for novel-view synthesis.

## 3 OVERVIEW

The input to our method is a set of images of a static scene, together with the corresponding cameras calibrated by SfM [Sch√∂nberger and Frahm 2016] which produces a sparse point cloud as a sidee/ffect. From these points we create a set of 3D Gaussians (Sec. 4), de/fined by a position (mean), covariance matrix and opacity ùõº , that allows a very /flexible optimization regime. This results in a reasonably compact representation of the 3D scene, in part because highly anisotropic volumetric splats can be used to represent /fine structures compactly. The directional appearance component (color) of the radiance /field is represented via spherical harmonics (SH), following standard practice [Fridovich-Keil and Yu et al. 2022; M√ºller et al. 2022]. Our algorithm proceeds to create the radiance /field representation (Sec. 5) via a sequence of optimization steps of 3D Gaussian parameters, i.e., position, covariance, ùõº and SH coe/fficients interleaved with operations for adaptive control of the Gaussian density. The key to the e/fficiency of our method is our tile-based rasterizer (Sec. 6) that allows ùõº -blending of anisotropic splats, respecting visibility order thanks to fast sorting. Out fast rasterizer also includes a fast backward pass by tracking accumulated ùõº values, without a limit on the number of Gaussians that can receive gradients. The overview of our method is illustrated in Fig. 2.

## 7.3 Ablations

We isolated the di/fferent contributions and algorithmic choices we made and constructed a set of experiments to measure their e/ffect. Speci/fically we test the following aspects of our algorithm: initialization from SfM, our densi/fication strategies, anisotropic covariance, the fact that we allow an unlimited number of splats to have gradients and use of spherical harmonics. The quantitative e/ffect of each choice is summarized in Table 3.

Initialization from SfM. We also assess the importance of initializing the 3D Gaussians from the SfM point cloud. For this ablation, we uniformly sample a cube with a size equal to three times the extent of the input camera's bounding box. We observe that our method performs relatively well, avoiding complete failure even without the SfM points. Instead, it degrades mainly in the background, see Fig. 7. Also in areas not well covered from training views, the random initialization method appears to have more /floaters that cannot be removed by optimization. On the other hand, the synthetic NeRF dataset does not have this behavior because it has no background and is well constrained by the input cameras (see discussion above).

Densi/fication. We next evaluate our two densi/fication methods, more speci/fically the clone and split strategy described in Sec. 5. We disable each method separately and optimize using the rest of the method unchanged. Results show that splitting big Gaussians is important to allow good reconstruction of the background as seen in Fig. 8, while cloning the small Gaussians instead of splitting them allows for a better and faster convergence especially when thin structures appear in the scene.

<!-- image -->

<!-- image -->

<!-- image -->

Fig. 7. Initialization with SfM points helps. Above: initialization with a random point cloud. Below: initialization using SfM points.

<!-- image -->

No Split-5k

No Clone-5k

Fig. 8. Ablation of densification strategy for the two cases "clone" and "split" (Sec. 5).

<!-- image -->

Unlimited depth complexity of splats with gradients. We evaluate if skipping the gradient computation after the ùëÅ front-most points

<!-- image -->

Fig. 9. If we limit the number of points that receive gradients, the e/ffect on visual quality is significant. Le/f\_t: limit of 10 Gaussians that receive gradients. Right: our full method.

<!-- image -->

will give us speed without sacri/ficing quality, as suggested in Pulsar [Lassner and Zollhofer 2021]. In this test, we choose N=10, which is two times higher than the default value in Pulsar, but it led to unstable optimization because of the severe approximation in the gradient computation. For the T/r.sc/u.sc/c.sc/k.sc scene, quality degraded by 11dB in PSNR (see Table 3, Limited-BW), and the visual outcome is shown in Fig. 9 for G/a.sc/r.sc/d.sc/e.sc/n.sc.

Anisotropic Covariance. An important algorithmic choice in our method is the optimization of the full covariance matrix for the 3D Gaussians. To demonstrate the e/ffect of this choice, we perform an ablation where we remove anisotropy by optimizing a single scalar value that controls the radius of the 3D Gaussian on all three axes. The results of this optimization are presented visually in Fig. 10. We observe that the anisotropy signi/ficantly improves the quality of the 3D Gaussian's ability to align with surfaces, which in turn allows for much higher rendering quality while maintaining the same number of points.

Spherical Harmonics. Finally, the use of spherical harmonics improves our overall PSNR scores since they compensate for the viewdependent e/ffects (Table 3).

## 7.4 Limitations

Our method is not without limitations. In regions where the scene is not well observed we have artifacts; in such regions, other methods also struggle (e.g., Mip-NeRF360 in Fig. 11). Even though the anisotropic Gaussians have many advantages as described above, our method can create elongated artifacts or 'splotchy' Gaussians (see Fig. 12); again previous methods also struggle in these cases.

We also occasionally have popping artifacts when our optimization creates large Gaussians; this tends to happen in regions with view-dependent appearance. One reason for these popping artifacts is the trivial rejection of Gaussians via a guard band in the rasterizer. A more principled culling approach would alleviate these artifacts. Another factor is our simple visibility algorithm, which can lead to Gaussians suddenly switching depth/blending order. This could be addressed by antialiasing, which we leave as future work. Also, we currently do not apply any regularization to our optimization; doing so would help with both the unseen region and popping artifacts.

While we used the same hyperparameters for our full evaluation, early experiments show that reducing the position learning rate can be necessary to converge in very large scenes (e.g., urban datasets).

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Fig. 10. We train scenes with Gaussian anisotropy disabled and enabled. The use of anisotropic volumetric splats enables modelling of fine structures and has a significant impact on visual quality. Note that for illustrative purposes, we restricted F/i.sc/c.sc/u.sc/s.sc to use no more than 5k Gaussians in both configurations.

<!-- image -->

Even though we are very compact compared to previous pointbased approaches, our memory consumption is signi/ficantly higher than NeRF-based solutions. During training of large scenes, peak GPU memory consumption can exceed 20 GB in our unoptimized prototype. However, this /figure could be signi/ficantly reduced by a careful low-level implementation of the optimization logic (similar to InstantNGP). Rendering the trained scene requires su/fficient GPU memory to store the full model (several hundred megabytes for large-scale scenes) and an additional 30-500 MB for the rasterizer, depending on scene size and image resolution. We note that there are many opportunities to further reduce memory consumption of our method. Compression techniques for point clouds is a wellstudied /field [De Queiroz and Chou 2016]; it would be interesting to see how such approaches could be adapted to our representation.

<!-- image -->

Fig. 11. Comparison of failure artifacts: Mip-NeRF360 has 'floaters' and grainy appearance (le/f\_t, foreground), while our method produces coarse, anisoptropic Gaussians resulting in low-detail visuals (right, background). T/r.sc/a.sc/i.sc/n.sc scene.

<!-- image -->

<!-- image -->

Fig. 12. In views that have li/t\_tle overlap with those seen during training, our method may produce artifacts (right). Again, Mip-NeRF360 also has artifacts in these cases (le/f\_t). D/r.scJ/o.sc/h.sc/n.sc/s.sc/o.sc/n.sc scene.

<!-- image -->

## 8 DISCUSSION AND CONCLUSIONS

We have presented the /first approach that truly allows real-time, high-quality radiance /field rendering, in a wide variety of scenes and capture styles, while requiring training times competitive with the fastest previous methods.

Our choice of a 3D Gaussian primitive preserves properties of volumetric rendering for optimization while directly allowing fast splat-based rasterization. Our work demonstrates that - contrary to widely accepted opinion - a continuous representation is not strictly necessary to allow fast and high-quality radiance /field training.

The majority ( ‚àº 80%) of our training time is spent in Python code, since we built our solution in PyTorch to allow our method to be easily used by others. Only the rasterization routine is implemented as optimized CUDA kernels. We expect that porting the remaining optimization entirely to CUDA, as e.g., done in InstantNGP [M√ºller et al. 2022], could enable signi/ficant further speedup for applications where performance is essential.

We also demonstrated the importance of building on real-time rendering principles, exploiting the power of the GPU and speed of software rasterization pipeline architecture. These design choices are the key to performance both for training and real-time rendering, providing a competitive edge in performance over previous volumetric ray-marching.

It would be interesting to see if our Gaussians can be used to perform mesh reconstructions of the captured scene. Aside from practical implications given the widespread use of meshes, this would allow us to better understand where our method stands exactly in the continuum between volumetric and surface representations.

In conclusion, we have presented the /first real-time rendering solution for radiance /fields, with rendering quality that matches the best expensive previous methods, with training times competitive with the fastest existing solutions.

