## Abstract

ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called Visual ChatGPT , incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at https: //github.com/microsoft/visual-chatgpt .

## 1. Introduction

In recent years, the development of Large language models (LLMs) has shown incredible progress, such as T5 [32], BLOOM [36], and GPT-3 [5]. One of the most significant breakthroughs is ChatGPT, which is built upon InstructGPT[29], specifically trained to interact with users in a genuinely conversational manner, thus allowing it to maintain the context of the current conversation, handle follow-up questions, and correct answer produced by itself.

Although powerful, ChatGPT is limited in its ability to process visual information since it is trained with a

Figure 1. Architecture of Visual ChatGPT.

<!-- image -->

single language modality, while Visual Foundation Models (VFMs) have shown tremendous potential in computer vision, with their ability to understand and generate complex images. For instance, BLIP Model [22] is an expert in understanding and providing the description of an image. Stable Diffusion [35] is an expert in synthesizing an image based on text prompts. However, suffering from the task specification nature, the demanding and fixed input-output formats make the VFMs less flexible than conversational language models in human-machine interaction.

Could we build a ChatGPT-like system that also supports image understanding and generation? One intuitive idea is to train a multi-modal conversational model. However, building such a system would consume a large amount of data and computational resources. Besides, another challenge comes that what if we want to incorporate modalities beyond languages and images, like videos or voices? Would it be necessary to train a totally new multi-modality model every time when it comes to new modalities or functions?

We answer the above questions by proposing a system named Visual ChatGPT . Instead of training a new multimodal ChatGPT from scratch, we build Visual ChatGPT directly based on ChatGPT and incorporate a variety of VFMs. To bridge the gap between ChatGPT and these VFMs, we propose a Prompt Manager which supports the following functions: 1) explicitly tells ChatGPT the capa-

bility of each VFM and specifies the input-output formats; 2) converts different visual information, for instance, png images, the depth images and mask matrix, to language format to help ChatGPT understand; 3) handles the histories, priorities, and conflicts of different Visual Foundation Models. With the help of the Prompt Manager, ChatGPT can leverage these VFMs and receives their feedback in an iterative manner until it meets the requirements of users or reaches the ending condition.

As shown in Fig. 1, a user uploads an image of a yellow flower and enters a complex language instruction 'please generate a red flower conditioned on the predicted depth of this image and then make it like a cartoon, step by step'. With the help of Prompt Manager, Visual ChatGPT starts a chain of execution of related Visual Foundation Models. In this case, it first applies the depth estimation model to detect the depth information, then utilizes the depth-to-image model to generate a figure of a red flower with the depth information, and finally leverages the style transfer VFM based on the Stable Diffusion model to change the style of this image into a cartoon. During the above pipeline, Prompt Manager serves as a dispatcher for ChatGPT by providing the type of visual formats and recording the process of information transformation. Finally, when Visual ChatGPT obtains the hints of 'cartoon' from Prompt Manager, it will end the execution pipeline and show the final result.

In summary, our contributions are as follows:

- · We propose Visual ChatGPT, which opens the door of combining ChatGPT and Visual Foundation Models and enables ChatGPT to handle complex visual tasks;
- · We design a Prompt Manager, in which we involve 22 different VFMs and define the internal correlation among them for better interaction and combination;
- · Massive zero-shot experiments are conducted and abundant cases are shown to verify the understanding and generation ability of Visual ChatGPT.

## 4.1. Setup

We implement the LLM with ChatGPT [29] (OpenAI 'text-davinci-003' version), and guide the LLM with LangChain [7] 1 . We collect foundation models from HuggingFace Transformers [43] 2 , Maskformer [10] 3 and ControlNet [53] 4 . The fully deployment of all the 22 VFMs requires 4 Nvidia V100 GPUs, but users are allowed to deploy fewer foundation models to save GPU resources flexibly. The maximum length of chat history is 2,000 and excessive tokens are truncated to meet the input length of ChatGPT.

## 5. Limitations

Although Visual ChatGTP is a promising approach for multi-modal dialogue, it has some limitations, including:

- · Dependence on ChatGPT and VFMs Visual ChatGPT relies heavily on ChatGPT to assign tasks and on VFMs to execute them. The performance of Visual ChatGPT is thus heavily influenced by the accuracy and effectiveness of these models.
- · Heavy Prompt Engineering Visual ChatGPT requires a significant amount of prompt engineering to convert VFMs into language and make these model descriptions distinguishable. This process can be timeconsuming and requires expertise in both computer vision and natural language processing.
- · Limited Real-time Capabilities Visual ChatGPT is designed to be general. It tries to decompose a complex task into several subtasks automatically. Thus, when handling a specific task, Visual ChatGPT may invoke multiple VFMs, resulting in limited real-time capabilities compared to expert models specifically trained for a particular task.
- · Token Length Limitation The maximum token length in ChatGPT may limit the number of foundation models that can be used. If there are thousands or millions of foundation models, a pre-filter module may be necessary to limit the VFMs fed to ChatGPT.
- · Security and Privacy The ability to easily plug and unplug foundation models may raise security and privacy concerns, particularly for remote models accessed via APIs. Careful consideration and automatic check must be given to ensure that sensitive data should not be exposed or compromised.

## 6. Conclusion

In this work, we propose Visual ChatGPT, an open system incorporating different VFMs and enabling users to interact with ChatGPT beyond language format. To build such a system, we meticulously design a series of prompts to help inject the visual information into ChatGPT, which thus can solve the complex visual questions step-by-step. Massive experiments and selected cases have demonstrated the great potential and competence of Visual ChatGPT for different tasks. Apart from the aforementioned limitations, another concern is that some generation results are unsatisfied due to the failure of VFMs and the instability of the prompt. Thus, one self-correction module is necessary for checking the consistency between execution results and human intentions and accordingly making the corresponding editing. Such self-correction behavior can lead to more complex thinking of the model, significantly increasing the inference time. We will solve such an issue in the future.

## A. Tool Details



