## Your Language Model is Secretly a Reward Model

Rafael Rafailov ∗†

Archit Sharma ∗†

Eric Mitchell ∗†

Stefano Ermon †‡

Christopher D. Manning †

Chelsea Finn †

† Stanford University ‡ CZ Biohub {rafailov,architsh,eric.mitchell}@cs.stanford.edu

## Abstract

While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.

## 1 Introduction

Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities [11, 7, 42, 8]. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to understand common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be aware of a common misconception believed by 50% of people, but we certainly do not want the model to claim this misconception to be true in 50% of queries about it! In other words, selecting the model's desired responses and behavior from its very wide knowledge and abilities is crucial to building AI systems that are safe, performant, and controllable [28]. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL),

Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification objective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.

<!-- image -->

we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.

At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [12, 2]). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.

In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimization (DPO) , an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [5]) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data.

Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.

## 3 Preliminaries

We review the RLHF pipeline in Ziegler et al. (and later [40, 1, 28]). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.

SFT : RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model π SFT .

Reward Modelling Phase : In the second phase the SFT model is prompted with prompts x to produce pairs of answers ( y 1 , y 2 ) ∼ π SFT ( y | x ) . These are then presented to human labelers who express preferences for one answer, denoted as y w ≻ y l | x where y w and y l denotes the preferred and dispreferred completion amongst ( y 1 , y 2 ) respectively. The preferences are assumed to be generated by some latent reward model r ∗ ( y, x ) , which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular choice (although more general Plackett-Luce ranking models [32, 23] are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution p ∗ can be written as:

p ∗ ( y 1 ≻ y 2 | x ) = exp( r ∗ ( x, y 1 )) exp( r ∗ ( x, y 1 )) + exp ( r ∗ ( x, y 2 )) . (1)

Assuming access to a static dataset of comparisons D = { x ( i ) , y ( i ) w , y ( i ) l } N i =1 sampled from p ∗ , we can parametrize a reward model r ϕ ( x, y ) and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:

L R ( r ϕ , D ) = -E ( x,y w ,y l ) ∼D [ log σ ( r ϕ ( x, y w ) -r ϕ ( x, y l )) ] (2)

where σ is the logistic function. In the context of LMs, the network r ϕ ( x, y ) is often initialized from the SFT model π SFT ( y | x ) with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value [51]. To ensure a reward function with lower variance, prior works normalize the rewards, such that E x,y ∼D [ r ϕ ( x, y )] = 0 for all x .

RL Fine-Tuning Phase : During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works [17, 18], the optimization is formulated as

max π θ E x ∼D ,y ∼ π θ ( y | x ) [ r ϕ ( x, y ) ] -β D KL [ π θ ( y | x ) || π ref ( y | x ) ] , (3)

where β is a parameter controlling the deviation from the base reference policy π ref, namely the initial SFT model π SFT . In practice, the language model policy π θ is also initialized to π SFT . The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach [51, 40, 1, 28] has been to construct the reward function r ( x, y ) = r ϕ ( x, y ) -β (log π θ ( y | x ) -log π ref ( y | x )) , and maximize using PPO [39].

## 5.1 Your Language Model Is Secretly a Reward Model

DPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. 5 is equivalent to a Bradley-Terry model with a reward parameterization r ∗ ( x, y ) = β log π ∗ θ ( y | x ) π ref ( y | x ) and we optimize our parametric model π θ , equivalently to the reward model optimization in Eq. 2 under the change of variables. In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions.

Definition 1. We say that two reward functions r ( x, y ) and r ' ( x, y ) are equivalent iff r ( x, y ) -r ' ( x, y ) = f ( x ) for some function f .

It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. We can state the following two lemmas:

Lemma 1. Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.

Lemma 2. Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.

The proofs are straightforward and we defer them to Appendix A.5. The first lemma is a well-known under-specification issue with the Plackett-Luce family of models [32]. Due to this under-specification,

we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. 2 [4]. The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class. We prove the following Theorem in Appendix A.6:

Theorem 1. Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization r ( x, y ) = β log π ( y | x ) π ref ( y | x ) for some model π ( y | x ) and a given reference model π ref ( y | x ) .

Proof Sketch. Consider any reward function r ( x, y ) , which induces a corresponding optimal model π r ( y | x ) , specified by Eq. 4. We will show that a reward function from the equivalence class of r can be represented using the reparameterization given above. We define the projection f as

f ( r ; π ref , β )( x, y ) = r ( x, y ) -β log ∑ y π ref ( y | x ) exp ( 1 β r ( x, y ) ) (8)

The operator f simply normalizes the reward function with the logarithm of the partition function of π r . Since the added normalization term is only a function of the prefix x , f ( r ; π ref , β )( x, y ) is a reward function in the equivalence class of r ( x, y ) . Finally, replacing r with the RHS of Eq. 5 (which holds for any reward function), we have f ( r ; π ref , β )( x, y ) = β log π r ( y | x ) π ref ( y | x ) . That is, the projection f produces a member of the equivalence class of r with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization.

We can alternatively view Theorem 1 as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying:

∑ y π ref ( y | x ) exp ( 1 β r ( x, y ) ) ︸ ︷︷ ︸ = π ( y | x ) , using Thm. 1 reparam. = 1 , (9)

i.e., π ( y | x ) is a valid distribution (probabilities are positive and sum to 1). However, following Eq. 4, we can see that Eq. 9 is the partition function of the optimal policy induced by the reward function r ( x, y ) . The key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. 4 analytically tractable for all prompts x .

## 6.1 How well can DPO optimize the RLHF objective?

The KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KLdiscrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. Figure 2 shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target KL ∈ { 3 , 6 , 9 , 12 } for PPO, β ∈ { 0 . 05 , 0 . 1 , 1 , 5 } , α ∈ { 0 . 05 , 0 . 1 , 0 . 5 , 1 } for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level KL 3 with the reference policy KL ( π || π ref ) . We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient;

DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, even when PPO can access ground truth rewards (PPO-GT).

## 7 Discussion

Learning from preferences is a powerful, scalable framework for training capable, aligned language models. We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences directly , with a simple cross-entropy loss, without reinforcement learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.

Limitations & Future Work. Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure 3-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.

## A.1 Deriving the Optimum of the KL-Constrained Reward Maximization Objective

In this appendix, we will derive Eq. 4. Analogously to Eq. 3, we optimize the following objective:

max π E x ∼D ,y ∼ π [ r ( x, y ) ] -β D KL [ π ( y | x ) || π ref ( y | x ) ] (11)

under any reward function r ( x, y ) , reference model π ref and a general non-parametric policy class. We now have:

max π E x ∼D ,y ∼ π [ r ( x, y ) ] -β D KL [ π ( y | x ) || π ref ( y | x ) ] = max π E x ∼D E y ∼ π ( y | x ) [ r ( x, y ) -β log π ( y | x ) π ref ( y | x ) ] = min π E x ∼D E y ∼ π ( y | x ) [ log π ( y | x ) π ref ( y | x ) -1 β r ( x, y ) ] = min π E x ∼D E y ∼ π ( y | x )   log π ( y | x ) 1 Z ( x ) π ref ( y | x ) exp ( 1 β r ( x, y ) ) -log Z ( x )   (12)

where we have partition function:

Z ( x ) = ∑ y π ref ( y | x ) exp ( 1 β r ( x, y ) ) .

Note that the partition function is a function of only x and the reference policy π ref, but does not depend on the policy π . We can now define

π ∗ ( y | x ) = 1 Z ( x ) π ref ( y | x ) exp ( 1 β r ( x, y ) ) ,

which is a valid probability distribution as π ∗ ( y | x ) ≥ 0 for all y and ∑ y π ∗ ( y | x ) = 1 . Since Z ( x ) is not a function of y , we can then re-organize the final objective in Eq 12 as:

min π E x ∼D [ E y ∼ π ( y | x ) [ log π ( y | x ) π ∗ ( y | x ) ] -log Z ( x ) ] = (13)

min π E x ∼D [ D KL ( π ( y | x ) || π ∗ ( y | x )) -log Z ( x )] (14)

Now, since Z ( x ) does not depend on π , the minimum is achieved by the policy that minimizes the first KL term. Gibbs' inequality tells us that the KL-divergence is minimized at 0 if and only if the two distributions are identical. Hence we have the optimal solution:

π ( y | x ) = π ∗ ( y | x ) = 1 Z ( x ) π ref ( y | x ) exp ( 1 β r ( x, y ) ) (15)

for all x ∈ D . This completes the derivation.

## A.2 Deriving the DPO Objective Under the Bradley-Terry Model

It is straightforward to derive the DPO objective under the Bradley-Terry preference model as we have

p ∗ ( y 1 ≻ y 2 | x ) = exp( r ∗ ( x, y 1 )) exp( r ∗ ( x, y 1 )) + exp ( r ∗ ( x, y 2 )) (16)

In Section 4 we showed that we can express the (unavailable) ground-truth reward through its corresponding optimal policy:

r ∗ ( x, y ) = β log π ∗ ( y | x ) π ref ( y | x ) + β log Z ( x ) (17)

Substituting Eq. 17 into Eq. 16 we obtain:

p ∗ ( y 1 ≻ y 2 | x ) = exp ( β log π ∗ ( y 1 | x ) π ref ( y 1 | x ) + β log Z ( x ) ) exp ( β log π ∗ ( y 1 | x ) π ref ( y 1 | x ) + β log Z ( x ) ) +exp ( β log π ∗ ( y 2 | x ) π ref ( y 2 | x ) + β log Z ( x ) ) = 1 1 + exp ( β log π ∗ ( y 2 | x ) π ref ( y 2 | x ) -β log π ∗ ( y 1 | x ) π ref ( y 1 | x ) ) = σ ( β log π ∗ ( y 1 | x ) π ref ( y 1 | x ) -β log π ∗ ( y 2 | x ) π ref ( y 2 | x ) ) .

The last line is the per-instance loss in Equation 7.

## A.3 Deriving the DPO Objective Under the Plackett-Luce Model

The Plackett-Luce model [32, 23] is a generalization of the Bradley-Terry model over rankings (rather than just pair-wise comparisons). Similar to to the Bradley-Terry model, it stipulates that when presented with a set of possible choices, people prefer a choice with probability proportional to the value of some latent reward function for that choice. In our context, when presented with a prompt x and a set of K answers y 1 , . . . , y K a user would output a permutation τ : [ K ] → [ K ] , giving their ranking of the answers. The Plackett-Luce model stipulates that

p ∗ ( τ | y 1 , . . . , y K , x ) = K ∏ k =1 exp( r ∗ ( x, y τ ( k ) )) ∑ K j = k exp( r ∗ ( x, y τ ( j ) )) (18)

Notice that when K = 2 , Equation 18 reduces to the Bradley-Terry model. However, for the general Plackett-Luce model, we can still utilize the results of Eq. 5 and substitute the reward function parameterized by its optimal policy. Similarly to Appendix A.2, the normalization constant Z ( x ) cancels out and we're left with:

p ∗ ( τ | y 1 , . . . , y K , x ) = K ∏ k =1 exp ( β log π ∗ ( y τ ( k ) | x ) π ref ( y τ ( k ) | x ) ) ∑ K j = k exp ( β log π ∗ ( y τ ( j ) | x ) π ref ( y τ ( j ) | x ) ) (19)

Similarly to the approach of Section 4, if we have access to a dataset D = { τ ( i ) , y ( i ) 1 , . . . , y ( i ) K , x ( i ) } N i =1 of prompts and user-specified rankings, we can use a parameterized model and optimize this objective with maximum-likelihood.:

L DPO ( π θ , π ref ) = -E τ,y 1 ,...,y K ,x ∼D   log K ∏ k =1 exp ( β log π θ ( y τ ( k ) | x ) π ref ( y τ ( k ) | x ) ) ∑ K j = k exp ( β log π θ ( y τ ( j ) | x ) π ref ( y τ ( j ) | x ) )   (20)

## A.4 Deriving the Gradient of the DPO Objective

In this section we derive the gradient of the DPO objective:

∇ θ L DPO ( π θ ; π ref ) = -∇ θ E ( x,y w ,y l ) ∼D [ log σ ( β log π θ ( y l | x ) π ref ( y l | x ) -β log π θ ( y w | x ) π ref ( y w | x ) )] (21)

We can rewrite the RHS of Equation 21 as

∇ θ L DPO ( π θ ; π ref ) = -E ( x,y w ,y l ) ∼D [ σ ' ( u ) σ ( u ) ∇ θ ( u ) ] , (22)

where u = β log π θ ( y l | x ) π ref ( y l | x ) -β log π θ ( y w | x ) π ref ( y w | x ) .

Using the properties of sigmoid function σ ' ( x ) = σ ( x )(1 -σ ( x )) and σ ( -x ) = 1 -σ ( x ) , we obtain the final gradient

∇ θ L DPO ( π θ ; π ref ) = -E ( x,y w ,y l ) ∼D [ βσ ( β log π θ ( y w | x ) π ref ( y w | x ) -β log π θ ( y l | x ) π ref ( y l | x ) )[ ∇ θ log π ( y w | x ) -∇ θ log π ( y l | x ) ]] ,

After using the reward substitution of ˆ r θ ( x, y ) = β log π θ ( y | x ) π ref ( y | x ) we obtain the final form of the gradient from Section 4.

## C Further Details on the Experimental Set-Up

In this section, we include additional details relevant to our experimental design.

## C.1 IMDb Sentiment Experiment and Baseline Details

The prompts are prefixes from the IMDB dataset of length 2-8 tokens. We use the pre-trained sentiment classifier siebert/sentiment-roberta-large-english as a ground-truth reward model and gpt2-large as a base model. We use these larger models as we found the default ones to generate low-quality text and rewards to be somewhat inaccurate. We first use supervised fine-tuning on a subset of the IMDB data for 1 epoch. We then use this model to sample 4 completions for 25000 prefixes and create 6 preference pairs for each prefix using the ground-truth reward model. The RLHF reward model is initialized from the gpt2-large model and trained for 3 epochs on the preference datasets, and we take the checkpoint with the highest validation set accuracy. The 'TRL' run uses the hyper-parameters in the TRL library. Our implementation uses larger batch samples of 1024 per PPO step.

## D.3 Human study details

In order to validate the usage of GPT4 for computing win rates, our human study collects human preference data for several matchups in the TL;DR summarization setting. We select three different algorithmic matchups, evaluating DPO (temp. 0.25), SFT (temp. 0.25), and PPO (temp 1.0) compared to the reference algorithm PPO (temp 0.). By selecting matchups for three unique algorithms as well as algorithms with a wide range of win rates vs the reference, we capture the similarity of human and GPT-4 win rates across the response quality spectrum. We sample 150 random comparisons of DPO vs PPO-0 and 100 random comparisons PPO-1 vs PPO-0, assigning two humans to each comparison, producing 275 judgments for DPO-PPO 7 and 200 judgments for PPO-PPO. We sample 125 SFT comparisons, assigning a single human to each. We ignore judgments that humans labeled as ties (which amount to only about 1% of judgments), and measure the raw agreement percentage between human A and human B (for comparisons where we have two human annotators, i.e., not SFT) as well as between each human and GPT-4.

