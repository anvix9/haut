## 1. Introduction



## Abstract

We present Text2Room † , a method for generating roomscale textured 3D meshes from a given text prompt as input. To this end, we leverage pre-trained 2D text-to-image models to synthesize a sequence of images from different poses. In order to lift these outputs into a consistent 3D scene representation, we combine monocular depth estimation with a text-conditioned inpainting model. The core idea of our approach is a tailored viewpoint selection such that the content of each image can be fused into a seamless, textured 3D mesh. More specifically, we propose a continuous alignment strategy that iteratively fuses scene frames with the existing geometry to create a seamless mesh. Unlike existing works that focus on generating single objects [57, 42] or zoom-out trajectories [18] from text, our method generates complete 3D scenes with multiple objects and explicit 3D geometry. We evaluate our approach using qualitative and quantitative metrics, demonstrating it as the first method to generate room-scale 3D geometry with compelling textures from only text as input.

Mesh representations of 3D scenes are a crucial component for many applications, from AR/VR asset creation to computer graphics, yet creating these 3D assets remains a painstaking process that requires considerable expertise. In the 2D domain, recent works have successfully created high-quality images from text using generative models, such as diffusion models [66, 59, 68]. These methods significantly reduce the barriers to creating images that contain a user's desired content, effectively helping towards the democratization of content creation. An emerging line of work has sought to apply similar methods to create 3D models from text [9, 57, 30, 42, 39], yet existing approaches come with a number of significant limitations and lack the generality of 2D text-to-image models.

One of the core challenges of generating 3D models is coping with the lack of available 3D training data, as 3D datasets are vastly smaller than those available in many other applications, such as 2D image synthesis. For example, methods that directly use 3D supervision, such as Chen et al . [9], are often limited to datasets of simple shapes, such as ShapeNet [8]. To address these data limitations, recent methods [57, 30, 42, 39, 89] lift the expressive power of 2D text-to-image models into 3D by formulating 3D generation as an iterative optimization problem in the image domain. This allows them to generate 3D ob-

Figure 1. Textured 3D mesh generation from text prompts. We generate textured 3D meshes from a given text prompt using 2D text-to-image models. (a) The scene is iteratively created from different viewpoints (marked in blue). (b) Our generated mesh contains compelling textures and geometry. We remove the ceiling in the top-down views for better visualization of the scene layout.

<!-- image -->

jects stored in a radiance field representation, demonstrating the ability to generate arbitrary (neural) shapes from text. However, these methods cannot easily be extended to create room-scale 3D structure and texture. The challenge of generating large scenes is ensuring that the generated output is dense and coherent across outward-facing viewpoints, and that these views contain all of the required structures, such as walls, floors, and furniture. Additionally, a mesh remains a desired representation for many end-user tasks, such as rendering on commodity hardware (which requires an additional conversion step as presented in Lin et al . [42]).

To address these shortcomings, we propose a method that extracts scene-scale 3D meshes from off-the-shelf 2D text-to-image models. Our method iteratively generates a scene through inpainting and monocular depth estimation. We produce an initial mesh by generating an image from text, and backproject it into 3D using a depth estimation model. Then, we iteratively render the mesh from novel viewpoints. From each one, we fill in holes in the rendered images via inpainting, then fuse the generated content into the mesh (Fig. 1a).

Our iterative generation scheme has two important design considerations: how we choose the viewpoints, and how we merge generated scene content with the existing mesh. We first select viewpoints from predefined trajectories that will cover large amounts of scene content, then adaptively select viewpoints that close remaining holes. When merging generated content with the mesh, we align the two depth maps to create smooth transitions, and remove parts of the mesh that contain distorted textures. Together, these decisions lead to large, scene-scale 3D meshes with compelling textures and consistent geometry (Fig. 1b), that can represent a wide range of rooms.

To summarize, our contributions are:

- · Generating 3D meshes of room-scale indoor scenes with compelling textures and geometry from any text input.
- · A method that leverages 2D text-to-image models and monocular depth estimation to lift frames into 3D in an iterative scene generation. Our proposed depth alignment and mesh fusion steps, enable us to create seamless and undistorted geometry and textures.
- · A two-stage tailored viewpoint selection that samples camera poses from optimal positions to first create the room layout and furniture and then close any remaining holes, creating a watertight mesh.

## 3. Method

Our method creates a textured 3D mesh of a complete scene from text input. To this end, we continuously fuse generated frames from a 2D text-to-image model at different poses into a joint 3D mesh, creating the scene over time. The core idea of our approach is a two-stage tailored viewpoint selection, that first generates the scene layout and objects and then closes remaining holes in the 3D geometry (Section 3.4). We visualize this workflow in Figure 2. For each pose in both stages, we apply an iterative scene generation scheme to update the mesh (Section 3.1). We first align each frame with the existing geometry with a depth align-

(a) Scene Generation Stage

<!-- image -->

(b) Scene Completion Stage

Figure 2. Method overview . We iteratively create a textured 3D mesh in two stages. (a) First, we sample predefined poses and text to generate the complete scene layout and furniture. Each new pose (marked in green) adds newly generated geometry to the mesh (depicted by green triangles) in an iterative scene generation scheme (see Figure 3 for details). Blue poses/triangles denote viewpoints that created geometry in a previous iteration. (b) Second, we fill in the remaining unobserved regions by sampling additional poses (marked in red) after the scene layout is defined.Figure 3. Iterative scene generation . For each new pose, we render the current mesh to obtain partial RGB and depth renderings. We complete both, utilizing respective inpainting models and the text prompt. Next, we perform depth alignment (see Section 3.2) and mesh filtering (see Section 3.3) to obtain an optimal next mesh patch, that is finally fused with the existing geometry.

<!-- image -->

ment strategy (Section 3.2). Next, we triangulate and filter the novel content to merge it into the mesh (Section 3.3).

## 4.3. Ablations

The key ingredients of our method are depth alignment (Section 3.2), mesh fusion (Section 3.3) and the two-stage viewpoint selection (Section 3.4). We demonstrate the importance of each component in Figure 7 and Table 1.

Depth alignment creates seamless scenes. Monocular depth predictions from subsequent frames can be inconsistent in scale. This leads to disconnected components in the mesh that are backprojected from multiple viewpoints (see Figure 7a). Our depth alignment strategy allows fusing multiple frames into a seamless mesh, eventually creating a complete scene with flat floors, walls, ceilings and no holes.

Stretch removal creates undistorted scene geometry. During mesh fusion, we update the scene geometry with the contents of the next frame. Due to noisy depth prediction, the objects become stretched out, if they are observed

from small grazing angles. Thus, we propose two filters (edge length and surface normal thresholds) that alleviate this issue. Instead of baking in stretched-out geometry (see Figure 7b), we disregard the corresponding faces and let the object be completed from a more suitable, later viewpoint.

Two-stage generation creates complete scenes. Our approach chooses camera poses in two stages to create a complete scene without holes. After generating the scene from predefined trajectories, the scene still contains some holes (see Figure 7c). Because the scene is built-up over time, it is impossible to choose camera poses a-priori , that view all unobserved regions. To this end, our completion stage samples poses a-posteriori to refine those regions. The resulting mesh is watertight and contains no holes (see Figure 7d).

## 4.5. Limitations

Our approach allows to generate 3D room geometry from arbitrary text prompts that are highly detailed and contain consistent geometry. Nevertheless, our method can still fail under certain conditions (see supplemental material). First, our thresholding scheme (see Section 3.3) may not detect all stretched-out regions, which may lead to remaining distortions. Additionally, some holes may still not be completed fully after the second stage (see Section 3.4),

which results in over-smoothed regions after applying poisson reconstruction. Our scene representation does not decompose material from lighting, which bakes in shadows or bright lamps, that are generated from the diffusion model.

## 5. Conclusion

Wehave shown a method to generate textured 3D meshes from only text input. We use text-to-image 2D generators to create a sequence of images. The core insight of our method is a tailored viewpoint selection, that allows to create a 3D mesh with seamless geometry and compelling textures. Specifically, we lift the images into a 3D scene, by employing our alignment strategy that iteratively fuses all images into the mesh. Our output meshes represent arbitrary indoor scenes that can be rendered with classical rasterization pipelines. We believe our approach demonstrates an exciting application of large-scale 3D asset creation, that only requires text as input.

## B. Societal Impact

Our method leverages text-to-image models to generate a sequence of images from text, specifically we use the Stable Diffusion model [66]. Thus it inherits possible drawbacks of these 2D models. First, our method could be exploited to generate harmful content, by forcing the text-to-image model to generate respective images. Furthermore, our method is biased towards the cultural or stereotypical data distribution, that was used to train the text-to-image model. Lastly, we note that text-to-image models are trained on large-scale text-image datasets [72]. Thus, the model learns to reproduce and combine the style of artists, whose works are contained in these datasets. This raises questions regarding the correct way to credit these artists or if it is ethical to benefit from their works in this way at all.

Our method can be used to generate meshes, that depict entire scenes, from only text as input. This significantly reduces the required expertise to model and design such 3D assets. Thus, we believe our work proposes a promising step towards the democratization of large-scale 3D content creation.

## C. Limitations

Given a text prompt, our approach allows to generate 3D room geometry that is highly detailed and contains consistent 3D geometry. Nevertheless, our method can still fail under certain conditions (see Figure 10).

First, our completion stage (see Section 3.4) might not be able to inpaint all holes (Figure 10b). For example this can happen, if an object contains holes that are close to a wall. These angles are hard to see from additional cameras and thus might remain untouched. We still close these holes by applying Poisson surface reconstruction [34]. However, this can results in overly smoothed geometry.

Second, our mesh fusion stage (see Section 3.3) might not remove all stretched-out faces. Faces can appear stretched-out because of imperfect depth estimation and alignment. Over time this can yield unusual room shapes

such as the curved wall in Figure 10c. We apply two filtering schemes to remove stretched-out faces before fusing them with the existing geometry. Both use thresholds δ sn =0 . 1 , δ edge =0 . 1 , that we fix during all our experiments. It can happen that some faces are not removed by the filtering schemes, but are still stretched-out unnaturally. However, we find that lowering the thresholds would also remove unstretched geometry. This would make creating a complete scene harder, because more holes need to be inpainted in the completion stage.

## D. Details on User Study

We conduct a user study and ask n =61 users to score Perceptual Quality ( PQ ) and 3D Structure Completeness ( 3DS ) of the whole scene on a scale of 1 -5 . We show an example of how we asked the users to score these two metrics in Figure 11. We present users with multiple images from each scene, that show it from multiple angles. Then we ask them to rate the scene on a scale from 1 -5 by asking them about the 3D structure completeness and the overall perceptual quality. In total, we received 1098 datapoints from multiple scenes and report averaged results per method.

## E. Additional Implementation Details

We give additional implementation details in the following subsections.

