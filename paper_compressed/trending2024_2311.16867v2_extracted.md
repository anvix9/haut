## Abstract

We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoderonly models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text-the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to e ffi ciently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7 / 40 / 180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.

Figure 1: The Falcon series of models achieves competitive performance, with Falcon-180B nearly matching the 1-shot performance of PaLM-2 Large. 1-shot performance of PaLM (Chowdhery et al., 2022), PaLM-2 (Anil et al., 2023), and Falcon-180B, on a set of tasks from Brown et al. (2020). These evaluation results are only a small snapshot of our evaluations, see Section 6 for details and comparisons with GPT-3.5 / 4, LLaMA-1 / 2, and Inflection-1.

<!-- image -->

## 1 Introduction

The on-going Cambrian explosion of language models has been primarily fueled by the unique scalability of popular Transformer-based recipes. This scalability manifests over multiple axes:

- · Performance scalability (and predictability). Increase in pretraining compute budgets systematically yield improvements in language modeling capabilities, in a consistent and predictable way (Kaplan et al., 2020). Falcon-180B is the first publicly documented GPT-3sized model to follow the updated scaling law recommendations of Ho ff mann et al. (2022), with a total pretraining length of 3,500 billion tokens, without any upsampling.
- · Data scalability. To scale-up pretraining e ffi ciently, and to decouple pretraining and inference compute, increasingly large models should be trained for longer, on larger corpora. To sustain the Falcon series, we developed RefinedWeb (Penedo et al., 2023), a 5 trillion tokens high-quality filtered and deduplicated web dataset-the largest publicly documented.
- · Hardware scalability. Transformer models (Vaswani et al., 2017) are naturally suited for modern GEMM optimized hardware, allowing their training and inference to be e ffi ciently distributed over a large number of accelerators (Narayanan et al., 2021b; Pope et al., 2023). With Falcon-180B, we demonstrate scaling-up pretraining to 4,096 A100 40GB with only 50 Gbps interconnect per accelerator on cost-e ffi cient AWS cloud infrastructure.

Building upon these fundamentals, increasingly large language models give rise to so-called emergent capabilities (Wei et al., 2022a). These capabilities can be further tailored to human preference, to build instruction-following or chatty models (Ouyang et al., 2022). All together these methods have lead to the widespread deployment of large language models in customer-facing applications, such as ChatGPT (GPT-3.5 / 4, OpenAI (2023a)), Claude, Bard (PaLM-2, Anil et al. (2023)), or Pi (Inflection-1, Inflection (2023)). In this paper, we primarily report on the pretraining alone of the Falcon series of models, and leave further downstream finetuning and alignment to future works.

The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)-a massive filtered and deduplicated web dataset. The architecture of the models is based on PaLM (Chowdhery et al., 2022), although we independently validated each decision, ultimately resulting in some minor tweaks-see Section 4 for details. The Falcon series leverages extensive custom tooling (i.e., pretraining codebase, data pipeline), of which development started in August 2022, with training of the models kicked-o ff in December 2022. In-depth evaluation shows that the Falcon series is competitive across scale, and that Falcon-180B nears the performance of PaLM-2 Large, positioning it as the best open model and in the top-3 of the best language models.

Contributions. With this paper and the Falcon series, we make the following contributions:

- · Public documentation of the pretraining of a large-scale model. Recent state-of-the-art models have been scarcely documented, hindering further research and progress in the field. At variance with these works, we extensively document the pretraining of the Falcon series.
- · Open data and models. To accelerate research, and to enable community-driven improvements of large language models, we openly release Falcon-7 / 40 / 180B, and a 600 billion tokens extract of the RefinedWeb dataset: https://huggingface.co/tiiuae/ .

Table 1: The Falcon series of models covers a wide range of capabilities and inference requirements, enabled by large-scale web data. Falcon-7B can e ffi ciently run on consumer hardware (e.g., Apple M2), while Falcon-180B typically requires dedicated inference infrastructure (e.g., 8 × A100 80GB ). We report steady zero-shot performance gains across the entire Falcon series.

|                                | Falcon-7B   | Falcon-40B   | Falcon-180B             |
|--------------------------------|-------------|--------------|-------------------------|
| Pretraining [tokens]           | 1,500B      | 1,000B       | 3,500B                  |
| Compute [PF-days]              | 730         | 2,800        | 43,500                  |
| Training [A100s]               | 384         | 384          | 4,096                   |
| Availability                   | Apache 2.0  | Apache 2.0   | Responsible use license |
| Agg. performance (Section 6.5) | 60.8        | 67.1         | 70.3                    |
| Closest model                  | < GPT-3     | Chinchilla   | PaLM-2 Large            |

## 4.3.1 Extending multiquery into multigroup for tensor parallel training and inference

Background. Unanimously, large language models first adopted the multihead attention scheme described in Vaswani et al. (2017). Each token produces n head triplets of (query, keys, and values), and the result of each head is then summed to produce the final output of the attention module. However, this scheme can be altered. Shazeer (2019) found that one can share the same keys and values between all attention heads with only a small degradation in performance. In this so-called multiquery attention, the number of heads for the queries remains nq = n head but there is only one head for the keys and values, nkv = 1 . This significantly reduces inference memory consumption: during autoregressive generation, the keys and values are cached to accelerate generation-with multiquery, the K,V-cache size is divided by n head compared to vanilla attention, resulting in a 10-100x reduction in memory consumption for common models-see Table 8. Multiquery improves the scalability of inference for large models (Pope et al., 2023). Chowdhery et al. (2022) has recently popularized this architectural modification, which has notably been adopted by LLaMA-2 (Touvron et al., 2023b).

Table 8: Multiquery / group schemes significantly reduces the size of the K,V-cache for inference. Assuming TP = 1 for Falcon-7B and TP = 8 for Falcon-40 / 180B, sequence length 2,048.

| Attention scheme                  | nq    | nkv   | K,V-cache for a 2,048 sequence   | K,V-cache for a 2,048 sequence   | K,V-cache for a 2,048 sequence   | K,V-cache for a 2,048 sequence   |
|-----------------------------------|-------|-------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|
|                                   |       |       |                                  | 7B                               | 40B                              | 180B                             |
| Vanilla                           | nhead | nhead | O ( √ N log( n ))                | 1GB                              | 4GB                              | 10GB                             |
| Multiquery (Shazeer, 2019)        | nhead | 1     | O (log( N ))                     | 20MB                             | 30MB                             | 40MB                             |
| Multigroup (Ainslie et al., 2023) | nhead | TP    | O (log( N ))                     | N / A                            | 250MB                            | 335MB                            |

Scaling. Interestingly, multiquery is disproportionately e ff ective for larger models. With N the total number of parameters, d model the model size, n layer the number of layers, and assuming a fixed d head = d model / n head-which is typically the case when using FlashAttention (Dao et al., 2022). For e ffi cient scaling, it is recommended that n layer ∼ O (log( N )) (Levine et al., 2020); since we can approximate N ≃ n layer( d model) 2 (Kaplan et al., 2020), it follows that the size of the K,V-cache with multihead attention scales in O ( √ N log( n )). Conversely, for multiquery the K,V-cache only stores a fixed 2 d head per layer, which does not increase with width; this results in overall more e ffi cient scaling, in O (log( N )) instead.

Multigroup. One caveat of multiquery attention is that it is di ffi cult to e ffi ciently parallelize when relying on tensor parallelism, as is common with GPU-based infrastructure (Shoeybi et al., 2019). Either each GPU keeps a copy of the shared key / value, recomputing them individually and then sharing gradients to keep them in sync, or they are computed on a single GPU and then communicated as necessary. We propose to introduce separate key / value pairs for each tensor parallel rank, simplifying the required communications. As in Shazeer (2019), we keep nq = n head, but now have nkv = TP . This scheme doesn't change the scaling of the K,V-cache, as it only applies a fixed TP factor. Concurrently to the development of the Falcon series, Ainslie et al. (2023) also proposed this modification; we refer to this variant of attention as grouped query attention or multigroup. We note that the communication reduction applies not just during inference, but also during training.

Results. We train 1B / 3B models on 30 / 60B tokens from The Pile (Gao et al., 2020), with multiquery and varying degrees of multigroup attention. Importantly, we do not control for the reduction in parameters caused by the loss of additional keys and values-some degradation is thus expected. Results are presented in Table 9. Both multiquery and multigroup do not result in any large reduction in zero-shot performance, even without compensating for the reduction in trainable parameters.

Recipe decision. To improve performance scalability for the largest models, the Falcon series implement multigroup with KV = TP for all models (respectively 1 / 8 / 8 for Falcon-7 / 40 / 180B).

Table 9: Even without controlling for the reduction in parameters, multiquery only comes at a limited zero-shot performance cost. Impact on perplexity is more directly measurable, whereas impact on zero-shot performance is less consistent. Multigroup with KV = 8 consistently performs close to the vanilla baseline. Underlined values have crossed the very likely 2σ degradation threshold.

| Model size   | KV                | Performance       | Performance        | Performance          |
|--------------|-------------------|-------------------|--------------------|----------------------|
| Very likely  | threshold (2- σ ) | zs-main ↑ ± 2 . 2 | zs-small ↑ ± 0 . 8 | ppl-pile ↓ ± 0 . 005 |
| 1B           | 1                 | 48.5              | 42.6               | 0.908                |
| 1B           | 2                 | 48.2              | 42.1               | 0.899                |
| 1B           | 4                 | 48.9              | 42.4               | 0.908                |
| 1B           | 8                 | 48.6              | 42.8               | 0.903                |
| 1B           | Vanilla           | 49.2              | 43.1               | 0.895                |
| 3B           | 1                 | 52.7              | 48.6               | 0.825                |
| 3B           | 8                 | 54.6              | 50.1               | 0.819                |
| 3B           | Vanilla           | 54.4              | 49.8               | 0.807                |

Table 10: Although at small-scale URPE and RoPE may likely be better than ALiBi, that advantage isn't as clear at increased size. We find ALiBi to be likely worst than rotary on two of our three aggregates for 1B models, but to be closer to the performance of rotary at the 3B scale. Underlined values have crossed the likely 1σ degradation threshold over RoPE.

| Model size    | Pos. Emb.   | Performance zs-main ↑ ± 1 . 1   | zs-small ↑ ± 0 . 4   | ppl-pile ± 0 . 002   |
|---------------|-------------|---------------------------------|----------------------|----------------------|
| threshold (1- | σ )         |                                 |                      | ↓                    |
| Likely 1B     | ALiBi       | 49.2                            | 43.1                 | 0.895                |
| Likely 1B     | URPE        | 49.6                            | 43.1                 | 0.885                |
| Likely 1B     | RoPE        | 50.0                            | 44.2                 | 0.883                |
| 3B            | ALiBi       | 54.4                            | 49.8                 | 0.807                |
| 3B            | RoPE        | 54.4                            | 50.5                 | 0.799                |

## 5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset

Our web data processing pipeline is extensively described in the dedicated paper R efined W eb paper (Penedo et al., 2023). In this section, we only highlight key components and decisions. To scale-up pretraining data, two approaches are possible:

- · Repeat data. This is the easiest option, and was the norm in computer vision originally. However, most large language models have been far more conservative, usually only upsampling specific corpora for 2-6 times (see Table 3). This is largely due to concerns with memorization (Carlini et al., 2022) and with deduplicates disproportionately degrading the quality of models (Lee et al., 2022; Hernandez et al., 2022). Recently, Muennigho ff et al. (2023) has argued that while up to 4 epochs on the same data may be acceptable, further repetition will cause degradation-this leads us to eschew this strategy.
- · Scale-up web data processing. While scaling curated sources is cumbersome and requires extensive manual work, web data is a massive, plentiful source. Improvements to web data have high leverage, as they impact a large amount of tokens at once: public crawls such as CommonCrawl may contain in excess of 50-100 trillion tokens, such that even a 90% rejection rate would result in a trillion-scale dataset. However, raw web data is also of extremely poor quality (Trinh and Le, 2018; Kreutzer et al., 2022), containing large amounts of undesirable adult content and machine generated spam. We choose to focus our work on improving the quality of web data, through large-scale filtering and deduplication.

These approaches are orthogonal, but not antagonist; scaling to frontier models, with pretraining datastes of 10-100 trillion tokens, will likely require repeating massive web datasets for a few epochs.

Philosophy. RefinedWeb di ff erentiates itself from previous web datasets in the following ways:

- · Extreme-scale. Our Macrodata Refinement pipeline focuses on scalability: we used up to 20,000 CPU cores to produce RefinedWeb. With nearly five trillion deduplicated tokens, the RefinedWeb dataset is the largest documented pretraining dataset, supporting the training of larger models than previously though possible without relying on multiple epochs.
- · Stringent deduplication and filtering. Inspired by Lee et al. (2022), RefinedWeb is fully deduplicated. Fuzzy deduplication with MinHash is used to first massively shrink the dataset, and then extract substring deduplication is applied. Filtering heuristics are also first used to reduce text extraction artefacts, and to remove machine-generated content. We find in Section 4.2.1 that this allows web data to match curated corpora.
- · Neutral filtering. With the exception of language identification, the Macrodata Refinement pipeline does not rely on ML-based filtering strategies. Indeed, such filters can easily introduce or amplify biases into the data Dodge et al. (2021); Welbl et al. (2021).

Loading [MathJax]/extensions/MathMenu.js Figure 6: Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Notably, filtering and deduplication each result in a halving of the data available: around 50% of documents are discarded for not being English, 24% of remaining for being of insu ffi cient quality, and 12% for being duplicates. We report removal rate (grey) with respect to each previous stage, and kept rate (shade) overall. Figure from Penedo et al. (2023).

<!-- image -->

Overview. The Macrodata Refinement pipeline is split into three subsequent stages (see also Fig. 6 for detailed removal rates): (1) document preparation; (2) filtering; (3) deduplication.

For the document preparation, before undertaking any compute-heavy processing, we first filter documents based on the URL alone, using a blocklist of adult sites and scoring URLs based on their name. We found that the preprocessed .WET files o ff ered by CommonCrawl still contain undesirable content (e.g., navigation menus) so we instead process raw .WARC files (HTML response) with trafilatura to extract natural text. Finally, we use the fastText classifier from CCNet (Wenzek et al., 2020) to identify the top language of documents. For English, about 48% of documents remain.

In the filtering stage, we apply a number of heuristics to remove repeated text (which may be an artefact of crawling / text extraction), and documents which are outliers in terms of length, symbolto-word ratio, etc. These heuristics are inspired by Rae et al. (2021). We also introduce so-called line-wise corrections, which remove lingering artefacts such as likes counter or navigation buttons. The size of the suitable data is against halved, resulting in about 23% of CommonCrawl being kept.

Finally, we apply large-scale deduplication in two steps: first, we remove approximate duplicates at the document-level with MinHash (Broder, 1997), before removing exact substring matches with a su ffi x array (Manber and Myers, 1993). With the deduplication settings of Lee et al. (2022), this results in a final halving of the usable data, down to only about 12% of the total data in CommonCrawl.

## 7 Limitations

We highlight in this section limitations of our findings and of the Falcon series of models itself, both to nuance potential applications of our models, and to highlight further research directions. Broadly speaking, these limitations stem from two factors: (1) most of the research on the Falcon series was conducted prior to December 2022, when model training started-in the meanwhile significant developments have occurred in the world of large language models; (2) constraints in compute resources preventing a more exhaustive exploration of some directions.

## 7.1 Limitations of our findings and ablations

Scale. Our ablations are conducted with 1B and 3B models trained to optimality; although we validated the final architecture and datasets with 1B and 7B models trained for 350B tokens, this remains order of magnitude below the compute budget of the actual models in the Falcon series. On the model side, as scale increases, other works have noted the emergence of outlier features, which may drive important dynamics which are not captured by our set-up (Dettmers et al., 2022). On the data side, increasingly large models are known to become increasingly sensitive to memorization (Carlini et al., 2022), potentially leading to catastrophic quality degradation (Hernandez et al., 2022).

Benchmarks. In our ablations, we focused on measuring zero-shot performance on a limited set of tasks, using only logprobs-based evaluations. Not only can downstream task performance be at odds with actual human preference (see Fig. 12 for our own experience of that issue), but our set of tasks does not capture code or multilingual performance. Evaluating general purpose language models is di ffi cult, and single aggregated metrics often poorly translate the nuances brought forth by modeling or data interventions. Broadly speaking, on our set of tasks, we never experienced a consistent trend with one intervention systematically improving a specific metrics but not the others (e.g., adding technical data did not consistently uplift performance on SciQ or PubMedQA); however, this is likely to be a limitation of our small-scale setup, which end-up predominantly measuring general gains. It is likely the performance on popular benchmarks such as MMLU (Hendrycks et al., 2020) or AGIEval (Zhong et al., 2023) could be improved with dedicated in-domain data. We notably believe in the value of larger-scale ablations on optimal pretraining data composition, and on better understanding quantitative versus qualitative aspects of deduplication (i.e., deduplication on web data may be unreasonably e ff ective because it happens to filter out some of the worst samples).

Overall, we view Section 4 as a set of cursory experiments that helped us ground and broadly validate some decisions for the Falcon series. Future practitioners will likely want to revisit these experiments and results at increased scale, enabling broader benchmarking to better capture performance.

## 8 Conclusion

In this paper, we have introduced and extensively described the Falcon series of pretrained models, with Falcon-7 / 40 / 180B, trained on up to 3,500B tokens.

First, we described some of the ablations and experiments we performed to prepare the training dataset and architecture of the Falcon models. We found adequately filtered and deduplicated web data to be a surprisingly strong baseline; concerns around memorization for larger models (Carlini et al., 2022; Hernandez et al., 2022) led us to elect not to upsample any sources, and to predominantly train on this web data. On the architecture-side, we found most interventions to have a limited e ff ect, and adopted multigroup attention (an extension of multiquery (Shazeer, 2019)) as a way to improve inference scalability by significantly reducing the size of the K,V-cache. For future generations of the Falcon series, we see promise in significantly increasing the fraction of code in the pretraining data, and in training with longer sequence lengths in a staged process (e.g., half of the training up to 8k, and second half up to 16k, with downstream adaptation to 32-256k).

Then, we described our implementation of our final strategy for the pretraining of the Falcon series. We report extensively on our data pipeline in Penedo et al. (2023). We described our approach to conversational masking, and our distributed training strategy for running on cost-e ffi cient cloud infrastructure, relying notably on 3D parallelism combined with ZeRO. We also discussed some of our interventions for fast memory e ffi cient training, such as dedicated FlashAttention (Dao et al., 2022) kernels in Triton (Tillet et al., 2019) and our monolayer strategy. We also discussed some of the details around setting hyper-parameters and managing runs over multiple thousand GPUs.

Finally, we outlined some of the results obtained by the Falcon series on key benchmarks. We found Falcon-180B to near the performance of PaLM-2 Large (Anil et al., 2023), and to end-up in-between GPT-3.5 and GPT-4 (OpenAI, 2023a). Falcon-180B is the best open-source model currently available, and likely one of the best models overall. We note our evaluation predominantly focuses on classic natural language tasks, and that further work will be required for evaluating human preferences on downstream versions of Falcon having undergone dedicated finetuning or reinforcement learning.

To foster open research on large language models, and accelerate technological development in this space, we make the following artefacts public available under permissive open-source licenses:

- · Falcon-7 / 40 / 180B. We make all models in the Falcon series available, with Falcon-7 / 40B under an Apache 2.0 license and Falcon-180B under a dedicated responsible AI use license. At time of release, Falcon-180B is the most powerful open large language model available.
- · A 600B tokens extract of RefinedWeb. We make a 600B tokens extract of our web dataset available, for use by researchers to study large-scale filtered and deduplicated web data, and for other practioners to adopt as a standard for high-quality web data. We also open-source 1 / 7B models trained on 350B tokens from this extract.
- · Detailed research. With this paper and the RefinedWeb paper (Penedo et al., 2023), we have detailed numerous of our decisions and experiments concerning the Falcon series.

We believe large language models to be a foundational technology for the future of our civilization, and in turn we believe they should be shared responsibly. Widespread exchange of ideas is a staple of accelerated technological and economical progress in our history; in turn, this acceleration uplifts all. By open-sourcing artificial intelligence research and models, we can foster a broader and more diverse community, and benefit from vibrant collaborative e ff orts to improve the safety and reliability of large language models. We hope the Falcon series can be a small step towards this vision.

