## Abstract

Most language models (LMs) are trained and applied in an autoregressive left-to-right fashion, assuming that the next token only depends on the preceding ones. However, this assumption ignores the potential benefits of using the full sequence information during training, and the possibility of having context from both sides during inference. In this paper, we propose a new pre-training paradigm with techniques that jointly improve the training data efficiency and the capabilities of the LMs in the infilling task. The first is a training objective that aligns the predictions of a left-to-right LM with those of a right-to-left LM, trained on the same data but in reverse order. The second is a bidirectional inference procedure that enables both LMs to meet in the middle. We show the effectiveness of our pre-training paradigm with extensive experiments on both programming and natural language models, outperforming strong baselines. 1

| 1                    | Introduction                                                                                                                                 | 2   |
|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------|-----|
| 2                    | Preliminaries                                                                                                                                | 4   |
| 2.1 2.2              | The Infilling task . . . . . . . . . . . . . . . . . . . . . . . . . . . Bidirectional Language Modeling . . . . . . . . . . . . . . . . . . | 4 5 |
| 3 Meet in the Middle |                                                                                                                                              | 5   |
| 3.1                  | Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                 | 5   |
| 3.2 . . .            | Infilling . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                              | 6   |
|                      | 3.2.1 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                        | 6   |
|                      | 3.2.2                                                                                                                                        |     |
|                      | Optional Enhancements . . . . . . . . . . . . . . . . . . .                                                                                  | 9   |

| 4 Experiments 10 4.1 Data and Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 10   |                                                |                                                                                                                                                                                |                                                                                    |     |       |       |    |    |       |          |     |         |    |
|-------------------------------------------------------------------------------------------------|------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|-----|-------|-------|----|----|-------|----------|-----|---------|----|
|                                                                                                 | 4.2                                            | 4.2.1 Code generation and infilling . .                                                                                                                                        | Benchmarks and metrics . . . . . . . . . . . . . . . . . . . . . . . . . . .       | .   |       | . .   | .  | .  | . . . | 11 11    | .   | . .     |    |
|                                                                                                 | 4.2.2                                          | Language Modeling . . .                                                                                                                                                        | .                                                                                  |     |       | . .   | .  | .  | .     | 11       |     |         |    |
|                                                                                                 | 4.3                                            | . .                                                                                                                                                                            | . . . . . Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . |     |       |       |    |    |       |          |     |         |    |
| 4.3.1 Code generation and infilling                                                             |                                                | . .                                                                                                                                                                            | . . . . . .                                                                        | .   | .     |       | .  | .  | .     | 12       |     | . .     |    |
| 4.4                                                                                             |                                                | Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Effect of Optional Enhancements . . . . . . . . . . . . . . 4.4.2 Effect of Agreement Regularizer | . .                                                                                | . . | .     |       | .  | .  |       | 13 13 14 | .   | . .     |    |
| 4.3.2 Language Modeling . .                                                                     |                                                | . . . . . . . . . . .                                                                                                                                                          | . .                                                                                | . . | . . . | . . . | .  | .  |       | 13       | . . | . . . . | 12 |
|                                                                                                 | 4.5                                            | . . . . . . . . . . .                                                                                                                                                          |                                                                                    | . . | . .   |       |    | .  |       | 14       | .   | .       |    |
| 5                                                                                               | Efficiency of Inference . . . . . Related work |                                                                                                                                                                                |                                                                                    | 15  |       |       |    |    |       |          |     |         |    |
| 6                                                                                               | Conclusion 18                                  |                                                                                                                                                                                |                                                                                    |     |       |       |    |    |       |          |     |         |    |
|                                                                                                 | A Appendix 23                                  |                                                                                                                                                                                |                                                                                    |     |       |       |    |    |       |          |     |         |    |
| A.1 A.2                                                                                         |                                                |                                                                                                                                                                                |                                                                                    |     |       |       |    |    |       |          |     |         |    |

## 1 Introduction

Language models (LMs) are powerful tools for generating natural and programming language, and have been widely used for various assisted authoring tasks, such as text summarization, code completion, and paraphrasing. In order to be usable in many different applications, most LMs have to be able to generate the next token from the sequence of previous tokens. Given the importance of this operation, pre-training has focused on optimizing the model's ability to predict the next token given the previous tokens, as measured by perplexity. However, at pre-training time we have additional information that we are not utilizing. In particular, when training the model to predict one token we condition on the previous tokens (prefix) but completely ignore the subsequent tokens (suffix). While the suffix cannot be used as an input to the model, there are other ways to incorporate it into pre-training which have not received attention in the literature. Our goal is to utilize the pre-training data more efficiently while preserving the autoregressive nature of the underlying LM.

The approach we advocate involves additional modeling which at first blush may seem wasteful. After all, the main artifact produced during pre-training is an autoregressive left-to-right LM and the pre-training objective closely matches how the LM is applied. Still, there are two reasons to consider alternative training objectives. The first is about data efficiency. The LM is trained by a cheap-to-obtain but rather sparse signal: it produces a probability distribution over all possible choices for the next token yet it is only supervised by the actual next token in the training data. What if during training we provided a

denser form of supervision, where the probability distribution over next tokens is compared with another probability distribution?

The second reason has to do with other related tasks. In particular, in many real-world scenarios, the user may not want to generate text from scratch, but to rather infill or modify an existing sequence of tokens. For example, a programmer may want to add a new argument to a function, or a writer may want to insert a sentence or a phrase to improve the coherence of a paragraph. In these cases, a left-to-right LM cannot use the context from both sides of the insertion position, and may produce suboptimal results. The additional modeling we do during training will also help us develop a state-of-the-art infilling technique.

In this work, we propose a unified pre-training and inference paradigm that we call 'Meet in the Middle' (MIM) to tackle both pre-training as well as infilling. MIM leverages two main ideas. The first idea is to introduce an additional language model that processes tokens right-to-left and use the two models to co-regularize each other. This allows each LM to benefit from the context provided by the other LM, which improves data efficiency and consistency. Here the models 'meet in the middle' metaphorically in the sense of adjusting their output probabilities to agree with the other side. The second idea is a simple and effective inference procedure for infilling that takes advantage of all the artifacts produced during pre-training: both language models, as well as their tendency to agree. In this case, the two models will be building the completion each from their own side until they literally 'meet in the middle'. Our agreement regularizer has two important benefits: it regularizes the two language models and makes them more consistent, and it helps us stop the generation process early in the infilling task, by detecting when the two models converge to the same token.

In other words, to train MIM, we use two decoding flows under a single shared decoder-only architecture [BMR + 20], [CND + 22]. The two LMs generate tokens in opposite directions. The forward direction predicts the next token given the prefix and the tokens it generates. The backward direction predicts the previous token given the suffix and the tokens it generates. We pre-train the two models jointly on a large corpus of text, using a combination of the standard language modeling loss and the agreement regularizer. Once, pre-training is complete, the forward model is a drop-in replacement for existing autoregressive LMs. The backward model can either be discarded or be used for related tasks such as infilling.

In our experiments, we aim to evaluate the effectiveness of MIM for pretraining LMs on different domains and tasks. We use public code and language data to pre-train LMs of different sizes and measure their performance in terms of perplexity and code completion tasks. We compare MIM with FIM [BJT + 22] and other baselines, and show that MIM outperforms them in terms of both perplexity as well as task-specific evaluation metrics. We also conduct ablation studies to show the effectiveness of our main proposals during training and inference. To summarize, our main contributions are:

- · We introduce a new pre-training paradigm for LMs that uses the training

data more efficiently by leveraging both the prefix and the suffix while still maintaining the autoregressive nature of LMs. We do this by training both a forward and a backward model and encourage them to agree.

- · Propose a simple and efficient inference procedure for the infilling task, that takes advantage of context from both sides and the tendency of the forward and backward models to agree. Our procedure can use parallelism more effectively than existing infilling procedures and on average achieves better quality and latency than the state of the art.
- · Pre-train language models of different sizes on public code and language data using MIM, evaluate them both with human and programming languages, and show that MIM outperforms many baselines in terms of standard evaluation metrics. Finally, some models and code are made publicly available.

## 2 Preliminaries

We introduce some notation here we use throughout the paper. For a sequence of tokens x 1 , x 2 , . . . , x N we denote x <i the prefix x 1 , x 2 , . . . x i -1 . We use x >i for the suffix x i +1 , x i +2 , . . . x N . The definitions for x ≤ i and x ≥ i are analogous. To reduce notation clutter, we are suppressing all dependence of models on learnable parameters and when it is clear from context we even suppress the inputs to the models. We will use arrows to distinguish the two models and their outputs. For example, -→ p is the forward model and ←-p is the backward model. Similarly, -→ H will be a hidden representation from the forward model while ←-H will be the corresponding representation from the backward model.

## 3.2.1 Inference

At inference time, our goal is to have an efficient and low-latency generation procedure. At a conceptual level, a naive procedure could work as follows: First

Figure 1: Inference procedure for infilling. Given the prefix 'The quick' and the suffix 'dog', the models -→ p and ←-p generate tokens until a candidate meet-in-themiddle token is detected (shaded area). We use a single token for illustration purposes, although the method can use more tokens. Given the candidate MIM token, the -→ p (respectively ←-p ) model can in parallel verify that the tokens generated by ←-p (resp. -→ p ) are acceptable completions (only the -→ p verification is shown to reduce clutter). The numbers in the top right of each box show the order of operations. Two boxes with the same number can be executed in parallel. Similarly, thick lines show that information (embeddings, tokens) can flow in parallel, while thin lines denote sequential steps.

<!-- image -->

generate candidates from both models. As mentioned earlier, M the length of the desired infilling is unknown in many applications. So in practice we would generate from the two models until each meets a condition (e.g. an application specific token, such as the newline or the EOS token is generated). If -→ p generates -→ y 1 , -→ y 2 , . . . , -→ y F and ←-p generates ←-y 1 , ←-y 2 , . . . , ←-y B then we would need to find the best stitching -→ y 1 , . . . , -→ y i , ←-y j , . . . ←-y B among all ( i, j ) pairs 1 ≤ i ≤ F , 1 ≤ j ≤ B . However, this can be time-consuming as we would need to examine and assign a score to all F × B possible stitchings.

Instead we propose a simplified procedure that interleaves generation and scoring and can terminate as quickly as a unidirectional LM. Moreover, with enough parallelism, it can even be faster. This approach is shown in Figure 1. At a high level, the two models start building a completion each from their own side and they try to literally meet in the middle.

The steps of our approach are as follows. Initially, the prefix and suffix are consumed by -→ p and ←-p respectively. Then -→ p and ←-p generate tokens synchronously one at a time. For each generated token from -→ p we check whether it is in the generated tokens from ←-p (or the first token of the suffix). Likewise, for each generated token from ←-p we check whether it is in the generated tokens from -→ p (or the last token of the prefix). If there is a match, we have a 'meet in the middle' candidate position for joining the two generated sequences.

In the most optimistic scenario, -→ p and ←-p produce the same sequence ( ←-p produces it in reverse order). In that case we will detect that we can join the two generated sequences after each model has generated half of the tokens. Thus the two models have 'met in the middle'. The importance of the agreement regularizer should now be more clear: if -→ p and ←-p produce completely different sequences, then our method is slower than FIM as it has no chance to terminate the generation early. But if the two models completely agree (and we have enough parallelism) it can even beat FIM in terms of generation latency as each model only needs to autoregressively generate half of the completion (while the other half is generated in parallel). As we will see in the experiments, our method achieves lower latency than FIM which suggests that in most cases the two generated sequences meet somewhere near the middle.

Admittedly, the above procedure can suffer from false positives: just because one side generated 'the' and the other side had generated 'the' it does not necessarily mean that joining the two generated sequences at that position would produce a coherent infilling. To reduce false positives we use n-grams instead of a single token. In all of the infilling experiments, we use 4-gram matching. Finally, to be confident that the resulting sequence could have been generated by running -→ p or ←-p until the end, we run a parallel verification procedure which we describe next.

Our parallel verification procedure is adapted from [GXS + 22] where they used it in the context of certain tasks with special structure. Fortunately, our setup is the ideal place to apply their techniques. To make things more concrete, let's assume that the latest token generated from -→ p matches with a token in position s generated from ←-p , as is the case in Figure 1 for the token 'over'. Then we can use the tokens that were generated by ←-p before s as inputs to -→ p in parallel . In the context of Figure 1 we provide the tokens 'over', 'the', 'lazy' as inputs to -→ p in parallel. If each output of -→ p matches the corresponding input, we have verified that -→ p would have autoregressively generated the tokens it copied from ←-p 's output. Grounding the discussion back to Figure 1, if -→ p generates (in parallel) 'the' and 'lazy' we have verified that -→ p would have autoregressively generated the same tokens. If there is a partial match we can fast-forward the generation from -→ p to the point of the first disagreement. If there is no partial match we can return to autoregressive generation from -→ p and ←-p . So far, we have described verification as performing greedy top-1 sampling and checking whether the output from one position is the input we provided into the next position. While this closely matches our parallel verification implementation, more relaxed acceptance criteria could be adopted such as the provided candidate token having a high enough probability in the previous output. If the generations from -→ p and ←-p terminate without meeting or passing verification, we return the sequence with higher probability according to the model that generated it.

To sum up, we generate by running both the forward and backward direction in parallel, and after each step check whether there is a candidate meeting point where the two generated sequences are likely to be compatible. If so, we then apply a parallel verification procedure [SSU18], [GXS + 22] to the joined sequence and decide whether we should stop the generation process early. With this

technique, 'Meet in the Middle' (MIM) can produce high-quality outputs with better latency than FIM.

## 4.4 Ablation Study



## 4.5 Efficiency of Inference

In this section, we further look into the efficiency of our MIM inference procedure for infilling and compare to FIM in terms of inference latency with various batch sizes. Figure 2 shows the speedup of MIM in terms of inference latency over FIM baselines in both single and half precision format with the same model size

Table 2: pass @ k ( % ) results on the HumanEval-X benchmarks in the zero-shot settings for the baselines FIM and our MIM approach. Results of k = 1 , 10 and 100 are reported across all categories.

| Methods      | C++    | C++    | C++    | Java   | Java   | Java   | Go     | Go     | Go     |
|--------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| k            | 1      | 10     | 100    | 1      | 10     | 100    | 1      | 10     | 100    |
| Incoder-6B   | 10 . 0 | 20 . 0 | 35 . 0 | 9 . 0  | 19 . 0 | 40 . 0 | 8 . 0  | 14 . 0 | 29 . 0 |
| CodeGen-6B   | 12 . 0 | 20 . 0 | 36 . 0 | 15 . 0 | 18 . 0 | 40 . 0 | 9 . 0  | 22 . 0 | 40 . 0 |
| CodeGen-16B  | 18 . 0 | 30 . 0 | 50 . 0 | 15 . 0 | 38 . 0 | 60     | 13 . 0 | 25 . 0 | 47 . 0 |
| CodeGeeX-13B | 20 . 0 | 31 . 0 | 50 . 0 | 16 . 0 | 38 . 0 | 58 . 0 | 15 . 0 | 25 . 0 | 49 . 0 |
| FIM-350M     | 8 . 5  | 18 . 3 | 24 . 3 | 11 . 3 | 21 . 5 | 27 . 6 | 10 . 2 | 16 . 8 | 31 . 4 |
| FIM-1.3B     | 16 . 7 | 31 . 5 | 43 . 2 | 15 . 4 | 25 . 2 | 32 . 6 | 12 . 5 | 23 . 7 | 39 . 6 |
| FIM-2.7B     | 24 . 5 | 38 . 6 | 51 . 3 | 18 . 3 | 28 . 6 | 38 . 7 | 15 . 2 | 30 . 4 | 50 . 2 |
| MIM-350M     | 10 . 2 | 19 . 6 | 26 . 3 | 11 . 1 | 22 . 4 | 28 . 2 | 10 . 8 | 17 . 5 | 31 . 8 |
| MIM-1.3B     | 19 . 3 | 36 . 5 | 45 . 7 | 17 . 6 | 27 . 4 | 34 . 7 | 13 . 6 | 25 . 1 | 41 . 7 |
| MIM-2.7B     | 27.4   | 41.3   | 54.1   | 21.6   | 30.8   | 39.1   | 17.4   | 32.7   | 53.6   |

Table 3: pass @ k ( % ) results on the APPS benchmarks in the zero-shot settings for the baselines FIM and our MIM approach. Results of k = 1 , 10 and 100 are reported across all categories.

| Methods   | Introductory   | Introductory   | Introductory   | Interview   | Interview   | Interview   | Competition   | Competition   | Competition   |
|-----------|----------------|----------------|----------------|-------------|-------------|-------------|---------------|---------------|---------------|
| k         | 1              | 10             | 100            | 1           | 10          | 100         | 1             | 10            | 100           |
| FIM-350M  | 3 . 6          | 7 . 8          | 11 . 5         | 0 . 0       | 0 . 3       | 1 . 2       | 0 . 0         | 0 . 04        | 0 . 9         |
| FIM-1.3B  | 8 . 2          | 11 . 6         | 17 . 4         | 0 . 12      | 0 . 59      | 1 . 9       | 0 . 01        | 0 . 07        | 1 . 7         |
| FIM-2.7B  | 12 . 4         | 15 . 7         | 20 . 8         | 0 . 27      | 0 . 72      | 2 . 4       | 0 . 03        | 0 . 095       | 2 . 4         |
| MIM-350M  | 4 . 7          | 9 . 2          | 14 . 3         | 0 . 2       | 0 . 51      | 2 . 3       | 0 . 02        | 0 . 06        | 1 . 4         |
| MIM-1.3B  | 10 . 6         | 14 . 2         | 21 . 2         | 0 . 36      | 0 . 76      | 3 . 6       | 0 . 043       | 0 . 09        | 2 . 2         |
| MIM-2.7B  | 14.3           | 18.2           | 24.6           | 0.52        | 1.4         | 5.2         | 0.067         | 0.18          | 3.3           |

of 1.3B average over all the examples in the HumanEval Infilling benchmark. In particular, the inference speed of MIM-1.3B is 4% to 6% faster compared to the inference speed of FIM-1.3B in single precision, and 3% to 5% faster if using half precision.

We speculate that the speedup of MIM over FIM baselines during inference is attributed to several factors. Firstly, the generation of tokens in the left-toright and right-to-left models are done in parallel. Furthermore, MIM inference procedure allows the generation from both sides to terminate early when there is an n-gram match and the sequence of tokens generated passes the verification. Our verification procedure based on [GXS + 22] is also very efficient as it can be parallelized over all the remaining time steps in the sequence.

## 6 Conclusion

In this paper we addressed two challanges faced by large LMs: Pre-training data efficiency and better handling of context for the task of infilling. We proposed 'Meet in the Middle', a method that uses both forward and backward LMs that share parameters and are trained to agree with each other in addition to predicting the next token. The resulting forward LM is a drop-in replacement for existing autoregressive LMs while also achieving better quality over strong baselines. Moreover, for the task of infilling, we proposed an inference procedure that employs both LMs and can in certain cases reduce the inference latency by up to 50%. Though in our experiments the latency reduction was modest, compared to FIM, the reduction in perplexity and the improvements over FIM in both autoregressive and infilling settings were substantial.

## A.1 Model training details

To evaluate the effectiveness of 'Meet in the Middle' (MIM) pre-training compared to left-to-right autoregressive and 'Fill in the Middle' (FIM) pre-training baselines, we adopt standard transformer-based autoregressive language models used in previous works [BMR + 20] for all the models we trained, varying the number of parameters (350M, 1.3B, 2.7B). Moreover, we replace the use of the Multi Head Attention [VSP + 17] with the use of the Multi Query Attention proposed in [Sha19] in all the models we trained, allowing faster inference and reducing the memory requirements to store multiple key and values embeddings that are not shared between attention heads.

For our bidirectional language models, we run the forward model and the backward model in parallel within a single decoder-only architecture, leveraging bidirectional context explicitly during pre-training. We use the sentinel token 〈 l 2 r 〉 to specify that the generation comes from the forward model and sentinel token 〈 r 2 l 〉 to specify that generation comes from the backward model.

Regarding optimization, we use the Adam optimizer [KB15] with β 1 = 0 . 9 , β 2 = 0 . 95 , glyph[epsilon1] = 10 -8 and a global gradient norm clipping of 1 . 0 . We follow [BMR + 20] to decay learning rate to 10% of its maximum value using cosine annealing with linear warm-up of 2% of the total number of training steps.

For scaling the training of these models, we employ the open source MegatronLM framework [SPP + 19] and partition the training across multiple GPUs along the batch dimension. All the training runs that we conducted use mixed precision training [MNA + 18] and FlashAttention [DFE + 22] to reduce memory requirements and increase training throughput. During pre-training of our models, we observed that MIM, FIM and autoregressive left-to-right pre-training have similar training wall-clock time, it is because the forward model and the backward model are executed in parallel in MIM pre-training. Our largest models of size 2.7B parameters are trained using 128 A100 GPU with 80GB memory each over 4 days, while the smaller models are trained using 64 A100 GPU with 80GB memory each over 3.5 days. See Table 8 for the details of all the training runs.

## A.2 Programming language dataset details

Table 7 details the statistics of the datasets of different programming languages we use to pre-train our code language models in terms of number of tokens and dataset size. We perform some filtering and deduplication to obtain the final dataset. Our tokenizer is based on the Byte-Pair Encoding algorithm widely used in previous work [CTJ + 21] to directly encode raw bytes with a vocabulary of size 100257 tokens. We pre-tokenize the text using a special regex pattern that accounts for splitting on digit and newlines together with the default GPT-2 pre-tokenization [BMR + 20].

Table 7: Approximate statistics of the programming language pre-training data

| Languages   | Size (GB)   | Tokens (B)   |
|-------------|-------------|--------------|
| C           | 34 . 3      | 12 . 3       |
| C++         | 215 . 6     | 70 . 8       |
| Python      | 252 . 3     | 75 . 5       |
| Java        | 178 . 5     | 46 . 7       |
| JavaScript  | 120 . 1     | 39 . 3       |
| TypeScript  | 21 . 8      | 8 . 6        |
| PHP         | 30 . 7      | 11           |
| Ruby        | 26 . 8      | 10 . 1       |
| C#          | 35 . 3      | 12 . 6       |
| Others      | 40 . 2      | 13 . 3       |
| Total       | 955 . 6     | 300          |

Table 8: Details of each training run for all of our model specifications.

| Hyper-parameters   | 350M     | 1.3B     | 2.7B     |
|--------------------|----------|----------|----------|
| Number of layers   | 24       | 24       | 32       |
| Number of heads    | 16       | 16       | 32       |
| Dimension per head | 64       | 128      | 80       |
| Context length     | 2048     | 2048     | 2048     |
| Batch size         | 786 k    | 1 M      | 1 M      |
| Weight decay       | 0 . 1    | 0 . 1    | 0 . 1    |
| Learning rate      | 0 . 0003 | 0 . 0002 | 0 . 0002 |
| Warmup steps       | 7 k      | 5 k      | 5 k      |
| Total steps        | 382 k    | 286 k    | 286 k    |

