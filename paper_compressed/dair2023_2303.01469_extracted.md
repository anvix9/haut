## Abstract

Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models , a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-ofthe-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64 ˆ 64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR10, ImageNet 64 ˆ 64 and LSUN 256 ˆ 256 .

## 1. Introduction

Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; 2020; Ho et al., 2020; Song et al., 2021), also known as score-based generative models, have achieved unprecedented success across multiple fields, including image generation (Dhariwal & Nichol, 2021; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022), audio synthesis (Kong et al., 2020; Chen et al., 2021; Popov et al., 2021), and video generation (Ho et al.,

Proceedings of the 40 th International Conference on Machine Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).

Figure 1: Given a Probability Flow (PF) ODE that smoothly converts data to noise, we learn to map any point ( e.g ., x t , x t 1 , and x T ) on the ODE trajectory to its origin ( e.g ., x 0 ) for generative modeling. Models of these mappings are called consistency models, as their outputs are trained to be consistent for points on the same trajectory.

<!-- image -->

2022b;a). A key feature of diffusion models is the iterative sampling process which progressively removes noise from random initial vectors. This iterative process provides a flexible trade-off of compute and sample quality, as using extra compute for more iterations usually yields samples of better quality. It is also the crux of many zero-shot data editing capabilities of diffusion models, enabling them to solve challenging inverse problems ranging from image inpainting, colorization, stroke-guided image editing, to Computed Tomography and Magnetic Resonance Imaging (Song & Ermon, 2019; Song et al., 2021; 2022; 2023; Kawar et al., 2021; 2022; Chung et al., 2023; Meng et al., 2021). However, compared to single-step generative models like GANs (Goodfellow et al., 2014), VAEs (Kingma & Welling, 2014; Rezende et al., 2014), or normalizing flows (Dinh et al., 2015; 2017; Kingma & Dhariwal, 2018), the iterative generation procedure of diffusion models typically requires 10-2000 times more compute for sample generation (Song &Ermon, 2020; Ho et al., 2020; Song et al., 2021; Zhang &Chen, 2022; Lu et al., 2022), causing slow inference and limited real-time applications.

Our objective is to create generative models that facilitate efficient, single-step generation without sacrificing important advantages of iterative sampling, such as trading compute for sample quality when necessary, as well as performing zero-shot data editing tasks. As illustrated in Fig. 1, we build on top of the probability flow (PF) ordinary differential equation (ODE) in continuous-time diffusion models (Song et al., 2021), whose trajectories smoothly transition

the data distribution into a tractable noise distribution. We propose to learn a model that maps any point at any time step to the trajectory's starting point. A notable property of our model is self-consistency: points on the same trajectory map to the same initial point . We therefore refer to such models as consistency models . Consistency models allow us to generate data samples (initial points of ODE trajectories, e.g ., x 0 in Fig. 1) by converting random noise vectors (endpoints of ODE trajectories, e.g ., x T in Fig. 1) with only one network evaluation. Importantly, by chaining the outputs of consistency models at multiple time steps, we can improve sample quality and perform zero-shot data editing at the cost of more compute, similar to what iterative sampling enables for diffusion models.

To train a consistency model, we offer two methods based on enforcing the self-consistency property. The first method relies on using numerical ODE solvers and a pre-trained diffusion model to generate pairs of adjacent points on a PF ODE trajectory. By minimizing the difference between model outputs for these pairs, we can effectively distill a diffusion model into a consistency model, which allows generating high-quality samples with one network evaluation. By contrast, our second method eliminates the need for a pre-trained diffusion model altogether, allowing us to train a consistency model in isolation. This approach situates consistency models as an independent family of generative models. Importantly, neither approach necessitates adversarial training, and they both place minor constraints on the architecture, allowing the use of flexible neural networks for parameterizing consistency models.

We demonstrate the efficacy of consistency models on several image datasets, including CIFAR-10 (Krizhevsky et al., 2009), ImageNet 64 ˆ 64 (Deng et al., 2009), and LSUN 256 ˆ 256 (Yu et al., 2015). Empirically, we observe that as a distillation approach, consistency models outperform existing diffusion distillation methods like progressive distillation (Salimans & Ho, 2022) across a variety of datasets in few-step generation: On CIFAR-10, consistency models reach new state-of-the-art FIDs of 3.55 and 2.93 for one-step and two-step generation; on ImageNet 64 ˆ 64 , it achieves record-breaking FIDs of 6.20 and 4.70 with one and two network evaluations respectively. When trained as standalone generative models, consistency models can match or surpass the quality of one-step samples from progressive distillation, despite having no access to pre-trained diffusion models. They are also able to outperform many GANs, and existing non-adversarial, single-step generative models across multiple datasets. Furthermore, we show that consistency models can be used to perform a wide range of zero-shot data editing tasks, including image denoising, interpolation, inpainting, colorization, super-resolution, and stroke-guided image editing (SDEdit, Meng et al. (2021)).

## 7. Conclusion

Wehave introduced consistency models, a type of generative models that are specifically designed to support one-step and few-step generation. We have empirically demonstrated that our consistency distillation method outshines the existing distillation techniques for diffusion models on multiple image benchmarks and small sampling iterations. Furthermore, as a standalone generative model, consistency models generate better samples than existing single-step generation models except for GANs. Similar to diffusion models, they also allow zero-shot image editing applications such as inpainting, colorization, super-resolution, denoising, interpolation, and stroke-guided image generation.

In addition, consistency models share striking similarities with techniques employed in other fields, including deep Q-learning (Mnih et al., 2015) and momentum-based contrastive learning (Grill et al., 2020; He et al., 2020). This offers exciting prospects for cross-pollination of ideas and methods among these diverse fields.

## C. Additional Experimental Details

Model Architectures We follow Song et al. (2021); Dhariwal & Nichol (2021) for model architectures. Specifically, we use the NCSN++ architecture in Song et al. (2021) for all CIFAR-10 experiments, and take the corresponding network architectures from Dhariwal & Nichol (2021) when performing experiments on ImageNet 64 ˆ 64 , LSUN Bedroom 256 ˆ 256 and LSUN Cat 256 ˆ 256 .

Parameterization for Consistency Models We use the same architectures for consistency models as those used for EDMs. The only difference is we slightly modify the skip connections in EDM to ensure the boundary condition holds for consistency models. Recall that in Section 3 we propose to parameterize a consistency model in the following form:

f θ p x , t q ' c skip p t q x ' c out p t q F θ p x , t q .

In EDM (Karras et al., 2022), authors choose

c skip p t q ' σ 2 data t 2 ' σ 2 data , c out p t q ' σ data t a σ 2 data ' t 2 ,

where σ data ' 0 . 5 . However, this choice of c skip and c out does not satisfy the boundary condition when the smallest time instant ϵ % 0 . To remedy this issue, we modify them to

c skip p t q ' σ 2 data p t ' ϵ q 2 ' σ 2 data , c out p t q ' σ data p t ' ϵ q a σ 2 data ' t 2 ,

which clearly satisfies c skip p ϵ q ' 1 and c out p ϵ q ' 0 .

Schedule Functions for Consistency Training As discussed in Section 5, consistency generation requires specifying schedule functions N pq and µ pq for best performance. Throughout our experiments, we use schedule functions that take the form below:

N p k q ' S c k K pp s 1 ' 1 q 2 ' s 2 0 q ' s 2 0 ' 1 W ' 1 µ p k q ' exp ˆ s 0 log µ 0 N p k q ˙ ,

where K denotes the total number of training iterations, s 0 denotes the initial discretization steps, s 1 ą s 0 denotes the target discretization steps at the end of training, and µ 0 ą 0 denotes the EMA decay rate at the beginning of model training.

Training Details In both consistency distillation and progressive distillation, we distill EDMs (Karras et al., 2022). We trained these EDMs ourselves according to the specifications given in Karras et al. (2022). The original EDM paper did not provide hyperparameters for the LSUN Bedroom 256 ˆ 256 and Cat 256 ˆ 256 datasets, so we mostly used the same hyperparameters as those for the ImageNet 64 ˆ 64 dataset. The difference is that we trained for 600k and 300k iterations for the LSUN Bedroom and Cat datasets respectively, and reduced the batch size from 4096 to 2048.

We used the same EMA decay rate for LSUN 256 ˆ 256 datasets as for the ImageNet 64 ˆ 64 dataset. For progressive distillation, we used the same training settings as those described in Salimans & Ho (2022) for CIFAR-10 and ImageNet 64 ˆ 64 . Although the original paper did not test on LSUN 256 ˆ 256 datasets, we used the same settings for ImageNet 64 ˆ 64 and found them to work well.

In all distillation experiments, we initialized the consistency model with pre-trained EDM weights. For consistency training, we initialized the model randomly, just as we did for training the EDMs. We trained all consistency models with the Rectified Adam optimizer (Liu et al., 2019), with no learning rate decay or warm-up, and no weight decay. We also applied EMA to the weights of the online consistency models in both consistency distillation and consistency training, as well as to the weights of the training online consistency models according to Karras et al. (2022). For LSUN 256 ˆ 256 datasets, we chose the EMA decay rate to be the same as that for ImageNet 64 ˆ 64 , except for consistency distillation on LSUN Bedroom 256 ˆ 256 , where we found that using zero EMA worked better.

When using the LPIPS metric on CIFAR-10 and ImageNet 64 ˆ 64 , we rescale images to resolution 224 ˆ 224 with bilinear upsampling before feeding them to the LPIPS network. For LSUN 256 ˆ 256 , we evaluated LPIPS without rescaling inputs. In addition, we performed horizontal flips for data augmentation for all models and on all datasets. We trained all models on a cluster of Nvidia A100 GPUs. Additional hyperparameters for consistency training and distillation are listed in Table 3.

