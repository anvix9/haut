## Abstract

GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT series models on NLU tasks does not increase gradually as the models evolve, especially with the introduction of the RLHF training strategy. While this strategy enhances the models' ability to generate humanlike responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.

## 1 Introduction

Large language models (LLMs), such as FLAN (Wei et al., 2022), OPT (Zhang et al., 2022b), and PaLM (Chowdhery et al., 2022), have demonstrated exceptional performance in natural language understanding (NLU) tasks. Among these models, the Generative Pre-trained Transformer (GPT) (Brown et al., 2020) series has recently garnered significant interest due to their outstanding performance in unifying all NLU tasks into generative tasks. Specifically, the GPT series models comprise two sub-series: GPT-3 and GPT-3.5, with their evolutionary relationship depicted in Figure 1, as documented by OpenAI 1 .

Extensive research has been conducted to explore the capabilities of these models from various perspectives. On one hand, researchers have performed experiments to evaluate the performance of GPT series models in specific natural language processing (NLP) tasks. For instance, Zhang et al. (2022a) demonstrated that GPT-3 has acquired linguistic knowledge and can recognize semantic information in most continuous contexts. In addition, Yang et al. (2023) and Hendy et al. (2023) investigated the potential of ChatGPT (i.e., gpt-3.5-turbo in Figure 1) in ascpect-based text summarization and machine translation tasks, respectively. Furthermore, (Qin et al., 2023) analyzed the zero-shot capability of ChatGPT across seven representative task categories. On the other hand, some researchers have investigated the limitations of GPT series models. For example, Koco'n et al. (2023) compared the performance of ChatGPT with state-of-the-art models on 25 different NLP tasks, revealing certain biases and shortcomings of ChatGPT. Additionally, Chen et al. (2023) conducted robustness tests on the GPT

Figure 1: The evolutionary relationship of the GPT series models. FeedME and PPO are two distinct training strategies officially described by OpenAI. A dashed arrow ( 99K ) is used between GPT-3 and GPT-3.5 since the official documentation does not provide specific information on the differences between the two series when trained.

<!-- image -->

series models on 9 NLU tasks and demonstrated that these models still experience similar problems with robustness as fine-tuned models.

However, while many studies have focused on comparing the performance of specific GPT series models to fine-tuned models for particular tasks or analyzing their shortcomings relative to fine-tuned models, a comprehensive analysis of the evolution of GPT series models is still lacking. Specifically, there is a need to investigate how the different strategies used in training GPT series models impact their capabilities in NLU tasks.

In order to conduct a comprehensive analysis of the capabilities of the GPT-3 and GPT-3.5 series models, we evaluate the performance of six GPT series models across nine different NLU tasks using 21 datasets and corresponding transformation data generated by TextFlint (Gui et al., 2021). These models include two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). Our analysis focused on three main perspectives: 1) comparing the performance of different models across various NLU tasks; 2) examining the effect of the training strategies employed by the GPT series models on their capabilities; and 3) analyzing the effect of zero-shot and few-shot scenarios on the capabilities of the models.

Our findings are summarized as follows:

- · Davinci lacks instruction comprehension. The davinci model cannot produce an answer in the zero-shot scenario for prompts that are declarative sentences and do not end with a word such as 'Answer', indicating a lack of instruction comprehension (Section 4.1.2).
- · In-context learning improves prompt understanding for davinci. For the davinci model, in the named entity recognition (NER) and part-of-speech (POS) tasks, in-context learning substantially improves the proportion of outputs that meet the instruction requirements in the few-shot scenario, while in the inference-based tasks (e.g., natural language inference (NLI), relation extraction (RE) and the winograd scheme challenge (WSC)), in-context learning does not significantly improve performance, suggesting its usefulness in helping the model understand prompts (Section 4.1.1).
- · All models are sensitive to prompts. We select three prompts for each task in different scenarios to test the ability of the models other than davinci 2 , and the results show that all models exhibit prompt sensitivity in both zero-shot and few-shot scenarios, but the extent of sensitivity varies across different models and tasks and requires further investigation (Section 4.2).
- · Different models perform differently in zero-shot scenarios. In the zero-shot scenario, code-davinci-002 performs best in aspect-based sentiment analysis (ABSA), machine reading comprehension (MRC), and sentiment classification (SC) tasks; text-davinci-003 performs best in POS, RE, and semantic matching (SM) tasks; gpt-3.5-turbo performs better in NLI and wSC tasks, but has difficulty following instructions in the POS task, which is similar to that of text-davinci-001.

Figure 2: The performance of different models in zero-shot scenario. Missing bars in some datasets mean that the model cannot perform the specified task on that dataset. See Appendix A.1 for specific data.

<!-- image -->

This may be due to gpt-3.5-turbo using a smaller model and weakening the ability of tasks where interaction with humans is not important (Section 4.2 and Figure 2).

- · Few-shot scenarios do not always improve model performance. Although models generally perform better in the few-shot scenario than in the zero-shot scenario, this is not always the case and depends on the model, task, prompt design, and example selection, which deserves further study. (Section 4.2).
- · Text-davinci-001 has relatively weak capabilities compared to other models. Compared to other models except davinci, text-davinci-001 has the weakest overall ability on most tasks, but still showed moderate performance in two tasks, MRC and SC (Section 4.2).
- · Gpt-3.5-turbo and text-davinci-003 have comparable capabilities. Compared to text-davinci003, gpt-3.5-turbo has similar performance to it on most tasks, and only has a disadvantage in MRC, POS, and RE tasks, which may be due to its smaller model size. This of course needs to be studied in more depth (Section 4.2).
- · Increasing model capability does not always improve robustness. With the exception of the ABSAtask, where different models show some differences in robustness, the robustness of different models in other tasks is relatively similar, indicating that there is still much room for improvement in model robustness (Section 4.2).

Based on these findings, we draw the following conclusions :

- · The pre-training phase provides the model with fundamental comprehension and in-context learning abilities. For example, the davinci model is a text generation model that does not require explicit instructions during pre-training. However, even in the zero-shot scenario, it can understand task instructions for tasks including NLI, SC, SM, WSC, and generate effective answers. In the few-shot scenario, the model's understanding of instructions for complex tasks like NER and POS is greatly improved, leading to more analyzable answers (Section 4.1.1).
- · The inclusion of a certain type of task in the supervised fine-tuning phase may have a significant impact on the model's performance on that type of task. For instance, while textdavinci-001 performs poorly on NER and POS tasks, it shows similar performance to text-davinci002 and text-davinci-003 on the MRC task. However, since we cannot determine from official documentation which tasks the model uses for supervised fine-tuning, this issue warrants further investigation (Section 4.2).
- · Alignment with human cognition to some extent impairs the performance of the model on certain tasks. Text-davinci-002, an InstructGPT model based on code-davinci-002, exhibits performance advantages over the latter in SM and WSC tasks, but its performance on other tasks is similar or even worse than code-davinci-002, particularly in few-shot scenarios. OpenAI refers to this phenomenon as the 'alignment tax' (Ouyang et al., 2022) (Figure 1 and Section 4.2).
- · RLHF (Christiano et al., 2017) is leveraged to enhance the model's ability to produce human-like responses, rather than directly improving its performance. Text-davinci-003 is an improvement over text-davinci-002, as it incorporates RLHF as a training strategy. However, its performance is comparable to that of text-davinci-002 in most tasks and inferior to text-davinci-002 in SC and SM tasks. This is due to the fact that RLHF provides limited knowledge to support the model's deeper understanding of the task, thereby not significantly improving the model's performance in NLU tasks (Figure 1 and Section 4.2).

## 2 Background



## 3 Experiment Setup



## 4.2.1 Aspect-based Sentiment Analysis

Table 2: Performance and robustness test results (accuracy) of GPT series models in zero-shot and fewshot scenarios on SemEval2014-Laptop dataset.

| Model            | AddDiff # 331 samples   | AddDiff # 331 samples   | ReverseNonTarget # 104 samples   | ReverseNonTarget # 104 samples   | ReverseTarget # 331 samples   | ReverseTarget # 331 samples   |
|------------------|-------------------------|-------------------------|----------------------------------|----------------------------------|-------------------------------|-------------------------------|
|                  | ori                     | trans                   | ori                              | trans                            | ori                           | trans                         |
| 0-shot           | 0-shot                  | 0-shot                  | 0-shot                           | 0-shot                           | 0-shot                        | 0-shot                        |
| code-davinci-002 | 92.88±2.14              | 90.18±7.42              | 91.39±2.96                       | 53.09±2.78                       | 93.23±1.65                    | 58.61±0.89                    |
| text-davinci-001 | 85.21±1.70              | 80.10±2.11              | 85.89±1.68                       | 47.35±4.16                       | 85.26±2.17                    | 53.56±0.92                    |
| text-davinci-002 | 86.38±0.11              | 81.90±0.35              | 85.57±0.21                       | 52.97±2.96                       | 86.40±0.26                    | 56.68±5.17                    |
| text-davinci-003 | 83.84±0.33              | 77.50±2.43              | 82.43±0.42                       | 39.61±4.83                       | 83.62±0.12                    | 47.04±4.64                    |
| gpt-3.5-turbo    | 85.57±1.27              | 86.55±8.67              | 88.78±2.22                       | 41.78±7.36                       | 85.67±1.36                    | 49.75±9.51                    |
| 1-shot           | 1-shot                  | 1-shot                  | 1-shot                           | 1-shot                           | 1-shot                        | 1-shot                        |
| code-davinci-002 | 96.33±0.58              | 92.67±0.58              | 94.00±1.00                       | 53.33±1.53                       | 96.33±0.58                    | 65.00±1.00                    |
| text-davinci-001 | 82.87±1.04              | 72.69±0.63              | 82.85±1.68                       | 45.74±2.29                       | 82.94±0.88                    | 47.50±3.15                    |
| text-davinci-002 | 86.05±0.43              | 82.20±1.98              | 85.22±0.24                       | 55.18±2.38                       | 86.05±0.43                    | 56.77±3.08                    |
| text-davinci-003 | 85.77±0.69              | 87.17±4.62              | 85.63±1.05                       | 52.22±8.86                       | 85.84±0.57                    | 57.07±7.43                    |
| gpt-3.5-turbo    | 88.99±0.73              | 85.17±4.90              | 93.22±1.00                       | 41.93±5.21                       | 89.18±0.90                    | 47.95±6.93                    |
| 3-shot           | 3-shot                  | 3-shot                  | 3-shot                           | 3-shot                           | 3-shot                        | 3-shot                        |
| code-davinci-002 | 97.00±1.00              | 94.00±1.00              | 94.00±0.00                       | 52.00±2.65                       | 97.00±1.00                    | 64.33±1.53                    |
| text-davinci-001 | 83.33±0.69              | 71.90±0.12              | 83.40±0.24                       | 48.40±1.98                       | 83.44±0.87                    | 50.26±0.77                    |
| text-davinci-002 | 85.41±0.43              | 81.55±1.86              | 84.80±0.97                       | 54.01±2.28                       | 85.48±0.33                    | 56.65±2.43                    |
| text-davinci-003 | 85.91±0.12              | 88.73±4.11              | 85.08±0.48                       | 55.59±11.11                      | 85.98±0.12                    | 59.14±7.11                    |
| gpt-3.5-turbo    | 90.75±1.57              | 90.23±6.40              | 93.09±1.98                       | 47.93±5.83                       | 90.45±1.08                    | 53.62±4.82                    |

Table 3: Performance and robustness test results (accuracy) of GPT series models in zero-shot and fewshot scenarios on SemEval2014-Restaurant dataset.

| Model            | AddDiff # 492 samples   | AddDiff # 492 samples   | ReverseNonTarget # 227 samples   | ReverseNonTarget # 227 samples   | ReverseTarget # 492 samples   | ReverseTarget # 492 samples   |
|------------------|-------------------------|-------------------------|----------------------------------|----------------------------------|-------------------------------|-------------------------------|
|                  | ori                     | trans                   | ori                              | trans                            | ori                           | trans                         |
| 0-shot           | 0-shot                  | 0-shot                  | 0-shot                           | 0-shot                           | 0-shot                        | 0-shot                        |
| code-davinci-002 | 94.65±2.09              | 57.23±29.53             | 97.00±2.00                       | 74.33±3.51                       | 94.31±2.33                    | 72.92±5.25                    |
| text-davinci-001 | 89.25±0.94              | 54.56±12.55             | 90.07±1.24                       | 63.35±1.58                       | 88.89±1.45                    | 63.40±1.94                    |
| text-davinci-002 | 91.38±0.21              | 70.41±13.90             | 92.52±0.26                       | 66.26±1.66                       | 91.52±0.33                    | 68.68±3.02                    |
| text-davinci-003 | 89.45±0.87              | 55.25±20.03             | 91.58±0.96                       | 47.93±0.45                       | 89.38±0.85                    | 54.60±7.18                    |
| gpt-3.5-turbo    | 90.70±0.41              | 64.21±18.97             | 92.64±1.54                       | 70.40±6.69                       | 90.94±0.47                    | 63.39±7.76                    |
| 1-shot           | 1-shot                  | 1-shot                  | 1-shot                           | 1-shot                           | 1-shot                        | 1-shot                        |
| code-davinci-002 | 98.00±1.00              | 75.67±14.64             | 100.00±0.00                      | 78.67±4.16                       | 98.00±1.00                    | 78.67±4.51                    |
| text-davinci-001 | 88.07±3.09              | 33.70±4.97              | 88.49±3.02                       | 61.90±1.22                       | 88.03±3.05                    | 58.03±3.50                    |
| text-davinci-002 | 91.86±0.79              | 74.05±19.15             | 92.00±0.91                       | 69.50±3.12                       | 91.90±0.79                    | 69.96±5.18                    |
| text-davinci-003 | 92.08±0.74              | 78.30±17.09             | 92.84±0.52                       | 64.97±11.58                      | 92.05±0.78                    | 66.62±8.78                    |
| gpt-3.5-turbo    | 92.34±1.11              | 60.23±20.70             | 94.42±2.08                       | 67.79±6.21                       | 92.34±1.11                    | 60.45±5.68                    |
| 3-shot           | 3-shot                  | 3-shot                  | 3-shot                           | 3-shot                           | 3-shot                        | 3-shot                        |
| code-davinci-002 | 98.00±0.00              | 84.67±9.29              | 100.00±0.00                      | 76.33±2.89                       | 98.00±0.00                    | 74.00±3.00                    |
| text-davinci-001 | 89.49±0.21              | 50.57±3.50              | 90.43±0.52                       | 62.07±1.04                       | 89.57±0.25                    | 60.73±1.02                    |
| text-davinci-002 | 92.48±0.54              | 86.46±7.66              | 92.39±0.48                       | 68.94±2.88                       | 92.41±0.61                    | 72.39±4.25                    |
| text-davinci-003 | 92.56±0.83              | 89.47±7.53              | 93.01±0.52                       | 70.95±5.76                       | 92.52±0.78                    | 70.68±4.60                    |
| gpt-3.5-turbo    | 94.62±0.12              | 69.98±16.42             | 96.31±0.29                       | 71.75±5.24                       | 94.55±0.13                    | 65.18±2.83                    |

Weanalyze the performance of various models on two ABSA datasets, namely SemEval2014-Laptop and SemEval2014-Restaurant, and present the outcomes in Table 2 and 3. While some models demonstrate good performance on these datasets, there are still issues with robustness. Our analysis comprises two scenarios: zero-shot and few-shot, and the tables display the results.

In the zero-shot scenario, all models' performance on the ABSA task is nearly identical, presumably because it is a simpler task. Specifically, code-davinci-002 exhibits the most consistent performance, followed by gpt-3.5-turbo and text-davinci-002. Text-davinci-001's performance is poor in most tasks but relatively better in the ABSA task. All five models demonstrate poor performance in other variations except for 'AddDiff', particularly the davinci series models.

In the few-shot scenario, code-davinci-002 demonstrates further enhancement relative to the zero-shot scenario, achieving zero errors in the 'ReverseNonTarget' variation of SemEval2014Restaurant. The five models' performance in zero-shot and few-shot is comparable, indicating that these two datasets are not significantly influenced by the number of examples in the prompt. Concerning robustness, the GPT series models do not demonstrate any significant changes with iterative updates.

## 4.2.4 Natural Language Inference

Table 12: Performance and robustness test results (accuracy) of GPT series models in zero-shot and fewshot scenarios on MNLI-m dataset.Table 13: Performance and robustness test results (accuracy) of GPT series models in zero-shot and fewshot scenarios on MNLI-mm dataset.

| Model            | AddSent # 9815 samples   | AddSent # 9815 samples   | NumWord # 745 samples   | NumWord # 745 samples   | SwapAnt # 199 samples   | SwapAnt # 199 samples   |
|------------------|--------------------------|--------------------------|-------------------------|-------------------------|-------------------------|-------------------------|
|                  | ori                      | trans                    | ori                     | trans                   | ori                     | trans                   |
| 0-shot           | 0-shot                   | 0-shot                   | 0-shot                  | 0-shot                  | 0-shot                  | 0-shot                  |
| code-davinci-002 | 48.38±3.06               | 37.13±2.43               | 45.08±3.74              | 25.81±19.73             | 76.33±19.40             | 51.33±32.58             |
| text-davinci-001 | 42.20±4.15               | 36.66±2.06               | 38.91±3.03              | 25.76±8.35              | 42.41±23.67             | 28.89±23.00             |
| text-davinci-002 | 52.62±6.64               | 36.27±1.61               | 52.72±5.83              | 29.21±14.81             | 52.27±15.07             | 50.95±19.87             |
| text-davinci-003 | 64.26±0.53               | 34.04±1.85               | 68.12±1.42              | 51.62±13.17             | 70.11±1.57              | 78.96±6.20              |
| gpt-3.5-turbo    | 68.96±2.93               | 58.83±4.30               | 70.07±2.46              | 22.37±6.82              | 80.07±9.73              | 44.06±9.43              |
| 1-shot           | 1-shot                   | 1-shot                   | 1-shot                  | 1-shot                  | 1-shot                  | 1-shot                  |
| code-davinci-002 | 72.67±3.06               | 39.67±4.04               | 70.92±3.51              | 54.33±15.95             | 76.67±9.50              | 81.00±5.29              |
| text-davinci-001 | 39.90±8.01               | 39.40±7.63               | 40.44±7.19              | 1.39±2.40               | 98.16±3.19              | 4.36±6.28               |
| text-davinci-002 | 68.60±6.20               | 37.20±1.10               | 69.44±9.26              | 45.37±14.51             | 90.29±1.05              | 86.26±10.18             |
| text-davinci-003 | 73.14±4.80               | 41.07±5.05               | 74.43±4.90              | 43.80±3.61              | 93.35±1.66              | 72.87±5.45              |
| gpt-3.5-turbo    | 71.69±0.31               | 57.77±3.91               | 71.32±1.30              | 43.04±6.98              | 92.46±1.81              | 74.70±5.13              |
| 3-shot           | 3-shot                   | 3-shot                   | 3-shot                  | 3-shot                  | 3-shot                  | 3-shot                  |
| code-davinci-002 | 74.99±6.10               | 41.67±3.51               | 67.81±7.67              | 55.00±7.81              | 75.74±7.61              | 87.67±9.29              |
| text-davinci-001 | 48.80±3.64               | 43.87±3.91               | 50.16±6.62              | 4.34±5.34               | 91.79±7.33              | 28.64±16.34             |
| text-davinci-002 | 70.30±5.47               | 36.87±2.12               | 71.18±6.00              | 44.61±11.68             | 97.49±1.51              | 88.27±12.51             |
| text-davinci-003 | 72.07±5.69               | 41.02±3.14               | 71.59±6.01              | 46.49±4.12              | 98.66±0.58              | 82.41±5.10              |
| gpt-3.5-turbo    | 68.98±1.53               | 42.69±4.75               | 69.71±1.57              | 46.98±1.91              | 94.64±0.77              | 83.25±2.77              |

| Model            | AddSent # 9832 samples   | AddSent # 9832 samples   | NumWord # 775 samples   | NumWord # 775 samples   | SwapAnt # 255 samples   | SwapAnt # 255 samples   |
|------------------|--------------------------|--------------------------|-------------------------|-------------------------|-------------------------|-------------------------|
|                  | ori                      | trans                    | ori                     | trans                   | ori                     | trans                   |
| 0-shot           | 0-shot                   | 0-shot                   | 0-shot                  | 0-shot                  | 0-shot                  | 0-shot                  |
| code-davinci-002 | 48.49±1.82               | 43.50±5.22               | 50.67±2.89              | 22.67±17.16             | 81.00±16.64             | 55.67±31.07             |
| text-davinci-001 | 44.07±3.54               | 35.72±3.46               | 45.34±6.09              | 28.11±9.66              | 33.42±18.59             | 31.07±18.61             |
| text-davinci-002 | 49.93±6.44               | 36.05±2.27               | 52.93±7.57              | 28.45±14.29             | 51.40±15.02             | 58.39±23.61             |
| text-davinci-003 | 64.56±0.32               | 34.26±2.15               | 67.50±0.25              | 39.81±3.04              | 67.76±0.54              | 77.65±2.71              |
| gpt-3.5-turbo    | 69.24±2.21               | 60.37±3.42               | 69.09±1.94              | 19.18±8.02              | 78.30±12.00             | 43.27±9.97              |
| 1-shot           | 1-shot                   | 1-shot                   | 1-shot                  | 1-shot                  | 1-shot                  | 1-shot                  |
| code-davinci-002 | 57.14±14.48              | 43.88±10.34              | 61.00±15.72             | 27.36±17.51             | 79.25±11.63             | 54.88±28.82             |
| text-davinci-001 | 47.54±2.85               | 44.87±2.63               | 45.22±2.84              | 11.10±5.54              | 91.76±8.38              | 36.69±6.32              |
| text-davinci-002 | 51.12±18.88              | 35.20±6.03               | 52.96±20.00             | 16.95±7.03              | 55.30±36.10             | 51.29±18.91             |
| text-davinci-003 | 70.28±5.21               | 37.01±3.34               | 71.39±4.39              | 28.45±1.80              | 93.20±2.94              | 66.80±6.64              |
| gpt-3.5-turbo    | 72.96±1.53               | 53.49±3.24               | 72.78±1.40              | 19.53±2.91              | 81.05±4.84              | 53.85±6.61              |
| 3-shot           | 3-shot                   | 3-shot                   | 3-shot                  | 3-shot                  | 3-shot                  | 3-shot                  |
| code-davinci-002 | 68.15±11.92              | 48.93±8.10               | 62.79±7.14              | 47.09±31.37             | 63.33±40.69             | 90.82±3.91              |
| text-davinci-001 | 54.43±3.50               | 49.03±10.00              | 52.16±5.08              | 7.87±10.58              | 89.41±9.90              | 32.29±23.66             |
| text-davinci-002 | 61.81±11.09              | 34.79±0.70               | 61.40±14.05             | 49.71±20.48             | 70.67±19.95             | 83.12±26.82             |
| text-davinci-003 | 73.66±3.44               | 37.92±1.28               | 74.99±2.36              | 41.59±4.71              | 96.34±1.20              | 89.61±2.90              |
| gpt-3.5-turbo    | 73.20±1.55               | 42.94±5.07               | 74.88±1.56              | 46.80±5.25              | 90.20±3.14              | 82.22±4.65              |

Table 14: Performance and robustness test results (accuracy) of GPT series models in zero-shot and fewshot scenarios on SNLI dataset.

| Model            | AddSent # 10000 samples   | AddSent # 10000 samples   | NumWord # 108 samples   | NumWord # 108 samples   | SwapAnt # 523 samples   | SwapAnt # 523 samples   |
|------------------|---------------------------|---------------------------|-------------------------|-------------------------|-------------------------|-------------------------|
|                  | ori                       | trans                     | ori                     | trans                   | ori                     | trans                   |
| 0-shot           | 0-shot                    | 0-shot                    | 0-shot                  | 0-shot                  | 0-shot                  | 0-shot                  |
| code-davinci-002 | 56.67±4.51                | 38.33±5.86                | 52.00±4.36              | 55.33±26.76             | 74.67±17.47             | 60.00±25.12             |
| text-davinci-001 | 38.61±7.26                | 33.00±3.80                | 48.30±26.79             | 46.42±30.83             | 24.75±15.46             | 63.38±37.26             |
| text-davinci-002 | 47.40±8.24                | 35.45±2.04                | 45.98±11.81             | 30.84±25.56             | 58.76±41.98             | 27.89±20.28             |
| text-davinci-003 | 67.16±3.26                | 34.11±0.03                | 63.89±2.45              | 73.77±22.76             | 81.39±8.06              | 61.44±17.44             |
| gpt-3.5-turbo    | 62.57±1.94                | 49.40±2.69                | 68.57±5.08              | 51.23±12.09             | 88.75±5.59              | 42.77±9.83              |
| 1-shot           | 1-shot                    | 1-shot                    | 1-shot                  | 1-shot                  | 1-shot                  | 1-shot                  |
| code-davinci-002 | 72.00±14.73               | 39.67±7.37                | 66.67±5.86              | 52.00±7.81              | 59.33±24.79             | 81.67±4.04              |
| text-davinci-001 | 36.27±0.31                | 34.50±0.61                | 41.97±2.33              | 62.66±11.50             | 2.10±3.48               | 38.24±22.93             |
| text-davinci-002 | 72.20±3.04                | 40.70±3.72                | 71.60±2.98              | 46.91±10.65             | 94.45±4.45              | 60.48±13.19             |
| text-davinci-003 | 71.81±4.12                | 34.63±0.26                | 73.30±1.63              | 54.63±6.68              | 97.77±0.67              | 49.46±8.36              |
| gpt-3.5-turbo    | 68.97±1.91                | 51.51±3.13                | 68.82±1.07              | 85.49±3.74              | 93.83±1.55              | 87.19±1.99              |
| 3-shot           | 3-shot                    | 3-shot                    | 3-shot                  | 3-shot                  | 3-shot                  | 3-shot                  |
| code-davinci-002 | 74.67±8.08                | 47.67±4.16                | 69.00±2.65              | 68.33±20.40             | 94.33±3.51              | 49.00±10.44             |
| text-davinci-001 | 44.60±4.45                | 34.10±0.78                | 47.33±6.43              | 87.94±11.21             | 41.27±26.07             | 89.68±6.84              |
| text-davinci-002 | 72.43±3.11                | 37.83±2.73                | 70.68±2.97              | 63.27±15.28             | 94.39±3.75              | 66.35±11.72             |
| text-davinci-003 | 72.18±2.37                | 40.00±2.68                | 75.00±3.34              | 67.29±7.54              | 94.65±2.20              | 45.41±7.14              |
| gpt-3.5-turbo    | 68.23±1.99                | 38.31±1.75                | 69.14±2.14              | 84.57±2.67              | 94.70±1.12              | 93.31±1.70              |

We analyze the performance of different GPT series models on the MNLI-m, MMLI-mm, and SNLI NLI datasets, and analyze their performance and robustness in both zero-shot and few-shot scenarios. Overall, the performance of different models on the three NLI datasets shows a similar trend. Please refer to Table 12 to Table 14 for more details.

In the zero-shot scenario, gpt-3.5-turbo performes the best most of the time, followed by textdavinci-003. Meanwhile, Code-davinci-002 and text-davinci-002 also performe well on a few datasets, such as the SwapAnt variation of the original MNLI-m dataset, where the performance of code-davinci002 even exceeds that of gpt-3.5-turbo and text-davinci-003, but this good performance is not stable. However, text-davinci-001 performes poorly in most cases, with a significant gap compared to the other four models.

In the few-shot scenario, the advantage of gpt-3.5-turbo in performance is no longer as significant as in the zero-shot scenario. Although text-davinci-001 still has a significant gap compared to the other four models, the performance gap among these five models is significantly reduced compared to the zero-shot scenario, and overall the best performer is text-davinci-003. In addition, on the three NLI datasets, different models generally perform better in the three-shot scenario than in the one-shot scenario, indicating that more prompts can help improve the performance of this series of models.

Surprisingly, the robustness of gpt-3.5-turbo in NLI tasks is often not as good as earlier models. For example, in the zero-shot scenario, in the zero-shot scenario, gpt-3.5-turbo shows poor robustness on the NumWord variation of all three datasets, and performs much worse than the other four models.

## 5 Conclusion

In this paper, we comprehensively analyze the capabilities of six GPT series models, including GPT-3 and GPT-3.5, by evaluating their performance and robustness on 21 datasets across nine NLU tasks. Our findings reveal that the evolution of GPT series models does not necessarily lead to universal improvements across all NLU tasks, which is influenced by the training strategy employed and the specific characteristics of each task. Moreover, we observe that despite the improved performance of the models, their robustness does not show significant enhancements, which warrants further investigation. We hope that our study will offer new insights to future work on how to balance the model's task-solving ability with its user-friendly response capabilities, as well as on how to improve its robustness while enhancing its performance.

## 6 Limitations

In this paper, we systematically analyze the GPT-3 and GPT-3.5 series and summarize some findings and conclusions. However, we acknowledge that there are some limitations. Firstly, we do not use the full dataset for testing some models due to the OpenAI API limiting the rate of accesses, but this does not affect the overall trend analysis. Secondly, OpenAI releases GPT-4 during our study and notes that it has more powerful capabilities. Unfortunately, the GPT-4 API has not been made available yet, which has made it difficult for us to test whether GPT-4 addresses some of the issues with the previous model. Investigating this will be a critical area for future research.

## A Performance Details



