## Abstract

Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the humanlabeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve imagetext alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models.

## 1. Introduction

Deep generative models have recently shown remarkable success in generating high-quality images from text prompts (Ramesh et al., 2021; 2022; Saharia et al., 2022; Yu et al., 2022b; Rombach et al., 2022). This success has been driven in part by the scaling of deep generative models to large-scale datasets from the web such as LAION (Schuhmann et al., 2021; 2022). However, major challenges remain in domains where large-scale text-to-image models fail to generate images that are well-aligned with text prompts (Feng et al., 2022; Liu et al., 2022a;b). For instance, current text-to-image models often fail to produce reliable visual text (Liu et al., 2022b) and struggle with compositional image generation (Feng et al., 2022).

In language modeling, learning from human feedback has emerged as a powerful solution for aligning model behavior with human intent (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021; Nakano et al., 2021; Ouyang et al., 2022; Bai et al., 2022a). Such methods first learn a reward function intended to reflect what humans care about in the task, using human feedback on model outputs. The language model is then optimized using the learned reward function by a reinforcement learning (RL) algorithm, such as proximal policy optimization (PPO; Schulman et al. 2017). This RL with human feedback (RLHF) framework has successfully aligned large-scale language models (e.g., GPT-3; Brown et al. 2020) with complex human quality assessments.

Motivated by the success of RLHF in language domains, we propose a fine-tuning method for aligning text-to-image models using human feedback. Our method consists of the following steps illustrated in Figure 1: (1) We first generate diverse images from a set of text prompts designed to test output alignment of a text-to-image model. Specifically, we examine prompts where pre-trained models are more prone to errors - generating objects with specific colors, counts, and backgrounds. We then collect binary human feedback assessing model outputs. (2) Using this humanlabeled dataset, we train a reward function to predict human feedback given the image and text prompt. We propose an auxiliary task-identifying the original text prompt within a set of perturbed text prompts-to more effectively exploit human feedback for reward learning. This technique improves the generalization of reward function to unseen images and text prompts. (3) We update the text-to-image model via reward-weighted likelihood maximization to better align it with human feedback. Unlike the prior work (Stiennon et al., 2020; Ouyang et al., 2022) that uses RL for optimization, we update the model using semi-supervised learning to measure model-output quality w.r.t. the learned reward function.

We fine-tune the stable diffusion model (Rombach et al., 2022) using 27K image-text pairs with human feedback. Our fine-tuned model shows improvement in generating objects with specified colors, counts, and backgrounds. Moreover, it improves compositional generation (i.e., can bet-

Figure 1. The steps in our fine-tuning method. (1) Multiple images sampled from the text-to-image model using the same text prompt, followed by collection of (binary) human feedback. (2) A reward function is learned from human assessments to predict image-text alignment. We also utilize an auxiliary objective called prompt classification, which identifies the original text prompt within a set of perturbed text prompts. (3) We update the text-to-image model via reward-weighted likelihood maximization.

<!-- image -->

ter generate unseen objects 1 given unseen combinations of color, count, and background prompts). We also observe that the learned reward function is better aligned with human assessments of alignment than CLIP score (Radford et al., 2021) on tested text prompts. We analyze several design choices, such as using an auxiliary loss for reward learning and the effect of using 'diverse' datasets for fine-tuning.

We can summarize our main contributions as follows:

- · We propose a simple yet efficient fine-tuning method for aligning a text-to-image model using human feedback.
- · We show that fine-tuning with human feedback significantly improves the image-text alignment of a text-toimage model. On human evaluation, our model achieves up to 47% improvement in image-text alignment at the expense of mildly degraded image fidelity.
- · We show that the learned reward function predicts human assessments of the quality more accurately than the CLIP score (Radford et al., 2021). In addition, we show that rejection sampling based on our learned reward function can also significantly improve the image-text alignment.
- · Naive fine-tuning with human feedback can significantly reduce the image fidelity, despite better alignment. We find that careful investigations on several design choices are important in balancing alignment-fidelity tradeoffs.

Even though our results do not address all the failure modes of the existing text-to-image models, we hope that this work highlights the potential of learning from human feedback for aligning these models.

## 3. Main Method

To improve the alignment of generated images with their text prompts, we fine-tune a pre-trained text-to-image model (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022) by repeating the following steps shown in

Figure 1. We first generate a set of diverse images from a collection of text prompts designed to test various capabilities of the text-to-image model. Human raters provide binary feedback on these images (Section 3.1). Next we train a reward model to predict human feedback given a text prompt and an image as inputs (Section 3.2). Finally, we fine-tune the text-to-image model using reward-weighted log likelihood to improve text-image alignment (Section 3.3).

## 3.3. Updating the Text-to-Image Model

We use our learned r φ to update the text-to-image model p with parameters θ by minimizing the loss

L ( θ ) = E ( x , z ) ∼D model [ -r φ ( x , z ) log p θ ( x | z ) ] + β E ( x , z ) ∼D pre [ -log p θ ( x | z ) ] , (2)

where D model is the model-generated dataset (i.e., images generated by the text-to-image model on the tested text prompts), D pre is the pre-training dataset , and β is a penalty parameter. The first term in (2) minimizes the rewardweighted negative log-likelihood (NLL) on D model . 6 By

Table 1. Examples of text categories.

| Category    | Examples                                                    |
|-------------|-------------------------------------------------------------|
| Count       | One dog ; Two dogs ; Three dogs ; Four dogs ; Five dogs ;   |
| Color       | A green colored dog ; A red colored dog ;                   |
| Background  | A dog in the forest ; A dog on the moon ;                   |
| Combination | Two blue dogs in the forest ; Five white dogs in the city ; |

Table 2. Details of image-text datasets and human feedback.

| Category    | Total # of images   | Human feedback (%)   | Human feedback (%)   | Human feedback (%)   |
|-------------|---------------------|----------------------|----------------------|----------------------|
|             |                     | Good                 | Bad                  | Skip                 |
| Count       | 6480                | 34 . 4               | 61 . 0               | 4 . 6                |
| Color       | 3480                | 70 . 4               | 20 . 8               | 8 . 8                |
| Background  | 2400                | 66 . 9               | 33 . 1               | 0 . 0                |
| Combination | 15168               | 35 . 8               | 59 . 9               | 4 . 3                |
| Total       | 27528               | 46 . 5               | 48 . 5               | 5 . 0                |

evaluating the quality of the outputs using a reward function aligned with the text prompts, this term improves the image-text alignment of the model.

Typically, the diversity of the model-generated dataset is limited, which can result in overfitting. To mitigate this, similar to Ouyang et al. (2022), we also minimize the pretraining loss , the second term in (2). This reduces NLL on the pre-training dataset D pre . In our experiments, we observed regularization in the loss function L ( θ ) in (2) enables the model to generate more natural images.

Different objective functions and algorithms (e.g., PPO; Schulman et al. 2017) could be considered for updating the text-to-image model similar to RLHF finetuning (Ouyang et al., 2022). We believe RLHF fine-tuning may lead to better models because it uses online sample generation during updates and KL-regularization over the prior model. However, RL usually requires extensive hyperparameter tuning and engineering, thus, we defer the extension to RLHF fine-tuning to future work.

## 4.1. Experimental Setup

Models. For our baseline generative model, we use stable diffusion v1.5 (Rombach et al., 2022), which has been

## Original model

Figure 2. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). (a) Our model generates high-quality seen object ( dog ) with specified color, count and background. on seen text prompts. (b) Our model generates an unseen object ( tiger ) with specified color, count, and background. (c) Our model still generates reasonable images from unseen text categories (artistic generation).

<!-- image -->

## 4.4. Ablation Studies

Effects of human dataset size. To investigate how human data quality affects reward learning, we conduct an ablation study, reducing the number of images per text prompt by half before training the reward function. Figure 3(b) shows that model accuracy decreases on both seen and unseen prompts as data size decreases, clearly demonstrating the importance of diversity and the amount of rater data.

Effects of using diverse datasets. To verify the importance of data diversity, we incrementally include unlabeled and pre-training datasets during fine-tuning. We measure the reward score (image-text alignment) on 120 tested text prompts and FID score (Heusel et al., 2017)-the similarity between generated images and real images-on MS-CoCo validation data (Lin et al., 2014). Table 3 shows that FID score is significantly reduced when the model is fine-tuned using only human data, despite better image-text alignment.

Table 3. Comparison with the original Stable Diffusion. For evaluating image fidelity, we measure FID scores on the MS-CoCo. For evaluating the image-text alignment, we measure reward scores and CLIP scores on 120 tested text prompts. ↑ ( ↓ ) indicates that the higher (lower) number is the better.

|                                            | FID on MS-CoCo ( ↓ )   | Average rewards on tested prompts ( ↑ )   |
|--------------------------------------------|------------------------|-------------------------------------------|
| Original model                             | 13 . 97                | 0 . 43                                    |
| Fine-tuned model w.o unlabeled & pre-train | 26 . 59                | 0 . 69                                    |
| Fine-tuned model w.o pre-train             | 21 . 02                | 0 . 79                                    |
| Fine-tuned model                           | 16 . 76                | 0 . 79                                    |

However, by adding the unlabeled and pre-training datasets, FID score is improved without impacting image-text alignment. We provide image samples from unseen text prompts in Figure 5. We see that fine-tuned models indeed generate more natural images when exploiting more diverse datasets.

## 5. Discussion

In this work, we have demonstrated that fine-tuning with human feedback can effectively improve the image-text alignment in three domains: generating objects with a specified count, color, or backgrounds. We analyze several design choices (such as using an auxiliary loss and collecting diverse training data) and find that it is challenging to balance the alignment-fidelity tradeoffs without careful investigations on such design choices. Even though our results do not

address all the failure modes of the existing text-to-image models, we hope that our method can serve as a starting point to study learning from human feedback for improving text-to-image models.

Limitations and future directions . There are several limitations and interesting future directions in our work:

- · More nuanced human feedback . Some of the poor generations we observed, such as highly saturated image colors, are likely due to similar images being highly ranked in our training set. We believe that instructing raters to look for a more diverse set of failure modes (oversaturated colors, unrealistic animal anatomy, physics violations, etc.) will improve performance along these axes.
- · Diverse and large human dataset . For simplicity, we consider a limited class of text categories (count, color, background) and thus consider a simple form of human feedback (good or bad). Due to this, the diversity of our human data is bit limited. Extension to more subjective text categories (like artistic generation) and informative human feedback such as ranking would be an important direction for future research.
- · Different objectives and algorithms . For updating the text-to-image model, we use a reward-weighted likelihood maximization. However, similar to prior work in language domains (Ouyang et al., 2022), it would be an interesting direction to use RL algorithms (Schulman et al., 2017). We believe RLHF fine-tuning may lead to better models because (a) it uses online sample generation during updates and (b) KL-regularization over the prior model can mitigate overfitting to the reward function.

## Original model

Fine-tuned model (ours)

<!-- image -->

(a) Text prompt: Three wolves in the forest.

<!-- image -->

Figure 8. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). The fine-tuned model can generate an unseen object ( wolf ) with specified counts. However, the counts are not always perfect, showing a room for improvement.

<!-- image -->

(b) Text prompt: Four wolves in the forest.

<!-- image -->

<!-- image -->

(c) Text prompt: Five wolves in the forest.

<!-- image -->

## Original model

Figure 9. Samples from the original Stable Diffusion model (left) and our fine-tuned model (right). The fine-tuned model can generate cake with specified backgrounds.

<!-- image -->

## B. Image-text Dataset

In this section, we describe our image-text dataset. We generate 2774 text prompts by combining a word or phrase from that category with some object. Specifically, we consider 9 colors ( red, yellow, green, blue, black, pink, purple, white, brown ), 6 numbers ( 1-6 ), 8 backgrounds ( forest, city, moon, field, sea, table, desert, San Franciso ) and 25 objects ( dog, cat, lion, orange, vase, cup, apple, chair, bird, cake, bicycle, tree, donut, box, plate, clock, backpack, car, airplane, bear, horse, tiger, rabbit, rose, wolf ). 15 For each text prompt, we generate 60 or 6 images according to the text category. In total, our image-text dataset consists of 27528 image-text pairs. Labeling for training is done by two human labelers.

For evaluation, we use 120 text prompts listed in Table 4. Given two (anonymized) sets of 4 images, we ask human raters to assess which is better w.r.t. image-text alignment and fidelity (i.e., image quality). Each query is rated by 9 independent human raters in Figure 4 and Figure 6.

Table 4. Examples of text prompts for evaluation.

| Category   | Examples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Seen       | A red colored dog.; A red colored donut.; A red colored cake.; A red colored vase.; A green colored dog.; A green colored donut.; A green colored cake.; A green colored vase.; A pink colored dog.; A pink colored donut.; A pink colored cake.; A pink colored vase.; A blue colored dog.; A blue colored donut.; A blue colored cake.; A blue colored vase.; A black colored apple.; A green colored apple.; A pink colored apple.; A blue colored apple.; A dog on the moon.; A donut on the moon.; A cake on the moon.; A vase on the moon.; An apple on the moon.;                                                                                                                 |
| Unseen     | A red colored bear.; A red colored wolf.; A red colored tiger.; A red colored rabbit.; A green colored bear.; A green colored wolf.; A green colored tiger.; A green colored rabbit.; A pink colored bear.; A pink colored wolf.; A pink colored tiger.; A pink colored rabbit.; A blue colored bear.; A blue colored wolf.; A blue colored tiger.; A blue colored rabbit.; A black colored rose.; A green colored rose.; A pink colored rose.; A blue colored rose.; A bear on the moon.; A wolf on the moon.; A tiger on the moon.; A rabbit on the moon.; A rose on the moon.; A bear in the sea.; A wolf in the sea.; A tiger in the sea.; A rabbit in the sea.; A rose in the sea.; |

## D. Experimental Details

Model architecture . For our baseline generative model, we use stable diffusion v1.5 (Rombach et al., 2022), which has been pre-trained on large image-text datasets (Schuhmann et al., 2021; 2022). For the reward model, we use ViT-L/14 CLIP model (Radford et al., 2021) to extract image and text embeddings and train a MLP using these embeddings as input. Specifically, we use two-layer MLPs with 1024 hidden dimensions each. We use ReLUs for the activation function between layers, and we use the Sigmoid activation function for the output. For auxiliary task, we use temperature T = 2 and penalty parameter λ = 0 . 5 .

Training . Our fine-tuning pipeline is based on publicly released repository ( https://github.com/huggingface/ diffusers/tree/main/examples/text\_to\_image ). We update the model using AdamW (Loshchilov & Hutter, 2017) with β 1 = 0 . 9 , β 2 = 0 . 999 , glyph[epsilon1] = 1 e -8 and weight decay 1 e -2 . The model is trained in half-precision on 4 40GB NVIDIA A100 GPUs, with a per-GPU batch size of 8, resulting in a toal batch size of 512 (256 for pre-training data and 256 for model-generated data). 16 It is trained for a total of 10,000 updates.

FID measurement using MS-CoCo dataset . We measure FID scores to evaluate the fidelity of different models using MS-CoCo validation dataset (i.e., val2014 ). There are a few caption annotations for each MS-CoCo image. We randomly choose one caption for each image, which results in 40,504 caption and image pairs. MS-CoCo images have different resolutions and they are resized to 256 × 256 before computing FID scores. We use pytorch-fid Python implementation for the FID measurement ( https://github.com/mseitzer/pytorch-fid ).

