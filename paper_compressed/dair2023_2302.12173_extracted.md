## ABSTRACT

Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be /flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection , that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, e/ffective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.

## 1 INTRODUCTION

Foundation and instruction-following [63] Large Language Models (LLMs) [43, 62] are changing our lives on many levels, not only for researchers and practitioners but also for the general public. Shortly after its release, ChatGPT [1] gained immense popularity, attracting over 100 million users in a short period of time [10]. Furthermore, there is a constant stream of new models, including the moreadvanced GPT-4 [62] and smaller white-box models [68, 68].

Figure 1: With LLM-integrated applications, adversaries could control the LLM, without direct access, by indirectly injecting it with prompts placed within sources retrieved at inference time.

<!-- image -->

LLM-Integrated Applications. Beyond their impressive capabilities, LLMs are now integrated into other applications at a widespread fast-paced rate. Such tools can o/ffer interactive chat and summary of the retrieved search results or documents and perform actions on behalf of the user by calling other APIs [9]. In the few months after ChatGPT, we witnessed Bing Chat [31], Bard [4], Microsoft 365 and Security Copilots [19, 20], and numerous ChatGPT plugins [9]- with new announcements on almost a daily basis. However, we argue that this AI-integration race is not accompanied by adequate guardrails and safety evaluations.

Indirect Prompt Injection. Augmenting LLMs with retrieval blurs the line between data and instructions . Adversarial prompting has been so far assumed to be performed directly by a malicious user exploiting the system. In contrast, we show that adversaries

Prompt Injection. Attacks against ML models typically involve powerful algorithms and optimization techniques [35]. However, the easily extensible nature of LLMs' functionalities via natural prompts can enable more straightforward attack tactics. Even under black-box settings with mitigation already in place [53], malicious users can exploit the model through Prompt Injection (PI) attacks that circumvent content restrictions or gain access to the model's original instructions [18, 33, 67].

Shailesh Mishra Saarland University shmi00001@uni-saarland.de

can now remotely a/ffect other users' systems by strategically injecting the prompts into data likely to be retrieved at inference time. If retrieved and ingested, these prompts can indirectly control the model (see Figure 1). Recent incidents already show that retrieved data can accidentally elicit unwanted behaviors (e.g., hostility) [25]. In this work, we take this idea further and investigate what an adversary can purposefully do to modify the behavior of LLMs in applications, potentially a/ffecting millions of benign users. Given the unprecedented nature of this attack vector, there are numerous new approaches to delivering such attacks and the myriad of threats they can cause. To address this unexplored challenge, we /first develop a systematic taxonomy that examines these emerging vulnerabilities from a computer security perspective.

In summary, our main contributions are:

Impact. We show that Indirect Prompt Injection can lead to full compromise of the model at inference time analogous to traditional security principles. This can entail remote control of the model, persistent compromise, theft of data, and denial of service. Furthermore, advanced AI systems add new layers of threat: Their capabilities to adapt to minimal instructions and autonomously advance the attacker's goals make them a potent tool for adversaries to achieve, e.g., disinformation dissemination and user manipulation. In this paper, we construct the /first examples of such attacks.

- · We introduce the concept of Indirect Prompt Injection (IPI) to compromise LLM-integrated applications-a completely uninvestigated attack vector in which retrieved prompts themselves can act as 'arbitrary code'.
- · Weshowcase the practical feasibility of these attacks on both real-world and synthetic systems, emphasizing the need for robust defenses.
- · We develop the /first taxonomy and systematic analysis of the threat landscape associated with IPI in LLM-integrated applications.
- · We share all our demonstrations on our GitHub repository 1 and all developed attack prompts in the Appendix of this paper to foster future research and contribute to building an open framework for the security assessment of LLMintegrated applications.

## 3.1 Injection Methods

There are potentially several ways to deliver the injection of malicious prompts, depending on the application itself. We broadly outline them below.

Passive Methods. These methods rely on retrieval to deliver injections. For example, for search engines, the prompts could be placed within public sources (e.g., a website or social media posts) that would get retrieved by a search query. Attackers could use

Search Engine Optimization (SEO) techniques to promote their poisonous websites. Moreover, Microsoft Edge has a Bing Chat sidebar; if enabled by the user, the model can read the current page and, e.g., summarize it. We found that any prompts/instructions written on a page (while being invisible to the user) can be e/ffectively injected and a/ffect the model. For code auto-completion models, the prompts could be placed within imported code available via code repositories. Even with o/ffline models that retrieve personal or documentation /files (e.g., the ChatGPT Retrieval Plugin [9]), the prompts could be injected by poisoning the input data.

User-Driven Injections. There could be even simpler techniques for injection by tricking the users themselves into entering the malicious prompt. A recent exploit [29] shows that an attacker could inject a malicious prompt into a text snippet that the user has copied from the attacker's website. A user could then rashly paste the copied text with the prompt in it as a question to ChatGPT, delivering the injection. Attackers could also leverage 'classic' social engineering (i.e., not AI-powered) to disseminate malicious prompts, by convincing users to try prompts where the instructions are written in a di/fferent language (e.g., 'You won't believe ChatGPT's answer to this prompt!').

Active Methods. Alternatively, the prompts could be actively delivered to the LLM, e.g., by sending emails containing prompts that can be processed by automated spam detection, personal assistant models, or new LLMs-augmented email clients [19].

Hidden Injections. To make the injections more stealthy, attackers could use multiple exploit stages, where an initial smaller injection instructs the model to fetch a larger payload from another source. Additionally, improvements in models' capabilities and supported modalities could open new doors for injections. For example, with multi-modal models (e.g., GPT-4), the prompts could be hidden in images (see Figure 28 for an example we constructed). To circumvent /filtering, prompts can also be encoded. Moreover, instead of feeding prompts to the model directly, they could be the result of Python programs that the model is instructed to run enabling encrypted payloads to pass safeguards. These possibilities would make the prompts harder to detect.

## 4.1 Experimental Setup

4.1.1 Synthetic Applications. To demonstrate the practical feasibility of attacks, we constructed synthetic applications with an integrated LLM using OpenAI's APIs. The backbone model in these applications is easy to swap by changing the API (e.g., text-davinci-003 , gpt-4 , etc.). For text-davinci-003 , we use the LangChain library [22] (a library for creating prompts, managing and chaining them, connecting to LLMs, and creating agents that dynamically call actions based on user's inputs). For gpt-4 , we directly use OpenAI's chat format. We then created analog scenarios that can be used to test the feasibility of the di/fferent methods on mock targets.

Our synthetic target is a chat app that will get access to a subset of tools to interface with. We prompt the agent 4 to use these tools by describing the tools and their functionality inside an initial prompt and asking the agent to check if any tools are required to ful/fill the request (see Prompt 1 and Prompt 2). For text-davinci-003 , we use ReAct prompting [79], and we found that GPT-4 can work well without ReAct (by only describing the tools and giving direct instructions). We integrate the following interfaces:

- · Search: Allows search queries to be answered with external content (which can potentially be malicious).
- · Retrieve URL: Sends an HTTP GET request to a speci/fied URL and returns the response.
- · View: Gives the LLM the capability to read the current website the user has opened.

- · Read/Send Email: Lets the agent read current emails, and compose and send emails at the user's request.
- · Memory: Lets the agent read/write to simple key-value storage per user's request.
- · Read Address Book: Lets the agent read the address book entries as (name, email) pairs.

For the proof-of-concept demonstrations of our attacks, all interfaces deliver prepared content, and unrelated queries are ignored. The agent cannot make any requests to real systems or websites. All attacks are run at a sampling temperature of 0 for reproducibility. Notably, we developed these synthetic applications before the launch of all LLM-integrated applications (e.g., Bing Chat, plugins, etc.) as potential futuristic applications. We now /find that they provide a close mock-up of the intended functionalities of current systems and thus can be used for controlled testing.

4.1.2 Bing Chat. Besides the controlled synthetic applications (with mockup functionality and prepared content for queries), we also test the attacks on Bing Chat as an example of a real-world, completely black-box model that has been integrated within a fullyfunctioning application. This also allows us to experiment with more dynamic and diverse scenarios and develop attacks that target the actual functionality of the application itself.

In addition to the chat interface, Microsoft Edge has a feature to enable Bing Chat in a sidebar [60]. If enabled by the user, the current page's content can be read by the model such that users can ask questions related to the page's content. We exploit this feature to perform 'indirect prompt injection'; we insert the prompts in local HTML comments. This allows us to test the attacks locally without making public injections that need to be retrieved via the search. Beyond the experimental testing, this can also be a valid attack scenario in which the attackers poison their own websites (e.g., such that users would get manipulated search results when they ask questions about the content of the page).

Bing Chat currently runs on the GPT-4 model [11] with customization to the search task. Full details of how Bing Chat works are not available. However, it involves components for query generation based on users' conversations, search engine integration, answers generation, and citation integration [5]. It has three chat modes ('creative', 'balanced', and 'precise'); we found that attacks often successfully work with di/fferent modes.

4.1.3 Github Copilot. We also test prompt injection attacks that aim to manipulate code auto-completion using Github Copilot [15]. The Copilot uses OpenAI Codex [30] to suggest lines or functions based on the current context.

## 5 DISCUSSION

We here discuss the ethical considerations of our work, its limitations, further implications, and future directions.

## 5.2 Limitations

Experimental Setup. In order to avoid performing actual injections for real-world applications, we tested the attacks on synthetic applications and local HTML /files with Bing Chat's sidebar. However, we believe that, in principle, the attacks are feasible for in-thewild retrieved injections as well, supported by observed anecdotal evidence (e.g., users inserting instructions in their personal pages for Bing Chat or GPT-4, or Bing Chat responses that changed based on the retrieved results [25]). We also could not test the attacks on other applications (e.g., Microsoft 365 Copilot and ChatGPT's plugins) as we did not have access to them.

Deception and Believability. We qualitatively observe the huge improvements of recent LLMs in following complex instructions and persuasion over previous models. This is not without /flaws. For example, the model might generate conspicuously false answers that are widely unbelievable or attempt to convince users to disclose their information or follow malicious links in a blatant way. Carefully crafting prompts could lead to more believable utterances. Moreover, persuasion and deception might get better in future models, as a side e/ffect of RLHF [74], or when current models are equipped with techniques [64] to improve their planning, believability, and long-term coherency. Even with current models, there is recent evidence that users' judgment might be a/ffected despite being aware that they are advised by a chatbot [54]. Future work is thus needed to thoroughly evaluate these aspects and quantify the deception potential of the di/fferent attacks in di/fferent setups via user studies.

Evaluation. In contrast to static one-shot malicious text generation, quantifying our attacks' success rate can be challenging in the setup of dynamically evolving and interactive chat sessions with users [55]. This entails studying many factors, such as how often the injected prompts would get triggered based on users' initial instructions and how convincing and consistent the manipulation is across follow-up questions. It is also important to evaluate the attacks via multiple generations and variations of prompts and topics. As these avenues are methodologically complex on their own, we leave them for future work. We note, however, that developing the prompts that execute our attacks turned out to be rather simple, often working as intended on the very /first attempt at writing them. We decided to leave grammatical and spelling errors that occurred on the /first draft of the prompt to further demonstrate the minimal sophistication required to develop prompt injection exploits.

## 6 CONCLUSION

LLMs in applications are no longer stand-alone units with controlled input-output channels; they are presented with arbitrarily retrieved inputs and can call other external APIs. We argue that this allows attackers to remotely a/ffect users and cross crucial security boundaries via Indirect Prompt Injection . To kick-start the investigation of this novel attack vector, we /first draw insights from computer security principles to derive a taxonomy that studies potential vulnerabilities in a systematic manner. We next develop speci/fic demonstrations of the threats and injection methods, which we run on our developed synthetic applications and real-world systems, such as Bing Chat. Based on our attacks, we discuss key messages and qualitative observations in addition to implications on users, future applications, further attacks, and defenses. Our work sets an essential step towards the urgently-needed security evaluation of LLM-integrated applications and future autonomous agents, which we hope will pave the way to safer deployment.

