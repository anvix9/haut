## Abstract

Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.

## 1. Introduction

The main criteria for success when dealing with complex outputs in computer vision is not how well the model optimizes the training objective, but rather how well the predictions are aligned with the task risk, i.e. the model's performance on the intended usage. In order to improve this alignment, as a community we iterate on model architectures, data, optimization, sampling procedures, postprocessing, etc. As an example, in the context of object detection, researchers use non-maximum suppression postprocessing (Ren et al., 2015; Lin et al., 2017), set-based global loss (Carion et al., 2020) or even alter the input data (Chen et al., 2022) to obtain models with improved behavior at test time. Although these approaches deliver significant gains, they are often highly specialized to the task and method at hand, while only indirectly optimizing for the task risk.

|

Before

Figure 1. By tuning a strong, pretrained model with a reward that relates to the task, we can significantly improve the model's alignment with the intended usage.

<!-- image -->

After

<!-- image -->

(a) Optimize mAP: 39 → 54 , results in a much high recall and learns box prediction confidences.

<!-- image -->

Input

|

Before

After

<!-- image -->

<!-- image -->

(b) Optimize PQ: 43 . 1 → 46 . 1 , removes many incoherent predictions, especially for small-scale objects.

<!-- image -->

Input

|

Before

After

<!-- image -->

<!-- image -->

(c) Optimize 'colorfulness' score: 0 . 41 → 1 . 79 , improves color diversity and saturation.

<!-- image -->

This problem is not new. It has been extensively studied by the natural language processing (NLP) and reinforcement learning (RL) fields, where it is notoriously hard to formulate an optimization objective for tasks with less tangible goals, such as translation (Kreutzer et al., 2018) or summarization (Stiennon et al., 2020). A popular approach when dealing with this type of problem is to learn to imitate example outputs, followed by reinforcement-learning to align the model with a reward function . Using this approach, the NLP field is now producing exciting results with systems that use large pretrained language models and rewards defined by human feedback to tackle tasks that were otherwise

hard to specify (Ouyang et al., 2022). Additionally, the same approach is widely adopted for the image captioning task (Rennie et al., 2017), where CIDEr (Vedantam et al., 2015) is used as a reward. Despite that, to the best of our knowledge, reward optimization has not been previously explored for (non-textual) computer vision tasks.

In this work, we demonstrate that tuning a pretrained model with a reward function using REINFORCE (Williams, 1992) works out-of-the-box for a wide range of computer vision tasks. We illustrate some of our key results in Figure 1, highlighting both quantitative and qualitative improvements brought by reward optimization for object detection, panoptic segmentation, and image colorization. The simplicity and effectiveness of our approach on a diverse set of computer vision tasks demonstrates its versatility and adaptability. Although in this work we mostly use rewards in the form of evaluation metrics, we believe these initial results show promising paths to optimizing computer vision models with more complex and harder to specify rewards, e.g. human feedback or holistic system performance.

## 5. Analysis



## 6. Discussion and Limitations

Reward hacking. Our work shows the feasibility of tuning a model with rewards beyond standard MLE. However, it is important to note that there is no guarantee that a given reward function will result in improvements in the intended usage. The model may instead exploit weaknesses in the reward definition, and it is crucial to take that in consideration when validating the models. In saying that, we think that our demonstrated RL-based approach opens up the possibilities of different forms of rewards, unlocking the potential of using real-risk or human-feedback in training computer vision models, which we believe will lead to great advancements in the current field.

Reward design. In this work we mostly use simple rewards based on existing evaluation metrics. There are however many more options, including combinations of filter-based, input-output checks, simulation checks, use of pretrained

(a) Object detection.

<!-- image -->

(b) Panoptic segmentation.

<!-- image -->

Figure 6. Plot of metrics and rewards measured on the validation set during training for object detection (mAP) and panoptic segmentation (PQ). Overall we observe a good correlation between the reward being optimized and the task risk.

models to detect undesired outputs or to keep the model closer to a initial distribution, entropy to encourage diversity and exploration or use of real-world or human feedback. Another thing to keep in mind is that the reward does not needs to aim to be as exact as the task risk. Alternatives functions may be easier to obtain, control or optimize, for example if the reward provides more guidance than a sparse risk value. We leave the exploration of this in computer vision tasks to future work.

Advanced RL techniques. The presented approach with warm-up and constant learning rate setup suffices across the explored applications. As such, we saw no need to add regularisation to remain close to the original policy, encourage exploration or attempt to reduce the number of reward calls or model updates. We believe the efficacy is in part due to the MLE-pretrained initialization, allowing the model to avoid potential issues with the simple technique. Although this may not hold in more complex setups, we encourage to try the simple approach in other similar applications.

Data for imitation learning. Can MLE-training alone also reach better alignment with the task goal? This question was part of the motivation of this work. Although we expect more data to help MLE models imitate the ground truth, we found it hard to know what data to collect or how to augment it to have a particular alignment effect with a task risk. By tuning a model with a reward we obtain that effect

by optimizing a model to avoid undesired outputs. Since the space of undesired outputs where the MLE model assigns high likelihood is hard to predict, it is critical to observe the model in action and focus on the examples the model misassigns high-likelihood.

Training cost. There are two main costs to consider: model sampling cost and the number of queries to the reward function. Sampling auto-regressive models is notoriously more expensive than computing the likelihood of a given sequence. This is due to difficulties utilizing hardware efficiently and not due to an increase in the number of flops. Note however that this cost is still proportional to inference usage. Additionally, the presented method only requires a model where the likelihoods of samples can be optimized with gradients. It does not depend on the model being auto-regressive, though that can be an important piece to modelling complex distributions. For more complex applications the number of queries to the reward function can be a bigger concern. In such cases it is worth to explore off-policy RL techniques and approximate a target reward with a value network.

## 7. Conclusion

Our work shows that reward optimization is a viable option to optimize a variety of computer vision tasks. Using the simple approach of pretraining to imitate ground truth followed by reward optimization, we were able to: (a) improve models for object detection and panoptic segmentation trained without other task-specific components to the level comparable to ones obtained through clever manipulation of data, architectures and losses; (b) qualitatively affect the results of colorization models to align with a goal of creating vivid and colorful images; (c) show that the simple approach is competitive with recent works in captioning.

We believe these results demonstrate the possibilities to have more precise control on how models align the nontrivial task risk. We look forward to more challenging use cases such as tuning scene understanding outputs for robot grasping, where one can optimize the perception models for the probability of a successful grasp.

## Model

Table 4. Panoptic segmentation settings.

| ENCODER:    | VIT-L/16   |
|-------------|------------|
| DECODER:    | 24 LAYERS  |
| SEQ LENGTH: | 256 TOKENS |
| RESOLUTION: | 512 × 512  |

MLE pretraining

KOLESNIKOV ET AL. (2022)

Table 5. Object detection settings.

| BATCH SIZE:    | 128        |
|----------------|------------|
| SCHEDULE:      | CONSTANT   |
| LEARNING-RATE: | 1 · 10 - 6 |
| TOTAL STEPS:   | 30 000     |
| WARMUP STEPS:  | 4 000      |

|             | Model       |
|-------------|-------------|
| ENCODER:    | VIT-B/16    |
| DECODER:    | 6 LAYERS    |
| SEQ LENGTH: | 600 TOKENS  |
| RESOLUTION: | 1280 × 1280 |

## Model

ENCODER:

VIT-B/16 / VIT-L/16

DECODER:

6 LAYERS

SEQ LENGTH:

128 TOKENS

RESOLUTION:

512 × 512

