## Abstract

Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models early dropout : dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models late dropout , where dropout is not used in the early iterations and is only activated later in training. Experiments on ImageNet and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available at https://github. com/facebookresearch/dropout .

## 1. Introduction

The year 2022 marks a full decade since AlexNet's pivotal 'ImageNet moment' (Krizhevsky et al., 2012), which launched a new era in deep learning. It is no coincidence that dropout (Hinton et al., 2012) also celebrates its tenth

Proceedings of the 40 th International Conference on Machine Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).

birthday in 2022: AlexNet employed dropout to substantially reduce its overfitting, which played a critical role in its victory at the ILSVRC 2012 competition. Without the invention of dropout, the advancements we currently see in deep learning might have been delayed by years.

Dropout has since become widely adopted as a regularizer to mitigate overfitting in neural networks. It randomly deactivates each neuron with probability p , preventing different features from co-adapting with each other (Hinton et al., 2012; Srivastava et al., 2014). After applying dropout, training loss typically increases, while test error decreases, narrowing the model's generalization gap.

Deep learning evolves at an incredible speed. Novel techniques and architectures are continuously introduced, applications expand, benchmarks shift, and even convolution can be gone (Dosovitskiy et al., 2021) - but dropout has stayed. It continues to function in the latest AI achievements, including AlphaFold's protein structure prediction (Jumper et al., 2021), and DALL-E 2's image generation (Ramesh et al., 2022), demonstrating its versatility and effectiveness.

Despite the sustained popularity of dropout, its strength, represented by the drop rate p , has generally been decreasing over the years. In the original dropout work (Hinton et al., 2012), a default drop rate of 0.5 was used. However, lower drop rates, such as 0.1, have been frequently adopted in recent years. Examples include training BERT (Devlin et al., 2018) and Vision Transformers (Dosovitskiy et al., 2021).

The primary driver for this trend is the exploding growth of available training data, making it increasingly difficult to overfit. In addition, advancements in data augmentation techniques (Zhang et al., 2018; Cubuk et al., 2020) and algorithms for learning with unlabeled or weakly-labeled data (Brown et al., 2020; Radford et al., 2021; He et al., 2021) have provided even more data to train on than the model can fit to. As a result, we may soon be confronting more problems with underfitting instead of overfitting.

Would dropout lose its relevance should such a situation arise? In this study, we demonstrate an alternative use of dropout for tackling underfitting. We begin our investigation into dropout training dynamics by making an intriguing observation on gradient norms, which then leads us to a key empirical finding: during the initial stages of train-

whole-dataset gradient mini-batch gradient gradient error Figure 1. Dropout in early training helps the model produce minibatch gradient directions that are more consistent and aligned with the overall gradient of the entire dataset.

<!-- image -->

ing, dropout reduces gradient variance across mini-batches and allows the model to update in more consistent directions. These directions are also more aligned with the entire dataset's gradient direction (Figure 1). Consequently, the model can optimize the training loss more effectively with respect to the whole training set, rather than being swayed by individual mini-batches. In other words, dropout counteracts SGD and prevents excessive regularization due to randomness in sampling mini-batches during early training.

Based on this insight, we introduce early dropout - dropout is only used during early training - to help underfitting models fit better. Early dropout lowers the final training loss compared to no dropout and standard dropout. Conversely, for models that already use standard dropout, we propose to remove dropout during earlier training epochs to mitigate overfitting. We refer to this approach as late dropout and demonstrate that it improves generalization accuracy for large models. Figure 2 provides a comparison of standard dropout, early dropout, and late dropout.

We evaluate early and late dropout using different models on image classification and downstream tasks. Our methods consistently yield better results than both standard dropout and no dropout. We hope our findings can offer novel insights into dropout and overfitting, and motivate further research in developing neural network regularizers.

## 4. Approach

From the analysis above, we know that using dropout early can potentially improve the model's ability to fit the training data. Based on this observation, we present our approaches.

Underfitting and overfitting regimes. Whether it is desirable to fit the training data better depends on whether the model is in an underfitting or overfitting regime, which can be difficult to define precisely. In this work, we use the following criterion and find it is effective for our purpose: if a model generalizes better with standard dropout, we consider it to be in an overfitting regime; if the model performs better without dropout, we consider it to be in an underfitting regime. The regime a model is in depends not only on the model architecture but also on the dataset used and other training parameters.

Early dropout. In their default settings, models at underfitting regimes do not use dropout. To improve their ability to fit the training data, we propose early dropout : using dropout before a certain iteration, and then disabling it for the rest of training . Our experiments show that early dropout reduces final training loss and improves accuracy.

Late dropout. Overfitting models already have standard dropout included in their training settings. During the early stages of training, dropout may cause overfitting unintentionally, which is not desirable. To reduce overfitting, we propose late dropout : not using dropout before a certain iteration, and then using it for the rest of training. This is a symmetric approach to early dropout.

Hyper-parameters. Our methods are straightforward both in concept and implementation, illustrated in Figure 2. They require two hyper-parameters: 1) the number of epochs to wait before turning dropout on or off. Our results show that this choice can be robust enough to vary from 1% to 50% of the total epochs. 2) The drop rate p , which is similar to the standard dropout rate and is also moderately robust.

## 5.2. Analysis

We carry out ablation studies to understand the characteristics of early dropout. Our default setting is ViT-T training with early dropout using the improved recipe.

Dropout epochs. We investigate the impact of the number of epochs for early dropout. By default, we use 50 epochs. We vary the number of early dropout epochs and observe its effect on the final accuracy. The results, shown in Figure 9, are based on the average of 3 runs with different random seeds. The results indicate that the favorable range of epochs for both early dropout is quite broad, ranging from as few as 5 epochs to as many as 300, out of a total of 600 epochs. This robustness makes early dropout easy to adopt in practical settings.

Figure 9. Early dropout epochs . Early dropout is effective with a wide range of dropout epochs.

<!-- image -->

Drop rates. The dropout rate is another hyper-parameter, similar to standard dropout. The impact of varying the rate for early dropout and early s.d. is shown in Figure 10. The results indicate that the performance of early s.d. is not that sensitive to the rate, but the performance of early dropout is highly dependent on it. This could be related to the fact that dropout layers are more densely inserted in ViTs than s.d. layers. In addition, the s.d. rate represents the maximum rate among layers (Huang et al., 2016), but the dropout rate represents the same rates for all layers, so the same increase in dropout rate results in a much stronger regularizing effect. Despite that, both early dropout and early s.d. are less sensitive to the rate than standard dropout, where a drop rate of 0.1 can significantly degrade accuracy (Table 2).

Scheduling strategies. In previous studies, different strategies for scheduling dropout or related regularizers have been explored. These strategies typically involve either gradually

| strategy        | acc.   |   train loss |
|-----------------|--------|--------------|
| no dropout 76.3 |        |        3.033 |
| constant        | 71.5   |        3.437 |
| increasing      | 75.2   |        3.285 |
| decreasing      | 74.7   |        3.113 |
| annealed        | 76.3   |        3.004 |
| curriculum      | 70.4   |        3.49  |
| early           | 76.7   |        2.996 |

(a) Scheduling strategies . Early dropout outperforms alternative strategies.

| schedule   |   acc. |   train loss |
|------------|--------|--------------|
| linear     |   76.7 |        2.991 |
| constant   |   76.6 |        3.025 |
| cosine     |   76.6 |        2.988 |

- (b) Early dropout scheduling . Early dropout is robust to various schedules.

| model   |      |   baseline early dropout |
|---------|------|--------------------------|
| ViT-T   | 76.3 |                     76.7 |
| ViT-S   | 80.4 |                     80.8 |
| ViT-B   | 78.7 |                     78.7 |

- (c) Model size . Early dropout does not help models at overfitting regimes.

Table 3. Early dropout ablation results with ViT-T/16 on ImageNet-1K.Figure 10. Drop rates . The performance of early dropout on ViTT is affected by the dropout rate (top) but is more stable with the stochastic depth rate (bottom).

<!-- image -->

increasing (Morerio et al., 2017; Zoph et al., 2018; Tan & Le, 2021) or decreasing (Rennie et al., 2014) the strength of dropout over the entire or nearly the entire training process. The purpose of these strategies, however, is to reduce overfitting rather than underfitting.

For comparison, we also evaluate linear decreasing / increasing strategies where the drop rate starts from p / 0 and ends at 0 / p , as well as previously proposed curriculum (Morerio et al., 2017) and annealed (Rennie et al., 2014) strategies. For all strategies, we conduct a hyper-parameter sweep for the rate p . The results are presented in Table 3a. All strategies produce either similar or much worse results than no-dropout. This suggests existing dropout scheduling strategies are not effective for underfitting.

Early dropout scheduling. There is still a question on how to schedule the drop rate in the early phase. Our experiments use a linear decreasing schedule from an initial value p to 0

by default. A simpler alternative is to use a constant value. It can also be useful to consider a cosine decreasing schedule commonly adopted for learning rate schedules. The optimal p value for each option may differ and we compare the best result for each option. Table 3b presents the results. All three options manifest similar results and can serve as valid choices. This indicates early dropout does not depend on one particular schedule to work. Additional results for constant early dropout can be found in Appendix D.

Model sizes. According to our analysis in Section 3, early dropout helps models fit better to the training data. This is particularly useful for underfitting models like ViT-T. We take ViTs of increasing sizes, ViT-T, ViT-S, and ViT-B, and examine the trend in Table 3c. The baseline column represents the results obtained by the best standard dropout rates (0.0 / 0.0 / 0.1) for each of the three models. Our results show that early dropout is effective in improving the performance of the first two models, but was not effective in the case of the larger ViT-B.

Learning rate warmup. Learning rate (lr) warmup (He et al., 2016; Goyal et al., 2017) is a technique that also specifically targets the early phase of training, where a smaller lr is used. We are curious in the effect lr warmup on early dropout. Our default recipe uses a 50-epoch linear lr warmup. We vary the lr warmup length from 0 to 100 and compare the accuracy with and without early dropout in Figure 11. Our results show that early dropout consistently improves the accuracy regardless of the use of lr warmup.

Figure 11. Early dropout leads to accuracy improvement when the number of learning rate warmup epochs varies.

<!-- image -->

Batch size. We vary the batch size from 1024 to 8192 and scale the learning rate linearly (Goyal et al., 2017) to examine how batch size influences the effect of early dropout. Our default batch size is set at 4096. In Figure 12, we note that early dropout becomes less beneficial as the batch size increases to 8192. This observation supports our hypothesis: as the batch size grows, the mini-batch gradient tends to approximate the entire-dataset gradient more closely. Consequently, the importance of gradient error reduction may diminish, and early dropout no longer yields meaningful improvement over the baseline.

Figure 12. Early dropout is not as effective when the batch size is increased to 8192, but consistent improvement is observed for smaller batch sizes. This supports our hypothesis on the gradient error reduction effect of early dropout.

<!-- image -->

Training curves. We plot the training loss and test accuracy curves for ViT-T with early dropout and compare it with a no-dropout baseline in Figure 13. The early dropout is set to 50 epochs and uses a constant dropout rate. During the early dropout phase, the train loss for the dropout model is higher and the test accuracy is lower. Intriguingly, once the early dropout phase ends, the train loss decreases dramatically and the test accuracy improves to surpass the baseline.

Figure 13. Training Curves. When early dropout ends, the model experiences a significant decrease in training loss and a corresponding increase in test accuracy.

<!-- image -->

## 8. Conclusion

Dropout has shined for 10 years for its excellence in tackling overfitting. In this work, we unveil its potential in aiding stochastic optimization and reducing underfitting. Our key insight is dropout counters the data randomness brought by SGD and reduces gradient variance at early training. This also results in stochastic mini-batch gradients that are more aligned with the underlying whole-dataset gradient. Motivated by this, we propose early dropout to help underfitting models fit better, and late dropout, to improve the generalization of overfitting models. We hope our discovery stimulates more research in understanding dropout and designing regularizers for gradient-based learning, and our approaches help model training with increasingly large datasets.

Acknowledgement. We would like to thank Yubei Chen, Yida Yin, Hexiang Hu, Zhiyuan Li, Saining Xie and Ishan Misra for valuable discussions and feedback.

| References                                                                                                                                                                                                                                                                   | Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:                                                                                                                                                                            |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Ba, J. and Frey, B. Adaptive dropout for training deep neural networks. In NeurIPS , 2013.                                                                                                                                                                                   | Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805 , 2018.                                                                                                                 |
| Baldi, P. and Sadowski, P. J. Understanding dropout. In NeurIPS , 2013.                                                                                                                                                                                                      | DeVries, T. and Taylor, G. W. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552 , 2017.                                                                                           |
| Balles, L. and Hennig, P. Dissecting adam: The sign, magni- tude and variance of stochastic gradients. In International Conference on Machine Learning . PMLR, 2018.                                                                                                         | Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image      |
| Bossard, L., Guillaumin, M., and Gool, L. V. Food-101- mining discriminative components with random forests. In EECV , 2014.                                                                                                                                                 | recognition at scale. In ICLR , 2021.                                                                                                                                                                                                 |
| Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,                                                                                                    | Gal, Y. and Ghahramani, Z. Dropout as a bayesian approxi- mation: Representing model uncertainty in deep learning. In ICML , 2016.                                                                                                    |
| Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,                                                                                                                                                           | Gal, Y., Hron, J., and Kendall, A. Concrete dropout. NeurIPS , 30, 2017.                                                                                                                                                              |
| Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In NeurIPS , 2020.                                                                                                             | Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained cnns are biased towards texture; increasing shape bias improves ac- curacy and robustness. arXiv preprint arXiv:1811.12231 , |
| Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., Zhang, Z., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R., Wu, Y., Dai, J., Wang, J., Shi, J., Ouyang, W., Loy, C. C., and Lin,                              | 2018. Ghiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regu- larization method for convolutional networks. NeurIPS , 2018.                                                                                                          |
| D. MMDetection: Open mmlab detection toolbox and benchmark. arXiv:1906.07155 , 2019.                                                                                                                                                                                         | Gomez, A. N., Zhang, I., Kamalakara, S. R., Madaan,                                                                                                                                                                                   |
| Chen, X., Xie, S., and He, K. An empirical study of training self-supervised Vision Transformers. In ICCV , 2021.                                                                                                                                                            | D., Swersky, K., Gal, Y., and Hinton, G. E. Learning sparse networks using targeted dropout. arXiv preprint arXiv:1905.13678 , 2019.                                                                                                  |
| Chen, X., Hsieh, C.-J., and Gong, B. When vision trans- formers outperform resnets without pre-training or strong data augmentations. In ICLR , 2022.                                                                                                                        | Goyal, P., Doll'ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv:1706.02677 , 2017.                                |
| Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artificial intelligence and statistics , pp. 215-223. JMLR Workshop and Conference Proceedings, 2011. | He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pool- ing in deep convolutional networks for visual recognition. In ECCV , 2014.                                                                                              |
| Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran- daugment: Practical automated data augmentation with a                                                                                                                                                                | He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR , 2016.                                                                                                                                 |
| reduced search space. In CVPR Workshops , 2020.                                                                                                                                                                                                                              | He, K., Gkioxari, G., Doll'ar, P., and Girshick, R. Mask R-CNN. In ICCV , 2017.                                                                                                                                                       |
| De Vries, T., Misra, I., Wang, C., and Van der Maaten, L. Does object recognition work for everyone? In CVPR Workshops , 2019.                                                                                                                                               | He, K., Chen, X., Xie, S., Li, Y., Doll'ar, P., and Girshick, R. Masked autoencoders are scalable vision learners. arXiv:2111.06377 , 2021.                                                                                           |
| Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In CVPR , 2009.                                                                                                                                      | Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. In ICLR , 2018.                                                                                                    |

| Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV , 2021a.        | Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the loss landscape of neural nets. In NeurIPS , 2018a.                                               |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. In CVPR , 2021b.                                                                                                                           | Li, Z., Peng, C., Yu, G., Zhang, X., Deng, Y., and Sun, J. DetNet: A backbone network for object detection. arXiv:1804.06215 , 2018b.                                      |
| Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. Improving neural net- works by preventing co-adaptation of feature detectors. arXiv:1207.0580 , 2012.                                               | Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C. Learning efficient convolutional networks through network slimming. In ICCV , 2017.                           |
| Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In ECCV , 2016.                                                                                                                         | Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , 2021.       |
| Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., and Geras, K. The break-even point on optimization trajectories of deep neural networks. In                                                                        | Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. In CVPR , 2022.                                                          |
| ICLR , 2020.                                                                                                                                                                                                                               | Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization. In ICLR , 2019.                                                                                     |
| Johnson, R. and Zhang, T. Accelerating stochastic gradient descent using predictive variance reduction. In NeurIPS , 2013.                                                                                                                 | MMSegmentation-contributors. MMSegmentation: Open- mmlab semantic segmentation toolbox and bench- mark. https://github.com/open-mmlab/                                     |
| Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., ˇ Z'ıdek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold. Nature , 596(7873):583-589, | mmsegmentation , 2020. Molchanov, D., Ashukha, A., and Vetrov, D. Variational dropout sparsifies deep neural networks. In ICML , 2017.                                     |
| 2021. Kavis, A., Skoulakis, S., Antonakopoulos, K., Dadi, L. T.,                                                                                                                                                                           | Morerio, P., Cavazza, J., Volpi, R., Vidal, R., and Murino, V. Curriculum dropout. In ICCV , 2017.                                                                         |
| and Cevher, V. Adaptive stochastic variance reduction for non-convex finite-sum minimization. arXiv preprint arXiv:2211.01851 , 2022.                                                                                                      | Mu, N., Kirillov, A., Wagner, D., and Xie, S. Slip: Self- supervision meets language-image pre-training. In ECCV , 2022.                                                   |
| Kingma, D. P., Salimans, T., and Welling, M. Varia- tional dropout and the local reparameterization trick. In NeurIPS , 2015.                                                                                                              | Nilsback, M.-E. and Zisserman, A. Automated flower classi- fication over a large number of classes. In Indian Confer- ence on Computer Vision, Graphics & Image Processing |
| Krizhevsky, A. Learning multiple layers of features from tiny images. Tech Report , 2009.                                                                                                                                                  | , 2008.                                                                                                                                                                    |
| Krizhevsky, A., Sutskever, I., and Hinton, G. Imagenet classification with deep convolutional neural networks. In NeurIPS , 2012.                                                                                                          | Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. In CVPR , 2012.                                                                                  |
| Kumar Singh, K. and Jae Lee, Y. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In ICCV , 2017.                                                                                    | Polyak, B. T. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization , 1992.                                  |
| Labach, A., Salehinejad, H., and Valaee, S. dropout methods for deep neural networks.                                                                                                                                                      | Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,                                                                                                                 |
| Survey of arXiv preprint arXiv:1904.13310 , 2019.                                                                                                                                                                                          | Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML , 2021.                 |
| Larsson, G., Maire, M., and Shakhnarovich, G. Fractal- net: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648 , 2016.                                                                                          | Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.      |

| Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet? In ICML , 2019.                                                                                                | Wan, L., Zeiler, M., Zhang, S., Cun, Y. L., and Fergus, R. Regularization of neural networks using dropconnect. In ICML , 2013.                                                                              |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Rennie, S. J., Goel, V., and Thomas, S. Annealed dropout training of deep networks. In 2014 IEEE Spoken Lan- guage Technology Workshop (SLT) , pp. 159-164. IEEE, 2014.                                             | Wang, H., Ge, S., Xing, E. P., and Lipton, Z. C. Learning ro- bust global representations by penalizing local predictive power. In NeurIPS , 2019. Wang, S. and Manning, C. Fast dropout training. In ICML , |
| Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In ICLR , 2015.                                                                                                  | 2013. Wightman, R. Pytorch image models. https://github.                                                                                                                                                     |
| Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine                                          | com/rwightman/pytorch-image-models , 2019.                                                                                                                                                                   |
| Learning Research , pp. 1929-1958, 2014.                                                                                                                                                                            | Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified perceptual parsing for scene understanding. In ECCV , 2018.                                                                                      |
| Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkor- eit, J., and Beyer, L. How to train your vit? data, augmen- tation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270 , 2021. | Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV , 2019.                                          |
| Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In CVPR , 2015.                                                   | Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. In ICLR , 2018.                                                                                           |
| Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In CVPR , 2016.                                                                         | Zhang, M., Lucas, J., Ba, J., and Hinton, G. E. Lookahead optimizer: k steps forward, 1 step back. NeurIPS , 2019. Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y. Ran-                                 |
| Tan, M. and Le, Q. Efficientnetv2: Smaller models and faster training. In ICML , 2021.                                                                                                                              | dom erasing data augmentation. In AAAI , 2020. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso,                                                                                                 |
| lasso. Journal of the Royal Statistical Society: Series B (Methodological) , 58(1):267-288, 1996. Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L.,                                                        | through the ADE20K dataset. IJCV , 2019. Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning transferable architectures for scalable image recognition. In CVPR , 2018.                              |
| Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architec- ture for vision. In NeurIPS , 2021. Tompson, J., Goroshin, R., Jain, A., LeCun, Y., and Bre-   |                                                                                                                                                                                                              |
| Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J'egou, H. Training data-efficient image transform- ers & distillation through attention. arXiv:2012.12877 , 2020.                               |                                                                                                                                                                                                              |
| Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and J'egou, H. Going deeper with image transformers. ICCV ,                                                                                                  |                                                                                                                                                                                                              |
| 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,                                                                                                                                                   |                                                                                                                                                                                                              |
| L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In NeurIPS , 2017.                                                                                                                      |                                                                                                                                                                                                              |

## G. Limitations

We show that early and late dropout can benefit the training of small and large networks in a range of supervised visual recognition tasks. However, the application of deep learning extends far beyond this, and further research is needed to determine the impact of early and late dropout on other areas, such as self-supervised pre-training or natural language processing. It would also be valuable to explore the interplay between early / late dropout and other factors such as training duration or optimizer choice.

Another intriguing behavior that our current analysis cannot fully explain is shown in the training curves in Figure 13. Early dropout does not result in a lower training loss during the early dropout phase, even though it eventually leads to a lower final loss. This observation holds true even when evaluating the training loss with dropout turned off. Therefore, it appears that early dropout and gradient error reduction enhance optimization not by accelerating the process, but possibly by finding a better local optimum. This behavior warrants further study for a deeper understanding.

## H. Societal Impact

The training and inference of deep neural networks can take an excessive amount of energy, especially in the large model and large data era. Our discovery on early dropout could spark more interest in developing training techniques for small models, which have far lower total energy usage and carbon emission than large models.

It is also important to note that the benchmark datasets used in this study were primarily designed for research purposes, and may contain certain biases (De Vries et al., 2019) and not accurately reflect the real-world distributions. Further research is needed to address these biases and develop training techniques that are robust to real-world data variability.

