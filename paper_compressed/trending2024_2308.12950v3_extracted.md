## Abstract

We release Code Llama , a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models ( Code Llama ), Python specializations ( Code Llama - Python ), and instruction-following models ( Code Llama - Instruct ) with 7B, 13B, 34B, and 70B parameters each. These models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. The 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use. 1

## 1 Introduction

Large language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency in natural language that allows them to be commanded and prompted to perform a variety of tasks (OpenAI, 2023; Touvron et al., 2023b). By utilizing large, in-domain datasets, their efficacy can be greatly improved for applications that require a combination of both natural and domain-specific language and understanding of specialized terminology. By training on domain-specific datasets, they have proved effective more broadly on applications that require advanced natural language understanding. A prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation (for a survey, see Xu & Zhu, 2022, also see Section 5). In this work, we present Code Llama , a family of LLMs for code generation and infilling derived from Llama 2 (Touvron et al., 2023b) and released under the same custom permissive license. We provide inference code for both completion and infilling models in the accompanying repository. 1 Our approach is based on gradually specializing and increasing the capabilities of Llama 2 models by applying a cascade of training and fine-tuning steps (Figure 2):

- · Code-training from foundation models. While most LLMs for code generation such as AlphaCode (Li et al., 2022), InCoder (Fried et al., 2023) or StarCoder (Li et al., 2023) are trained on code only, Codex (Chen et al., 2021) was fine-tuned from a general language model. We also start from a foundation model ( Llama 2 , Touvron et al., 2023b) pretrained on general-purpose text and code data. Our comparison (Section 3.4.1) shows that initializing our model with Llama 2 outperforms the same architecture trained on code only for a given budget.

Prompt: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month

Response: You can use the 'find' command in Bash to list all text files in the current directory (excluding subdirectories) that have been modified in the last month. The command would be:

find . -maxdepth 1 -type f -mtime -30 -name "*.txt"

Here's a breakdown of the command:

- * '.' specifies the current directory.
- * '-maxdepth 1' tells 'find' to only search the current directory and not any subdirectories.
- * '-type f' tells 'find' to only search for files (not directories or other types of files).
- * '-mtime -30' tells 'find' to only search for files that have been modified in the last 30 days.
- * '-name "*.txt"' tells 'find' to only search for files with the '.txt' extension.

Note that the '-mtime' option uses the number of days since the file was last modified, so '-mtime -30' means "modified in the last 30 days". If you want to search for files modified in the last month, you can use the '-mtime -30' option.

Figure 1: Example of response of Code Llama - Instruct (34B) when queried for a specific shell command.

- · Infilling. Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does not provide the capability to fill a missing portion of text while taking the full surrounding context into account. Our code-training for 7B, 13B and 70B Code Llama models features a multitask objective (Fried et al., 2023) consisting of both autoregressive and causal infilling prediction, enabling applications such as real-time completion in source code editors or docstring generation.
- · Long input contexts. Unlocking repository-level reasoning for completion or synthesis - as opposed to function-level or file-level - requires prompting the model with much longer context than the 4,096 tokens supported by Llama 2 . We propose an additional fine-tuning stage that extends the maximum context length from 4,096 tokens to 100,000 tokens by modifying the parameters of the RoPE positional embeddings (Su et al., 2021) used in Llama 2 . Our experiments show Code Llama operating on very large contexts with a moderate impact on performances on standard coding benchmarks (Section 3.3).
- · Instruction fine-tuning. For end-users, the utility of LLMs is significantly improved by instruction fine-tuning (Ouyang et al., 2022; Wei et al., 2022; OpenAI, 2023; Touvron et al., 2023b), which also helps preventing unsafe, toxic or biased generations. Code Llama - Instruct variants are further fine-tuned on a mix of proprietary instruction data for improved safety and helpfulness, and a new machine-generated self-instruct dataset created by prompting Llama 2 for coding problems and Code Llama to generate associated unit tests and solutions. Our results show that Code Llama - Instruct significantly improves performance on various truthfulness, toxicity and bias benchmarks at moderate cost in terms of code generation performance (Section 4).

Different combinations of these approaches lead to a family of code-specialized Llama 2 models with three main variants that we release in four sizes (7B, 13B, 34B and 70B parameters):

- · Code Llama : a foundational model for code generation tasks,
- · Code Llama - Python : specialized for Python,
- · Code Llama - Instruct : fine-tuned with human instructions and self-instruct code synthesis data.

An example of using Code Llama - Instruct is given in Figure 1. It show-cases that the model interprets natural language to determine suitable options for a command-line program and provides an explanation of the solution. We provide further qualitative examples in Appendix L. We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023), where our best models establish a new state of the art amongst open-source LLMs. The technical details of our training and fine-tuning procedures are provided in Section 2, followed by in-depth experiments and ablation studies, details of the safety/helpfulness evaluations and a discussion of related work.

<latexit sha1\_base64="xxjQ0qU69VzePnVfp3QwO6CZats=">AAAB/HicbVDLSsNAFJ3UV42vaJduBovgqiRS1GXRjcsK9gFtKJPppB06mYSZGyWU+ituXCji1g9x5984abPQ1gMDh3Pu4d45QSK4Btf9tkpr6xubW+Vte2d3b//AOTxq6zhVlLVoLGLVDYhmgkvWAg6CdRPFSBQI1gkmN7nfeWBK81jeQ5YwPyIjyUNOCRhp4FTsvuKjscmFQJSKH7VtD5yqW3PnwKvEK0gVFWgOnK/+MKZpxCRQQbTueW4C/pQo4FSwmd1PNUsInZAR6xkqScS0P50fP8OnRhniMFbmScBz9XdiSiKtsygwkxGBsV72cvE/r5dCeOVPuUxSYJIuFoWpwBDjvAk85IpREJkhhCpubsV0TBShYPrKS/CWv7xK2uc176JWv6tXG9dFHWV0jE7QGfLQJWqgW9RELURRhp7RK3qznqwX6936WIyWrCJTQX9gff4A8lSUUw==</latexit>

Figure 2: The Code Llama specialization pipeline . The different stages of fine-tuning annotated with the number of tokens seen during training. Infilling-capable models are marked with the ⇄ symbol.

<!-- image -->

## 2.2 Dataset

We train Code Llama 7B, 13B and 34B on 500B tokens, and Code Llama 70B on 1T tokens during the initial phase, starting from the 7B, 13B, 34B, and 70B versions of Llama 2 . As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of publicly available code. We also source 8% of our samples data from natural language datasets related to code. This dataset contains many discussions about code and code snippets included in natural language questions or answers. To help the model retain natural language understanding skills, we also sample a small proportion of our batches from a natural language dataset. Data is tokenized via byte pair encoding (BPE, Sennrich et al. (2016)), employing the same tokenizer as Llama and Llama 2 . Preliminary experiments suggested that adding batches sampled from our natural language dataset improves the performance of our models on MBPP.

## 2.6 Training details

Optimization. Our optimizer is AdamW (Loshchilov & Hutter, 2019) with β 1 and β 2 values of 0.9 and 0.95. We use a cosine schedule with 1000 warm-up steps, and set the final learning rate to be 1/30th of the peak learning rate. We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each. Despite the standard practice of using lower learning rates in fine-tuning stages than in pre-training stages,

Table 2: Code Llama pass@ scores on HumanEval and MBPP. The pass@1 scores of our models are computed with greedy decoding. The pass@10 and pass@100 scores are computed with nucleus sampling with p=0.95 and temperature 0.8 following our findings from Figure 6. Models are evaluated in zero-shot on Human Eval and 3-shot on MBPP. The instruct models are trained to be safe and aligned from the base Code Llama models. Results for other models as provided by Li et al. (2023) (code-cushman-001, StarCoder), OpenAI (2023) (GPT-3.5, GPT-4), and Chowdhery et al. (2022); Anil et al. (2023) (PaLM).

| Model                 | Size   | HumanEval pass@1 pass@10 pass@100 pass@1 pass@10 pass@100   | HumanEval pass@1 pass@10 pass@100 pass@1 pass@10 pass@100   | HumanEval pass@1 pass@10 pass@100 pass@1 pass@10 pass@100   | MBPP    | MBPP    | MBPP    |
|-----------------------|--------|-------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|---------|---------|---------|
| code-cushman-001      | 12B    | 33 . 5%                                                     | -                                                           | -                                                           | 45 . 9% | -       | -       |
| GPT-3.5 (ChatGPT)     | -      | 48 . 1%                                                     | -                                                           | -                                                           | 52 . 2% | -       | -       |
| GPT-4                 | -      | 67 . 0%                                                     | -                                                           | -                                                           | -       | -       | -       |
| PaLM                  | 540B   | 26 . 2%                                                     | -                                                           | -                                                           | 36 . 8% | -       | -       |
| PaLM-Coder            | 540B   | 35 . 9%                                                     | -                                                           | 88 . 4%                                                     | 47 . 0% | -       | -       |
| PaLM 2-S              | -      | 37 . 6%                                                     | -                                                           | 88 . 4%                                                     | 50 . 0% | -       | -       |
| StarCoder Base        | 15.5B  | 30 . 4%                                                     | -                                                           | -                                                           | 49 . 0% | -       | -       |
| StarCoder Python      | 15.5B  | 33 . 6%                                                     | -                                                           | -                                                           | 52 . 7% | -       | -       |
| StarCoder Prompted    | 15.5B  | 40 . 8%                                                     | -                                                           | -                                                           | 49 . 5% | -       | -       |
| Llama 2               | 7B 12  | . 2%                                                        | 25 . 2%                                                     | 44 . 4%                                                     | 20 . 8% | 41 . 8% | 65 . 5% |
| Llama 2               | 13B    | 20 . 1%                                                     | 34 . 8%                                                     | 61 . 2%                                                     | 27 . 6% | 48 . 1% | 69 . 5% |
| Llama 2               | 34B    | 22 . 6%                                                     | 47 . 0%                                                     | 79 . 5%                                                     | 33 . 8% | 56 . 9% | 77 . 6% |
| Code Llama            | 7B 33  | . 5%                                                        | 59 . 6%                                                     | 85 . 9%                                                     | 41 . 4% | 66 . 7% | 82 . 5% |
| Code Llama            | 13B    | 36 . 0%                                                     | 69 . 4%                                                     | 89 . 8%                                                     | 47 . 0% | 71 . 7% | 87 . 1% |
| Code Llama            | 34B    | 48 . 8%                                                     | 76 . 8%                                                     | 93 . 0%                                                     | 55 . 0% | 76 . 2% | 86 . 6% |
| Code Llama            | 70B    | 53 . 0%                                                     | 84 . 6%                                                     | 96 . 2%                                                     | 62 . 4% | 81 . 1% | 91 . 9% |
| Code Llama - Instruct | 7B 34  | . 8%                                                        | 64 . 3%                                                     | 88 . 1%                                                     | 44 . 4% | 65 . 4% | 76 . 8% |
| Code Llama - Instruct | 13B    | 42 . 7%                                                     | 71 . 6%                                                     | 91 . 6%                                                     | 49 . 4% | 71 . 2% | 84 . 1% |
| Code Llama - Instruct | 34B    | 41 . 5%                                                     | 77 . 2%                                                     | 93 . 5%                                                     | 57 . 0% | 74 . 6% | 85 . 4% |
| Code Llama - Instruct | 70B    | 67 . 8%                                                     | 90 . 3%                                                     | 97 . 3%                                                     | 62 . 2% | 79 . 6% | 89 . 2% |
| Unnatural Code Llama  | 34B    | 62 . 2%                                                     | 85 . 2%                                                     | 95 . 4%                                                     | 61 . 2% | 76 . 6% | 86 . 7% |
| Code Llama - Python   | 7B 38  | . 4%                                                        | 70 . 3%                                                     | 90 . 6%                                                     | 47 . 6% | 70 . 3% | 84 . 8% |
| Code Llama - Python   | 13B    | 43 . 3%                                                     | 77 . 4%                                                     | 94 . 1%                                                     | 49 . 0% | 74 . 0% | 87 . 6% |
| Code Llama - Python   | 34B    | 53 . 7%                                                     | 82 . 8%                                                     | 94 . 7%                                                     | 56 . 2% | 76 . 4% | 88 . 2% |
| Code Llama - Python   | 70B    | 57 . 3%                                                     | 89 . 3%                                                     | 98 . 4%                                                     | 65 . 6% | 81 . 5% | 91 . 9% |

we obtained best results when retaining the original learning rate of the Llama 2 base model. We carry these findings to the 13B, 34B and 70B models, and set their learning rates to 3 e -4 , 1 . 5 e -4 , and 1 . 5 e -4 respectively. For python fine-tuning, we set the initial learning rate to 1 e -4 instead. For Code Llama - Instruct , we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.

Long context fine-tuning. For long context fine-tuning (LCFT), we use a learning rate of 2 e -5 , a sequence length of 16,384, and reset RoPE frequencies with a base value of θ = 10 6 . The batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default. We observed instabilities in downstream performance for certain configurations, and hence set the number of gradient steps to 11,000 for the 34B models and to 3,000 for Code Llama 7B.

## 3.2 Infilling evaluations

Performance cost of infilling training. Previous studies on infilling (or fill-in-the-middle, FIM ) code models assert that the traditional next token prediction objective can be replaced by a multitask infilling objective with an infilling rate of up to 90 % at no cost for left-to-right autoregressive test losses (Bavarian et al., 2022) and only small cost for downstream evaluation performance (Allal et al., 2023). In Table 5, we independently validate both findings at the scale of 7B and 13B parameters and 500B training tokens of code. The 7B model loses 0.6 percentage points on average across HumanEval and MBPP pass@1, pass@10 and pass@100 scores if trained with an infilling objective, while the 13B model loses 1.1 percentage points.

Figure 3: Correlations between Languages. Correlation scores between the Python, C++, Java, PHP, C#, TypeScript (TS), and Bash, reported for different model sizes. The code for this figure was generated by Code Llama - Instruct , the prompt and code can be seen in Figure 22.

<!-- image -->

Because of this modest decline in performance and the wide applicability of models with infilling capability, we decide to release Code Llama 7B, 13B and 70B in this configuration.

Code infilling benchmarks. Our infilling models reach state-of-the-art performances in code infilling benchmarks among models of their size. We evaluate on two related code infilling benchmarks based on the HumanEval benchmark (Chen et al., 2021).

The HumanEval infilling benchmark (Fried et al., 2023) turns the reference solutions of the HumanEval benchmark (Chen et al., 2021) into infilling problems by masking out either individual lines or blocks consisting of multiple consecutive lines. It has been extended in Bavarian et al. (2022) with a random span infilling task in which the masking is applied to a randomly selected substring at the character level. Predictions are scored with a pass@1 score based on the test cases of the original HumanEval problems. According to the results in Table 14, our models outperform all other infilling models of their size. Note, however, that the results in random span infilling are significantly worse in suffix-prefix-middle (SPM) format than in prefix-suffix-middle (PSM) format as it would require token healing (Microsoft, 2023), which we have not implemented for this evaluation (see Appendix E for further discussion).

Allal et al. (2023) translates the HumanEval infilling benchmark to other programming languages using MultiPL-E (Cassano et al., 2023). Single lines are masked and predictions are scored with an exact match metric against the ground truth solution. Our models, including Code Llama 7B, outperform all open infilling models across the three programming languages contained in the benchmark (Table 6). We observe a further increase in performance when prompting the models in SPM format, like witnessed in Bavarian et al. (2022).

## 3.3 Long context evaluations

We explore Code Llama 's ability to work with long sequences by measuring perplexity, key retrieval accuracy and performance during generation on code completion tasks. These tasks, and our results are detailed below. For full results and comparisons to alternative techniques of increasing the context length of LLMs, we refer to Appendix G.

Perplexity during extrapolation. In Figure 4a, perplexity is computed over 4M tokens from the code dataset, using a subset of our validation data consisting of large source files ( ≥ 50kB). For all model sizes, we observe a steady decrease in perplexity well beyond 16384 tokens, which is the sequence length we use for long-context fine-tuning. After 100K tokens, the perplexity increases only slightly, in contrast to the well-known instability phenomenon when testing transformer models on sequences larger than those seen during training (Press et al., 2022).

Table 5: Comparison of models with and without FIM training. pass@1, pass@10 and pass@100 scores on HumanEval and MBPP evaluated at temperature 0.1 for models trained with and without infilling (FIM) objective. Infilling training incurs no cost on autoregressive test set loss, but a small cost on HumanEval and MBPP pass@k metrics that is aggravated at higher sample counts k . The models are compared prior to long context fine-tuning (LCFT).

| Model                 |       | FIM Size   | HumanEval   | HumanEval   | HumanEval                                       | MBPP    | MBPP    | Test loss   | Test loss   |
|-----------------------|-------|------------|-------------|-------------|-------------------------------------------------|---------|---------|-------------|-------------|
|                       |       |            |             |             | pass@1 pass@10 pass@100 pass@1 pass@10 pass@100 |         |         |             |             |
| Code Llama (w/o LCFT) | ✗     | 7B         | 33 . 2%     | 43 . 3%     | 49 . 9%                                         | 44 . 8% | 52 . 5% | 57 . 1%     | 0 . 408     |
| Code Llama (w/o LCFT) | ✗     | 13B        | 36 . 8%     | 49 . 2%     | 57 . 9%                                         | 48 . 2% | 57 . 4% | 61 . 6%     | 0 . 372     |
| Code Llama (w/o LCFT) | ✓     | 7B         | 33 . 6%     | 44 . 0%     | 48 . 8%                                         | 44 . 2% | 51 . 4% | 55 . 5%     | 0 . 407     |
| Code Llama (w/o LCFT) | ✓     | 13B        | 36 . 2%     | 48 . 3%     | 54 . 6%                                         | 48 . 0% | 56 . 8% | 60 . 8%     | 0 . 373     |
| Absolute gap          | ✗ - ✓ | 7B         | - 0 . 4%    | - 0 . 7%    | 1 . 1%                                          | 0 . 6%  | 1 . 1%  | 1 . 6%      | 0 . 001     |
| Absolute gap          | ✗ - ✓ | 13B        | 0 . 7%      | 0 . 9%      | 3 . 3%                                          | 0 . 2%  | 0 . 6%  | 0 . 8%      | - 0 . 001   |

Table 6: Multilingual HumanEval single line infilling with MultiPL-E. Exact match rates on the line infilling benchmark from Allal et al. (2023) with greedy decoding. Evaluated in both prefix-suffix-middle (PSM) and suffix-prefix-middle (SPM) format. Numbers for InCoder, SantaCoder and StarCoder are reported from Li et al. (2023).

| Model      | Size   | Python     | Python     | Java    | Java    | JavaScript   | JavaScript   |
|------------|--------|------------|------------|---------|---------|--------------|--------------|
|            |        | PSM        | SPM        | PSM     | SPM     | PSM          | SPM          |
| InCoder    | 6B     |            | 31 . 0%    |         | 49 . 0% |              | 51 . 0%      |
| SantaCoder | 1.1B   |            | 44 . 0%    |         | 62 . 0% |              | 60 . 0%      |
| StarCoder  | 15.5B  |            | 62 . 0%    |         | 73 . 0% |              | 74 . 0%      |
| Code Llama | 7B 67  | . 6%       | 72 . 7% 74 | . 3% 77 | . 6% 80 | . 2% 82      | . 6%         |
| Code Llama | 13B    | 68 . 3% 74 | . 5% 77    | . 6% 80 | . 0% 80 | . 7% 85      | . 0%         |

Key retrieval. In Figure 4b, we investigate key retrieval performance in synthetic task. The prompt consists of a large amount of syntactically valid Python code, with a function returning a scalar inserted at a specified position. The model is asked to complete an assert statement with the return value of the inserted function. Liu et al. (2023b) showed that the inability to recall content placed in the middle of long prompts is a common failure mode in LLMs; our retrieval task is analogous to their setup, albeit tailored to code models which are not fine-tuned to follow instructions. All models exhibit strong retrieval performance on the sequence length they were trained on, with the exception of the 7B model for test cases in which the function is placed at the beginning of the prompt. We include OpenAI's gpt-3.5-turbo-16k-0613 as a reference. We query GPT with a system prompt of 'Complete the following code.' and a temperature of 0. For sequences beyond 16K tokens, i.e., when extrapolating, our models exhibit a decrease in performance (Appendix G.3).

Single line completion. Finally, we test the benefits of the ability to handle long context sizes in a single line code completion task. Our task is based on the Long Code Completion (LCC) benchmark (Guo et al., 2023). 2 The LCC test set is skewed towards shorter files and we hence sample a new set of examples from LCC's validation and test set with an equalized distribution over file size (Appendix G.2). In Table 7, we compare the completion accuracy of the Code Llama models to their counterparts prior to long-context fine-tuning. Non-LCFT models fail to generate meaningful completions on long sequences and we thus truncate their prompts to the 4,000 tokens immediate preceding the line to complete. Across all metrics, models fine-tuned to handle long contexts achieve significantly higher performance. This demonstrates that long contexts are informative for code completion, and that with LCFT our models are able to leverage this information to improve their generations. We note that the longest example's prompt in this test consists

<!-- image -->

Figure 4: Code Llama behavior on long sequences. (a) Perplexity on large source files ( ≥ 50 kB) from the validation data from the code dataset. The dashed line marks the fine-tuning context length. Perplexity decreases for up to 100K tokens for all Code Llama sizes. (b) Accuracy on a synthetic key retrieval task, with a context of 16K tokens and comparison to gpt-3.5-turbo.

<!-- image -->

Table 7: Average single line completion performance on LCC-balanced. Comparison of models before and after long-context fine-tuning in terms of exact match (EM) and BLEU. For non-LCFT models, context size limits are respected by truncating prompts to 4,000 tokens.

| Model      |     |    |    EM |   BLEU |    EM |   BLEU |    EM |   BLEU |
|------------|-----|----|-------|--------|-------|--------|-------|--------|
| Code Llama | 7B  | ✗  | 36.86 |  60.16 | 47.82 |  69.2  | 46.29 |  67.75 |
| Code Llama | 7B  | ✓  | 39.23 |  61.84 | 51.94 |  71.89 | 50.2  |  70.22 |
| Code Llama | 13B | ✗  | 37.96 |  61.33 | 50.49 |  69.99 | 49.22 |  69.87 |
| Code Llama | 13B | ✓  | 41.06 |  62.76 | 52.67 |  72.29 | 52.15 |  71    |
| Code Llama | 34B | ✗  | 42.52 |  63.74 | 54.13 |  72.38 | 52.34 |  71.36 |
| Code Llama | 34B | ✓  | 44.89 |  65.99 | 56.8  |  73.79 | 53.71 |  72.69 |

of 103K tokens, for which all Code Llama models generate syntactically correct completions, with the 7B model producing an exact match.

Performance impact on short contexts. While our models are effective on long sequences, we observe that LCFT slightly hurts performance on standard code synthesis benchmarks consisting of short sequences. In Table 10, we observe an average decrease of 0.52 percentage points on HumanEval pass@1 and 1.9 points on MBPP for the pass@1 metric. Similarly, a breakdown of the code completion results in Table 7 by the number of tokens in each example shows that for prompts shorter than 4k tokens, long context fine-tuning induces a reduction of up to 2 BLEU points from base models after code training (Figure 9b). We observe similar decreases in performance for infilling tasks (Table 14).

LCFT comes at a cost for short sequences, and slightly decreases our scores on standard coding benchmarks such as HumanEval and MBPP. However, many real-world use cases are not captured by these benchmarks, and we believe that this cost is more than offset by the potential of handling long sequences for real downstream applications. Hence we opt to release all our Code Llama , Code Llama - Python and Code Llama - Instruct models with long-context capabilities.

<!-- image -->

<!-- image -->

Figure 5: (a) Training perplexity of Code Llama models. The continued decrease at 500B tokens suggests further training would be beneficial. Results are presented without infilling for 7B and 13B models. (b) Training losses of both Code Llama 7B versus an identical model trained from scratch (c) MBPP (coding benchmark) vs. Helpfulness according to the helpfulness reward model from Llama 2 (Touvron et al., 2023b).

<!-- image -->

## 3.4 Ablation studies



## 6 Discussion

We release a family of code-specialized Llama 2 models called Code Llama , with three main variants that we release with four sizes (7B, 13B, 34B, and 70B parameters): Code Llama , Code Llama - Python , Code Llama - Instruct . With real-world applications in mind, we trained our 7B, 13B, and 70B models to support infilling, and all our models to leverage large contexts. We tested their stability in inference up to 100K tokens (Figure 4a). Large context fine-tuning and infilling come at a cost on standard benchmarks left-to-right code generation benchmarks (Table 10), that are all based on short sequences (i.e. function level). Still, our 70B model is state-of-the-art among public models on standard python completion benchmarks, and our other models are competitive compared to models with similar numbers of parameters. On multilingual benchmarks, even our smallest model ( Code Llama 7B) outperforms every other public model.

The Code Llama - Instruct models are trained to provide zero-shot instruction ability to Code Llama . In this further fine-tuning, where we somewhat distillate Llama 2 -Chat, we focused not only on being more directly helpful (Figure 5c) but also sought to provide a safer model to use and deploy (Section 4). Following instruction and being overly safe can cost some points on evaluations (e.g. on HumanEval for the 34B model in Table 2), as exemplified in Figure 15. Further work is needed for LLMs to understand context and nuance in their instructions.

## G.1 Further Discussion

For illustrating the effect of increasing the base period of rotary position embeddings, we plot expectations for attention scores when varying the distance between key and query vectors in Figure 9a. Compared to the default base period of 10,000, θ = 1 , 000 , 000 reduces the decay in attention scores, which helps far-away tokens contribute to the current prediction. Notably, this change in rotation frequencies can be applied to pretrained models, with loss curves stabilizing within a few gradient steps at a low learning rate. While the uniform frequency scaling proposed by Chen et al. (2023b) is motivated by maintaining the overall range of rotations when increasing the context from the sequence length used for pretraining, our modification explicitly addresses the problem of performing attention over long distances.

## G.4 Ablations

In Table 18, we report key-retrieval accuracy for ablations performed on an earlier version of our 7B model. Without long context fine-tuning, retrieval is possible on sequence lengths seen during training only (4,096); increasing RoPE's base period θ for inference only has no effect here. Performing LCFT without changing the base period results in failure to retrieve far-away keys at a context length of 8,000 already, despite fine-tuning with a 16,384 sequence length. This failure suggests that adapting the rotation frequencies is indeed necessary. We evaluate frequency scaling with a factor of 1/4 (Chen et al., 2023b), corresponding to the 4x increase of sequence length during fine-tuning. Retrieval performance at 16,00 tokens for keys placed at the beginning is low in this configuration, and extrapolation to longer sequences fails.

