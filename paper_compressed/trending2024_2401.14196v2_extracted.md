## Abstract

The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.

Figure 1 | The Performance of DeepSeek-Coder

<!-- image -->

## 1. Introduction

The field of software development has been significantly transformed by the swift advancement of large language models (OpenAI, 2023; Touvron et al., 2023), which have brought about a new era of code intelligence. These models have the potential to automate and streamline many aspects of coding, from bug detection to code generation, thereby enhancing productivity and reducing the likelihood of human error. However, a major challenge in this field is the performance gap between open-source models (Li et al., 2023; Nijkamp et al., 2022; Roziere et al., 2023; Wang et al., 2021) and closed-source models (Gemini Team, 2023; OpenAI, 2023). The giant closed-source models, while powerful, are often inaccessible to many researchers and developers due to their proprietary nature.

In response to this challenge, we present the DeepSeek-Coder series. This series comprises a range of open-source code models, varying in size from 1.3B to 33B, including the base version and instructed version for each size. Each model in the series has been trained from scratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive understanding of coding languages and syntax. Besides, we attempt to organize the pretraining data at the repository level to enhance the pre-trained model's understanding capability within the context of cross-files within a repository. In addition to employing the next token prediction loss during pre-training, we have also incorporated the Fill-In-Middle (FIM) approach (Bavarian et al., 2022; Li et al., 2023). This approach is designed to further bolster the model's code completion capabilities. To meet the requirements of handling longer code inputs, we have extended the context length to 16K. This adjustment allows our models to handle more complex and extensive coding tasks, thereby increasing their versatility and applicability in various coding scenarios.

We have carried out comprehensive experiments using a variety of public code-related benchmarks. The findings reveal that among open-source models, DeepSeek-Coder-Base 33B consistently delivers superior performance across all benchmarks. Furthermore, DeepSeekCoder-Instruct 33B surpasses OpenAI GPT-3.5 Turbo in the majority of the evaluation benchmarks, significantly narrowing the performance gap between OpenAI GPT-4 and open-source models. Remarkably, despite having fewer parameters, DeepSeek-Coder-Base 7B demonstrates competitive performance when compared to models that are five times larger, such as CodeLlama-33B (Roziere et al., 2023). To summarize, our main contributions are:

- · We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced codefocused large language models (LLMs). Developed through extensive training on an expansive code corpus, these models exhibit proficiency in understanding 87 programming languages. Additionally, they are available in various model scales to cater to a wide range of computational and application needs.
- · We make the first attempt to incorporate repository-level data construction during the pre-training phase of our models. We find that it can significantly boost the capability of cross-file code generation.
- · Our analysis rigorously examines the impact of FIM training strategies on the pretraining phase of code models. The outcomes of these comprehensive studies shed light on intriguing aspects of FIM configurations, offering valuable insights that significantly contribute to the enhancement and development of code pretrained models.
- · We conduct extensive evaluations of our code LLMs against a wide array of benchmarks encompassing numerous code-related tasks. The findings demonstrate that DeepSeek-CoderBase surpasses all existing open-source code LLMs across these benchmarks. Furthermore,

with meticulous fine-tuning using instructional data, DeepSeek-Coder-Instruct achieves better performance compared to the OpenAI GPT-3.5 Turbo model in code-related tasks.

## Algorithm 1 Topological Sort for Dependency Analysis

```
1: procedure TOPOLOGICALSORT( 𝑓 𝑖𝑙𝑒𝑠 ) 2: 𝑔𝑟𝑎𝑝ℎ𝑠 ←{} ⊲ Initialize an empty adjacency list 3: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 ←{} ⊲ Initialize an empty dictionary for in-degrees 4: for each 𝑓 𝑖𝑙𝑒 in 𝑓 𝑖𝑙𝑒𝑠 do 5: 𝑔𝑟𝑎𝑝ℎ𝑠 [ 𝑓 𝑖𝑙𝑒 ] ← [] 6: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 [ 𝑓 𝑖𝑙𝑒 ] ← 0 7: end for 8: 9: for each 𝑓 𝑖𝑙𝑒𝐴 in 𝑓 𝑖𝑙𝑒𝑠 do 10: for each 𝑓 𝑖𝑙𝑒𝐵 in 𝑓 𝑖𝑙𝑒𝑠 do 11: if HASDEPENDENCY( 𝑓 𝑖𝑙𝑒𝐴 , 𝑓 𝑖𝑙𝑒𝐵 ) then ⊲ If fileA depends on fileB 12: 𝑔𝑟𝑎𝑝ℎ𝑠 [ 𝑓 𝑖𝑙𝑒𝐵 ] .append ( 𝑓 𝑖𝑙𝑒𝐴 ) ⊲ Add edge from B to A 13: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 [ 𝑓 𝑖𝑙𝑒𝐴 ] ← 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 [ 𝑓 𝑖𝑙𝑒𝐴 ] + 1 ⊲ Increment in-degree of A 14: end if 15: end for 16: end for 17: 18: 𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ𝑠 ← getDisconnectedSubgraphs ( 𝑔𝑟𝑎𝑝ℎ𝑠 ) ⊲ Identify disconnected subgraphs 19: 𝑎𝑙𝑙𝑅𝑒𝑠𝑢𝑙𝑡𝑠 ← [] 20: for each 𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ in 𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ𝑠 do 21: 𝑟𝑒𝑠𝑢𝑙𝑡𝑠 ← [] 22: while length ( 𝑟𝑒𝑠𝑢𝑙𝑡𝑠 ) ≠ NumberOfNodes ( 𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ ) do 23: 𝑓 𝑖𝑙𝑒 ← argmin ({ 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 [ 𝑓 𝑖𝑙𝑒 ] | 𝑓 𝑖𝑙𝑒 ∈ 𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ and 𝑓 𝑖𝑙𝑒 ∉ 𝑟𝑒𝑠𝑢𝑙𝑡𝑠 }) 24: for each 𝑛𝑜𝑑𝑒 in 𝑔𝑟𝑎𝑝ℎ𝑠 [ 𝑓 𝑖𝑙𝑒 ] do 25: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 [ 𝑛𝑜𝑑𝑒 ] ← 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 [ 𝑛𝑜𝑑𝑒 ] -1 26: end for 27: 𝑟𝑒𝑠𝑢𝑙𝑡𝑠 .append ( 𝑓 𝑖𝑙𝑒 ) 28: end while 29: 𝑎𝑙𝑙𝑅𝑒𝑠𝑢𝑙𝑡𝑠 .append ( 𝑟𝑒𝑠𝑢𝑙𝑡𝑠 ) 30: end for 31: 32: return 𝑎𝑙𝑙𝑅𝑒𝑠𝑢𝑙𝑡𝑠 33: end procedure
```

will consider how to leverage the dependencies between files within the same repository in this step. Specifically, we first parse the dependencies between files and then arrange these files in an order that ensures the context each file relies on is placed before that file in the input sequence. By aligning the files in accordance with their dependencies, our dataset more accurately represents real coding practices and structures. This enhanced alignment not only makes our dataset more relevant but also potentially increases the practicality and applicability of the model in handling project-level code scenarios. It's worth noting that we only consider the invocation relationships between files and use regular expressions to extract them, such as "import" in Python, "using" in C#, and "include" in C.

The algorithm 1 describes a topological sort for dependency analysis on a list of files within the same project. Initially, it sets up two data structures: an empty adjacency list named "graphs" to represent dependencies between files and an empty dictionary called "inDegree" for storing the in-degrees of each file. The algorithm then iterates over each file pair to identify depen-

dencies, updating "graphs" and "inDegree" accordingly. Next, it identifies any disconnected subgraphs within the overall dependency graph. For each subgraph, the algorithm employs a modified topological sort. Unlike the standard approach that selects nodes with zero in-degrees, this algorithm selects nodes with minimal in-degrees, which allows it to handle cycles within the graph. Selected nodes are added to a "results" list, and the in-degrees of their connected nodes are decreased. This process continues until a topologically sorted sequence is generated for each subgraph. The algorithm concludes by returning a list of these sorted sequences, and each sequence's files are concatenated to form a single training sample. To incorporate file path information, a comment indicating the file's path is added at the beginning of each file. This method ensures that the path information is preserved in the training data.

## 3.3. Model Architecture

Wedevelop a range of models with varying parameters to cater to diverse applications, including models with 1.3B, 6.7B, and 33B parameters. These models are built upon the same framework as the DeepSeek Large Language Model (LLM) outlined by DeepSeek-AI (2024). Each model is a decoder-only Transformer, incorporating Rotary Position Embedding (RoPE) as described by Su et al. (2023). Notably, the DeepSeek 33B model integrates Grouped-Query-Attention (GQA) with a group size of 8, enhancing both training and inference efficiency. Additionally, we employ FlashAttention v2 (Dao, 2023) to expedite the computation involved in the attention mechanism. The architectural details of our models are summarized in Table 2.

## 6. Conclusion

In this technical report, we introduce a series of specialized Large Language Models (LLMs) for coding, named DeepSeek-Coder, available in three distinct scales: 1.3B, 6.7B, and 33B parameters. These models are uniquely trained on a meticulously curated project-level code corpus, utilizing a "fill-in-the-blank" pre-training objective to enhance code infilling capabilities. A significant advancement is the extension of the models' context window to 16,384 tokens, thereby greatly improving their effectiveness in handling extensive code generation tasks. Our evaluations reveal that the most advanced model in our series, DeepSeek-Coder-Base 33B surpasses existing open-source code models across a variety of standard tests. Impressively, the DeepSeek-CoderBase 6.7B model, despite its smaller scale, delivers performance on par with the 34B parameter CodeLlama, a testament to the high quality of our pretraining corpus.

To augment the zero-shot instruction capabilities of the DeepSeek-Coder-Base models, we have fine-tuned them with high-quality instructional data. This has led to the DeepSeek-CoderInstruct 33B model outperforming OpenAI's GPT-3.5 Turbo in a range of coding-related tasks, showcasing its exceptional proficiency in code generation and understanding.

To further improve the natural language understanding capabilities of the DeepSeek-CoderBase models, we have conducted additional pretraining based on the DeepSeek-LLM 7B checkpoint. This additional training involved processing a diverse dataset comprising 2 billion tokens, including natural language, code, and mathematical data. The result is the creation of a new

and improved code model, DeepSeek-Coder-v1.5. Our observations indicate that DeepSeekCoder-v1.5 not only maintains its predecessor's high-level coding performance but also exhibits enhanced natural language comprehension. This advancement underscores our belief that the most effective code-focused Large Language Models (LLMs) are those built upon robust general LLMs. The reason is evident: to effectively interpret and execute coding tasks, these models must also possess a deep understanding of human instructions, which often come in various forms of natural language. Looking ahead, our commitment is to develop and openly share even more powerful code-focused LLMs based on larger-scale general LLMs.

