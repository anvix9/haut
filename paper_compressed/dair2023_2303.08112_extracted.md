## Abstract

We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the tuned lens , is a refinement of the earlier 'logit lens' technique, which yielded useful insights but is often brittle.

We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/ AlignmentResearch/tuned-lens .

## 1. Introduction

The impressive performance of transformers in natural language processing (Brown et al., 2020) and computer vision (Dosovitskiy et al., 2020) suggests that their internal representations have rich structure worthy of scientific investigation. One common approach is to train classifiers to extract specific concepts from hidden states, like part-of-speech and syntactic structure (Hewitt and Manning, 2019; Tucker et al., 2021; Li et al., 2022).

In this work, we instead examine transformer representations from the perspective of iterative inference (Jastrz˛ebski et al., 2017). Specifically, we view each layer in a transformer language model as performing an incremental update to a latent prediction of the next token. 1 We decode these latent predictions through early exiting, converting the hidden

Logit Lens (theirs)

Figure 1. Comparison of our method, the tuned lens (bottom), with the 'logit lens' (top) for GPT-Neo-2.7B prompted with an except from the abstract of Vaswani et al. (2017). Each cell shows the top-1 token predicted by the model at the given layer and token index. The logit lens fails to elicit interpretable predictions before layer 21, but our method succeeds.

<!-- image -->

state at each intermediate layer into a distribution over the vocabulary. This yields a sequence of distributions we call the prediction trajectory , which exhibits a strong tendency to converge smoothly to the final output distribution, with each successive layer achieving lower perplexity.

We build on the 'logit lens' (nostalgebraist, 2020), an early exiting technique that directly decodes hidden states into vocabulary space using the model's pretrained unembedding matrix. We find the logit lens to be unreliable (Section 2), failing to elicit plausible predictions for models like BLOOM (Scao et al., 2022) and GPT Neo (Black et al., 2021). Even when the logit lens appears to work, its outputs are hard to interpret due to representational drift : features

r

e

y

a

L

may be represented differently at different layers of the network. Other early exiting procedures also exist (Schuster et al., 2022), but require modifying the training process, and so can't be used to analyze pretrained models. Simultaneous to this work is (Din et al., 2023) which proposes a relatively similar methodology which we will compare with in future work.

To address the shortcomings of the logit lens, we introduce the tuned lens . We train L affine transformations, one for each layer of the network, with a distillation loss: transform the hidden state so that its image under the unembedding matches the final layer logits as closely as possible. We call these transformations translators because they 'translate' representations from the basis used at one layer of the network to the basis expected at the final layer. Composing a translator with the pretrained unembedding yields a probe (Alain and Bengio, 2016) that maps a hidden state to a distribution over the vocabulary.

We find that tuned lens predictions have substantially lower perplexity than logit lens predictions, and are more representative of the final layer distribution. We also show that the features most influential on the tuned lens output are also influential on the model itself (Section 4). To do so, we introduce a novel algorithm called causal basis extraction (CBE) and use it to locate the directions in the residual stream with the highest influence on the tuned lens. We then ablate these directions in the corresponding model hidden states, and find that these features tend to be disproportionately influential on the model output.

We use the tuned lens to gain qualitative insight into the computational process of pretrained language models, by examining how their latent predictions evolve during a forward pass (Figure 1, Appendix B)

Finally, we apply the tuned lens in several ways: we extend the results of Halawi et al. (2023) to new models (Section 5.1), we find that tuned lens prediction trajectories can be used to detect prompt injection attacks (Perez and Ribeiro, 2022) often with near-perfect accuracy (Section 5.2), and find that data points which require many training steps to learn also tend to be classified in later layers (Section 5.3).

## 6. Discussion

In this paper, we introduced a new tool for transformer interpretability research, the tuned lens , which yields new qualitative as well as quantitative insights into the functioning of large language models. It is a drop-in replacement for the logit lens that makes it possible to elicit interpretable pre-

Table 2. Correlation between two measures of example difficulty, the iteration learned and prediction depth, across tasks. Prediction depth is measured using the tuned lens in the first column and the logit lens in the second.

| Task          | Tuned lens ρ   | Logit lens ρ   | Final acc    |
|---------------|----------------|----------------|--------------|
| ARC-Easy      | 0 . 577        | 0 . 500        | 69.7%        |
| ARC-Challenge | 0 . 547        | 0 . 485        | 32 . 4%      |
| LogiQA        | 0 . 498        | 0 . 277        | 21 . 4%      |
| MNLI          | 0 . 395        | 0 . 435        | 40 . 4%      |
| PiQA          | 0 . 660        | 0 . 620        | 76 . 1%      |
| QNLI          | 0 . 409        | - 0 . 099      | 53 . 0%      |
| QQP           | 0 . 585        | - 0 . 340      | 0 . 381 (F1) |
| RTE           | 0 . 156        | 0 . 347        | 60 . 0%      |
| SciQ          | 0 . 530        | 0 . 505        | 91 . 9%      |
| SST-2         | 0 . 555        | 0.292          | 64 . 7%      |
| WinoGrande    | 0 . 517        | 0 . 537        | 63 . 9%      |

diction trajectories from essentially any pretrained language model in use today. We gave several initial applications of the tuned lens, including detecting prompt injection attacks.

Finally, we introduced causal basis extraction , which identifies influential features in neural networks. We hope this technique will be generally useful for interpretability research in machine learning.

Limitations and future work. One limitation of our method is that it involves training a translator layer for each layer of the network, while the logit lens can be used on any pretrained model out-of-the-box. This training process, however, is quite fast: our code can train a full set of probes in under an hour on a single 8 × A40 node, and further speedups are likely possible. We have also released tuned lens checkpoints for the most commonly used pretrained models as part of our tuned-lens library, which should eliminate this problem for most applications.

Causal basis extraction, as presented in this work, is computationally intensive, since it sequentially optimizes d model

Figure 11. Token-level prediction depths for Pythia 12B computed on the abstract of OpenAI (2023). Warm colors have high prediction depth, while cool colors indicate low depth.

<!-- image -->

causal basis vectors for each layer of the network. Future work could explore ways to make the algorithm more scalable. One possibility would be to optimize a whole k -dimensional subspace, instead of an individual direction, at each iteration.

Due to space and time limitations, we focused on language models in this work, but we think it's likely that our approach is also applicable to other modalities.

## C. Transformers Perform Iterative Inference

Jastrz˛ebski et al. (2017) argue that skip connections encourage neural networks to perform 'iterative inference' in the following sense: each layer reliably updates the hidden state in a direction of decreasing loss. While their analysis focused on ResNet image classifiers, their theoretical argument applies equally well to transformers, or any neural network featuring residual connections. We reproduce their theoretical analysis below.

A residual block applied to a representation h i updates the representation as follows:

h ℓ +1 = h ℓ + F ℓ ( h ℓ )

Let L denote the final linear classifier followed by a loss function. We can Taylor expand L ( h L ) around h i to yield

L ( h L ) = L ( h i ) + L ∑ j = i 〈 F j ( h j ) , ∂ L ( h j ) ∂ h j 〉 ︸ ︷︷ ︸ gradient-residual alignment + O ( F 2 j ( h j )) (19)

Thus to a first-order approximation, the model is encouraged to minimize the inner product between the residual F ( h i ) and the gradient ∂ L ( h i ) ∂ h i , which can be achieved by aligning it with the negative gradient.

To empirically measure the extent to which residuals do align with the negative gradient, Jastrz˛ebski et al. (2017) compute the cosine similarity between F ( h i ) and ∂ L ( h i ) ∂ h i at each layer of a ResNet image classifier. They find that it is consistently negative, especially in the final stage of the network.

We reproduced their experiment using Pythia 6.9B, and report the results in Figure 19. We show that, for every layer in the network, the cosine similarity between the residual and the gradient is negative at least 95% of the time. While the magnitudes of the cosine similarities are relatively small in absolute terms, never exceeding 0.05, we show that they are much larger than would be expected of random vectors in this very high dimensional space. Specifically, we first sample 250 random Gaussian vectors of the same dimensionality as ∂ L ( h i ) ∂ h i , which is (hidden size) × (sequence length) = 8 , 388 , 608 . We then compute pairwise cosine similarities between the vectors, and find the 5 th percentile of this sample to be -6 × 10 -4 . Virtually all of the gradient-residual pairs we observed had cosine similarities below this number.

<!-- image -->

Figure 19. Left: Cosine similarity between the observed update to the hidden state and ∂ L ( h i ) ∂ h i . The similarity is almost always negative, and of much larger magnitude than would be expected by chance in this very high dimensional space. Boxplot whiskers are 5th and 95th percentiles. Results were computed over a sample of roughly 16M tokens from the Pile validation set. Right: Cross-entropy loss of Pythia 6.9B on the Pile validation set, after replacing each of its 32 layers with the identity transformation. The dashed line indicates Pythia's perplexity on this dataset with all layers intact.

<!-- image -->

## D. Causal intervention details



## E. Static interpretability analysis

Several interpretability methods aim to analyze parameters that 'write" (in the sense of Elhage et al. (2021)) to intermediate hidden states of a language model (Millidge and Black, 2022; Dar et al., 2022; Geva et al., 2022; 2020), and even use this analysis to successfully edit model behavior (Geva et al., 2022; Millidge and Black, 2022; Dai et al., 2022). Given that the tuned lens aims to provide a 'less biased" view of intermediate hidden states, we should expect the tuned lens to preserve their effectiveness. We test both static analysis and model editing methods, and confirm this hypothesis. With static analysis, the tuned lens appears to never decrease performance, and for some models, even increases their performance. With model editing, we found the tuned lens to outperform the logit lens on OPT-125m and perform equivalently on other models for the task of toxicity reduction.

## E.1. Static Analysis

Many parameters in transformer language models have at least one dimension equal to that of their hidden state, allowing us to multiply by the unembedding matrix to project them into token space. Surprisingly, the resulting top k tokens are often meaningful , representing interpretable semantic and syntactic clusters, such as 'prepositions' or 'countries' (Millidge and Black, 2022; Dar et al., 2022; Geva et al., 2022). This has successfully been applied to the columns of the MLP output matrices (Geva et al., 2022), the singular vectors of the MLP input and output matrices (Millidge and Black, 2022), and the singular vectors of the attention output-value matrices W OV (Millidge and Black, 2022).

In short 7 , we can explain this as: many model parameters directly modify the model's hidden state (Elhage et al., 2021), and

Figure 20. Causal fidelity in Pythia 410M across layers. In the top left corner of each plot is the Spearman rank correlation between the causal influence of each feature on the tuned lens and its influence on the model.

<!-- image -->

Eliciting Latent Predictions from Transformers with the Tuned Lens

|                     |   OV SVD (L) |   OV SVD (R) |   QK SVD (L) |   QK SVD (R) |   W in SVD |   W in columns |   W out SVD |   W out rows |
|---------------------|--------------|--------------|--------------|--------------|------------|----------------|-------------|--------------|
| Logit lens (real)   |       0.0745 |       0.1164 |       0.0745 |       0.1164 |     0.0745 |         0.0864 |      0.0728 |       0.0796 |
| Logit lens (random) |       0.0698 |       0.0695 |       0.0697 |       0.0688 |     0.0689 |         0.0689 |      0.0691 |       0.0688 |
| Tuned lens (real)   |       0.124  |       0.1667 |       0.124  |       0.1667 |     0.1193 |         0.1564 |      0.1196 |       0.163  |
| Tuned lens (random) |       0.1164 |       0.1157 |       0.1177 |       0.1165 |     0.1163 |         0.1262 |      0.1163 |       0.1532 |

Table 3. The mean interpretability scores (as measured in Appendix E.3) for Pythia 125M, with several different interpretability techniques (Millidge and Black, 2022; Geva et al., 2020), comparing both the tuned lens and logit lens to randomly generated matrices. Where applicable, the notation (L) and (R) indicates that the results are for the left and right singular vectors, respectively.

when viewed from the right angle, the modifications they make are often interpretable. These 'interpretable vectors' occur significantly more often than would be expected by random chance (Geva et al., 2022), and can be used to edit the model's behaviour, like reducing probability of specific tokens (Millidge and Black, 2022), decreasing toxicity (Geva et al., 2022), or editing factual knowledge (Dai et al., 2022).

Although these interpretability methods differ in precise details, we can give a generic model for the interpretation of a model parameter W using the unembedding matrix: 8

T ( W ) = topk ( f i ( W ) W U )

where f i is some function from our parameter to a vector with the same dimensions as the hidden state, and W U is the unembedding matrix. In words: we extract a hidden state vector from our parameter W according to some procedure, project into token space, and take the top k matching tokens. The resulting T ( W ) will be a list of tokens of size k , which functions as a human interpretable view of the vector f i ( W ) , by giving the k tokens most associated with that vector. As an example, the model parameter W could be the MLP output matrix W out for a particular layer, and f i the function selecting the i th column of the matrix.

With the tuned lens, we modify the above to become:

T ( W ) = topk ( L ℓ ( f i ( W )) W U )

where L ℓ is the tuned lens for layer number ℓ , projecting from the hidden state at layer ℓ to the final hidden state.

To test this hypothesis, we developed a novel automated metric for evaluating the performance of these parameter interpretability methods, based on the pretrained BPEmb encoder (Heinzerling and Strube, 2018), enabling much faster and more objective experimentation than possible with human evaluation. We describe this method in Appendix E.3.

The results for Pythia 125M can be seen in Table 3. The parameters under both the tuned and logit lens consistently appear more interpretable than random. And the tuned lens appears to show benefit: the difference between random/real average scores is consistently higher with the tuned lens than the logit lens. However, preliminary experiments found much less improvement with larger models, where both the tuned and logit lens appeared to perform poorly.

## E.3. Automated interpretability analysis method

While humans can easily tell if a list of words represents some coherent category, this can be subjective and time-consuming to evaluate manually. Some previous work has attempted to automate this process, using GPT-3 in Millidge and Black

where:

OV circuit SVD interpretability scores (logit lens)

OV circuit SVD interpretability scores (tuned lens)

Figure 21. The interpretability scores for the right singular vectors of the OV matrices W OV (following Millidge and Black (2022)) in Pythia 125M, compared with a randomly generated matrix, for both the logit lens (left), and the tuned lens (right).

<!-- image -->

(2022) or the Perspective API in Geva et al. (2022), but these can still be too slow for quick experimentation.

We created our own method, motivated by the observation that much of the human interpretable structure of these tokens appears to be related to their mutual similarity. Then, the mutual similarity of words can easily be measured by their cosine similarity under a word embedding (Sitikhu et al., 2019). In particular, we use the pretrained BPEmb encoder (Heinzerling and Strube, 2018) 9 . We encode each token, measure the cosine similarity pairwise between all tokens, and take the average value:

I ( T ( W )) = ∑ k,k i,j E ( T i ( W )) · E ( T j ( W )) k 2

where the subscript on T i ( W ) denotes the i -th token of T ( W ) , and E denotes the normalized BPEmb encoding function. This creates an interpretability metric I that measures something like the 'monosemanticity" of a set of tokens.

Because BPEmb is a pretrained model, unlike most CBOW or skip-gram models, there is no ambiguity in the training data used to produce word similarities and results are easy to replicate. We find the interpretability scores given by the model to correspond with human judgment in most cases: it rarely identifies human-uninterpretable lists of tokens as interpretable, although it occasionally struggles to recognize some cases of interpretable structure, like syntactically related tokens, or cases where a list as a whole is more interpretable than a pairwise measure can capture.

