## Abstract

With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis has recently made a big leap forward. At the core, NeRF proposes that each 3D point can emit radiance, allowing to conduct view synthesis using differentiable volumetric rendering. While neural radiance fields can accurately represent 3D scenes for computing the image rendering, 3D meshes are still the main scene representation supported by most computer graphics and simulation pipelines, enabling tasks such as real time rendering and physics-based simulations. Obtaining 3D meshes from neural radiance fields still remains an open challenge since NeRFs are optimized for view synthesis, not enforcing an accurate underlying geometry on the radiance field. We thus propose a novel compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach. Upon having trained the radiance field, we distill the volumetric 3D representation into a Signed Surface Approximation Network, allowing easy extraction of the 3D mesh and appearance. Our final 3D mesh is physically accurate and can be rendered in real time on an array of devices.

## 1. Introduction

Accurate 3D scene and object reconstruction is a key problem in areas such as robotics, photogrammetry, AR/VR, where applications often rely on precise 3D geometry to perform physics-based simulations, real-time 3D visualizations, rendering and interactions. Moreover, the related field of novel view synthesis (NVS) has made tremendous advances in recent years. Recently Mildenhall et al. [13] proposed to perform NVS by means of neural radiance fields (NeRFs), a novel 3D representation where each 3D location in space can emit radiance, see Sec 3.1 for more details. Novel views are synthesized by means of differentiable volumetric rendering [13, 23]. Due to the impressive results and simplicity of the approach, most related work has focused on improving NeRF in terms of image qual-

Figure 1. Our method extracts meshes with accurate geometry and view dependent appearance given a collection of posed images. We show a composition of meshes extracted from the Chair , Hotdog , Lego (Blender dataset) and Garden (MipNeRF 360 dataset) scenes using our method (Top left: scene rendered with colors, top right: geometry visualization). Our method enables physics based simulations. We show the results of a simulation of a cloth falling on the objects from the Blender Synthetic dataset (Bottom).

<!-- image -->

ity [1], robustness [9, 11, 16], as well as training speed [14] and rendering speed [3, 5]. Unfortunately, as these representations are commonly optimized for the NVS task and not explicitly for the underlying geometry [3, 6], it is yet unclear how to best obtain accurate 3D meshes from radiance fields. Indeed, while the volumetric representation of NeRF enables accurate renderings from new views, the underlying 3D geometry of each object is not uniquely defined as a level-set surface. NeRF methods often rely on layering and transparency effects to approximate complex appearance and geometry. The surface of objects is therefore approximated by dense regions of the volume instead of surfaces of zero thickness. Moreover, most related work still lacks the capability to be rendered in real-time [16,21],

especially on commodity hardware. Finally, NeRFs cannot be directly integrated with most computer graphics (CG) pipelines, as they still rely on standard 3D meshes due to their compactness and physical properties.

Despite some recent work proposing alternative scene representations to re-enable real-time rendering even for NeRFs, they are again not designed to produce accurate 3D representations of the input objects or scenes to be used with standard CG pipelines [3,6].

To deal with these limitations, we thus introduce NeRFMeshing , an end-to-end pipeline for efficiently extracting geometrically accurate meshes from trained NeRFbased networks, merely adding a very small overhead in time. Our method produces meshes with neural colors having accurate geometry that can be rendered in real time on commodity hardware. Introducing a novel signed surface approximation network (SSAN), we train a post-processing NeRF pipeline, defining the underlying surface and appearance. SSAN produces an accurate 3D triangle mesh of the scene that we render using a small appearance network to produce view-dependent colors. In contrast to other works that leverage distance fields, requiring significant modifications in the used NeRF architecture [22, 24], our method can be leveraged together with any NeRF, enabling to easily incorporate new advances, such as improved handling of unbounded scenes [1] or reflective objects [21]. Essentially, SSAN estimates a Truncated Signed Distance Field (TSDF) and a feature appearance field. Harnessing the NeRF approximated geometry as well as the used training views, we distill the trained NeRF into the SSAN model. We then extract the 3D mesh from the trained SSAN which can be rendered on embedded devices at high frame-rate using rasterization and the appearance network. Thanks to the flexibility of our method, we can generate these 3D meshes fast [14], are not tied to object-centric scenes [1], and can even model complex and non-lambertian surfaces [21].

To summarize, we propose NeRFMeshing, a novel method for capturing both accurate 3D meshes of the scene as well as enabling realistic view dependent rendering. The extracted meshes from our end-to-end pipeline can be integrated in graphics and simulation pipelines. Our model also preserves the high fidelity of neural radiance fields like view-dependent effects and reflections and can be used for real-time novel view synthesis.

## 3. Method

In this section we present our approach for extracting accurate 3D meshes with neural features from NeRF for subsequent real-time rendering. We present an overview of the method in Fig. 2. We first briefly outline the general concept of NeRFs in Sec. 3.1. In Sec. 3.2 we present our method for approximating surface from NeRF. Finally in Sec. 3.3 we describe mesh extraction and real-time rendering.

## 3.4. Implementation details

We use a backbone similar to Instant NGP [14] for the SSAN module. We divide the module in two separate branches with separate weights: i) the geometry branch that outputs TSDF approximation and the normal prediction. ii) The appearance branch which outputs color features. Each

Figure 4. Rendering of the 3D meshes from test views on the Synthetic Blender dataset.

<!-- image -->

branch is a separate network with the same architecture. We use Instant NGP [14] architectures with hash table size 2 19 , coarsest resolution of 16, highest resolution of 2048, 15 levels and a number of feature dimension per entry of 2. For the appearance network we use a network of 4 layers of MLP each with width of 32. We use the JAX framework for our implementation. We can train and extract a mesh end-to-end in less than an hour using 8 V100 NVIDIA GPUs. Finally we experimentally set the hyper-parameters n c = 10 and glyph[epsilon1] = 0 . 1 .

## 5. Conclusion

In this work we propose a novel approach to extract geometrically accurate meshes from NeRF based architectures. Our SSAN model can be trained from any NeRF architecture without a significant penalty in training time. Our neural mesh representation can be rendered at high frame rates on commodity hardware. Thanks to their geometric accuracy, our extracted meshes can be quickly visualized, and also be used in physically accurate settings in simulations, to compute accurate occlusions and interactions with other objects. Our work is nevertheless limited in some aspects: rendering detailed surfaces causes us to generate meshes with a high number of faces and vertices, thus, our method would gain from using an adaptative mesh reconstruction strategy. Moreover, on large (unbounded) and detailed scenes, we are limited to lower resolutions to not hinder the overall size of the model. Finally, the appearance network that we are using is a non-standard component that requires customization of existing renderers. Being able to learn the underlying materials, as methods like [2] would further simplify the integration of meshes in other tools.

## B. Ablation study

We evaluate the effect of some of our design choices for our method trained with a Mip-NeRF 360 with Instant NGP on the Blender Synthetic dataset inTab. 4.

Effect of choice of percentiles. We train our method on the blender synthetic dataset with different choices of percentile values. We notice that this choice only has a minor effect ( c.f. Tab. 4 row 2, 3, 5).

Effect of separate appearance network. We train our method with a single network to represent both the geometry and the appearance. We notice that separating the network leads to a small improvement over PSNR whilst geometric metrics remain comparable Tab. 4 row 1, 5.

Effect of the projection. Wetrain our method without the projection defined in Equation 10. We notice that it leads to a small drop in PSNR while geometric metrics are not much affected ( c.f. Tab. 4 row 4, 5).

## C. Additional implementation details



