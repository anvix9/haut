## Abstract

Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient visionlanguage model that leverages an ensemble of task-specific experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from multiple readily-available, pre-trained experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-arts, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/ NVlabs/prismer .

## 1 Introduction

Large pre-trained models have demonstrated exceptional generalisation capabilities across a wide range of tasks. However, these capabilities come at a hefty cost in terms of computational resources required for training and inference, as well as the need for large amounts of training data. In the language domain, models with hundreds of billions of learnable parameters typically require a compute budget on the yottaFLOP scale [18, 8, 7, 69].

The problems in vision-language learning are arguably more challenging. This domain is a strict super-set of language processing, whilst also requiring extra skills unique to visual and multi-modal reasoning. For example, many image captioning and visual question answering problems require the model to be capable of fine-grained object recognition, detection, counting, and 3D perception [4, 14]. A typical solution is to use a massive amount of imagetext data to train one giant, monolithic model that learns to develop these task-specific skills from scratch, simultaneously, and within the same generic architecture.

Figure 1: Prismer model overview. Prismer is a data-efficient vision-language model that leverages diverse pre-trained experts through its predicted multi-task signals. It can perform vision-language reasoning tasks such as image captioning and visual question answering. The analogy is with an optical prism: Prismer splits a single reasoning task into diverse domain-specific reasoning.

<!-- image -->

Instead, we investigate an alternative approach to learning these skills and domain knowledge via distinct and separate sub-networks , referred to as 'experts'. As such, each expert can be optimised independently for a specific task, allowing for the use of domain-specific data and architectures that would not be feasible with a single large network. This leads to improved training efficiency, as the model can focus on integrating specialised skills and domain knowledge, rather than trying to learn everything at once, making it an effective way to scale down multi-modal learning.

To achieve this, we propose Prismer 1 , a visually conditioned autoregressive text generation model, trained to better use diverse pre-trained task experts for open-ended vision-language reasoning tasks. Prismer's key design elements include i) powerful vision-only and languageonly models for web-scale knowledge to construct our core network backbones, and ii) multitask vision experts encoding multiple types of visual information, including low-level vision signals such as depth, and high-level vision signals such as instance and semantic labels, as a form of auxiliary knowledge , directly from their corresponding network outputs. All expert modelsareindividually pre-trained and frozen, and are connected through some lightweight trainable components which contribute to roughly 20% of the total network parameters.

Despite Prismer being trained on only 13M publicly available image/alt-text data examples, it shows strong multi-modal reasoning performance in tasks such as image captioning, image

classification, and visual question answering, competitive with many state-of-the-art visionlanguage models [3, 88, 90], that were trained with one or two orders of magnitude more data. Finally, we conduct an in-depth analysis of Prismer's learning behaviours and observe someencouraging properties. For example, i) Prismer exhibits strong robustness against the inclusion of noisy experts , and ii) the learning performance also scales favourably with increases in both the quantity or quality of experts.

## 3.1 Model Overview

The design of the Prismer model is illustrated in Fig. 2. Prismer is an encoder-decoder transformer model [86] that leverages a library of existing pre-trained experts. It consists of a vision encoder and an auto-regressive language decoder. The vision encoder takes an RGB image and its corresponding multi-task labels as input ( e.g. depth, surface normal, segmentation labels, predicted from the frozen pre-trained experts), and outputs a sequence of RGB and multi-task features. The language decoder is then conditioned on these multi-task features via cross attention, and produces a sequence of text tokens.

Oneofthekeyadvantages of the Prismer model is its exceptional data efficiency during training. This is achieved by leveraging a combined power of strong task-specific experts , resulting in a significant reduction in the number of GPU hours required to achieve comparable perfor-

<latexit sha1\_base64="wYTJ/0oUqHYhBNNBS/WgByxgjsI=">AAADAXicfVLLahRBFK1pH4ntK9Glm8ZBEJGhR4JmIwR14UaN4CQD00O4XXO7U0y9qLptnDSz8g/c6g+4E7d+iWt/xOqZHnCS4IWiD+fcVx9ubqXwlKa/O9Gly1eubmxei6/fuHnr9tb2nQNvKsdxwI00bpiDRyk0DkiQxKF1CCqXeJhPXzb64Ud0Xhj9gWYWxwpKLQrBgQI1zEgo9Mnbo61u2ksXkZwH/RZ0WRv7R9udP9nE8EqhJi7B+1E/tTSuwZHgEudxVnm0wKdQ4ihADWHOuF4sPE8eBGaSFMaFpylZsP9W1Aro2MKpWetTU2nDR0pYp0F5P1N5aNuU+bNaQ16kjSoqdse10LYi1Hy5VVHJhEzSWJVMhENOchYAcCfCjyX8GBxwCoauTbFlYaUhP4/jOHuFwRCHb8K8dxYdkHGP6gxcqeDTPBhUZo8b9L9EoVeJATU9VwM8UvBQWaDnGk/QU+tEK0uRO3CzunSmsu1GF+kTIGyIpUpierpSwDlz4nsKKZgch7Ponz2C8+DgSa//tLfzfqe796I9kE12j91nD1mfPWN77DXbZwPGmWRf2Ff2LfocfY9+RD+XqVGnrbnL1iL69RcOzvw7</latexit>

Figure 2: Prismer architecture design overview. Prismer has two main trainable components: the Experts Resampler that converts variable multi-task signals to a fixed number of outputs, and the Adaptor that enhances the model's expressivity for vision-language reasoning. To ensure that the model takes advantage of the rich domain-specific knowledge encoded in the pre-trained experts, the majority of network weights are frozen during training, as represented by ^ .

<!-- image -->

Vision Encoder

## 3.4 Training Objective

For simplicity, we train Prismer with a single objective - to predict the next text token autoregressively. Following the standard encoder-decoder architecture, the vision encoder pre-

dicts the multi-task features z , and the language decoder learns to maximise the conditional likelihood of the paired text caption y with its length T under the forward autoregressive factorisation: L = -âˆ‘ T t = 1 log p ( yt | y < t , z ) .

In practice, our one-time pre-processing step of collecting multi-task expert labels is computationally cheap and fast with data parallelism. The single generative objective then only requires one forward pass to compute gradients, which is significantly more efficient and streamlined than many other VLMs that may require a multi-stage and/or multi-step pretraining [45, 46, 89, 25, 15], with multiple objectives and data sources. However, because our model only focuses on multi-modal language generation, it is less suitable for multi-modal discriminative tasks such as image-text retrieval and visual entailment, which are the focus of other types of VLMs [28, 15, 35].

## 4.2 Training and Evaluation Details

Pre-training Datasets Weconstruct our pre-training data from the following datasets: two in-domain datasets: COCO [48] and Visual Genome [42]; and three web datasets: Conceptual Captions [76], SBU captions [65], and a much noisier Conceptual 12M [10]. The web datasets are pre-filtered and re-captioned by a pre-trained image captioner [45]. The pretraining datasets include 11M unique images or 12.7M image/alt-text pairs. 2 All datasets are available publicly and have been widely used for pre-training many VLMs [46, 45, 15].

Optimisation and Implementation All our models are trained with AdamW optimiser [56] with a weight decay of 0.05. Since only a small proportion of the model parameters are trainable, model sharding is only applied during fine-tuning on large-resolution images. Specifically, we employ ZeRO Stage 2 technique [70], which enables the sharding of optimiser states and parameter gradients across all GPU instances. Additionally, we also apply Automatic Mixed Precision (AMP) with fp16 precision to further reduce training time. For more details on our data processing techniques and hyper-parameter choices, please refer to Appendix A. An analysis of training costs compared to other vision-language models can be found in Appendix B.

Evaluation Setting We evaluate the performance of our models through language modelling, which is a more challenging task than discriminative learning (particularly in VQA tasks), and aligns with that used in other vision-language generative models [45, 3, 88, 13]. For example, the model must accurately generate all text tokens for a question (which is on average 2.2 tokens per question in the VQAv2 dataset [4] as reported in [88]), rather than just one correct prediction as required in discriminative models.

Specifically, we evaluate image captioning tasks in an open-ended generative setting, and we apply beam search with a beam size of 3 for text generation. A prefix prompt of 'A picture of' is added to the input text for fined-tuned image captioning tasks, similar to previous studies such as in [90, 45, 68], which have shown to improve the quality of image captions. We evaluate both VQA and image classification tasks in a close-ended generative setting, by ranking the per-token log-likelihood from a pre-defined answer list.

## 5 Additional Analysis

We now include a comprehensive evaluation of Prismer, characterised by a meticulous and fine-grained analysis of its learning strategy. We delve into various aspects of Prismer's performance, examining its behaviour with different types of multi-task experts (as discussed in Sec.5.1). Additionally, we explore the individual utility of each expert in addressing domainspecific reasoning tasks, allowing us to gain insights into the specific strengths and contributions of each expert (as discussed in Sec.5.2).

## 5.3 Architecture Design and Training Details

Adaptor Design and Size In our ablation study of adaptor designs, as shown in row (i) and (ii) of Table 6, we find that the most straightforward adaptor design, consisting of a standard residual connection and an encoder-decoder structure, performs the best. We have experimented with more intricate designs, such as adding an additional adaptor at the end of each transformer layer or incorporating a learnable gating mechanism akin to the one shown

(b) Text-VQA

| Baselines (fine-tuned)   | Baselines (fine-tuned)   | Baselines (fine-tuned)   | Baselines (fine-tuned)   |         | Prismer (zero-shot)   | Prismer (zero-shot)   | Prismer (zero-shot)   |            |            |            |
|--------------------------|--------------------------|--------------------------|--------------------------|---------|-----------------------|-----------------------|-----------------------|------------|------------|------------|
| VisualBERT               | LXMERT                   | ViLT                     | +Depth                   | +Normal | +Edge                 | +Seg.                 | +OCR Det.             | +Obj. Det. | No Experts | +6 Experts |
| 51.0                     | 61.2                     | 63.0                     | 68.4                     | 68.3    | 67.8                  | 68.4                  | 67.2                  | 68.3       | 65.6       | 68.7       |
| (a) VSR                  | (a) VSR                  | (a) VSR                  | (a) VSR                  | (a) VSR | (a) VSR               | (a) VSR               | (a) VSR               | (a) VSR    | (a) VSR    | (a) VSR    |
| Baselines (zero-shot)    | Baselines (zero-shot)    | Baselines (zero-shot)    |                          |         | Prismer (zero-shot)   | Prismer (zero-shot)   | Prismer (zero-shot)   |            |            |            |
| OFA                      | BLIP-2                   | Flamingo                 | +Depth                   | +Normal | +Edge                 | +Seg.                 | +OCR Det.             | +Obj. Det. | No Experts | +6 Experts |
| 18.3                     | 15.7                     | 35.0                     | 27.4                     | 28.0    | 28.2                  | 27.8                  | 28.4                  | 28.4       | 22.6       | 28.8       |

Table 5: Zero-shot accuracies in VSR (zero-shot split) and Text-VQA (validation split) datasets, considering various types of experts. These results shed light on the valuable contributions of individual experts for domain-specific reasoning tasks, offering insights into the versatility and adaptability of Prismer across different domains and problem-solving scenarios. The colour green represents the most helpful experts, while the colour red represents the least helpful experts.

in [50], but both have resulted in inferior performance. Furthermore, we observe that a larger bottleneck hidden size for the single adaptor has led to improved performance.

Resampler Design and Multi-modal Sampling Strategy In our ablation study of Experts Resampler designs and various strategies for encoding multi-modal signals, as shown in row (iii) - (v) of Table 6, we find that using lightweight designs for the resampler layers and latent variables is crucial for stable training. Our experiments also show that using a non-learnable random sampling approach resulted in a slightly lower performance compared to using a learnable resampler. We have also attempted to optimise the resampler by receiving all input signals, including RGB information, but this approach has also resulted in a significant decline in performance. Finally, incorporating an extra resampler at the end of the vision encoder is not beneficial, though it may help in reducing and keeping a constant memory usage independent of the image resolutions, it ultimately leads to a decrease in performance.

The Effect of Frozen Backbones In our experiments on pre-training and fine-tuning whilst freezing different parts of the model, as shown in row (vi) and (vii) of Table 6, we find that freezing pre-trained parameters is essential for achieving strong performance and avoiding over-fitting and catastrophic forgetting of the learned knowledge. 3 Freezing these parameters also saves a significant amount of GPU memory. Even when fine-tuning on different downstream tasks, we find that freezing the vision encoder is beneficial (while allowing the resampler and adaptors to be trainable). This observation is consistent with the findings in [97], which demonstrates that fine-tuning only the language model with a frozen vision model can produce a much stronger zero-shot vision-language retrieval performance.

Table 6: Ablation studies for architecture components and training strategies. Weperform ablation studies to evaluate the impact of different architectural components and training strategies on the VQAv2 test-dev performance. We compare the performance of our default setting to other design and training options. The number of parameters and pre-training step time of the changed setting relative to the default setting are reported. To ensure a fair comparison, all experiments are evaluated using a reduced amount of training data and 3 task experts: depth, normal and segmentation.

| Ablated Component                                | Our Setting                                      | Changed Setting                                  |   Params. (Rel.) |   Step Time (Rel.) |   VQAv2 (Acc.) |
|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|------------------|--------------------|----------------|
| Prismer BASE (our setting with reduced training) | Prismer BASE (our setting with reduced training) | Prismer BASE (our setting with reduced training) |             1    |               1    |          72.79 |
| (i) Adapter Design                               | Residual MLP                                     | Residual MLP Ã— 2                                 |             1.04 |               1.02 |          72.36 |
| (i) Adapter Design                               | Residual MLP                                     | Gated Residual MLP                               |             1.03 |               1.03 |          70.54 |
| (ii) Adapter Bottleneck Dim.                     | 1                                                | 1/2                                              |             0.95 |               0.96 |          72.52 |
| (ii) Adapter Bottleneck Dim.                     | 1                                                | 1/4                                              |             0.93 |               0.93 |          71.66 |
| (iii) Resampler Design                           | Experts Perceiver                                | Random Sampling                                  |             0.91 |               0.96 |          72.24 |
| (iii) Resampler Design                           | Experts Perceiver                                | Full Perceiver                                   |             1    |               0.9  |          65.05 |
| (iii) Resampler Design                           | Experts Perceiver                                | Dual Perceiver                                   |             1.08 |               1.02 |          71.56 |
| (iv) Resampler Layers                            | 4                                                | 1                                                |             0.94 |               0.93 |          70.61 |
| (iv) Resampler Layers                            | 4                                                | 2                                                |             0.96 |               0.96 |          72.39 |
| (iv) Resampler Layers                            | 4                                                | 6                                                |             1.04 |               1.01 |          72.78 |
| (v) Resampler Latents                            | 64                                               | 32                                               |             1    |               0.95 |          72.44 |
| (v) Resampler Latents                            | 64                                               | 128                                              |             1    |               1.01 |          70.28 |
| (v) Resampler Latents                            | 64                                               | 256                                              |             1    |               1.06 |          68.07 |
| (vi) Pre-training                                | Freeze Vision and Lang.                          | Freeze Vision Only                               |             1    |               1.07 |          70.49 |
| (vi) Pre-training                                | Freeze Vision and Lang.                          | Freeze Lang. Only                                |             1    |               1.05 |          67.77 |
| (vi) Pre-training                                | Freeze Vision and Lang.                          | All Parameters                                   |             1    |               1.15 |          68.13 |
| (vii) Fine-tuning                                | Freeze Vision                                    | Freeze Vision and Lang.                          |             1    |               1    |          71.36 |
| (vii) Fine-tuning                                | Freeze Vision                                    | Freeze Lang. Only                                |             1    |               1    |          70.37 |
| (vii) Fine-tuning                                | Freeze Vision                                    | All Parameters                                   |             1    |               1    |          68.69 |

## 6 Conclusions, Limitations and Discussion

In this paper, we have introduced Prismer, a vision-language model designed for reasoning tasks. Prismer is parameter-efficient and utilises a small number of trainable components to connect an ensemble of diverse, pre-trained experts. By leveraging these experts, Prismer achieves competitive performance in image captioning, VQA, and image classification benchmarks, comparable to models trained on up to two orders of magnitude more data.

For full transparency, we now discuss some limitations of Prismer during our implementation and explore potential future directions for this work.

Multi-modalIn-context Learning Zero-shot in-context generalisation is an emergent property that only exists in very large language models [8, 91]. In this work, we build Prismer on top of a small-scale language model with the main focus on parameter-efficient learning. Therefore, it does not have the ability to perform few-shot in-context prompting by design.

Zero-shot Adaptation on New Experts We experiment with inference on a pre-trained Prismer with a different segmentation expert pre-trained on a different dataset. Although we

apply the same language model to encode semantic labels, Prismer shows limited adaptability to a different expert with a different set of semantic information, which leads to a notable performance drop.

Free-form Inference on Partial Experts Similarly, we discover that Prismer entangles its multi-task features from all experts we include during pre-training. Therefore, only having a partial number of experts during inference will lead to a notable performance drop. We attempt to use a different training objective such as masked auto-encoding [5], to design Prismer to reason on an arbitrary number of experts, but it eventually leads to a degraded fine-tuned performance.

Representation of Expert Knowledge In our current design of Prismer, we convert all expert labels into an image-like 3-dimensional tensor via task-specific post-processing for simplicity. There are other efficient methods to represent expert knowledge, such as converting object detection labels into a sequence of text tokens [11, 12]. This may potentially lead to a stronger reasoning performance and a more stable training landscape in future works.

