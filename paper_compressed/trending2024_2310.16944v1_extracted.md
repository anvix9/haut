## ABSTRACT

We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, ZEPHYR7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHFbased model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook .

Figure 1: Model performance on MT-Bench. We compare ZEPHYR-7B, trained with distilled direct preference optimization (dDPO), to proprietary models as well as larger, open-access models like LLAMA2-CHAT-70B that were additionally trained using reinforcement learning on a large amount of human feedback.

<!-- image -->

## 1 INTRODUCTION

Smaller, open large language models (LLMs) have greatly increased in ability in recent years, from early GPT-2-like models (Wang & Komatsuzaki, 2021) to accurate and compact models (Touvron et al., 2023; Penedo et al., 2023; Jiang et al., 2023) that are trained on significantly more tokens than the 'compute-optimal' amount suggested by the Chincilla scaling laws (De Vries, 2023). In addition, researchers have shown that these models can be further trained through distilled supervised fine-tuning (dSFT) based on proprietary models to increase their accuracy (Taori et al., 2023). In this approach, the output of a more capable teacher model is used as supervised data for the student model.

Distillation has proven to be an effective tool for improving open models on a range of different tasks (Chiang et al., 2023); however, it does not reach the performance of the teacher models (Gudibande et al., 2023). Users have noted that these models are not 'intent aligned', i.e. they do not behave in a manner that aligns with human users' preferences. This property often leads to outputs that do not provide correct responses to queries.

Intention alignment has been difficult to quantify, but recent work has led to the development of benchmarks like MT-Bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023) that specifically target this behavior. These benchmarks yield scores that correlate closely with human ratings of model outputs and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation. This motivates careful collection of human feedback for alignment, often at enormous cost at scale, such as in LLAMA2-CHAT (Touvron et al., 2023).

In this work, we consider the problem of aligning a small open LLM entirely through distillation. The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective (Rafailov et al., 2023). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling compared to using other approaches like proximal preference optimization (PPO) (Schulman et al., 2017). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB).

To validate this approach, we construct ZEPHYR-7B, an aligned version of Mistral-7B (Jiang et al., 2023). We first use dSFT, based on the UltraChat (Ding et al., 2023) dataset. Next we use the AI feedback data collected in the UltraFeedback dataset (Cui et al., 2023). Finally, we apply dDPO based on this feedback data. Experiments show that this 7B parameter model can achieve performance comparable to 70B-parameter chat models aligned with human feedback. Results show improvements both in terms of standard academic benchmarks as well as benchmarks that take into account conversational capabilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available at https://github.com/huggingface/alignment-handbook .

We note an important caveat for these results. We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice (Bai et al., 2022). As distillation only works with the output of publicly available models this is technically more challenging to do because of added challenges in curating that type of synthetic data, and is an important subject for future work.

## 3 METHOD

The goal of this work is to align an open-source large-language model to the intent of the user. Throughout the work we assume access to a larger teacher model π T which can be queried by prompted generation. Our goal is be to produce a student model π θ and our approach follows similar stages as InstructGPT (Ouyang et al., 2022) as shown in Figure 2.

dd chocolate

e

,

,

A

dd chocolate

,

dd chocolate

e

,

dd chocolate

e

,

dd chocolate

e

,

dd chocolate

e

,

e

A

dd chocolate

Distilled Supervised Fine-Tuning (dSFT) Starting with a raw LLM, we first need to train it to respond to user prompts. This step is traditionally done through supervised fine tuning (SFT) on a dataset of high-quality instructions and responses (Chung et al., 2022; Sanh et al., 2021). Given access to a teacher language models, we can instead have the model generate instructions and responses (Taori et al., 2023), and train the model directly on these. We refer to this as distilled SFT (dSFT).

Approaches to dSFT follow the self-instruct protocol (Wang et al., 2023). Let x 0 1 , . . . , x 0 J be a set of seed prompts, constructed to represent a diverse set of topical domains. A dataset is constructed through iterative self-prompting where the teacher is used to both respond to an instruction and refine the instruction based on the response. For each x 0 , we first sample response y 0 ∼ π T ( ·| x 0 ) , and then refine by sampling a new instruction (using a prompt for refinement), x 1 ∼ π T ( ·| x 0 , y 0 ) . The end point is a final dataset, C = { ( x 1 , y 1 ) , . . . , ( x J , y J ) } . Distillation is performed by SFT,

π dSFT = max π E ( x,y ) ∼C log π ( y | x )

AI Feedback through Preferences (AIF) Human feedback (HF) can provide additional signal to align LLMs. Human feedback is typically given through preferences on the quality of LLM responses (Ouyang et al., 2022). For distillation, we instead use AI preferences from the teacher model on generated outputs from other models.

We follow the approach of UltraFeedback (Cui et al., 2023) which uses the teacher to provide preferences on model outputs. As with SFT, the system starts with a set of prompts x 1 , . . . , x J . Each prompt x is fed to a collection of four models π 1 , . . . , π 4 , e.g. Claude, Falcon, Llama, etc, each of which yield a response y 1 ∼ π 1 ( ·| x ) , . . . , y 4 ∼ π 4 ( ·| x ) . These responses are then fed to the teacher model, e.g. GPT-4, which gives a score for the response s 1 ∼ π T ( ·| x, y 1 ) , . . . , s 4 ∼ π T ( ·| x, y 4 ) . After collecting the scores for a prompt x , we save the highest scoring response as y w and a random lower scoring prompt as y l . The final feedback dataset D consists of a set of these triples ( x, y w , y l ) .

Distilled Direct Preference Optimization (dDPO) The goal of the final step is to refine the π dSFT by maximizing the likelihood of ranking the preferred y w over y l in a preference model. The preference model is determined by a reward function r θ ( x, y ) which utilizes the student language model π θ . Past work using AI feedback has primarily focused on using RL methods such as proximal policy optimization (PPO) to optimize θ with respect to this reward. These approaches optimize θ by first training the reward and then sampling from the current policy to compute updates.

Direct preference optimization (DPO) uses a simpler approach to directly optimize the preference model from the static data (Rafailov et al., 2023). The key observation is to derive the optimal reward function in terms of the optimal LLM policy π ∗ and the original LLM policy π dSFT. Under an appropriate choice of preference model they show, for constant β and partition function Z that,

r ∗ ( x, y ) = β π * ( y | x ) π dSFT ( y | x ) + β log Z ( x )

By plugging this function of the reward into the preference model, the authors show that the objective can be written as,

π θ = max π E ( x,y w ,y l ) ∼D log σ ( β log π ( y w | x ) π dSFT ( y w | x ) -β log π ( y l | x ) π dSFT ( y l | x ) ) . (1)

While this term looks complex, we note that it implies a simple training procedure. Starting with the dSFT version of the model, we iterate through each AIF triple ( x, y w , y l ) .

- 1. Compute the probability for ( x, y w ) and ( x, y l ) from the dSFT model (forward-only).
- 2. Compute the probability for ( x, y w ) and ( x, y l ) from the dDPO model.
- 3. Compute Eq 1 and backpropagate to update. Repeat.

## 4 EXPERIMENTAL DETAILS

We conduct all of our fine-tuning experiments using Mistral 7B (Jiang et al., 2023), which is the current state-of-the-art base LM at the 7B parameter scale, and matches the performance of much larger

models like LLaMa 34B on many NLP benchmarks. We use the Transformer Reinforcement Learning (TRL) library for fine-tuning (von Werra et al., 2020), in conjunction with DeepSpeed ZeRO3 (Rajbhandari et al., 2020) and FlashAttention-2 (Dao, 2023) to optimize memory and improve training speed. All models are trained with the AdamW optimizer and no weight decay. We did not experiment with parameter-efficient techniques such as LoRA (Hu et al., 2021), but expect similar results to hold with these methods. All experiments were run on 16 A100s using bfloat16 precision and typically took 2-4 hours to complete. For the full set of hyperparameters and instructions on how to train the models, see: https://github.com/huggingface/alignment-handbook .

## 5 RESULTS AND ABLATIONS

In this section we collect our main results; see Appendix A for sample model completions.

Table 1: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. A dash ( -) indicates model or alignment information that is not publicly available, or an evaluation that is absent on the public leaderboards. Scores marked with an asterisk ( ∗ ) denote evaluations done by ourselves.

| Model                 | Size   | Align   | MT-Bench (score)   | AlpacaEval (win %)   |
|-----------------------|--------|---------|--------------------|----------------------|
| StableLM-Tuned- α     | 7B     | dSFT    | 2.75               | -                    |
| MPT-Chat              | 7B     | dSFT    | 5.42               | -                    |
| Xwin-LM v0.1          | 7B     | dPPO    | 6.19 ∗             | 87.83 1 . 15         |
| Mistral-Instruct v0.1 | 7B     | -       | 6.84               | -                    |
| Zephyr                | 7B     | dDPO    | 7.34               | 90.60 1 . 03         |
| Falcon-Instruct       | 40B    | dSFT    | 5.17               | 45.71 1 . 75         |
| Guanaco               | 65B    | SFT     | 6.41               | 71.80 1 . 59         |
| Llama2-Chat           | 70B    | RLHF    | 6.86               | 92.66 0 . 91         |
| Vicuna v1.3           | 33B    | dSFT    | 7.12               | 88.99 1 . 10         |
| WizardLM v1.0         | 70B    | dSFT    | 7.71               | -                    |
| Xwin-LM v0.1          | 70B    | dPPO    | -                  | 95.57 0 . 72         |
| GPT-3.5-turbo         | -      | RLHF    | 7.94               | 89.37 1 . 08         |
| Claude 2              | -      | RLHF    | 8.06               | 91.36 0 . 99         |
| GPT-4                 | -      | RLHF    | 8.99               | 95.28 0 . 72         |

dDPO Improves Chat Capabilities. In Table 1 we compare the performance of ZEPHYR-7B on the MT-Bench and AlpacaEval benchmarks. Compared to other open 7B models, ZEPHYR-7B sets a new state-of-the-art and performs significantly better than dSFT models across both benchmarks. In particular, ZEPHYR-7B outperforms XWIN-LM-7B, which is one of the few open models to be trained with distilled PPO (dPPO). When compared to larger open models, ZEPHYR-7B achieves competitive performance with LLAMA2-CHAT 70B, scoring better on MT-Bench and within two standard deviations on AlpacaEval. However, ZEPHYR-7B performs worse than WIZARDLM-70B

and XWIN-LM-70B, which suggests that applying dDPO to larger model sizes may be needed to match performance at these scales. When compared to proprietary models, ZEPHYR-7B is competitive with GPT-3.5-TURBO and CLAUDE 2 on AlpacaEval, however these results should be interpreted with care since the prompts in AlpacaEval may not be representative of real-usage and advanced applications. This is partly visible in Figure 1, which shows the breakdown of model performance on MT-Bench across each domain. We can see that although ZEPHYR-7B is competitive with proprietary models on several categories, is much worse in math and coding.

dDPO Improves Academic Task Performance Table 2 shows the main chat results comparing the performance of the proposed model with a variety of other closed source and open-source LLMs. Results show that the dDPO model performs the best among all 7B models, with a large gap over the best dSFT models as well as Xwin-LM dPPO model. Model scale does matter more for these results and the larger models perform better than Zephyr on some of the knowledge intensive tasks. However, Zephyr does reach the performance of the 40B scale models.

Table 2: Academic benchmark results for open-access models on the Open LLM Leaderboard.

| Model                 | Size   | Align   |   ARC |   Hella Swag |   MMLU |   Truthful QA |
|-----------------------|--------|---------|-------|--------------|--------|---------------|
| StableLM-Tuned- α     | 7B     | dSFT    | 31.91 |        53.59 |  24.41 |         40.37 |
| MPT-Chat              | 7B     | dSFT    | 46.5  |        75.51 |  37.62 |         40.16 |
| Xwin-LM v0.1          | 7B     | dPPO    | 56.57 |        79.4  |  49.98 |         47.89 |
| Mistral-Instruct v0.1 | 7B     | dSFT    | 54.52 |        75.63 |  55.38 |         56.28 |
| Zephyr                | 7B     | dDPO    | 62.03 |        84.52 |  61.44 |         57.44 |
| Falcon-Instruct       | 40B    | dSFT    | 61.6  |        84.31 |  55.45 |         52.52 |
| Guanaco               | 65B    | SFT     | 65.44 |        86.47 |  62.92 |         52.81 |
| Llama2-Chat           | 70B    | RLHF    | 67.32 |        87.33 |  69.83 |         44.92 |
| Vicuna v1.3           | 33B    | dSFT    | 62.12 |        83    |  59.22 |         56.16 |
| WizardLM v1.0         | 70B    | dSFT    | 64.08 |        85.4  |  64.97 |         54.76 |
| Xwin-LM v0.1          | 70B    | dPPO    | 70.22 |        87.25 |  69.77 |         59.86 |

Is Preference Optimization Necessary? In Table 3 we examine the impact from different steps of the alignment process by fine-tuning Mistral 7B in four different ways:

- · dDPO - dSFT fine-tunes the base model directly with DPO for one epoch on UltraFeedback.
- · dSFT-1 fine-tunes the base model with SFT for one epoch on UltraChat.
- · dSFT-2 applies dSFT-1 first, followed by one more epoch of SFT on the top-ranked completions of UltraFeedback.
- · dDPO + dSFT applies dSFT-1 first, followed by one epoch of DPO on UltraFeedback.

First, we replicate past results (Ouyang et al., 2022) and show that without an initial SFT step (dSFT), models are not able to learn at all from feedback and perform terribly. Using dSFT improves model score significantly on both chat benchmarks. We also consider running dSFT directly on the feedback data by training on the most preferred output (dSFT2 ); however we find that this does not make an impact in performance. Finally, we see that the full Zephyr models (dDPO+dDSFT) gives a large increase in both benchmarks.

Does Overfitting Harm Downstream Performance? In the process of training ZEPHYR-7B we observed that after one epoch of DPO training, the model would strongly overfit, as indicated by perfect training set accuracies in Figure 3. Surprisingly, this did not harm downstream performance on MT-Bench and AlpacaEval; as shown in Figure 3, the strongest model was obtained with one epoch of SFT followed by three epochs of DPO. However, we do observe that if the SFT model is trained for more than one epoch, the DPO step actually induces a performance regression with longer training.

Table 3: Ablation of different alignment methods on the base Mistral 7B model.

| Align       |   MT-Bench (score) | AlpacaEval (win %)   |
|-------------|--------------------|----------------------|
| dDPO - dSFT |               4.76 | 30.76 1 . 63         |
| dSFT-1      |               6.64 | 85.65 1 . 23         |
| dSFT-2      |               6.19 | 78.54 1 . 44         |
| dDPO + dSFT |               7    | 86.07 1 . 22         |

<!-- image -->

Figure 3: Train and test set accuracy during DPO (left) and MT-Bench scores for MISTRAL-7B models fine-tuned first with dSFT and then dDPO for a varying number of epochs on the UltraChat and UltraFeedback datasets (right).

<!-- image -->

## 6 CONCLUSIONS AND LIMITATIONS

We consider the problem of alignment distillation from an LLM onto a smaller pretrained model. The method avoids the use of sampling-based approaches like rejection sampling or PPO, and distills conversational capabilities with direct preference optimization (DPO) from a dataset of AI feedback. The resulting model ZEPHYR-7B, based on MISTRAL-7B, sets a new state=of-the-art for 7B parameter chat models, and even outperforms LLAMA2-CHAT-70B on MT-Bench. We hope this approach motivates further exploration of the capacity of smaller, open-models by demonstrating their ability to align to the intent of user interactions.

There are several limitations associated with our study. The main one is the use of GPT-4 as an evaluator for the AlpacaEval and MT-Bench benchmarks, which is known to be biased towards models distilled from it, or those that produce verbose, but potentially incorrect responses. Another limitation is examining whether our method scales to much larger models like LLAMA2-70B, where the performance gains are potentially larger.

