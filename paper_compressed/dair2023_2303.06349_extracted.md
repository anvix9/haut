## 1. Introduction

Recurrent neural networks (RNNs) have played a central role since the early days of deep learning, and are a natural choice when modelling sequential data (Elman, 1990; Hopfield, 1982; McCulloch and Pitts, 1943; Rumelhart et al., 1985). However, while these networks have strong theoretical properties, such as Turing completeness (Chung and Siegelmann, 2021; Kilian and Siegelmann, 1996), it is well-known that they can be hard to train in practice. In particular, RNNs suffer from the vanishing and exploding gradient problem (Bengio et al., 1994; Hochreiter, 1991; Pascanu et al., 2013), which makes it difficult for these models to learn about the long-range dependencies in the data. Several techniques were developed that attempt to mitigate this issue, including orthogonal/unitary RNNs (Arjovsky et al., 2016; Helfrich et al., 2018), and gating mechanisms such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs) (Cho et al., 2014a). Nonetheless, these models are still slow to optimize due to the inherently sequential nature of their computation (Kalchbrenner et al., 2016), and are therefore hard to scale.

In recent years, Transformers (Vaswani et al., 2017) have gained increasing prominence for sequence modelling tasks, achieving remarkable success in a wide range of applications (Brown et al., 2020; Dosovitskiy et al., 2020; Jumper et al., 2021). Compared to RNNs, attention layers are easier to scale and parallelize during training, and crucially they do not suffer from the vanishing gradient problem, since the interaction between any two tokens in the sequence is modeled by direct edges in the network. A key issue with attention layers however is that their computational and memory costs scale quadratically as ùëÇ ' ùêø 2 ' with the sequence length ùêø . Transformers can therefore be especially expensive to deploy on long sequences. RNNs, which scale linearly with the sequence length, are therefore typically faster than transformers at inference time even for modest sequence lengths (Liu et al., 2019).

Motivated by these problems, Gu et al. (2021a) recently introduced the S4 model, a carefully designed deep state-space model (SSM) achieving remarkable performance on tasks from the Long Range Arena (LRA) (Tay et al., 2020), a benchmark explicitly designed to require very long-ranged reasoning. S4 is theoretically principled and inspired by continuous-time linear SSMs; well-established components of modern control systems. More importantly, the S4 layer and its variants (DSS, S4D, S5, etc) (Gu et al., 2022a; Gupta et al., 2022a; Smith et al., 2022) overcome the ùëÇ ' ùêø 2 ' bottleneck of attention layers by modeling interactions between

50% 60% 70% 80% 90% Figure 1 j (Left) Deep Linear Recurrent Unit (LRU) architecture introduced in this paper, inspired by S4 (Gu et al., 2021a). The model is a stack of LRU blocks, with nonlinear projections in between, and also uses skip connections and normalization methods like batch/layer normalization. We expand on the details in ¬ßD and provide pseudocode in ¬ßA. We also use the same architecture structure (Norm-Recurrence-GLU-Skip) for every variant of the recurrent module in our study ( tanh dense, linear dense, etc..). (Right) Summary of effects for the main steps outlined in the introduction towards designing LRUs starting from tanh RNNs. Shown is the average performance (3 seeds) of the recurrent module at each step on the Long Range Arena (LRA), compared to average performance of deep SSMs. For all LRA tasks, we match the performance of deep SSMs like S4/S4D/S5 with LRUs. Detailed results in ¬ß3.

<!-- image -->

7Dnh-5NN LLn-5NN

DLDg

6tDble

NRrm

tokens using a hidden state (like RNNs) under proper discretization techniques. These models can be made very efficient at inference time by simply unrolling the layer like an RNN. Futhermore, since SSMs are linear in the temporal dimension, they are easily parallelizable during training, in contrast to the slow sequential nature of training a typical RNN. This makes them very computationally efficient on long sequences.

While the S4 model is equivalent to an RNN during inference, it has a number of unique characteristics during training. For example, S4 is parameterized as a discretization of a latent continuous-time system of differential equations. S4 also uses specific initializations of the state matrices motivated from the theory of polynomial projections (Gu et al., 2020). While these characteristics might seem to motivate the impressive performance of these models, later works (Gu et al., 2022a; Gupta et al., 2022a,b; Smith et al., 2022) have suggested that the specific initialization used by S4 is often not crucial for performance, and that the discretization rules which achieve best performance may deviate from theory (Smith et al., 2022). It is therefore unclear what these unique characteristics of the deep SSMs are doing mechanistically, and how they can be simplified.

Motivated by the striking similarities between RNNs and deep SSMs, and in an attempt to better understand the underlying mechanism driving the performance of these models, we study the power and limitations of RNNs when used as core components of deep architectures for long-range reasoning. Our main goal is to answer the question:

' Can we match the performance and efficiency of deep continuous-time SSMs using deep RNNs? '

We give a positive answer to this question. We show that the performance boost provided by deep SSMs like S4 can also be achieved via a series of small changes to a vanilla deep RNN. With these changes, we can recover the performance and efficiency of these deep SSMs on the Long Range Arena (LRA) benchmark (Tay et al., 2020). We call this new RNN model the Linear Recurrent Unit (or LRU for short).

Main Steps. We outline here the main steps needed towards crafting performant and efficient RNN models. Note while some of these observations have been made in prior works (see ¬ßB), we provide novel perspectives and careful ablations leading to new insights. Each step presented in this paper unveils a specific property of

recurrent networks, and showcases the challenges and best practices in training and initializing deep RNNs.

- ¬∑ Linear Recurrences. When replacing SSM layers in a deep architecture with vanilla RNN layers using tanh or ReLU activations, the performance on Long Range Arena (LRA) drops significantly. Surprisingly, in ¬ß3.1 we find that simply removing the nonlinearities in the recurrence of the RNN (i.e., using linear recurrences) gives a substantial boost in test accuracy. We motivate this effect in ¬ßE.1 by showing that stacking linear RNN layers and nonlinear MLP blocks (Fig.1) can indeed model complex nonlinear sequence-to-sequence maps without the need for nonlinearities in the recurrence. While dropping the nonlinearity does not seem to harm expressivity, it leads to several advantages, from the ability to directly control how quickly the gradients might vanish or explode, to allowing us to parallelize training. Our findings also partially motivate the success of deep SSMs, where the recurrence is also linear.
- ¬∑ Complex Diagonal Recurrent Matrices. Dense linear RNN layers can be reparameterized to a complex diagonal form without affecting the expressivity of the network or the features at initialization (¬ß3.2). Diagonal linear RNN layers additionally allow for a highly parallelizable unrolling of the recurrence using parallel scans to substantially improve training speeds (Martin and Cundy, 2017). We validate that these observations, which have been leveraged by prior SSMs (Gupta et al., 2022a; Smith et al., 2022), also provide important efficiency improvements for linear RNN layers.
- ¬∑ Stable Exponential Parameterization. In ¬ß3.3 we show that using an exponential parameterization for the diagonal recurrent matrix has important benefits. Crucially, this enables us to easily enforce stability during training, which in turn allows us to modify the initialization distribution to facilitate long-range reasoning and improve performance. Our results indicate that rather than the specific deterministic initializations used by several recent SSMs, it is the eigenvalue distribution of the recurrent layer at initialization that determines if the model can capture long-range reasoning.
- ¬∑ Normalization. In ¬ß3.4 we show that normalizing the hidden activations on the forward pass is important when learning tasks with very long-range dependencies. With this final modification, our RNNs can match the performance of deep SSMs on all tasks in the LRA benchmark. Connecting back to state-space models, we show in ¬ß4 how our normalization can be linked to the discretization structure in S4.

We summarize the deep Linear Recurrent Unit (LRU) architecture used in this paper, and the effect of each of the above steps on performance in Fig.1. We emphasize that the main purpose of our work is not to surpass the performance of S4-based models, but rather to demonstrate that simple RNNs can also achieve strong performance on long range reasoning tasks when properly initialized and parameterized. We believe the insights derived in this paper can be useful to design future architectures, and to simplify existing ones.

## 2. Preliminaries

In this section, we compare the key architectural components (RNNs and SSMs) studied in this work, and also describe our methodology and experimental setup. For a more thorough discussion or related architectures, the reader can check our related work section ¬ßB.

## 2.2. Experimental setup

In this paper, we consider the Long Range Arena benchmark (Tay et al., 2020), a set of tasks designed to test the ability of models to do long-range sequence modelling (except we use coloured images instead of grayscale images for the sequential CIFAR-10 classification task). Transformers fail to perform well on most of these tasks,

while deep SSMs have shown remarkable performance on these tasks (Dao et al., 2022a; Gu et al., 2021a). This makes it an appropriate benchmark to explore the long-range modelling capabilities of deep RNNs.

For all our experiments, we use a network of 6 layers with residual connections and layer/batch normalization (Ba et al., 2016; Ioffe and Szegedy, 2015) similar to Gu et al. (2021a) (Fig.1), and we replace the SSM layers with RNN layers, building up to our LRU recurrence in a sequence of steps (see ¬ß3). All experiments are repeated three times, and we report the mean and standard error. Networks are trained using the AdamW optimizer (Loshchilov and Hutter, 2017). We use a smaller learning rate and no weight decay on the recurrent parameters, as suggested by Gu et al. (2021a); Steil (2004). We tune hyperparameters such as learning rates for all models on a logarithmic grid for best accuracy. See ¬ßD for more details on our experimental setup.

## 5. Conclusion

In this paper, we introduce a new RNN layer called the Linear Recurrent Unit or LRU and show how it can be effectively and efficiently used as core layers of deep sequence models. We provide theoretical insights and extensive ablations on a series of step-by-step modifications of a vanilla RNN-linearization, diagonalization, stable exponential parameterization and normalization-that substantially improve performance, especially on tasks requiring long range reasoning. While our recurrence shares similarities with modern deep SSMs, our design does not rely on discretization of a latent continous-time system or on structured transition matrices. Instead our improvements directly follow from initialization and forward pass analysis arguments standard in the deep learning community, starting from a Glorot-initialized RNNs. Our final model matches the performance of modern deep state-space models (e.g. S4 or S5) on all LRA tasks.

## Supplementary Materials Supplementary Materials

We present here a simplified JAX implementation (Bradbury et al., 2018) of the Linear Recurrent Unit (LRU). The state of the LRU is driven by the input ' ùë¢ùëò ' ùêø ùëò = 1 of sequence length ùêø according to the following formula (and efficiently parallelized using an associative scan): ùë•ùëò = Œõ ùë•ùëò GLYPH<0> 1 , exp ' ùõæ log ' GLYPH<12> ' ùêµùë¢ùëò ' , and the output is computed at each timestamp ùëò as follows: ùë¶ùëò = ùê∂ùë•ùëò , ùê∑ùë¢ùëò . In our code, ùêµGLYPH<148> ùê∂ follow Glorot initialization, with ùêµ scaled additionally by a factor 2 to account for halving the state variance by taking the real part of the output projection. ùê∑ is random ùêª -dimensional and mutiplies elementwise each ùë¢ùëò , where ùëò is the timestamp. Œõ is initialized with the help of Lemma 3.2, with phase potentially restricted to a thin slice (see ¬ß3.4). We present here a simplified JAX implementation (Bradbury et al., 2018) of the Linear Recurrent Unit (LRU). The state of the LRU is driven by the input ( /u1D462/u1D458 ) /u1D43F /u1D458 = 1 of sequence length /u1D43F according to the following formula (and efficiently parallelized using an associative scan): /u1D465 /u1D458 = /uni039B /u1D465 /u1D458 -1 + exp ( /u1D6FE log )/circledot( /u1D435/u1D462/u1D458 ) , and the output is computed at each timestamp /u1D458 as follows: /u1D466/u1D458 = /u1D436/u1D465/u1D458 + /u1D437/u1D462/u1D458 . In our code, /u1D435, /u1D436 follow Glorot initialization, with /u1D435 scaled additionally by a factor 2 to account for halving the state variance by taking the real part of the output projection. /u1D437 is random /u1D43B -dimensional and mutiplies elementwise each /u1D462/u1D458 , where /u1D458 is the timestamp. /uni039B is initialized with the help of Lemma 3.2, with phase potentially restricted to a thin slice (see ¬ß3.4).

## D. Detailed experimental setup

In this section, we describe our experimental details.

## D.1. Architecture

We consider the standard S4 architecture of Gu et al. (2021a) and replace the S4 layers with RNN layers or with S5 (Smith et al., 2022) or S4D (Gu et al., 2022a) layers for our baselines. We give an overview of the architecture used in Fig.1. The input is first encoded into ùêª features, followed by a stack of residual blocks. For all our experiments, we use networks with a depth of 6 residual blocks. Each residual block consists of identity skip connection, and the residual path containing a normalization layer (in our case, we always use batch normalization in our experiments), followed by the RNN/SSM block. While using the 'post-norm' option of adding the normalization layer after the skip and residual branches typically improves performance, we stick to this design due to this architecture being more scalable in general (De and Smith, 2020).

Each RNN/SSM block first contains the recurrent layer as described in Eqs.(1) and (3) in ¬ß2. This is followed by a mixing layer. For all experiments except PathX, we use the GLU activation function (Dauphin et al., 2017) with dropout as the mixing layer, similar to Gu et al. (2021a). For PathX, we instead use a GLU activation function without one additional linear transform; the same as used by Smith et al. (2022) for their experiments.

We use bidirectional models for our experiments on PathFinder and PathX, using a similar setup as Gu et al. (2021a), and use unidirectional models for the rest of our experiments.

Table 7 j Test accuracy of a linear diagonal complex RNNs under different parameterizations of the transition matrix (see ¬ß3.2). Performance directly improves the results in Tb. 1, and showcases the advantage of exponential (polar) representation of Œõ . In bold font is the best parameterization option for linear RNN blocks. Ring Init denotes a changed initialization where ùëü min and ùëü max are tuned. Performance and Text and Retrieval task already aligns with S4 results in the dense setting (c.f. Tb.1 with Tb. 3). No model with able to solve PathX, which requires normalization (see Tb.3).

|                                                                                                 | /s.sc C I FA R   | L/i.sc /s.sc /t.sc O /p.sc /s.sc   | T/e.sc /x.sc /t.sc   | R/e.sc /t.sc /r.sc /i.sc /e.sc /v.sc/a.sc /l.sc   | P/a.sc/t.sc /h.sc /f.sc /i.sc /n.sc /d.sc /e.sc /r.sc   | P/a.sc/t.sc /h.sc X   |
|-------------------------------------------------------------------------------------------------|------------------|------------------------------------|----------------------|---------------------------------------------------|---------------------------------------------------------|-----------------------|
| D/e.sc /n.sc /s.sc /e.sc ùê¥                                                                      | 72.2 (0.2)       | 50.4 (0.2)                         | 89.1 (0.1)           | 89.1 (0.1)                                        | %                                                       | %                     |
| Œõ R/e.sc /a.sc /l.sc + I/m.sc                                                                   | 86.5 (0.1)       | 58.8 (0.3)                         | 87.4 (0.3)           | 87.8 (0.5)                                        | %                                                       | %                     |
| Œõ E/x.sc /p.sc                                                                                  | 85.4 (0.7)       | 60.5 (0.3)                         | 86.5 (0.4)           | 89.4 (0.1)                                        | 65.4 (9.0)                                              | %                     |
| Œõ S/t.sc/a.sc /b.sc /l.sc /e.sc E/x.sc /p.sc                                                    | 87.2 (0.4)       | 59.4 (0.3)                         | 87.6 (0.3)           | 89.1 (0.2)                                        | 93.5 (0.5)                                              | %                     |
| + R/i.sc /n.sc/g.sc I/n.sc /i.sc /t.sc                                                          | 88.1 (0.0)       | 59.4 (0.3)                         | 89.4 (0.1)           | 90.1 (0.1)                                        | 94.4 (0.3)                                              | %                     |
| S4D (/o.sc/u.sc /r.sc /r.sc /e.sc /p.sc /r.sc /o.sc /d.sc /u.sc /c.sc /t.sc /i.sc /o.sc /n.sc ) | 91.5 (0.2)       | 60.2 (0.3)                         | 86.4 (0.0)           | 89.5 (0.0)                                        | 94.2 (0.3)                                              | 97.5 (0.0)            |
| S5 (/o.sc/u.sc /r.sc /r.sc /e.sc /p.sc /r.sc /o.sc /d.sc /u.sc /c.sc /t.sc /i.sc /o.sc /n.sc )  | 88.8 (0.1)       | 58.5 (0.3)                         | 86.2 (0.1)           | 88.9 (0.0)                                        | 95.7 (0.1)                                              | 96.0 (0.1)            |
| S4 (/p.sc/a.sc /p.sc /e.sc /r.sc /r.sc /e.sc /s.sc /u.sc /l.sc/t.sc /s.sc )                     | 91.1             | 59.6                               | 86.8                 | 90.9                                              | 94.2                                                    | 96.4                  |
| S4D-L/e.sc/g.sc S (/p.sc/a.sc /p.sc /e.sc /r.sc /r.sc /e.sc /s.sc /u.sc /l.sc/t.sc /s.sc )      | 89.9             | 60.5                               | 86.2                 | 89.5                                              | 93.1                                                    | 91.9                  |
| S5 (/p.sc/a.sc /p.sc /e.sc /r.sc /r.sc /e.sc /s.sc /u.sc /l.sc/t.sc /s.sc )                     | 90.1             | 62.2                               | 89.3                 | 91.4                                              | 95.3                                                    | 98.6                  |

Table 8 j Effects of normalization on linear diagonal RNNs with stable exponential parameterization (see ¬ß3.4). In bold is our best performing model, and we report the closely matching deep SSM results below. Tunings for our rings are also reported. Results showcase the advantage of taking initialization close to the unit circle under proper ùõæ normalization. For PathX, we initialize eigenvalues to have a phase range of ¬ª 0 GLYPH<148> ùúã GLYPH<157> 10 ‚Ä¶ , for all other tasks we use a range of ¬ª 0 GLYPH<148> 2 ùúã ‚Ä¶ (see ¬ß3.4).

|                                                                                                                   | /s.sc C I FA R   | L/i.sc /s.sc /t.sc O /p.sc /s.sc   | T/e.sc /x.sc /t.sc   | R/e.sc /t.sc /r.sc /i.sc /e.sc /v.sc/a.sc /l.sc   | P/a.sc/t.sc /h.sc /f.sc /i.sc /n.sc /d.sc /e.sc /r.sc   | P/a.sc/t.sc /h.sc X   |
|-------------------------------------------------------------------------------------------------------------------|------------------|------------------------------------|----------------------|---------------------------------------------------|---------------------------------------------------------|-----------------------|
| L/i.sc /n.sc /e.sc /a.sc /r.sc D/e.sc /n.sc /s.sc /e.sc RNN                                                       | 72.2 (0.2)       | 50.4 (0.2)                         | 89.1 (0.1)           | 89.1 (0.1)                                        | %                                                       | %                     |
| D/i.sc /a.sc /g.sc /o.sc /n.sc /a.sc /l.sc C/o.sc/m.sc /p.sc /l.sc /e.sc /x.sc RNN                                | 86.5 (0.1)       | 58.8 (0.3)                         | 87.4 (0.3)           | 87.8 (0.5)                                        | %                                                       | %                     |
| S/t.sc/a.sc /b.sc /l.sc /e.sc E/x.sc /p.sc P/a.sc /r.sc /a.sc /m.sc /w.sc / R/i.sc /n.sc /g.sc I/n.sc /i.sc /t.sc | 88.1 (0.0)       | 59.4 (0.3)                         | 89.4 (0.1)           | 90.1 (0.1)                                        | 94.4 (0.3)                                              | %                     |
| ¬ª ùëü min GLYPH<148> ùëü max ‚Ä¶                                                                                        | [0.9, 0.99]      | [0.0, 1.0]                         | [0.0, 0.9]           | [0.5, 0.9]                                        | [0.9, 0.999]                                            |                       |
| , ùõæ N/o.sc/r.sc /m.sc /a.sc /l.sc /i.sc /z.sc /a.sc/t.sc /i.sc /o.sc /n.sc (LRU)                                  | 89.0 (0.1)       | 60.2 (0.8)                         | 89.4 (0.1)           | 89.9 (0.1)                                        | 95.1 (0.1)                                              | 94.2 (0.4)            |
| ¬ª ùëü min GLYPH<148> ùëü max ‚Ä¶                                                                                        | [0.9, 0.999]     | [0.0, 0.99]                        | [0.5, 0.9]           | [0.5, 0.9]                                        | [0.9, 0.999]                                            | [0.999, 0.9999]       |
| S4D (/o.sc/u.sc /r.sc /r.sc /e.sc /p.sc /r.sc /o.sc /d.sc /u.sc /c.sc /t.sc /i.sc /o.sc /n.sc )                   | 91.5 (0.2)       | 60.2 (0.3)                         | 86.4 (0.0)           | 89.5 (0.0)                                        | 94.2 (0.3)                                              | 97.5 (0.0)            |
| S5 (/o.sc/u.sc /r.sc /r.sc /e.sc /p.sc /r.sc /o.sc /d.sc /u.sc /c.sc /t.sc /i.sc /o.sc /n.sc )                    | 88.8 (0.1)       | 58.5 (0.3)                         | 86.2 (0.1)           | 88.9 (0.0)                                        | 95.7 (0.1)                                              | 96.0 (0.1)            |
| S4 (/p.sc/a.sc /p.sc /e.sc /r.sc /r.sc /e.sc /s.sc /u.sc /l.sc/t.sc /s.sc )                                       | 91.1             | 59.6                               | 86.8                 | 90.9                                              | 94.2                                                    | 96.4                  |
| S4D-L/e.sc/g.sc S (/p.sc/a.sc /p.sc /e.sc /r.sc /r.sc /e.sc /s.sc /u.sc /l.sc/t.sc /s.sc )                        | 89.9             | 60.5                               | 86.2                 | 89.5                                              | 93.1                                                    | 91.9                  |
| S5 (/p.sc/a.sc /p.sc /e.sc /r.sc /r.sc /e.sc /s.sc /u.sc /l.sc/t.sc /s.sc )                                       | 90.1             | 62.2                               | 89.3                 | 91.4                                              | 95.3                                                    | 98.6                  |

## D.2. General experimental details

We use AdamW as our optimizer (Loshchilov and Hutter, 2017). We use warmup for the learning rate, where we start from a value of 10 GLYPH<0> 7 and increase the learning rate linearly up a specified value for the first 10% of training. This is followed by cosine annealing for the rest of training down to a value of 10 GLYPH<0> 7 .

We used a smaller learning rate for the RNN/SSM parameters ùê¥ and ùêµ . When using normalization in our RNNs, we also used a smaller learning rate on the normalization parameter ùõæ . For our S5 and S4D baselines, we used a smaller learning rate for the discretization step size Œî . This smaller learning rate was determined by multiplying the base learning rate by a factor GLYPH<157> 1 (See Tb.9 for the learning rate factor used for each task).

We use weight decay for all parameters except the RNN/SSM parameters ùê¥ and ùêµ (and ùõæ and Œî when applicable).

All experiments were carried out on accelerated hardware A100 GPUs.

