## Abstract

The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, visionlanguage tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models.

## 1. Introduction

Large language models (LLMs) largely promote the development of artificial general intelligence (AGI) systems with their impressive capabilities in open-world language tasks, and their model scale and performance are still increasing at a fast pace. Vision large language models (VLLMs) [3, 5, 21, 23, 34, 92, 115, 147, 187], which leverage LLMs, have also achieved significant breakthroughs, enabling sophisticated vision-language dialogues and interactions. However, the progress of vision and vision-language foundation models, which are also crucial for VLLMs, has lagged behind the rapid growth of LLMs.

To bridge vision models with LLMs, existing VLLMs [5, 81, 131, 177, 187] commonly employ lightweight 'glue' layers, such as QFormer [81] or linear projection [92], to align features of vision and language models. Such alignment contains several limitations: (1) Disparity in parameter scales. The large LLMs [48] now boosts up to 1000 billion parameters, while the widely-used vision encoders of VLLMs are still around one billion. This gap may lead to the under-use of LLM's capacity. (2) Inconsistent representation. Vision models, trained on pure-vision data or

155

135

115

95

75

55

35

Figure 2. Comparison results on various generic visual-linguistic tasks , including image classification, video classification, image-text retrieval, image captioning, and multi-modal dialogue. The proposed InternVL achieves the best performance on all these tasks. Note that only the models trained on public data are included. 'IN' is an abbreviation for ImageNet [38].

<!-- image -->

aligned with the BERT series [39, 70, 93], often exhibit representation inconsistencies with LLMs. (3) Inefficient connection. The 'glue' layers are usually lightweight and randomly initialized, which may not capture the rich crossmodal interactions and dependencies that are crucial for multi-modal understanding and generation.

These limitations reveal a large gap in both parameter scale and feature representation ability between the vision encoder and the LLM. To bridge this gap, our inspiration lies in elevating the vision encoder to align with the parameter scale of the LLM and subsequently harmonizing their representations. However, the training of such large-scale models necessitates a vast amount of image-text data obtained from the Internet. The significant heterogeneity and quality variations within this data pose considerable challenges to the training process. To enhance the efficacy of the training, generative supervision is considered as a complementary approach to contrastive learning, as depicted in Figure 1. This strategy aims to provide additional guidance to the model during training. Yet, the suitability of lowquality data for generative training remains a concern. Besides, how to effectively represent the users' commands and align the representations between the vision encoder and LLM is another open question.

To address these issues, we formulate the InternVL, a large-scale vision-language foundation model, which aligns the representation of the scaled-up vision encoder with the LLM and achieves state-of-the-art performance on various visual and vision-language tasks. As shown in Figure 1 (c), InternVL has three key designs: (1) Parameter-balanced vision and language components : It includes a vision encoder scaled up to 6 billion parameters and an LLM middleware with 8 billion parameters, where the middleware functions as a substantial 'glue' layer to reorganize visual features based on user commands. Unlike prior vision-only (Figure 1 (a)) or dual-tower (Figure 1 (b)) structures, our vision encoder and middleware offer flexible combinations for both contrastive and generative tasks. (2) Consistent representations : To maintain the consistency of represen-

tations between the vision encoder and LLM, we employ a pre-trained multilingual LLaMA [32], to initialize the middleware and align the vision encoder with it. (3) Progressive image-text alignment : We leverage image-text data from diverse sources, ensuring training stability through a progressive alignment strategy. This strategy initiates contrastive learning on large-scale noisy image-text data and subsequently transitions to generative learning on fine-grained data. This approach ensures a consistent enhancement of model performance and task scope.

These designs endow our model with several advantages: (1) Versatile. It functions as a standalone vision encoder for perception tasks, or collaborates with the language middleware for vision-language tasks and multi-modal dialogue systems. The language middleware bridges the gap between the vision encoder and the LLM decoder. (2) Strong. By leveraging the training strategy, large-scale parameters, and web-scale data, our model has a powerful representation that helps to achieve state-of-the-art results on various vision and vision-language tasks, as shown in Figure 2. (3) LLM-friendly. Due to the aligned feature space with LLMs, our model can smoothly integrate with existing LLMs, such as LLaMA series [138, 139], Vicuna [184], and InternLM [135]. These features distinguish our model from the previous approaches and establish a leading visionlanguage foundation model for various applications.

In summary, our contribution has three folds:

(1) We present a large-scale vision-language foundation model-InternVL, which aligns the large-scale vision encoder with LLMs for the first time. The model demonstrates strong performance on a wide range of generic visuallinguistic tasks, including visual perception tasks, visionlanguage tasks, and multi-modal dialogue.

(2) We introduce a progressive image-text alignment strategy for the efficient training of large-scale visionlanguage foundation models. This strategy maximizes the utilization of web-scale noisy image-text data for contrastive learning and fine-grained, high-quality data for generative learning.

(3) We extensively compare the proposed model with the current state-of-the-art vision foundation models and VLLMs. The results indicate that InternVL achieves leading performance on a broad range of generic visuallinguistic tasks, including image classification (ImageNet), semantic segmentation (ADE20K), video classification (Kinetics), image-text retrieval (Flickr30K & COCO), videotext retrieval (MSR-VTT), and image captioning (COCO & Flickr30K & NoCaps). Meanwhile, it is also effective for multi-modal dialogue (MME & POPE & Tiny LVLM).

## 3. Proposed Method



## 3.1. Overall Architecture

As depicted in Figure 3, unlike traditional vision-only backbones [57, 94, 148] and dual-encoder models [67, 117, 130], the proposed InternVL is designed with a vision encoder InternViT-6B and a language middleware QLLaMA. Specifically, InternViT-6B is a vision transformer with 6 billion parameters, customized to achieve a favorable tradeoff between performance and efficiency. QLLaMA is a language middleware with 8 billion parameters, initialized with a multilingual-enhanced LLaMA [32]. It could provide robust multilingual representation for image-text contrastive learning, or serve as a bridge to connect the vision encoder and the off-the-shelf LLM decoder.

To align the two large-scale components with substantial gaps in modalities and structures, we introduce a progressive alignment training strategy. The training strategy is conducted progressively, beginning with contrastive learning on large-scale noisy data, and gradually moving towards generative learning on exquisite and high-quality

Figure 3. The training strategy of the proposed InternVL model. It consists of three progressive stages, including vision-language contrastive training, vision-language generative training, and supervised fine-tuning. These stages effectively leverage public data from diverse sources, ranging from noisy image-text pairs on the web to high-quality caption, VQA, and multi-modal dialogue datasets.

<!-- image -->

Table 1. Architecture details of the InternViT-6B model.

| name                |   width |   depth |   MLP |   #heads |   #param (M) |
|---------------------|---------|---------|-------|----------|--------------|
| ViT-G [173]         |    1664 |      48 |  8192 |       16 |         1843 |
| ViT-e [23]          |    1792 |      56 | 15360 |       16 |         3926 |
| EVA-02-ViT-E [130]  |    1792 |      64 | 15360 |       16 |         4400 |
| ViT-6.5B [128]      |    4096 |      32 | 16384 |       32 |         6440 |
| ViT-22B [37]        |    6144 |      48 | 24576 |       48 |        21743 |
| InternViT-6B (ours) |    3200 |      48 | 12800 |       25 |         5903 |

data. In this way, we ensure the effective organization and full utilization of web-scale image-text data from a variety of sources. Then, equipped with the aligned vision encoder and language middleware, our model functions like a Swiss Army knife. It boasts a flexible composition that can be adapted for a wide array of generic visual-linguistic tasks. These tasks range from visual perception and image/videotext retrieval to image captioning, visual question answering, and multi-modal dialogue, among others.

## 4.1. Implementation Details

Stage 1. In this stage, the image encoder InternViT-6B is randomly initialized [7], and the text encoder LLaMA-7B is initialized with the pre-trained weights from [32]. All parameters are fully trainable.

Stage 2. In this stage, InternViT-6B and QLLaMA inherit their weights from the first stage, while the new learnable queries and cross-attention layers in QLLaMA are randomly initialized. Benefiting from the powerful representations learned in the first stage, we keep both InternViT-6B and QLLaMA frozen and only train the new parameters.

Stage 3. At this stage, we have two different configurations. One is to use InternViT-6B separately, as shown in Figure 4 (c). The other is to use the entire InternVL model simultaneously, as shown in Figure 4 (d). More details will be provided in the supplementary materials.

## 4.5. Ablation Study

Hyperparameters of InternViT-6B. As discussed in Section 3.2, we explored variations in model depth { 32, 48, 64, 80 } , head dimension { 64, 128 } , and MLP ratio { 4, 8 } , resulting in 16 distinct models. In selecting the optimal model, we initially narrowed down our focus to 6 models, chosen based on their throughput, as listed in Table 11. These models underwent further evaluation using contrastive learning on a 100M subset of LAION-en [120] over 10K iterations. For the experimental setup, the primary difference was the use of a randomly initialized text encoder from CLIP-L [117], in order to speed up the training. For the sake of accuracy, inference speed, and training stability, we ultimately chose variant 3 as the final InternViT-6B.

Consistency of Feature Representation. In this study, we validate the consistency of the feature representation of InternVL with off-the-shelf LLMs. We adopt a minimalist setting, i.e . conducting a single-stage SFT using only the LLaVA-Mix-665K [85] dataset. Moreover, only the MLP layers are trainable, thereby confirming the inherent alignment level among features from various vision foundation models and LLMs. The results are shown in Table 12. We observed that compared to EVA-E [130], our InternViT-6B achieves better performance under this simple setup. Additionally, it is noteworthy that performance across all three tasks saw significant improvement when using QLLaMA as the 'glue layer'. These significant improvements clearly delineate that the feature representation of InternVL is more consistent with the off-the-shelf LLM.

## 5. Conclusion

In this paper, we present InternVL, a large-scale visionlanguage foundation model that scales up the vision foundation model to 6 billion parameters and is aligned for generic visual-linguistic tasks. Specifically, we design a largescale vision foundation model InternViT-6B, progressively align it with an LLM-initialized language middleware QLLaMA, and leverage web-scale image-text data from various sources for efficient training. It bridges the gap between vision foundation models and LLMs, and demonstrates proficiency in a wide range of generic visual-linguistic tasks, such as image/video classification, image/video-text retrieval, image captioning, visual question answering, and multi-modal dialogue. We hope this work could contribute to the development of the VLLM community.

## A. Supplementary Materials



## A.2. More Ablation Studies

Compatibility with Other LLM. In this experiment, we test the compatibility of InternVL with LLMs other than Vicuna [184]. The experimental setup used here is the same as in Table 9 of the main paper. As shown in Table

<!-- image -->

Table 16. Comparison of zero-shot image classification performance on 20 other datasets. These results indicate that, in addition to ImageNet [38], InternVL also possesses good generalization capabilities in zero-shot image classification across various domains.

| method               | CIF AR-10 [ 74   | CIF AR-100 [ 74   | MNIST [ 78 ]   | Caltech-101 [ 49   | SUN397 [ 157 ]   | FGVC Aircraft   | Country-211 [   | Stanford Cars   | DTD [ 28 ]   | Eurosat [ 59 ]   | FER2013 [ 52 ]   | [    | Flo wers-102   | F ood-101 [ 13 ] GTSRB [ 129   | ]         | Pets [ 113 ] Rendered   | SST2   | Resisc45 [ 27 ] STL10 [ 30 ]   |      | V OC2007 [ 45 ]   | a vg. top-1 acc.   |
|----------------------|------------------|-------------------|----------------|--------------------|------------------|-----------------|-----------------|-----------------|--------------|------------------|------------------|------|----------------|--------------------------------|-----------|-------------------------|--------|--------------------------------|------|-------------------|--------------------|
| OpenAI CLIP-L+ [117] | 94.9             | 74.4              | 79.0           | 87.2               | 68.7             | 33.4            | 34.5            | 79.3            | 56.0         | 61.5             | 49.1             | 78.6 | 93.9           | 52.4                           | 93.8      | 70.7                    |        | 65.4                           | 99.4 | 78.1              | 69.6               |
| EVA-01-CLIP-g [130]  | 98.3             | 88.7              | 62.3           | 87.7               | 74.2             | 32.4            | 28.6            | 91.7            | 61.3         | 73.6             | 52.2             | 74.5 | 93.5           | 49.1                           | 94.2      |                         | 58.4   | 70.3                           | 98.9 | 83.2              | 71.2               |
| OpenCLIP-g [67]      |                  |                   |                |                    |                  | 44.6            | 30.9            | 94.0            |              |                  |                  |      |                |                                |           |                         | 56.7   | 69.6                           | 98.9 | 81.6              | 72.5               |
|                      | 98.2             | 84.7              | 71.9           | 88.1               | 74.1             |                 |                 |                 | 68.7         | 64.7             | 55.8             | 81.0 | 92.4           | 49.7                           |           | 93.9                    | 64.3   | 70.5                           | 98.5 | 77.7              | 73.2               |
| OpenCLIP-H [67]      | 97.4             | 84.7              | 72.9           | 85.0               | 75.2             | 42.8            | 30.0            | 93.5            | 67.8         | 72.7             | 52.0             | 80.1 |                | 92.7                           | 58.4 57.6 | 94.5                    | 64.6   | 69.8                           | 99.7 | 82.7              | 72.6               |
| EVA-02-CLIP-L+ [130] | 98.9             | 89.8              | 64.3           | 89.5               | 74.8             | 37.5            | 33.6            | 91.6            | 64.5         | 71.4             | 51.0             | 77.2 |                | 94.2                           | 94.2      |                         |        | 71.4                           | 99.5 |                   |                    |
| EVA-01-CLIP-g+ [130] | 99.1             | 90.1              | 71.8           | 88.1               | 74.3             | 39.4            | 30.8            | 90.7            | 67.3         | 73.2             | 56.0             | 79.7 | 93.7           | 66.5                           | 94.8      |                         | 58.6   |                                |      | 82.9              | 74.0               |
| OpenCLIP-G [67]      | 98.2             | 87.5              | 71.6           | 86.4               | 74.5             | 49.7            | 33.8            | 94.5            | 69.0         | 70.0             | 59.5             | 81.5 | 93.1           | 62.5                           | 95.2      |                         | 65.2   | 72.6                           | 98.5 | 80.7              | 74.9               |
| EVA-02-CLIP-E [130]  | 99.3             | 92.5              | 76.7           | 89.0               | 76.5             | 47.9            | 34.7            | 94.4            | 68.2         | 77.6             | 55.1             | 82.5 | 95.2           |                                | 67.1      | 95.6                    | 61.1   | 73.5                           | 99.2 | 83.0              | 76.3               |
| EVA-02-CLIP-E+ [130] | 99.3             | 93.1              | 74.7           | 90.5               | 75.1             | 54.1            | 35.7            |                 | 68.2         | 75.8             | 58.6             | 84.5 | 94.9           |                                |           | 95.8                    |        | 75.6                           | 99.2 | 85.6              | 77.1               |
| InternVL-C (ours)    | 99.4             | 93.2              | 80.6           | 89.5               | 76.0             | 52.7            | 34.1            | 94.6 94.2       | 70.7         | 79.4             | 56.2             | 86.1 |                | 67.7 95.3                      | 65.5      | 96.0 67.9               | 61.4   | 74.2                           | 99.5 | 80.0              | 78.1               |

Table 17. Evaluation of Tiny LVLM test set. Here we report five categories of multimodal capabilities, including visual reasoning (VR), visual perception (VP), visual knowledge acquisition (VKA), visual commonsense (VC), and object hallucination (OH).

| method                               | LLM                                 |           |      |           |   VR VP VKA VC OH Overall |
|--------------------------------------|-------------------------------------|-----------|------|-----------|---------------------------|
| MiniGPT-4 [187]                      | Vicuna-7B                           | 37.6 37.8 | 17.6 | 49.0 50.7 |                     192.6 |
| LLaVA [92]                           | Vicuna-7B                           | 41.6 38.3 | 18.7 | 49.4 49.0 |                     197   |
| VisualGLM [44]                       | ChatGLM-6B 37.3 36.3 46.9 37.6 54.0 |           |      |           |                     211.9 |
| Otter [79]                           | Otter-9B                            | 41.6 37.0 | 15.1 | 52.4 74.0 |                     216.4 |
| LLaMA-Adapter-V2 [51]                | LLaMA-7B                            | 43.5 46.8 | 22.3 | 56.0 60.7 |                     229.2 |
| Lynx [172]                           | Vicuna-7B                           | 52.2 65.8 | 17.6 | 57.4 86.3 |                     279.2 |
| BLIP-2 [81]                          | FlanT5xl                            | 44.9 49.0 | 64.1 | 44.0 82.7 |                     284.7 |
| InstructBLIP [34]                    | Vicuna-7B                           | 46.7 48.0 | 61.7 | 59.2 85.0 |                     300.6 |
| LLaVA-1.5 [91]                       | Vicuna-7B                           | 55.6 49.0 | 57.0 | 57.2 88.3 |                     307.2 |
| Qwen-VL-Chat [5]                     | Qwen-7B                             | 62.4 54.5 | 55.1 | 54.8 90.0 |                     316.8 |
| Bard [53]                            | Bard                                | 64.2 57.0 | 68.1 | 59.6 70.7 |                     319.6 |
| InternLM-XComposer [177] InternLM-7B |                                     | 55.8 53.8 | 64.1 | 61.8 87.0 |                     322.5 |
| InternVL-Chat (ours)                 | Vicuna-13B                          | 56.4 52.3 | 68.0 | 62.0 89.0 |                     327.6 |

- 18, InternLM-7B [135] achieves slightly better performance than Vicuna-7B [184]. This indicates that our InternVL exhibits promising compatibility with various LLMs.

Efficiency Analysis. In this study, we analyze the computational efficiency of InternVL in encoding image-text pairs. The entire encoding process consists of two parts: image encoding and text encoding. The analysis covered two models (InternVL-C and InternVL-G) and their performance across three different image sizes (224, 336, and 448). The results are shown in Table 19.

From these results, we find that: (1) As the image size increases, the encoding time also significantly increases, leading directly to a decrease in frame rate; (2) InternVL-G slightly increased the encoding time due to the introduction of QLLaMA for secondary image encoding, but it still maintains a reasonable frame rate across all image sizes; (3) Even though we scale up the text encoder, the additional cost of text encoding is not significant, as the main time expenditure lies in image encoding. In summary, when choosing between InternVL-C and InternVL-G, one should weigh the trade-off between computational efficiency and

Table 18. Compatibility with other LLM. Here we use InternLM [135] as an example to verify the compatibility of InternVL with LLMs other than Vicuna [184]. The experimental settings used here are the same as in Table 9 of the main paper.

| visual   | glue   | LLM             | visual question answering   | visual question answering   | visual question answering   | visual question answering   | dialogue   | dialogue   |
|----------|--------|-----------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|------------|------------|
| encoder  | layer  |                 | VQA v2                      |                             | GQA VizWiz VQA              | T                           |            | MME POPE   |
| IViT-6B  |        | MLP Vicuna-7B   | 79.3                        | 62.9                        | 52.5                        | 57.0                        | 1525.1     | 86.4       |
| IViT-6B  |        | MLP InternLM-7B | 79.7                        | 63.2                        | 53.1                        | 58.0                        | 1532.8     | 86.4       |

Table 19. Efficiency analysis of InternVL for encoding imagetext pairs. The total time to encode an image-text pair includes both the image encoding part and the text encoding part. We measure the time cost with a batch size of 128 on a single A100 GPU. Flash Attention [35] and bf16 precision are used during testing.

| method     | image   | encode image (ms)   | encode image (ms)   | encode text (ms)   | total   | FPS   |
|------------|---------|---------------------|---------------------|--------------------|---------|-------|
|            | size    | InternViT-6B        | QLLaMA              | QLLaMA             | time    |       |
| InternVL-C | 224     | 15.5                | -                   | 4.9                | 20.4    | 48.9  |
| InternVL-C | 336     | 35.2                | -                   | 4.9                | 40.1    | 24.9  |
| InternVL-C | 448     | 66.9                | -                   | 4.9                | 71.8    | 13.9  |
| InternVL-G | 224     | 15.5                | 8.2                 | 4.9                | 28.6    | 35.0  |
| InternVL-G | 336     | 35.2                | 10.3                | 4.9                | 50.4    | 19.8  |
| InternVL-G | 448     | 66.9                | 12.8                | 4.9                | 84.6    | 11.8  |

potential performance improvements based on specific requirements. Additionally, these results were measured using PyTorch with Flash Attention [35] and bf16 precision, and there is still considerable room for optimization, such as using model quantization and TensorRT.

