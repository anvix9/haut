## Abstract

Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational ine/fficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of e/fficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpli/fied end-to-end neural network architecture without attention or even MLP blocks ( Mamba ). Mamba enjoys fast inference (5 × higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.

## 1 Introduction

Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an e/ffective paradigm in modern machine learning. The backbone of these FMs are often sequence models , operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The e/fficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a /finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more e/fficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it e/ffective. As of yet, none of these variants have been shown to be empirically e/ffective at scale across domains.

Recently, structured state space sequence models (SSMs) (Gu, Goel, and Ré 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very e/fficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range

Arena (Tay, Dehghani, Abnar, et al. 2021). Many /flavors of SSMs (Gu, Goel, and Ré 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less e/ffective at modeling discrete and information-dense data such as text.

We propose a new class of selective state space models , that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.

Selection Mechanism. First, we identify a key limitation of prior models: the ability to e/fficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to /filter out irrelevant information and remember relevant information inde/finitely.

Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally e/fficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between di/fferent levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3 × faster on A100 GPUs).

Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design ( Mamba ) incorporating selective state spaces.

Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and e/fficiency together yield performance improvements on real data up to sequence length 1M.

We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspeci/fic task performance, on several types of modalities and settings:

- · Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions inde/finitely long ( > 1M tokens).
- · Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences .
- · Language Modeling. Mamba is the /first linear-time sequence model that truly achieves Transformer-quality performance , both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5 × generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).

Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba .

## Selective State Space Model

with Hardware-aware State Expansion

𝐴

Figure 1: ( Overview .) Structured SSMs independently map each channel (e.g. 𝐷 = 5) of an input 𝑥 to output 𝑦 through a higher dimensional latent state ℎ (e.g. 𝑁 = 4). Prior SSMs avoid materializing this large e/ffective state ( 𝐷𝑁 , times batch size 𝐵 and sequence length 𝐿 ) through clever alternate computation paths requiring time-invariance: the ( Δ , 𝑨 , 𝑩 , 𝑪 ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more e/fficient levels of the GPU memory hierarchy.

<!-- image -->

1-dimensional function or sequence 𝑥 ( 𝑡 ) ∈ R ↦→ 𝑦 ( 𝑡 ) ∈ R through an implicit latent state ℎ ( 𝑡 ) ∈ R 𝑁 .

Concretely, S4 models are de/fined with four parameters ( Δ , 𝑨 , 𝑩 , 𝑪 ) , which de/fine a sequence-to-sequence transformation in two stages.

ℎ ' ( 𝑡 ) = 𝑨 ℎ ( 𝑡 ) + 𝑩 𝑥 ( 𝑡 ) (1a) ℎ 𝑡 = 𝑨 ℎ 𝑡 -1 + 𝑩 𝑥 𝑡 (2a) (2b) 𝑲 = ( 𝑪𝑩 , 𝑪𝑨𝑩 , . . . , 𝑪𝑨 𝑘 𝑩 , . . . )

(3a)

𝑦 = 𝑥 ∗ 𝑲 (3b)

Discretization. The /first stage transforms the 'continuous parameters' ( Δ , 𝑨 , 𝑩 ) to 'discrete parameters' ( 𝑨 , 𝑩 ) through /fixed formulas 𝑨 = 𝑓 𝐴 ( Δ , 𝑨 ) and 𝑩 = 𝑓 𝐵 ( Δ , 𝑨 , 𝑩 ) , where the pair ( 𝑓 𝐴 , 𝑓 𝐵 ) is called a discretization rule . Various rules can be used such as the zero-order hold (ZOH) de/fined in equation (4).

𝑨 = exp ( Δ 𝑨 ) 𝑩 = ( Δ 𝑨 ) -1 ( exp ( Δ 𝑨 ) -𝑰 ) · Δ 𝑩 (4)

Discretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from a mechanical point of view discretization can simply be viewed as the /first step of the computation graph in the forward pass of an SSM. Alternate /flavors of SSMs can bypass the discretization step and parameterize ( 𝑨 , 𝑩 ) directly instead (Zhang et al. 2023), which may be easier to reason about.

Computation. After the parameters have been transformed from ( Δ , 𝑨 , 𝑩 , 𝑪 ) ↦→ ( 𝑨 , 𝑩 , 𝑪 ) , the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3).

Commonly, the model uses the convolutional mode (3) for e/fficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for e/fficient autoregressive inference (where the inputs are seen one timestep at a time).

Linear Time Invariance (LTI). An important property of equations (1) to (3) is that the model's dynamics are constant through time. In other words ( Δ , 𝑨 , 𝑩 , 𝑪 ) , and consequently ( 𝑨 , 𝑩 ) as well, are /fixed for all time-steps. This property is

called linear time invariance (LTI) , which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models.

Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental e/fficiency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the e/fficiency bottlenecks.

Structure and Dimensions. Finally, we note that structured SSMs are so named because computing them e/fficiently also requires imposing structure on the 𝑨 matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.

In this case, the 𝑨 ∈ R 𝑁 × 𝑁 , 𝑩 ∈ R 𝑁 × 1 , 𝑪 ∈ R 1 × 𝑁 matrices can all be represented by 𝑁 numbers. To operate over an input sequence 𝑥 of batch size 𝐵 and length 𝐿 with 𝐷 channels, the SSM is applied independently to each channel. Note that in this case, the total hidden state has dimension 𝐷𝑁 per input, and computing it over the sequence length requires 𝑂 ( 𝐵𝐿𝐷𝑁 ) time and memory; this is the root of the fundamental e/fficiency bottleneck addressed in Section 3.3.

General State Space Models. Wenote that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in di/fferent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman /filters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).

Throughout this entire paper we use the term 'SSM' to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and Ré 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.

SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.

- · Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM.
- · H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer.
- · Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021).
- · RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.
- · RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main 'WKV' mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs.

Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM.

## 3.4 A Simpli/fied SSM Architecture

As with structured SSMs, selective SSMs are standalone sequence transformations that can be /flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention.

This architecture involves expanding the model dimension 𝐷 by a controllable expansion factor 𝐸 . For each block, most of the parameters (3 𝐸𝐷 2 ) are in the linear projections (2 𝐸𝐷 2 for input projections, 𝐸𝐷 2 for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for Δ , 𝑩 , 𝑪 , and the matrix 𝑨 ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always /fix to 𝐸 = 2 in our experiments and use two stacks of the block to match the 12 𝐷 2 parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular 'SwiGLU' variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023). Finally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)), motivated by RetNet's usage of a normalization layer in a similar location (Y. Sun et al. 2023).

## 3.6 Additional Model Details

Real vs. Complex. Most prior SSMs use complex numbers in their state ℎ , which is necessary for strong performance on many tasks in perceptual modalities (Gu, Goel, and Ré 2022). However, it has been empirically observed that completely real-valued SSMs seem to work /fine, and possibly even better, in some settings (Ma et al. 2023). We use real values as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeo/ff is related to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g. audio, video) but not discrete (e.g. text, DNA).

Initialization. Most prior SSMs also suggest special initializations, particularly in the complex-valued case, which can help in several settings such as low-data regimes. Our default initialization for the complex case is S4D-Lin and for the real case is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu, Dao, et al. 2020). These de/fine the 𝑛 -th element of 𝑨 as -1 / 2 + 𝑛𝑖 and -( 𝑛 + 1 ) respectively. However, we expect many initializations to work /fine, particularly in the large-data and real-valued SSM regimes; some ablations are considered in Section 4.6.

Parameterization of Δ . We de/fined the selective adjustment to Δ as 𝑠 Δ ( 𝑥 ) = Broadcast 𝐷 ( Linear 1 ( 𝑥 )) , which was motivated by the mechanics of Δ (Section 3.5). We observe that it can be generalized from dimension 1 to a larger dimension R . We set this to be a small fraction of D , which uses a negligible number of parameters compared to the main Linear projections in the block. We additionally note that the broadcasting operation can instead be viewed as another Linear projection, initialized to a speci/fic pattern of 1's and 0's; if this projection is trainable, this leads to the alternative 𝑠 Δ ( 𝑥 ) = Linear 𝐷 ( Linear 𝑅 ( 𝑥 )) , which can be viewed as a low-rank projection.

In our experiments, the Δ parameter (which can be viewed as a bias term) is initialized to 𝜏 -1 Δ ( Uniform ([ 0 . 001 , 0 . 1 ])) , following prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023).

Remark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models , because they are S4 models with a selection mechanism and computed with a scan .

## 4.2.2 Downstream Evaluations

Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV was trained with context length 1024.)

Table 3: ( Zero-shot Evaluations .) Best results for each size in bold. We compare against open source LMs with various tokenizers, trained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches baselines at twice the model size.

| M/o.sc/d.sc/e.sc/l.sc   | T/o.sc/k.sc/e.sc/n.sc.   | P/i.sc/l.sc/e.sc /p.sc/p.sc/l.sc ↓   |   LAMBADA /p.sc/p.sc/l.sc ↓ |   LAMBADA /a.sc/c.sc/c.sc ↑ |   H/e.sc/l.sc/l.sc/a.scS/w.sc/a.sc/g.sc /a.sc/c.sc/c.sc ↑ |   PIQA /a.sc/c.sc/c.sc ↑ |   A/r.sc/c.sc/hyphen.scE /a.sc/c.sc/c.sc ↑ |   A/r.sc/c.sc/hyphen.scC /a.sc/c.sc/c.sc ↑ |   W/i.sc/n.sc/o.scG/r.sc/a.sc/n.sc/d.sc/e.sc /a.sc/c.sc/c.sc ↑ |   A/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc /a.sc/c.sc/c.sc ↑ |
|-------------------------|--------------------------|--------------------------------------|-----------------------------|-----------------------------|-----------------------------------------------------------|--------------------------|--------------------------------------------|--------------------------------------------|----------------------------------------------------------------|-----------------------------------------------------|
| Hybrid H3-130M          | GPT2                     | -                                    |                       89.48 |                       25.77 |                                                      31.7 |                     64.2 |                                       44.4 |                                       24.2 |                                                           50.6 |                                                40.1 |
| Pythia-160M             | NeoX                     | 29.64                                |                       38.1  |                       33    |                                                      30.2 |                     61.4 |                                       43.2 |                                       24.1 |                                                           51.9 |                                                40.6 |
| Mamba-130M              | NeoX                     | 10.56                                |                       16.07 |                       44.3  |                                                      35.3 |                     64.5 |                                       48   |                                       24.3 |                                                           51.9 |                                                44.7 |
| Hybrid H3-360M          | GPT2                     | -                                    |                       12.58 |                       48    |                                                      41.5 |                     68.1 |                                       51.4 |                                       24.7 |                                                           54.1 |                                                48   |
| Pythia-410M             | NeoX                     | 9.95                                 |                       10.84 |                       51.4  |                                                      40.6 |                     66.9 |                                       52.1 |                                       24.6 |                                                           53.8 |                                                48.2 |
| Mamba-370M              | NeoX                     | 8.28                                 |                        8.14 |                       55.6  |                                                      46.5 |                     69.5 |                                       55.1 |                                       28   |                                                           55.3 |                                                50   |
| Pythia-1B               | NeoX                     | 7.82                                 |                        7.92 |                       56.1  |                                                      47.2 |                     70.7 |                                       57   |                                       27.1 |                                                           53.5 |                                                51.9 |
| Mamba-790M              | NeoX                     | 7.33                                 |                        6.02 |                       62.7  |                                                      55.1 |                     72.1 |                                       61.2 |                                       29.5 |                                                           56.1 |                                                57.1 |
| GPT-Neo 1.3B            | GPT2                     | -                                    |                        7.5  |                       57.2  |                                                      48.9 |                     71.1 |                                       56.2 |                                       25.9 |                                                           54.9 |                                                52.4 |
| Hybrid H3-1.3B          | GPT2                     | -                                    |                       11.25 |                       49.6  |                                                      52.6 |                     71.3 |                                       59.2 |                                       28.1 |                                                           56.9 |                                                53   |
| OPT-1.3B                | OPT                      | -                                    |                        6.64 |                       58    |                                                      53.7 |                     72.4 |                                       56.7 |                                       29.6 |                                                           59.5 |                                                55   |
| Pythia-1.4B             | NeoX                     | 7.51                                 |                        6.08 |                       61.7  |                                                      52.1 |                     71   |                                       60.5 |                                       28.5 |                                                           57.2 |                                                55.2 |
| RWKV-1.5B               | NeoX                     | 7.70                                 |                        7.04 |                       56.4  |                                                      52.5 |                     72.4 |                                       60.5 |                                       29.4 |                                                           54.6 |                                                54.3 |
| Mamba-1.4B              | NeoX                     | 6.80                                 |                        5.04 |                       64.9  |                                                      59.1 |                     74.2 |                                       65.5 |                                       32.8 |                                                           61.5 |                                                59.7 |
| GPT-Neo 2.7B            | GPT2                     | -                                    |                        5.63 |                       62.2  |                                                      55.8 |                     72.1 |                                       61.1 |                                       30.2 |                                                           57.6 |                                                56.5 |
| Hybrid H3-2.7B          | GPT2                     | -                                    |                        7.92 |                       55.7  |                                                      59.7 |                     73.3 |                                       65.6 |                                       32.3 |                                                           61.4 |                                                58   |
| OPT-2.7B                | OPT                      | -                                    |                        5.12 |                       63.6  |                                                      60.6 |                     74.8 |                                       60.8 |                                       31.3 |                                                           61   |                                                58.7 |
| Pythia-2.8B             | NeoX                     | 6.73                                 |                        5.04 |                       64.7  |                                                      59.3 |                     74   |                                       64.1 |                                       32.9 |                                                           59.7 |                                                59.1 |
| RWKV-3B                 | NeoX                     | 7.00                                 |                        5.24 |                       63.9  |                                                      59.6 |                     73.7 |                                       67.8 |                                       33.1 |                                                           59.6 |                                                59.6 |
| Mamba-2.8B              | NeoX                     | 6.22                                 |                        4.23 |                       69.2  |                                                      66.1 |                     75.2 |                                       69.7 |                                       36.3 |                                                           63.5 |                                                63.3 |
| GPT-J-6B                | GPT2                     | -                                    |                        4.1  |                       68.3  |                                                      66.3 |                     75.4 |                                       67   |                                       36.6 |                                                           64.1 |                                                63   |
| OPT-6.7B                | OPT                      | -                                    |                        4.25 |                       67.7  |                                                      67.2 |                     76.3 |                                       65.6 |                                       34.9 |                                                           65.5 |                                                62.9 |
| Pythia-6.9B             | NeoX                     | 6.51                                 |                        4.45 |                       67.1  |                                                      64   |                     75.2 |                                       67.3 |                                       35.5 |                                                           61.3 |                                                61.7 |
| RWKV-7.4B               | NeoX                     | 6.31                                 |                        4.38 |                       67.2  |                                                      65.5 |                     76.1 |                                       67.8 |                                       37.5 |                                                           61   |                                                62.5 |

## 4.6 Model Ablations

We perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with size ≈ 350M models at Chinchilla token counts (same setting as Figure 4).

## 4.6.1 Architecture

- · Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.
- · Replacing the complex-valued S4 variant from previous work with a real-valued one does not a/ffect performance much, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware e/fficiency.
- · Replacing any of these with a selective SSM (S6) signi/ficantly improves performance, validating the motivation of Section 3.

Table 6: ( Ablations: Architecture and SSM layer .) The Mamba block performs similarly to H3 while being simpler. In the inner layer, there is little di/fference among di/fferent parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More speci/fically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.

| M/o.sc/d.sc/e.sc/l.sc   | A/r.sc/c.sc/h.sc.   | SSM L/a.sc/y.sc/e.sc/r.sc   | P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc   |
|-------------------------|---------------------|-----------------------------|--------------------------------------------------|
| Hyena                   | H3                  | Hyena                       | 10 . 24                                          |
| H3                      | H3                  | S4 (complex)                | 10 . 30                                          |
| -                       | H3                  | S4 (real)                   | 10 . 34                                          |
| -                       | H3                  | S6                          | 8 . 95                                           |

Table 7: ( Ablations: Selective parameters .) Δ is the most important parameter (Theorem 1), but using multiple selective parameters together synergizes.

| M/o.sc/d.sc/e.sc/l.sc   | A/r.sc/c.sc/h.sc.   | SSM L/a.sc/y.sc/e.sc/r.sc   | P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc   |
|-------------------------|---------------------|-----------------------------|--------------------------------------------------|
| -                       | Mamba               | Hyena                       | 10 . 75                                          |
| -                       | Mamba               | S4 (complex)                | 10 . 54                                          |
| -                       | Mamba               | S4 (real)                   | 10 . 56                                          |
| Mamba                   | Mamba               | S6                          | 8 . 69                                           |

Table 8: ( Ablations: Parameterization of 𝑨 .) The more standard initializations based on S4D-Lin (Gu, Gupta, et al. 2022) perform worse than S4D-Real or a random initialization, when the SSM is selective.

| S/e.sc/l.sc/e.sc/c.sc/t.sc/i.sc/v.sc/e.sc Δ   | S/e.sc/l.sc/e.sc/c.sc/t.sc/i.sc/v.sc/e.sc 𝑩   | S/e.sc/l.sc/e.sc/c.sc/t.sc/i.sc/v.sc/e.sc 𝑪   |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|-----------------------------------------------|-----------------------------------------------|-----------------------------------------------|--------------------------------------------------|
| ✗                                             | ✗                                             | ✗                                             |                                            10.93 |
| ✗                                             | ✓                                             | ✗                                             |                                            10.15 |
| ✗                                             | ✗                                             | ✓                                             |                                             9.98 |
| ✓                                             | ✗                                             | ✗                                             |                                             9.81 |
| ✓                                             | ✓                                             | ✓                                             |                                             8.71 |

Table 9 and Table 10 consider varying the dimension of the Δ and ( 𝑩 , 𝑪 ) projections respectively. Changing them from static to selective provides the most bene/fit, while increasing the dimensions further generally improves performance modestly with a small increase in parameter count.

| 𝑨 𝑛 I/n.sc/i.sc/t.sc/i.sc/a.sc/l.sc/i.sc/z.sc/a.sc/t.sc/i.sc/o.sc/n.sc   | F/i.sc/e.sc/l.sc/d.sc   |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|--------------------------------------------------------------------------|-------------------------|--------------------------------------------------|
| 𝑨 𝑛 = - 1 2 + 𝑛𝑖                                                         | Complex                 |                                             9.16 |
| 𝑨 𝑛 = - 1 / 2                                                            | Real                    |                                             8.85 |
| 𝑨 𝑛 = -( 𝑛 + 1 )                                                         | Real                    |                                             8.71 |
| 𝑨 𝑛 ∼ exp (N( 0 , 1 ))                                                   | Real                    |                                             8.71 |

- · The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selective layer).

We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid attention architecture) in Appendix E.2.2.

## 5 Discussion

We discuss related work, limitations, and some future directions.

Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models.

Table 10: ( Ablations: SSM state dimension .) ( Top ) Constant 𝑩 and 𝑪 ( Bottom ) Selective 𝑩 and 𝑪 . Increasing the SSM state dimension 𝑁 , which can be viewed as an expansion factor on the dimension of the recurrent state, can signi/ficantly improve performance for a negligible cost in parameters/FLOPs, but only when 𝑩 and 𝑪 are also selective. Size of Δ projection /fixed to 64.

| S/i.sc/z.sc/e.sc /o.sc/f.sc Δ /p.sc/r.sc/o.sc/j.sc.   |   P/a.sc/r.sc/a.sc/m.sc/s.sc (M) |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|-------------------------------------------------------|----------------------------------|--------------------------------------------------|
| -                                                     |                            358.9 |                                             9.12 |
| 1                                                     |                            359.1 |                                             8.97 |
| 2                                                     |                            359.3 |                                             8.97 |
| 4                                                     |                            359.7 |                                             8.91 |
| 8                                                     |                            360.5 |                                             8.83 |
| 16                                                    |                            362.1 |                                             8.84 |
| 32                                                    |                            365.2 |                                             8.8  |
| 64                                                    |                            371.5 |                                             8.71 |

Table 9: ( Ablations: Expressivity of Δ .) The selection mechanism of Δ constructs it with a projection of the input. Projecting it even to dim. 1 provides a large increase in performance; increasing it further provides further improvements at the cost of a modest increase in parameters. State size /fixed to 𝑁 = 16.

|   S/t.sc/a.sc/t.sc/e.sc /d.sc/i.sc/m.sc/e.sc/n.sc/s.sc/i.sc/o.sc/n.sc 𝑁 |   P/a.sc/r.sc/a.sc/m.sc/s.sc (M) |   P/e.sc/r.sc/p.sc/l.sc/e.sc/x.sc/i.sc/t.sc/y.sc |
|-------------------------------------------------------------------------|----------------------------------|--------------------------------------------------|
|                                                                       1 |                            367.1 |                                             9.88 |
|                                                                       2 |                            367.4 |                                             9.86 |
|                                                                       4 |                            368   |                                             9.82 |
|                                                                       8 |                            369.1 |                                             9.82 |
|                                                                      16 |                            371.5 |                                             9.81 |
|                                                                       1 |                            367.1 |                                             9.73 |
|                                                                       2 |                            367.4 |                                             9.4  |
|                                                                       4 |                            368   |                                             9.09 |
|                                                                       8 |                            369.1 |                                             8.84 |
|                                                                      16 |                            371.5 |                                             8.71 |

No Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally de/fined as discretizations of continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeo/ff in more detail.

Downstream A/ffordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as /fine-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and a/ffordances.

Scaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper.

## 6 Conclusion

We introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for di/fferent domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model backbone.

## E.2.1 Scaling Law Details

Scaling law experiments generally followed the GPT3 recipe. All models were trained on the Pile with the GPT2 tokenizer.

Model Sizes. Table 12 speci/fies the model sizes we use for scaling laws. This is taken directly from the GPT3 speci/fications (Brown et al. 2020), with very minor modi/fications. First, we changed the batch size of the 1.3B model from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling laws (Ho/ffmann et al. 2022), which specify that training tokens should increase proportionally to model size.

Training Recipes. All models used the AdamW optimizer with

Table 12: ( Scaling Law Model Sizes .) Our model sizes and hyperparameters for scaling experiments. (Model dimension and number of heads applies only to Transformer models.)

| P/a.sc/r.sc/a.sc/m.sc/s.sc   |   n \_ layers |   d \_ model | n \_ heads / d \_ head   |   T/r.sc/a.sc/i.sc/n.sc/i.sc/n.sc/g.sc /s.sc/t.sc/e.sc/p.sc/s.sc |   L/e.sc/a.sc/r.sc/n.sc/i.sc/n.sc/g.sc R/a.sc/t.sc/e.sc | B/a.sc/t.sc/c.sc/h.sc S/i.sc/z.sc/e.sc   | T/o.sc/k.sc/e.sc/n.sc/s.sc   |
|------------------------------|--------------|-------------|------------------------|------------------------------------------------------------------|---------------------------------------------------------|------------------------------------------|------------------------------|
| 125M                         |           12 |         768 | 12 / 64                |                                                             4800 |                                                 0.0006  | 0.5M tokens                              | 2.5B                         |
| 350M                         |           24 |        1024 | 16 / 64                |                                                            13500 |                                                 0.0003  | 0.5M tokens                              | 7B                           |
| 760M                         |           24 |        1536 | 16 / 96                |                                                            29000 |                                                 0.00025 | 0.5M tokens                              | 15B                          |
| 1.3B                         |           24 |        2048 | 32 / 64                |                                                            50000 |                                                 0.0002  | 0.5M tokens                              | 26B                          |

- · gradient clip value 1 . 0
- · weight decay 0 . 1
- · no dropout
- · linear learning rate warmup with cosine decay

By default, the peak learning rate is the GPT3 speci/fication.

We give several models an 'improved recipe', inspired by changes adopted by popular large language models such as PaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include:

- · linear learning rate warmup with cosine decay to 1 𝑒 -5, with a peak value of 5 × the GPT3 value
- · no linear bias terms
- · RMSNorm instead of LayerNorm
- · AdamW hyperparameter 𝛽 = ( . 9 , . 95 ) (the GPT3 value) instead of the PyTorch default of 𝛽 = ( . 9 , . 999 )

## E.2.2 Additional Scaling Law Ablations

We perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in Figure 4 ( Left ).

Mamba Architecture: Interleaving Blocks. We test the e/ffect of di/fferent architectural blocks combined with the Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra conv → SSM path added. This leads to two natural ablations:

- · What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be interpreted as taking Mamba and removing half of the SSMs.

Figure 9: ( Scaling laws: extra ablations .) ( Left ) Instead of ( Right ) Instead of

<!-- image -->

- · What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.

Figure 9 ( Right ) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao, Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022).

H3 Architecture: Training Recipes. Next we ablate di/fferences between the Hyena and H3++ models, our weakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the e/ffect of training recipes.

- · Hyena : The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).
- · Hyena+ : The same architecture but with the improved training recipe described above.
- · H3+ : The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.
- · H3++ : The same as H3+, but with a linear attention head dimension of 8. This increases computation inside the SSM recurrence but does not increase parameters.

Our general convention is that 'Model+' represents the base model with the improved training recipe, and 'Model++' also allows for architectural changes.

## E.2.3 Downstream Evaluation Details

This pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens and with the GPT-NeoX tokenizer (Black et al. 2022) instead of GPT2 tokenizer. For the 1.3B model, we use a batch size of 1M tokens to be consistent with the GPT3 speci/fications. We report the perplexity on the Pile validation set, and for this metric only compare to models trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV.

For downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021), as done by most work in this area. We evaluate on the following tasks/datasets that measure common sense reasoning:

- · LAMBADA (Paperno et al. 2016)
- · HellaSwag (Zellers et al. 2019)

- · PIQA (Bisk et al. 2020)
- · ARC-challenge (P. Clark et al. 2018)
- · ARC-easy: an easy subset of ARC-challenge
- · WinoGrande (Sakaguchi et al. 2021)

We report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task).

## E.3.1 Pretraining Details

We describe the dataset and training procedure of the HG38 pretraining task in more detail.

The dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains a total of 𝑆 = 34021 segments of length 2 17 = 131072 that cover the genome, for a total of approximately 4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if necessary (e.g. to get longer segments).

We deviate from HyenaDNA when the training sequence length is not 2 17 . HyenaDNA always takes a /fixed sub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is /fixed to 34021 samples and doesn't necessarily go through the whole genome. On the other hand, we use the entire training data:

- · When the context length 𝐿 is less than (or equal to) 2 17 , we divide up each segment into non-overlapping sub-segments of length 𝐿 , so that there are 𝑆 × 2 17 𝐿 total samples and 𝑆 × 2 17 ≈ 4 . 5 𝐵 tokens per epoch.
- · When the context length 𝐿 is greater than 2 17 , we turn each segment into two samples, one that begins with the prescribed segment and one that ends with the prescribed segment. Thus each epoch has 2 𝑆 items and 2 𝑆𝐿 tokens per epoch. For example, at sequence length 2 18 = 262144 there are 4 × as many tokens as the default, and at sequence length 2 20 there are 16 × as many tokens.

Other training details generally follow the same protocol as our language modeling experiments (Appendix E.2). For example, we use the AdamW with ( 𝛽 1 , 𝛽 2 ) = ( 0 . 9 , 0 . 95 ) , no dropout, weight decay 0 . 1. We use a cosine learning rate scheduler with linear warmup for 10% of total steps.

## E.3.2 Scaling: Model Size Details

Models. The models we consider are:

- · Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al. 2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017).
- · HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP.
- · Mamba: the standard Mamba architecture.

Model Sizes. We use the following model sizes.

| B/l.sc/o.sc/c.sc/k.sc/s.sc                                      | 4    | 5    | 6    | 7    | 8    | 10    | 12    |
|-----------------------------------------------------------------|------|------|------|------|------|-------|-------|
| M/o.sc/d.sc/e.sc/l.sc D/i.sc/m.sc/e.sc/n.sc/s.sc/i.sc/o.sc/n.sc | 64   | 96   | 128  | 192  | 256  | 384   | 512   |
| P/a.sc/r.sc/a.sc/m.sc/s.sc (A/p.sc/p.sc/r.sc/o.sc/x.sc.)        | 250K | 700K | 1.4M | 3.5M | 7.0M | 19.3M | 40.7M |

Note that the number of blocks for Mamba is doubled, because one Transformer 'layer' includes both the MHA and MLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).

Training. For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across { 1 𝑒 -3 , 2 𝑒 -3 , 4 𝑒 -3 , 8 𝑒 -3 } . The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possible that our results are still suboptimal.)

Note that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. The optimal LR should go down for larger models, but we didn't /find a noticeable e/ffect at the small model sizes (at most a few million parameters) we considered.

## E.3.3 Scaling: Context Length Details

We use a total batch size of 2 24 ≈ 16 𝑀 tokens per training step, for every sequence length (e.g. at length 2 20 there are 16 segments per batch and at length 2 10 there are 16384 segments per batch). This is a large batch size relative to the model size by usual LM standards, but note that a batch size of 2 23 is the minimum possible on a machine with 8 GPUs and sequence length of 2 2 0, and that HyenaDNA used much larger batches of 2 28 .

The learning rate used was 0 . 008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rate of 0 . 002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length.

Sequence Length Warmup. Following (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW) during pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 2 10 = 1024. (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally. In particular, each stage up to length 2 17 processes the same number of tokens, but 4 × as many tokens are processed at length 2 18 , 8 × as many at length 2 19 , and 16 × as many at length 2 20 .)

Unlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively halved as the sequence lengths are doubled in each stage.

Remark E.1. We also note that the schedule was not tuned, and we never experimented with turning o/ff sequence length warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similar lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.

## E.4 Audio Details



