## AUDITING LARGE LANGUAGE MODELS: A THREE-LAYERED APPROACH

Jakob Mökander 1 , 2 , ∗

Jonas Schuett 3 , 4

Hannah Rose Kirk 1

Luciano Floridi 1 , 5

1

Oxford Internet Institute, University of Oxford, Oxford, UK 2 Center for Information Technology Policy, Princeton University, Princeton, US 3 Centre for the Governance of AI, Oxford, UK 4 Faculty of Law, Goethe University Frankfurt, Frankfurt am Main, Germany 5 Department of Legal Studies, University of Bologna, Bologna, Italy

This paper is accepted for publication in AI & Ethics. The official citation is: Mökander, J., Schuett, J., Kirk, H.R., & Floridi, L., Auditing large language models: a three-layered approach. AI Ethics (2023). doi:10.1007/s43681-023-00289-2

## ABSTRACT

Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.

Keywords artificial intelligence · auditing · ethics · foundation models · governance · large language models · natural language processing · policy · risk management

## 1 Introduction

Auditing is a governance mechanism that technology providers and law enforcers can use to identify and mitigate risks associated with artificial intelligence (AI) systems [1]-[5]. 1 Specifically, auditing is a systematic and independent process of obtaining and evaluating evidence regarding an entity's actions or properties and communicating the results of that evaluation to relevant stakeholders [7]. Three ideas underpin the promise of auditing as an AI governance mechanism: that procedural regularity and transparency contribute to good governance [8], [9]; that proactivity in the design of AI systems helps identify risks and prevent harm before it occurs [10], [11]; and, that the independence between the auditor and the auditee contributes to the objectivity and professionalism of the evaluation [12], [13].

Previous work on AI auditing has focused on ensuring that specific applications meet predefined, often sectorspecific, requirements. For example, researchers have developed procedures for how to audit AI systems used in recruitment [14], online search [15] and medical diagnostics [16], [17]. However, the capabilities of AI systems tend to become ever more general. In a recent article, Bommasani et al. [18] coined the term foundation models to describe models that can be adapted to a wide range of downstream tasks via transfer learning. While foundation models are not necessarily new from a technical perspective, they differ from other AI systems insofar as they have proven to be effective across many different tasks and display emergent capabilities when scaled [19]. 2 The rise of foundation models also reflects a shift in how AI systems are designed and deployed, since these models are typically trained and released by one actor and subsequently adapted for a plurality of applications by other actors.

From an AI auditing perspective, foundation models pose significant challenges. For example, it is difficult to assess the risks that AI systems pose independent of the context in which they are deployed. Moreover, how to allocate responsibility between technology providers and downstream developers when harms occur remains unresolved. Taken together, the capabilities and training processes of foundation models have outpaced the development of tools and procedures to ensure that these are ethical, legal, and technically robust. 3 This implies that, while application-level audits have an important role in AI governance, they must be complemented with new forms of supervision and control.

This article addresses that gap by focusing on a subset of foundation models, namely large language models (LLMs). Language models start from a source input, called the prompt, to generate the most likely sequences of words, code, or other data [26]. Historically, different model architectures have been used in natural language processing (NLP), including probabilistic methods [27]. However, most recent LLMs - including those we focus on in this article - are based on deep neural networks trained on a large corpus of texts. Examples of such LLMs include GPT-3 [28], PaLM [29], LaMDA [30], Gopher [31] and OPT [32]. Once an LLM has been pre-trained, it can be adapted (with or without fine-tuning 4 ) to support various applications, from spell-checking [37] to creative writing [38].

Developing LLM auditing procedures is a relevant task for two reasons. First, previous research has demonstrated that LLMs pose many ethical and social challenges, including the perpetuation of harmful stereotypes, the leakage of personal data protected by privacy regulations, the spread of misinformation, plagiarism, and the misuse of copyrighted material [39]-[41]. In recent months, the scope of impact from these harms has been dramatically scaled by unprecedented public visibility and growing user bases of LLMs. For example, ChatGPT attracted over 100 million users just two months after its launch [42]. The urgency of addressing those challenges makes developing a capacity to audit LLMs' characteristics along different normative dimensions (such as privacy, bias, IP, etc.) a critical task in and of itself [43]. Second, LLMs can be considered proxies for other foundation models. 5 For example, CLIP [45] is a vision-language model trained to predict which text caption accompanied an image. Although not an LLM, CLIP - and other models that can be adapted for multiple downstream applications - faces similar governance challenges. The same holds of other automatic content producers, such as DALL·E 2 [46]. So, developing LLM auditing procedures may inform the auditing of other foundation models and even more powerful generative systems in the future. 6

The main contribution offered in this article is a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. Fig. 1 provides an overview of this three-layered approach. As we demonstrate throughout this article, many tools and methods already exist to conduct audits at each individual level. However, the

3 The European Commission's Ethics Guidelines for Trustworthy AI stipulate that AI systems should be legal, ethical, and technically robust [25]. That normative standard includes safeguards against both immediate and long-term concerns, e.g., those related to data privacy and wrongful discrimination and those related to the safety and control of AI systems, respectively.

4 To fine-tune LLMs for specific tasks, an additional dataset of in-domain examples can be used to adapt the final layers of a pre-trained model. In some cases, developers apply reinforcement learning (RL) - a feedback driven training paradigm whereby LLMs learn to adjust their behaviour to maximise a reward function [33]; especially reinforcement learning from human feedback (RLHF) - where the reward function is estimated based on human ratings of model outputs [34], [35]. LLMs can also be adapted with no additional training data and frozen weights - via in-context learning or prompt-based demonstrations [36].

key message we seek to stress is that, to provide meaningful assurance for LLMs, audits conducted on the governance, model, and application levels must be combined into a structured and coordinated procedure. Fig. 2 illustrates how outputs from audits on one level become inputs for which audits on other levels must account. To our best knowledge, our blueprint for how to audit LLMs is the first of its kind, and we hope it will inform both technology providers' and policymakers' efforts to ensure that LLMs are legal, ethical, and technically robust.

In the process of introducing and discussing our three-layered approach, the article also offers two secondary contributions. First, it makes seven claims about how LLM auditing procedures should be designed to be feasible and effective in practice. Second, it identifies the conceptual, technical, and practical limitations associated with auditing LLMs. Together, these secondary contributions lay a groundwork that other researchers and practitioners can build upon when designing new, more refined, LLM auditing procedures in the future.

Our efforts tie into an extensive research agenda and policy formation process. AI labs like Cohere, OpenAI, and AI21 have expressed interest in understanding what it means to develop LLMs responsibly [49], and DeepMind, Microsoft, and Anthropic have highlighted the need for new governance mechanisms to address the social and ethical challenges that LLMs pose [39], [50], [51]. Individual parts of our proposal (e.g., those related to model evaluation [29] and red teaming [52], [53]) have thus already started to be implemented across the industry, although not always in a structured manner or with full transparency. Policymakers, too, are interested in ensuring that societies benefit from LLMs while managing the associated risks. Recent examples of proposed AI regulations include the EU AI Act [54] and the US Algorithmic Accountability Act of 2022 [55]. The blueprint for auditing LLMs outlined in this article neither seeks to replace existing best practices for training and testing LLMs nor to foreclose forthcoming AI regulations. Instead, it complements them by demonstrating how governance, model, and application audits - when conducted in a structured and coordinated manner - can help ensure that LLMs are designed and deployed in ethical, legal, and technically robust ways.

A further remark is needed to narrow down this article's scope. Our three-layered approach concerns the procedure of LLM audits and answers questions about what should be audited, when, and according to which criteria. Of course, when designing a holistic auditing ecosystem, several additional considerations exist, e.g., who should conduct the audit and how to ensure post-audit action [13]. While such considerations are important, they fall outside the scope of this article. How to design an institutional ecosystem to audit LLMs is a non-trivial question that we have neither the space nor the capacity to address here. That said, the policy process required to establish an LLM auditing ecosystem will likely be gradual and involve negotiations between numerous actors, including AI labs, policymakers, and civil rights groups. For this reason, our early blueprint for how to audit LLMs is intentionally limited in scope to not forego but rather to initiate this policy formation process by eliciting stakeholder reactions.

The remainder of this article proceeds as follows. Sec. 2 highlights the ethical and social risks posed by LLMs and establishes the need to audit them. In doing so, it situates our work in relation to recent technological and societal developments. Sec. 3 reviews previous literature on AI auditing to identify transferable best practices, discusses the properties of LLMs that undermine existing AI auditing procedures, and derives seven claims for how LLM auditing procedures should be designed to be feasible and effective. Sec. 4 outlines our blueprint for how to audit LLMs, introducing a three-layered approach that combines governance, model, and application audits. The section explains in detail why these three types of audits are needed, what they entail, and the outputs they should produce. Sec. 5 discusses the limitations of our three-layered approach and demonstrates that any attempt to audit LLMs will face several conceptual, technical, and practical constraints. Finally, Sec. 6 concludes by discussing the implications of our findings for technology providers, policymakers, and independent auditors.

## 4 Auditing LLMs: A three-layered approach

This section offers a blueprint for auditing LLMs that satisfies the seven claims in Sec 3 about how to structure such procedures. While there are many ways to do that, our proposal focuses on a limited set of activities that are (i) jointly sufficient to identify LLM-related risks, (ii) practically feasible to implement, and (iii) have a justifiable cost-benefit ratio. The result is the three-layered approach outlined below.

## 6 Conclusion

Some of the features that make LLMs attractive also create significant governance challenges. For instance, the potential to adapt LLMs to a wide range of downstream applications undermines system verification procedures that presuppose well-defined demand specifications and predictable operating environments. Consequently, our analysis in Sec. 3 concluded that existing AI auditing procedures are not well-equipped to assess whether the checks and balances put in place by technology providers and downstream developers are sufficient to ensure good governance of LLMs.

In this article, we have attempted to bridge that gap by outlining a blueprint for auditing LLMs. In Sec. 4, we introduced a three-layered approach, whereby governance, model and application audits inform and complement each other. During governance audits , technology providers' accountability structures and quality management systems are evaluated for robustness, completeness, and adequacy. During model audits , LLMs' capabilities and limitations are

assessed along several dimensions, including performance, robustness, information security, and truthfulness. Finally, during application audits , products and services built on top of LLMs are first assessed for legal compliance and subsequently evaluated based on their impact on users, groups, and the natural environment.

Technology providers and policymakers have already started experimenting with some of the auditing activities we propose. Consequently, auditors can leverage a wide range of existing tools and methods, such as impact assessments, benchmarking, model evaluation, and red teaming, to conduct governance, model, and application audits. That said, the feasibility and effectiveness of our three-layered approach hinge on two factors. First, only when conducted in a combined and coordinated fashion can governance, model and application audits enable different stakeholders to manage LLM-related risks. Hence, audits on the three levels must be connected in a structured process. Governance audits should ensure that providers have mechanisms to take the output logs generated during application audits into account when redesigning LLMs. Similarly, application audits should ensure that downstream developers take the limitations identified during model audits into account when building on top of a specific LLM. Second, audits must be conducted by an independent third-party to ensure that LLMs are ethical, legal, and technically robust. The case for independent audits rests not only on concerns about the misaligned incentives that technology providers may face but also on concerns about the rapidly increasing capabilities of LLMs [281].

However, even when implemented under ideal circumstances, audits will not solve all tensions or protect against all risks of harm associated with LLMs. So, it is important to remain realistic about what auditing can achieve. Three limitations of our approach are worth reiterating. First, the feasibility of the model audits hinges on the construct validity of the metrics used to assess model characteristics like robustness and truthfulness. This is a limitation because it is notoriously difficult to operationalise normative concepts. Second, our blueprint for auditing LLMs does not specify who should conduct the audits it posits. No auditing procedure is stronger than the institutions backing it. Hence, the fact that an ecosystem of actors capable of implementing our blueprint has yet to emerge constrains its effectiveness. Third, not all risks associated with LLMs arise from processes that can be addressed through auditing. Some tensions are inherently political and require continuous management through public deliberation and structural reform.

Academics and industry researchers can contribute to overcoming these limitations by focusing on two avenues for further research. The first is to develop new methods and metrics to operationalise normative concepts in ways that are verifiable and maintain a high degree of construct validity. The second is to disentangle further the sources of different types of risks associated with LLMs. Such research would advance our understanding of how political reform can complement technically oriented mechanisms in holistic efforts to govern LLMs.

Policymakers can facilitate the emergence of an institutional ecosystem capable of carrying out and enforcing governance, model, and application audits of LLMs. For example, policymakers can encourage and strengthen private sector LLM auditing initiatives by supporting the standardisation of evaluation metrics [282], harmonising AI regulation [283], facilitating knowledge sharing [284] or rewarding achievements through monetary incentives [277]. Policymakers should also update existing and proposed AI regulations in line with our three-layered approach to address LLM-related risks. For example, while the EU AI Act's conformity assessments and post-market monitoring plans mirror application audits, the proposed regulation does not contain mechanisms akin to governance and model audits [91]. Without amendments, such regulations are unlikely to generate adequate safeguards against the risks associated with LLMs.

Our findings most directly concern technology providers as they are primarily responsible for ensuring that LLMs are legal, ethical, and technically robust. Such providers have moral and material reasons to subject themselves to independent audits, including the need to manage financial and legal risks [285] and build an attractive brand [286]. So, what ought technology providers do? Firstly, they should subject themselves to governance audits and their LLMs to model audits. That would create a demand for independent auditing and accreditation bodies and help spark methodological innovation in governance and model audits. Secondly, providers should demand that products and services built on top of their LLMs undergo application audits. That could be done through structured access procedures, whereby permission for using an LLM is conditional on such terms. Thirdly, like-minded providers should establish, and fund, an independent industry body that conducts or commissions governance, model, and application audits.

Taking a long-term perspective, our three-layered approach holds lessons for how to audit more capable and general future AI systems. This article has focused on LLMs because they have broad societal impacts via widespread applications. However, elements of the governance challenges - including generativity, emergence, lack of grounding, and lack of access - have some general applicability to other ML-based systems [287], [288]. Hence, we anticipate that our blueprint can inform the design procedures for auditing other generative, ML-based technologies.

That said, the long-term feasibility and effectiveness of our blueprint for how to audit LLMs may also be undermined by future developments. For example, governance audits make sense when only a limited number of actors have the ability and resources to train and disseminate LLMs. However, the democratisation of AI capabilities - either through the reduction of entry barriers or a turn to business models based on open-source software - would challenge this

status quo [289]. Similarly, if language models become more fragmented or personalised [101], there will be many user-specific branches or instantiations of a single LLM which would make model audits more complex to standardise. As a result, while maintaining the usefulness of our three-layered approach, we acknowledge that it will need to be continuously revised in response to the changing technological and regulatory landscape.

It is worth concluding with some words of caution. Our blueprint is not intended to replace existing governance mechanisms but to complement and interlink them by strengthening procedural transparency and regularity. Rather than being adopted wholesale by technology providers and policymakers, we hope that our three-layered approach can be adopted, adjusted, and expanded to meet the governance needs of different stakeholders and contexts.

Authorship statement JM is the first author of this paper. JS, HRK, and LF contributed equally to the paper.

Acknowledgements The authors would like to thank the following people for helpful comments on earlier versions of this manuscript: Markus Anderljung, Matthew Salganik, Arvind Narayanan, Deep Ganguli, Katherine Lee, Toby Shevlane, Varun Rao, and Lennart Heim. In the process of writing the article, the authors also benefitted from conversations with and input from Allan Dafoe, Ben Garfnkel, Anders Sandberg, Emma Bluemke, Andreas Hauschke, Owen Larter, Sebastien Krier, Mihir Kshirsagar, and Jade Leung. The article is much better for it.

Conflicts of interest The authors have no conflicts of interest to declare.

Funding JM's doctoral research at the Oxford Internet Institute is supported through a studentship provided by AstraZeneca. JM conducted part of this research during a paid Summer Fellowship at the Centre for Governance of AI. HRK's doctoral research at the Oxford Internet Institute is supported by the UK Economic and Social Research Council grant ES/P000649/1.

