## Abstract

Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose COLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that COLT5 achieves stronger performance than LONGT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, COLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.

## 1 Introduction

Many natural language processing tasks, such as summarization (Cohan et al., 2018) or question answering over long documents (Joshi et al., 2017), require machine learning models to encode longform text. Processing long documents with a Transformer model is computationally expensive, both because attention cost scales quadratically with input length and because feedforward and attention projection layers have to be applied to each input token.

Over the past few years, many 'efficient Transformer' approaches have been proposed that reduce the cost of the attention mechanism over long inputs (Child et al., 2019; Ainslie et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Wang et al., 2020; Tay et al., 2021; Guo et al., 2022). However, especially for larger models, the feedforward and projection layers actually make up the majority of

Figure 1: An overview of a COLT5 Transformer layer with conditional computation. All tokens are processed by light attention and MLP layers, while q routed query tokens perform heavier attention over v routed key-value tokens and m routed tokens are processed by a heavier MLP.

<!-- image -->

the computational burden and can render processing long inputs intractable.

This paper presents COLT5 (Conditional LongT5), a new family of models that, building on top of LONGT5 (Guo et al., 2022), enables fast processing of long inputs by combining architecture improvements for both attention and feedforward layers. COLT5 is based on the intuition that some tokens are more important than others, and we can achieve better quality for lower cost by devoting more computation to important tokens. Moreover, the fraction of important tokens is likely to diminish with document length, allowing for tractable processing of long documents.

In particular, COLT5 divides each feedforward layer and each attention layer into a light branch

Figure 2: COLT5 achieves stronger performance than LONGT5 at any speed. Average performance on all datasets as a function of inference and fine-tuning time per sample (ms) for LONGT5 and COLT5 Base, Large, and XL models. LONGT5 does not use MQA, but we report speed as though it had for a conservative baseline.

<!-- image -->

which is applied to all tokens and a heavy branch which is applied to a set of important tokens, selected specifically for that input and component. The light feedforward branch has lower hidden dimension than standard LONGT5 while the heavy feedforward branch has higher hidden dimension. The light attention branch has fewer heads and applies only local attention, while the heavy attention branch performs full attention over another separately selected set of important tokens. Figure 1 provides an overview of the COLT5 conditional mechanism.

Finally, COLT5 also includes two other modifications to the LONGT5 architecture. COLT5 adds multi-query cross-attention (Shazeer, 2019), significantly speeding up inference. COLT5 also employs the UL2 (Tay et al., 2022) pre-training objective, which we demonstrate allows for in-context learning over long inputs.

We show that COLT5 performs much faster finetuning and inference with similar or better model quality, improving over LONGT5 on arXiv summarization (Cohan et al., 2018) and TriviaQA question answering (Joshi et al., 2017) datasets and achieving SOTA on the SCROLLS benchmark (Shaham et al., 2022). Moreover, COLT5 achieves further gains in quality and speed for tasks with extremely long inputs (64k tokens), with less-than-linear scaling of 'focus' tokens.

## 2 Background

Transformer FLOPs COLT5 follows an extensive line of work in attempting to reduce the computational cost of Transformer models, particularly

over long inputs. The computational burden of Transformer models has several distinct elements, and different approaches focus on reducing the cost of different components. For that reason, it is helpful to start by providing a breakdown of the computational cost of Transformer components. Table 1 shows the FLOPs 1 for each component of a Transformer encoder layer (Kaplan et al., 2020).

Table 1: Computational cost of encoder layer transformer components measured in FLOPs. n is the input length, d is the model dimensionality, and w is the size of the local attention window.

| Encoder Layer Component              | Flops   |
|--------------------------------------|---------|
| Vanilla self-attention computation   | 2 n 2 d |
| Attention QKV and output projections | 4 nd 2  |
| Feedforward layer                    | 8 nd 2  |
| LONGT5 local attention computation   | 2 nwd   |
| LONGT5 global attention computation  | n 2 8 d |

Sparse attention The first challenge of applying a Transformer to a long input is that the FLOPs of the self-attention mechanism scales quadratically in the input length, becoming intractable for long inputs. A large body of work focuses on reducing self-attention cost, restricting attention between a subset of inputs (Child et al., 2019; Ainslie et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Wang et al., 2020; Guo et al., 2022) or to a subset of layers (Zemlyanskiy et al., 2021). In LONGT5 (Guo et al., 2022), the most closely related model to COLT5, tokens attend within a lo-

cal window as well as to a mean-pooled summary representation for each block of 16 tokens in the input. LONGT5 attention leads to sharply reduced (though still non-negligible) FLOPs (Table 1).

Conditional computation After applying a sparse attention mechanism, the feedforward and attention projection layers account for the majority of the FLOPs. These costs scale with the length of the input, such that processing long inputs is still prohibitively expensive. A common approach to reduce the remaining cost is to employ some form of conditional computation , avoiding applying all model parameters to the entire input. CALM (Schuster et al., 2022) applies a varying number of decoder layers to each decoded token, outputting a token early if the model is confident in its prediction. Mixture-of-Experts models (Shazeer et al., 2017; Fedus et al., 2021; Zoph et al., 2022) route inputs through a small proportion of expert sub-modules, bringing to bear only the parameters most relevant to the input. In the context of retrieval-augmented models, numerous works rerank retrieved passages by their relevance to the query and process only the highest scoring passages (Mao et al., 2021; Wang et al., 2018; Yu et al., 2022) and vary the number of processed passages depending on model confidence (Kratzwald and Feuerriegel, 2018; Varshney et al., 2022). Concurrent work CoDA (Lei et al., 2023) employs a related conditional computation mechanism, designed for efficient adaptation rather than modeling long documents.

Device utilization FLOPs do not tell the whole story, as modeling choices can influence the effective speed of operations achieved by accelerators. For long text inputs, autoregressive decoder inference is very slow due to memory bandwidth constraints from repeatedly loading the long sequence of keys and values (Shazeer, 2019; de Jong et al., 2022). Shazeer (2019) introduces multi-query attention (MQA), sharing heads for keys and values to reduce memory bandwidth overhead. Pope et al. (2022) studies how to shard large models, especially in the context of MQA, to obtain optimal device utilization and therefore speed.

Training objectives T5 introduced the span corruption objective (Raffel et al., 2020), a modification of masked language modeling (Devlin et al., 2019). LONGT5 made use of the PEGASUS (Zhang et al., 2020) sentence reconstruc-

tion objective for improved summarization performance. Tay et al. (2022) proposes UL2, a mixture of span corruption, prefix, and causal language modeling, and shows that it leads to strong performance on both short-output and generative tasks.

## 4.1 Experimental setup

Configurations COLT5 is based on the T5.1.1 architecture (Raffel et al., 2020), implemented with JAX (Bradbury et al., 2018), Flax (Heek et al., 2020), and Flaxformer 3 . Following LONGT5, we experiment with Base, Large, and XL model sizes. COLT5 models use the same embedding dimension, number of layers, and total attention heads as corresponding LONGT5 models of the same size, with more overall parameters (but less compute) due to the conditional branch. See Appendix B for additional details on model configuration.

Pre-training We pre-train COLT5 for 1M steps on the C4 dataset (Raffel et al., 2020) using a variant of the UL2 objective (Tay et al., 2022) with batch size 256, input length 4096, and output length 910. In particular, our mixture contains four objectives in equal proportion: prefix-LM with noise rate 0.5, and span corruption (Raffel et al., 2020) with noise rate 0.15 and average span lengths 3, 8, and 64. We use the Adafactor optimizer (Shazeer and Stern, 2018) with the T5.1.1 inverse square root learning rate schedule and no dropout. COLT5 is trained with the T5X (Roberts et al., 2022) framework. For pre-training, we route m = 512 tokens, 1 8 th of the input length.

Fine-tuning For fine-tuning we use a constant learning rate of 0.001, batch size 128, and dropout rate 0.1 for all tasks. Main results use input length of 16384 for all datasets other than ContractNLI, which uses 8192. Question answering datasets use output length 128 and summarization datasets use output length 512, except for GovRep which uses output length 1024. We route m = 1024 tokens, 1 16 th of the input length. We train until convergence

and select the checkpoint with the highest dev performance. We use greedy decoding for inference.

Data We evaluate COLT5 on TriviaQA (Joshi et al., 2017), arXiv (Cohan et al., 2018), and the SCROLLS benchmark (Shaham et al., 2022). SCROLLS contains question-answering datasets: NarrativeQA (Koˇciský et al., 2018), QASPER (Dasigi et al., 2021), and QuALITY (Pang et al., 2021), an NLI dataset: ContractNLI (Koreeda and Manning, 2021), and summarization datasets: SummScreenFD (Chen et al., 2022), QMSum (Zhong et al., 2021), and GovReport (Huang et al., 2021). Table 4 provides an overview of the size and input length for each dataset.

Table 4: Median and 90th percentile input length by dataset measured in SentencePiece tokens.

| Dataset     | Type   | Samples   | Median   | 90%     |
|-------------|--------|-----------|----------|---------|
| TriviaQA    | QA     | 157,053   | 8,858    | 28,956  |
| arXiv       | Sum    | 215,913   | 8,519    | 20,170  |
| NarrativeQA | QA     | 71,187    | 57,829   | 176,862 |
| QASPER      | QA     | 5,692     | 5,472    | 8,657   |
| QuALITY     | QA     | 6,737     | 7,171    | 8,276   |
| ContractNLI | NLI    | 10,319    | 2,148    | 4,485   |
| SummScreen  | Sum    | 4,348     | 9,046    | 15,172  |
| QMSum       | Sum    | 1,810     | 14,197   | 27,761  |
| GovRep      | Sum    | 19,402    | 8,841    | 18,835  |

Timing We report time per sample per TPUv4 chip, as measured by xprof (Google, 2020). For inference we use a single TPUv4 with batch size 16 or the largest that fits in memory. For fine-tuning we profile with 8 TPUv4 chips, sharded separately for each model to maximize throughput.

## 4.5 Ablations

This section studies the effect of different choices in the COLT5 recipe. Table 6 contains results of a series of experiments that change a single compo-

Table 6: COLT5 ablations evaluated on validation sets. Each experiment modifies a component of the COLT5 recipe for COLT5-Base. Static routing divides the input into equal-length blocks and selects the first token in each block to be routed. Shared QKV routing shares routing decisions for queries and keys/values. In v=all the routed queries attend to the entire input, while v=q selects the same number of key and value tokens as query tokens. m=512 and m=1536 use different numbers of routed tokens. LONGT5-B uses a LONGT5 encoder while retaining other parts of the COLT5 training recipe such as MQA and the UL2 objective. Multi-head refers to using multi-head cross-attention. The final ablation replaces the UL2 objective with PEGASUS as in LONGT5.

| Ablation   | Model      | Avg   | Inf   | TQA   | NQA   | QAS   | QuAL   | CNLI   | arX   | SumS   | QMS   | GovR   |
|------------|------------|-------|-------|-------|-------|-------|--------|--------|-------|--------|-------|--------|
|            |            |       | S/s   | F1    | F1    | F1    | EM     | EM     | Rgm   | Rgm    | Rgm   | Rgm    |
| Baseline   | COLT5-B    | 42.5  | 11.2  | 82.4  | 23.1  | 38.3  | 36.6   | 87.8   | 35.3  | 19.3   | 20.5  | 39.4   |
| Routing    | Static     | 40.5  | 11.6  | 79.7  | 19.2  | 34.2  | 34.5   | 86.4   | 34.9  | 18.1   | 18.9  | 38.8   |
| Routing    | Share QKV  | 42.0  | 11.8  | 82.1  | 21.9  | 37.5  | 36.2   | 87.0   | 35.2  | 18.2   | 20.4  | 39.7   |
| Attention  | v=all      | 42.5  | 9.4   | 82.4  | 22.3  | 38.6  | 37.2   | 87.8   | 35.3  | 19.1   | 20.3  | 39.8   |
| Attention  | v=q        | 42.3  | 11.5  | 82.5  | 22.5  | 37.3  | 37.0   | 85.9   | 35.2  | 19.0   | 20.5  | 39.7   |
| Routed     | m=512      | 41.6  | 12.2  | 81.9  | 22.1  | 37.3  | 35.4   | 84.6   | 35.2  | 18.9   | 19.5  | 39.6   |
| Tokens     | m=1536     | 42.9  | 10.4  | 82.6  | 23.5  | 39.8  | 37.5   | 87.5   | 35.4  | 19.4   | 20.8  | 40.0   |
| Encoder    | LONGT5-B   | 42.1  | 7.4   | 82.0  | 21.4  | 38.4  | 35.8   | 88.0   | 35.5  | 18.7   | 20.4  | 38.5   |
| Decoder    | Multi-head | 42.9  | 0.7   | 82.7  | 22.9  | 40.2  | 35.8   | 87.7   | 35.5  | 19.7   | 21.2  | 40.3   |
| Objective  | PEGASUS    | 42.8  | 11.2  | 82.6  | 22.6  | 40.5  | 37.3   | 87.3   | 35.3  | 19.6   | 20.8  | 39.6   |

nent for COLT5 Base.

Routing First, we note that static routing -evenly distributing routed tokens over the input -- leads to massive drop in performance. The importance of routing provides evidence that the model learns to devote capacity to important tokens and the advantage of COLT5 is not merely a result of additional parameters. Sharing routing decisions for query and KV tokens should be compared with v=q, and leads to a modest reduction in quality and increase in speed.

The optimal number of routed tokens represents a trade-off between improved performance and computational cost of applying heavier layers. Table 6 shows strong gains going from 512 to 1024 (baseline) routed tokens and diminishing returns for further increases.

Attention COLT5 relies on routing to identify not only tokens that can benefit from important information elsewhere in the input, but also which tokens contain such important information. We study whether COLT5 is successful in this task by comparing performance with two different attention settings -- v=all, in which routed tokens attend to the entire input, and v=q, which uses equal number of routed keys and values as queries, rather than twice as many. COLT5 appears to occupy a sweet spot, as using fewer routed key-values modestly decreases performance at similar speed but attending

to all inputs barely helps at sharply increased cost.

Other We compare COLT5 to LONGT5 with multi-query cross-attention, confirming that LONGT5 indeed does not achieve an unexpected quality gain from MQA, and our conservative assumptions in Figures 2, 4 are valid. Next, we evaluate multi-head cross-attention for COLT5, finding that it leads to modestly improved COLT5 performance. However, as MHA exhibits orderof-magnitude slower inference, MQA is clearly favored. Finally, PEGASUS appears to fine-tune slightly better than UL2, though the difference is small and UL2 enables few-shot learning.

## 4.6 Routing analysis

It is interesting to ask whether COLT5 routed tokens line up with what we consider intuitively important tokens in each document. We investigate this question by studying routing patterns of a Large COLT5 model fine-tuned on TriviaQA. We divide tokens into three categories: (1) question tokens, (2) answer tokens, and (3) other tokens. Figure 6 shows the average fraction of each type of token that is routed through the heavy path for MLP and attention layers on TriviaQA. We note that question and answer tokens are significantly more likely to be routed than other tokens, for feedforward as well as attention queries and keys/values. Appendix F presents more detailed routing analysis; e.g., semantically important tokens are much

Figure 6: Proportion of tokens routed for answer (string match), question, and other tokens by routing component for COLT5 Large model, averaged over examples in TriviaQA dev set and all layers of model.

<!-- image -->

more likely to be selected in later layers.

## 5 Conclusion

We propose COLT5, a new model for long-range inputs that employs conditional computation for higher quality and faster speed. COLT5 has light feedforward and attention layers that apply to the entire input, as well as heavy branches that are applied only to a subset of important tokens selected by a learned router. We show that COLT5 achieves stronger performance at any speed compared to LONGT5 on a variety of long-input datasets, and can effectively and efficiently make use of extremely long inputs up to 64k tokens.

## Limitations

COLT5 applies conditional computation only in the encoder. Applying conditional computation in the decoder is more complicated; the routing method in COLT5 is not causal, so it isn't applicable when generating token by token. Since decoder-only models and applications with long outputs have become more popular recently, this is a strong limitation of the current approach. Although the routing method in COLT5 could potentially be applied to the input context in a decoder-only model, we didn't investigate this setup.

COLT5 is specialized towards long sequences and has to be trained from scratch. For large-scale training and deployment, it is desirable to either train a single model that can handle both short and long sequences, or develop a long-input architecture that can be adapted from an existing large model.

## F Routing Analysis

In this section we take a closer look at the routing mechanisms in COLT5. There are three routing processes in each layer of COLT5: (1) Routing of attention keys and values ('KV-routing'), (2) routing of attention queries ('Q-routing') and (3) routing of MLP tokens ('MLP-routing'). For simplicity, we will say that a token is selected , when it is routed to the heavy alternative (of either MLP or attention). We are interested in understanding what tokens are selected and whether these mechanisms select similar or different tokens in each layer.

Which tokens are selected We divide input tokens into three categories: (1) question tokens, (2) answer tokens (found via simple normalized string match of the ground truth answer), and (3) other tokens. Figure 7 shows the proportion of each token type that is routed by a fine-tuned COLT5-Large model on the TriviaQA dev set, by layer and routing component.

Earlier we showed that question and answer tokens are more likely to be selected, but separating routing decisions by layer reveals interesting patterns. At early layers question and answer to-

Figure 7: Proportion of tokens routed for answer (string match), question, and other tokens by routing component and layer for COLT5 Large model, averaged over examples in TriviaQA dev set.

<!-- image -->

Figure 8: Visualization of token routing weights for some fragments of an example on TriviaQA.

<!-- image -->

kens are only modestly more likely to be selected, with routing probability sharply increasing at later layers and peaking in the last layer. This makes intuitive sense: in early layers the model has not yet had the opportunity to identify which tokens and parts of the document are important. However, the increase is not monotonic and there is strong variation between layers. This variation may imply that different layers focus on different types of tokens, or that some routing components do not successfully learn to identify important tokens.

To gain a better insight into this, Figure 8 visualizes routing on two sample fragments from a TriviaQA example (notice that, given the large input length used in COLT5, we do not show the complete example in the figure). The two fragments shown correspond to the beginning of the example (where the question is located), and the part of the context surrounding the correct answer. We have added a colored background to the figure, where each of the three CMY channels are mapped to the KV-routing weights in different layers of the model. Cyan corresponds to layer 1, Magenta to layer 12, and Yellow to layer 24. As we can see, question and answer are heavily yellow colored, showing those tokens are selected in the last layer.

Correlation between routing processes. Table 10 shows the Pearson correlation coefficient between the routing weights of the different routing mechanisms in each layer in a COLT5 Large model (MLP-routing correlation with KV-routing, MLProuting with Q-routing, and KV-routing with Qrouting). We show numbers for both the pre-trained checkpoint, as well as a fine-tuned model on TriviaQA. As we can see, the routing of keys/values and

routing of queries is highly correlated at all layers except the first two, while the routing of tokens in the MLP has lower correlation to the other two processes. Interestingly correlation between MLP and attention routing increases in the last layers of the model.

<!-- image -->

Table 10: Pearson correlation coefficient between the routing weights of the different routing mechanisms in each layer in a COLT5 Large model. We show numbers for both the pre-trained checkpoint, as well as a finetuned model on TriviaQA. Blue bars visualize positive correlation, whereas red bars visualize negative correlation.

|            | Pre-trained   | Pre-trained   | Pre-trained   | Fine-tuned   | Fine-tuned                          | Fine-tuned   |
|------------|---------------|---------------|---------------|--------------|-------------------------------------|--------------|
|            |               |               |               |              | MLP-KV MLP-Q KV-Q MLP-KV MLP-Q KV-Q |              |
| -0.06      | -0.06         | -0.09         |               | -0.06        | -0.09                               | -0.26        |
| 0.27       | 0.52          | 0.04          |               | 0.27         | 0.39                                | 0.02         |
| -0.05      | -0.03         | 0.75          |               | 0.05         | -0.01                               | 0.69         |
| 0.05       | 0.09          | 0.76          |               | 0.18         | 0.14                                | 0.72         |
| 0.02       | -0.01         | 0.75          |               | 0.22         | 0.26                                | 0.68         |
| 0.02       | -0.01         | 0.78          |               | 0.31         | 0.33                                | 0.70         |
| 0.02       | 0.00          | 0.73          |               | 0.26         | 0.27                                | 0.70         |
| 0.00       | -0.02         | 0.44          |               | 0.11         | -0.07                               | 0.29         |
| 0.13       | 0.11          | 0.74          |               | 0.36         | 0.40                                | 0.70         |
| -0.06      | -0.08         | 0.08          |               | -0.15        | -0.15                               | 0.12         |
| -0.05      | -0.07         | 0.31          | -0.08         |              | -0.03                               | 0.18         |
| -0.04      | -0.08         | 0.27          |               | 0.03         | 0.00                                | 0.28         |
| -0.10      | -0.09         | 0.87          |               | -0.13 -0.06  | -0.03 -0.12                         | 0.72         |
| -0.04 0.53 | -0.05 0.64    | 0.76          |               | 0.51         | 0.55                                | 0.67 0.67    |
|            |               | 0.69          |               | 0.06         | 0.57                                | 0.24         |
| 0.08       | 0.12          | 0.63          |               |              | 0.32                                |              |
| 0.28       | 0.30          | 0.65          |               | 0.27         |                                     | 0.69         |
| 0.28       | 0.02          | 0.84          |               | 0.31         | 0.20                                | 0.76         |
| 0.45       | 0.77          | 0.59          | 0.19          |              | 0.38                                | 0.64         |
| 0.30       | 0.39 -0.04    | 0.64          | 0.18          | 0.38         | 0.47 0.11                           | 0.62 0.47    |
| 0.05       | 0.00          | 0.49          |               | 0.21         | 0.16                                | 0.68         |
| 0.05 0.39  | 0.33          | 0.69 0.68     | 0.60          |              | 0.79                                | 0.69         |
| 0.43       | 0.39          | 0.59          |               | 0.57         | 0.63                                | 0.65         |

