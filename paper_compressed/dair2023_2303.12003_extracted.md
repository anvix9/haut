## Abstract:

A widespread view is that Artificial Intelligence cannot be creative. We tested this assumption by comparing human-generated ideas with those generated by six Generative Artificial Intelligence (GAI) chatbots:  alpa.ai, Copy.ai, ChatGPT (versions 3 and 4), Studio.ai, and YouChat. Humans and a specifically trained AI independently assessed the quality and quantity of ideas. We found no  qualitative  difference  between  AI  and  human-generated  creativity,  although  there  are differences in how ideas are generated. Interestingly, 9.4% of humans were more creative than the  most  creative  GAI,  GPT-4.  Our  findings  suggest  that  GAIs  are  valuable  assistants  in  the creative process. Continued research and development of GAI in creative tasks is crucial to fully understand this technology's potential benefits and drawbacks in shaping the future of creativity. Finally, we discuss the question of whether GAIs are capable of being 'truly' creative.

Keywords: Creativity, originality, AI, Generative Artificial Intelligence

## 3. Discussion

The question of whether GAIs such as ChatGPT, Studio.ai, and You.com can be considered creative is complex. Our research showed that their output for a standardized creativity measure for  broad-associative  "thinking"  is  as  original  as  the  human-generated  ideas.  Thus,  from  a scientific perspective, these chatbots are creative, as their output was judged as such by humans and AI and indistinguishable from human output. Some critics 8,9 have argued that chatbots cannot replicate the creativity of humans, as human creativity is a combination of real-world experience, emotion, and inspiration. However, the definition and common measurement of creativity do not require these elements. It is defined as the ability to produce something new and useful 16 , which

can  be  judged  by  those  engaging  with  the  potentially  creative  output.  We  believe  that  this definition can also be applied to GAIs. Our results show that when chatbots are asked the same simple question as humans, they generate more ideas, which are, on average, as original as ideas generated by humans. As the sheer number of ideas is less important, and the assessment style between humans and chatbot conversations is less comparable, we do not want to stress the results for fluency too much. However, GAI chatbots can recombine knowledge so that the ideas presented are considered original.

The argument against GAIs' creative potential stems from two distinct but linked arguments: GAI is missing (so far) a connection to the real world, with emotions and imagination, and second, GAI is thus not capable of "actual" creativity, as Big-C endeavors. Although we cannot speak against both positions, we aim to advance this debate by closely looking into human creativity: generating creative  output  is  much  closer  to  recombining  existing  knowledge  than  actually  developing anything  new 3,36 ,  and  secondly,  most  humans  do  not  come  close  to  creative  acts  which  are leading to Big-C. Instead, we use and apply our human creativity to improve (and improvise) everyday  tasks 7,25 .  This  is  not  to  belittle  human  creativity  but  instead  aims  to  show  the  GAI chatbot's potential to be comparable to human creative abilities.

Especially art as a creative output seems driven by our human ability to dream, visualize and imagine potential futures. However, developing new ideas, which can serve a specific intention, solve  an  issue,  or  deliver  an  abstract  meaning,  is  always  built  on  a  cumulative  tradition  of knowledge within the domain of art 37,38 . Most creativity-support systems in businesses thus focus on generating, processing, and retrieving knowledge 39,40 . Brain scan analyses showed that idea generation is similar to knowledge retrieval 41 . Thus, similar to GAIs, we retrieve and recombine existing knowledge to make it appear new. Arguably, current databases of these chatbots do have a much larger knowledge base than any human being could possess, which makes the potential idea recombination that chatbots can provide a much wider 42 .

The second argument, the missing potential for "actual" Big-C, seems unjust against the GAIs: human's ability of Big-C - bringing forward actual world-changing ideas - is also minimal. Mostly, we generate something new and useful for us in a specific and thus limited context. Our study shows GAI chatbots can compete with human ideation skills when it comes to everyday creativity. The prompts we used for the idea generation are very generic. When we consider more complex problems, a proper solution is achieved by including several factors, such as intense domain knowledge  and  creative  thinking,  individual  subjective  experiences,  emotions,  cultural background, and the capacity for abstract thinking. Here, current GAI chatbots appear to perform very well on complex knowledge-intensive tasks, such as complex coding tasks: ChatGPT can free up coders on tedious work 43 so that the coder can focus on more complex, creative work aspects 44 .  However,  ChatGPT  is  shown  to  be  rather  limited  in  emotional  responses  and evaluations and shows less reliable performance with more complex tasks 45 .

Overall, GAI chatbots show a convincing human-like performance for some tasks, whereas their performance is limited in others. Concerning creative performance, GAI can generate ideas based on specific input but cannot create the need to ideate. The motivation to engage with a specific creative task and problem understanding must come from the human interacting with the tool 23 .

Thus, GAI is limited considering the overall creative process:  it would not trigger the creative process. It can only respond to a prompt that is given. Thus, the problem definition is currently still  uniquely  human,  as  is  evaluating  whether  an  idea  fits  a  problem.  Although,  for  particular contexts, such as the assessment of the AUT output, we believe that an AI 32 sufficiently assessed the quality of the generated ideas.

GAI chatbots can therefore be used to identify seemingly new connections based on the broad knowledge base at certain points in the creative process. The person's responsibility is to embed in a relevant problem and the actual implementation of a selected solution. Our study shows that chatbots can generate ideas on the same level as humans, especially on the level of everyday creativity (with ChatGPT4 showing the best results, followed by Copy.ai, ChatGPT3, and YouChat scoring all similarly high in terms of the originality of ideas). Whether the person interacting with the GAI achieves little- or Big-C achievements are more up to the person than the GAI. GAI can successfully support the creative process and generate ideas, but it remains the task of humans to make sense of it and embed this in physical reality 46 .

Our experimental design likely led to an underestimation of the creativity of humans and GAIs. We paid participants to generate ideas for creative tasks they might not care about. However, intrinsic motivation strongly contributes to creative performance 47 , potentially leading to an overall underperformance. Regarding chatbots, smart prompting is how the best answers are obtained, which we did not use to allow a direct comparison between humans and chatbots. The actual potential for chatbots as creative assistants is likely much higher. Tailored prompts and reshaping answers given by the chatbots will likely lead to much more concise and, thus, relevant answers. Also,  chatbots  can  be  used  to  get  information  from  a  specific  angle,  such  as  for  a  certain profession, which can improve the quality of answers a user seeks, which we did not test here either.

There  are  some  limitations  to  our  research.  Although  the  AUT  is  a  widely  used  creativity measurement, there is an ongoing debate regarding its validity 29,48 . As chatbots use wide parts of the internet as a source of their data, it could be the case these databases include test material and thus previously given human answers to the prompts used by the AUT. We did not measure usefulness to assess the reported ideas because originality is the more important part of the "new and useful" definition 49 . Further, judging an idea's usefulness is difficult without a proper real-life application to serve as an anchor. When we assessed the AUT with the chatbots, we pushed for more answers,  with  a  relatively  arbitrary  number  maximum  of  three  times.  Thus,  the  fluency assessment is not very meaningful because chatbots are programmed to create vast amounts of text.

Research  has  shown  that  exposure  to  other  people's  creative  ideas  can  stimulate  cognitive activity  and  enhance  creativity 50 .  Participants  who  were  prompted  with  highly  creative  ideas generated more creativity than those who were given random, unrelated words. In this study, the comparison between the most original humans vs. chatbot shows that humans had the most creative answers in all but one case. Thus, humans serve as proper ideation partners. However, on a more pragmatic note, it might be easier to ask a chatbot than to find a motivated human to

run  ideas  by.  Our  study  and  a  lot  of  anecdotal  evidence  on  the  web  show  the  possibility  of generating  creative  output  in  combination  with  a  GAI,  be  it  a  writing  tool,  chatbot,  or  picture generation. The potential is real for GAIs to properly support human (creative) work. However, the ethical dilemma needs to be properly addressed, as the potential for misuse 19 or  harmful application is present as with any potent technology 51 .

In summary, whether GAI is creative can be answered pragmatically with "yes, as much or as little as humans". We recommend avoiding viewing GAI chatbots as omnipotent tools that may replace human performance. Instead, they can be valuable assistants in reviewing thoughts and ideas. The extensive knowledge base they build upon can be very useful in expanding one's ideas. The more our (working) lives are automated, and the more authority automation acquires, the more important the human role with its creative abilities becomes 52 .

## Materials

Participants completed the Alternative Use Test five times. They were instructed to write down as many ideas as possible for a ball, fork, pants, tire, and tooth, respectively. These objects are commonly used in creativity tests 53-55 and can therefore be reliably assessed by the AI-rater we used 32 (the AI had been trained on many prompts, including the five we used). Human participants were given three minutes for each object to write down as many ideas as possible. The order in which the prompts were presented was randomized. To get responses from the six GAI chatbots, we used the same prompt: "What can you do with [prompt]?". We used separate chat sessions for each prompt, so prior answers would not impact the following ones. For all chatbots, responses were limited to a certain length of answers, which we increased by asking "What else?" up to three times (for Copy.ai, we used the option "more like this"). In some instances, a chatbot would also respond with something like "I can't think of anything." This is similar to what some humans reported. These kinds of no-answers were excluded from the data set. In other cases, the chatbots would report unrelated answers (e.g., "I am not a big fan of the toothbrush. I think it is overrated."). This is again similar to human answers, and those were also excluded from the data. In one case, when asking for the use of pants, alpa.ai could not bring up any uses.

## Procedure

Data was collected in early February 2023. Six human raters rated the responses from human participants and five of the six GAI chatbots (GPT-4 was released on the 14th of March), blind to the origin of the responses. The order of the prompts was randomized throughout the raters, and the  list  of  ideas  was  randomized.  The  six  human  raters  were  instructed  to  follow  the  CAT method 31 , which comprised using the full range of the originality scale from 1-5. Additionally, we assessed originality  scores  for  all  human-generated  as  well  as  all  six  GAI  chatbot-generated answers by a trained large language model for assessing AUT prompts 32 . This model is trained on prior human assessments for the standardized AUT test. We selected five prompts for which the AI showed the highest reliability.

Fluency  scores  were  calculated  for  the  AI  and  the  six  raters  as  the  sum  of  ideas  from  each participant and the GAI chatbots. The sum of ideas varied slightly, as the raters differed in their assessment of non-relevant answers coded as no-answer.

