## Abstract

A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-1 2 , a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.

Figure 2: Selected examples generated from KOSMOS-1. Blue boxes are input prompt and pink boxes are KOSMOS-1 output. The examples include (1)-(2) visual explanation, (3)-(4) visual question answering, (5) web page question answering, (6) simple math equation, and (7)-(8) number recognition.

<!-- image -->

<!-- image -->

Figure 3: Selected examples generated from KOSMOS-1. Blue boxes are input prompt and pink boxes are KOSMOS-1 output. The examples include (1)-(2) image captioning, (3)-(6) visual question answering, (7)-(8) OCR, and (9)-(11) visual dialogue.

<!-- image -->

Table 1: We evaluate the capabilities of KOSMOS-1 on language, perception-language, and vision tasks under both zero- and few-shot learning settings.

| Dataset                           | Task description                                 | Metric      | Zero-shot   | Few-shot   |
|-----------------------------------|--------------------------------------------------|-------------|-------------|------------|
| Language tasks                    |                                                  |             |             |            |
| StoryCloze [MRL + 17]             | Commonsense reasoning                            | Accuracy    | 3           | 3          |
| HellaSwag [ZHB + 19]              | Commonsense NLI                                  | Accuracy    | 3           | 3          |
| Winograd [LDM12a]                 | Word ambiguity                                   | Accuracy    | 3           | 3          |
| Winogrande [SBBC20]               | Word ambiguity                                   | Accuracy    | 3           | 3          |
| PIQA [BZB + 20]                   | Physical commonsense                             | Accuracy    | 3           | 3          |
| BoolQ [CLC + 19]                  | Question answering                               | Accuracy    | 3           | 3          |
| CB [dMST19]                       | Textual entailment                               | Accuracy    | 3           | 3          |
| COPA [RBG11]                      | Causal reasoning                                 | Accuracy    | 3           | 3          |
| Rendered SST-2 [RKH + 21]         | OCR-free sentiment classification                | Accuracy    | 3           |            |
| HatefulMemes [KFM + 20]           | OCR-free meme classification                     | ROC AUC     | 3           |            |
| Cross-modal transfer              |                                                  |             |             |            |
| RelativeSize [BHCF16]             | Commonsense reasoning (object size)              | Accuracy    | 3           |            |
| MemoryColor [NHJ21]               | Commonsense reasoning (object color)             | Accuracy    | 3           |            |
| ColorTerms [BBBT12]               | Commonsense reasoning (object color)             | Accuracy    | 3           |            |
| Nonverbal reasoning tasks IQ Test | Raven's Progressive Matrices                     | Accuracy    | 3           |            |
| Perception-language tasks         |                                                  |             |             |            |
| COCO Caption [LMB + 14]           | Image captioning                                 | CIDEr, etc. | 3           | 3          |
| Flicker30k [YLHH14]               | Image captioning                                 | CIDEr, etc. | 3           | 3          |
| VQAv2 [GKSS + 17]                 | Visual question answering                        | VQA acc.    | 3           | 3          |
| VizWiz [GLS + 18]                 | Visual question answering                        | VQA acc.    | 3           | 3          |
| WebSRC [CZC + 21]                 | Web page question answering                      | F1 score    | 3           |            |
| Vision tasks                      |                                                  |             |             |            |
| ImageNet [DDS + 09]               | Zero-shot image classification                   | Top-1 acc.  | 3           |            |
| CUB [WBW + 11]                    | Zero-shot image classification with descriptions | Accuracy    | 3           |            |

## 2 KOSMOS-1: A Multimodal Large Language Model

As shown in Figure 1, KOSMOS-1 is a multimodal language model that can perceive general modalities, follow instructions, learn in context, and generate outputs. Given the previous context, the model learns to generate texts in an auto-regressive manner. Specifically, the backbone of KOSMOS-1 is a Transformer-based causal language model. Apart from text, other modalities are embedded and fed into the language model. The Transformer decoder serves as a general-purpose interface to multimodal input. We train KOSMOS-1 on multimodal corpora, including monomodal data, crossmodal paired data, and interleaved multimodal data. Once the models are trained, we can directly evaluate the models in zero-shot and few-shot settings on both language tasks and multimodal tasks.

## 2.3 Training Objective

The KOSMOS-1 training is conducted on web-scale multimodal corpora, including monomodal data (e.g., text corpus), cross-modal paired data (e.g., image-caption pairs), and interleaved multimodal data (e.g., documents of arbitrarily interleaved images and texts). To be specific, we use monomodal data for representation learning. For example, language modeling with text data pretrains instruction following, in-context learning, and various language tasks. Moreover, cross-modal pairs and interleaved data learn to align the perception of general modalities with language models. Interleaved data also naturally fit in the multimodal language modeling task. We present more details of training data collection in Section 3.1.

The models are trained with the next-token prediction task, i.e., learning to generate the next token depending on the previous context. The training objective is to maximize the log-likelihood of tokens in examples. Notice that only discrete tokens, such as text tokens, are accounted for in the training loss. Multimodal language modeling is a scalable way to train the models. More importantly, the emergence of various capabilities makes the training task favorable for downstream applications.

## 3.2 Training Setup

The MLLM component has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size, and 32 attention heads, resulting in about 1.3B parameters. We use Magneto's initialization for optimization stability. For faster convergence, the image representation is obtained from a pretrained CLIP ViT-L/14 model with 1,024 feature dimensions. The images are preprocessed into 224 × 224 resolution during training. We freeze the parameters of the CLIP model except for the last layer during training. The total number of parameters of KOSMOS-1 is about 1.6B. More details about hyperparameters can be found in Appendix A.

We use a batch size of 1.2 million tokens (0.5 million tokens from text corpora, 0.5 million tokens from image-caption pairs, and 0.2 million tokens from interleaved data) and train KOSMOS-1 for 300k steps, corresponding to about 360 billion tokens. We adopt the AdamW optimizer with β = (0 . 9 , 0 . 98) . We set the weight decay to 0.01 and the dropout rate to 0.1. The learning rate increases to 2e-4 for the first 375 warming-up steps and decays linearly to 0 for the rest of the training steps. We use SentencePiece [KR18] to tokenize the text. We preprocess the data in the 'full-sentence' format [LOG + 19], which packs each input sequence with full sentences that are sampled continuously from one or more documents.

## 4.1.1 Evaluation Setup

We evaluate the caption generation on MS COCO Caption [LMB + 14], and Flickr30k [YLHH14]. We use the test set of COCO Karpathy split [KFF17], which re-partitions the train2014 and val2014 images [LMB + 14] into 113,287, 5,000, and 5,000 for the training set, validation set, and test set, respectively. We conduct an evaluation on Flickr30k's Karpathy split test set. The image resolution is 224 × 224. We use beam search to generate the captions, and the beam size is 5. In the few-shot settings, we randomly sample demonstrations from the training set. We use COCOEvalCap 4 to compute CIDEr [VLZP15] and SPICE [AFJG16] scores as the evaluation metrics. We prompt KOSMOS-1 with 'An image of' for zero-shot and few-shot caption generation experiments.

For visual question-answering tasks, we evaluate zero-shot and few-shot results on test-dev set of VQAv2 [GKSS + 17] and test-dev set of VizWiz [GLS + 18], respectively. The resolution of images is 224 × 224. We use greedy search for the decoding. We follow the normalization rules of the VQAv2 evaluation code 5 when computing the VQA accuracy. We evaluate the performance of VQA in an open-ended setting that KOSMOS-1 generates answers and stops at the </s> ('end of sequence') token. The prompt is 'Question: {question} Answer: {answer}' for visual question answering tasks.

## 4.2.1 Evaluation Setup

To evaluate the KOSMOS-1 on zero-shot nonverbal reasoning, we construct a dataset of the Raven IQ test. It consists of 50 examples collected from different websites 6789 . Each example has three (i.e., 2 × 2 matrix), four, or eight (i.e., 3 × 3 matrix) given images. The goal is to predict the next one. Each instance has six candidate images with a unique correct completion. We measure accuracy scores to evaluate the models. The evaluation dataset is available at https://aka.ms/kosmos-iq50 .

Figure 4 illustrates how to evaluate KOSMOS-1 on the Raven IQ test. The matrix-style images are flattened and fed into the models one-by-one. To enable the model to better understand the desired task, we also use a textual instruction 'Here are three/four/eight images:' , 'The following image is:' , and 'Is it correct?' for conditioning. We append each possible candidate to the context separately and compare the probability that the model outputs 'Yes' in a close-ended setting. The candidate that yields the largest probability is regarded as the prediction.

## 4.3.1 Evaluation Setup

We evaluate OCR-free language understanding on the Rendered SST-2 [RKH + 21] test set and HatefulMemes [KFM + 20] validation set. We use accuracy as the metric for the Rendered SST-2 and report ROC AUC for the HatefulMemes dataset. We use the prompt 'Question: what is the sentiment of the opinion? Answer: {answer}' , where the answer is either positive or negative for the Rendered SST-2. For the HatefulMemes task, the prompt is 'Question: does this picture contain real hate speech? Answer: {answer}' , where the answer is either yes or no.

## 4.4.1 Evaluation Setup

We compare the performance on the Web-based Structural Reading Comprehension (WebSRC) dataset [CZC + 21]. For comparisons, we train a language model (LLM) on the same text corpora with the same training setup as in KOSMOS-1. The LLM takes the text extracted from the web page as input. Its template of the prompt is 'Given the context below from web page, extract the answer from the given text like this: Qusestion: Who is the publisher of this book? Answer: Penguin Books Ltd. Context: {WebText} Q: {question} A: {answer} ' , where the {WebText} presents the text extracted from the web page. Besides using the same prompt, KOSMOS-1 prepends the image before the prompt. Two example images from WebSRC are shown in Appendix C.3. Following the original paper [CZC + 21], we use exact match (EM) and F1 scores as our evaluation metrics.

## 4.5.1 Evaluation Setup

We evaluate the ability of multimodal chain-of-thought prompting on the Rendered SST-2. We use the prompt 'Introduce this picture in detail:' to generate the content in the picture as the rationale. Then, we use the prompt '{rationale} Question: what is the sentiment of the opinion? Answer: {answer}' to predict the sentiment, where the answer is either positive or negative.

## 4.6.1 Evaluation Setup

Given an input image, we concatenate the image with the prompt 'The photo of the' . The input is then fed into the model to obtain the category name of the image. We evaluate the model on ImageNet [DDS + 09], which contains 1.28M training images and 50k validation images in 1k object categories. The prediction is classified as correct if it is exactly the same as the ground-truth category name. The image resolution used for evaluation is 224 × 224. We use beam search to generate the category names and the beam size is 2.

## 4.7.1 Evaluation Setup

Following CUB [WBW + 11], we construct a bird classification dataset that contains images and natural-language descriptions of categories. The dataset has three groups of binary image classification. Each group contains two animal categories with similar appearances. Our goal is to classify images given the categories' descriptions. Table 11 presents the data samples. The first group is from [WBW + 11], while the other two groups are collected from the website. Each category contains twenty images.

The evaluation procedure is illustrated in Figure 6. For the zero-shot setting, we provide detailed descriptions of two specific categories and use the template 'Question:what is the name of {general category} in the picture? Answer:' to prompt the model for the specific category name in an openended manner. To evaluate the effect of providing verbal descriptions in context, we also implement a zero-shot baseline without prompting descriptions. Instead, we provide the corresponding specific names in the prompt.

## 4.8.1 Evaluation Setup

We train a language model (LLM) baseline with the same text corpora and training setup. We evaluate KOSMOS-1 and the LLM baseline on eight language tasks, including cloze and completion tasks (i.e, StoryCloze, HellaSwag), Winograd-style tasks (i.e, Winograd, Winogrande), commonsense reasoning (i.e, PIQA), and three datasets BoolQ, CB, and COPA from the SuperGLUE benchmark [WPN + 19]. The detailed descriptions of these datasets are provided in Appendix C.2. We conduct experiments under zero-shot and few-shot settings. We evaluate each test example by randomly sampling examples from the training set as demonstrations. We set the number of shots to 0, 1, and 4 in our experiments.

## 5 Conclusion

In this work, we introduce KOSMOS-1, a multimodal large language model that can perceive general modalities, follow instructions, and perform in-context learning. The models trained on web-scale

multimodal corpora achieve promising results across a wide range of language tasks and multimodal tasks. We show that going from LLMs to MLLMs enables new capabilities and opportunities. In the future, we would like to scale up KOSMOS-1 in terms of model size [MWH + 22, WMH + 22, CDH + 22], and integrate the speech [WCW + 23] capability into KOSMOS-1. In addition, KOSMOS-1 can be used as a unified interface for multimodal learning, e.g., enabling using instructions and examples to control text-to-image generation.

