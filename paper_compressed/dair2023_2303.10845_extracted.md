## ABSTRACT

The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors 2 and MindSpore framework 3 , and present the language model with 1.085T parameters named PanGuΣ . With parameter inherent from PanGuα [1], we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation (ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGuΣ provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.

K eywords Large Language Models · Distributed Training · Natural Language Processing

## 1 Introduction

Large Language Models (LLMs) [2, 3, 1, 4, 5, 6, 7, 8, 9, 10, etc.] have demonstrated unprecedented capabilities and potential in the areas of natural language understanding, generation and reasoning. By utilizing vast amount of textual data, the performance of language models scales up with compute budget and model parameters, demonstrating strong zero/few-shot learning abilities or even emergence abilities [4, 11]. Several large language models with hundreds of billion parameters have been published since GPT-3 [2], including but not limited to Megatron-Turing NLG [12], PanGuα [1], ERNIE 3.0 Titan [8], Gopher [5], PaLM [4], OPT [6], Bloom [10], and GLM-130B [9]. Researchers start to build even larger language models with more than one trillion parameters. Typically, this is accomplished by leveraging sparsely-activated models such as Mixture-of-Experts (MoE) [13]. Among the trillion-parameter models currently in existence, there are several noteworthy work such as Switch-C [14], GLaM [15], MoE-1.1T [16], Wu Dao 2.0 [17], and M6-10T [18]. However, only a select few have published comprehensive evaluation results over a wide range of tasks while simultaneously achieving anticipated performance. In our experience, the primary difficulty lies in the scaling efficiency.

Recent studies on the scaling laws of language models [19, 20, 21] demonstrate the necessity of training LLMs with sufficient amount of training data and corresponding compute budget to achieve optimal performance. Therefore, one of the main motivation for this work is to design a scalable model architecture and an efficient distributed training system that can consume the data with high training throughput.

- · Model Scaling . Model performance of LLMs is expected to scale up with larger model size. Comparing to the expensive computational cost for training dense Transformer model, sparse architectures such as Mixtureof-Experts (MoE) [13, 14, 15, 22] are considered to be an appealing choice to scale model size up without incuring linear increase in computational cost. However, MoE models suffer from the problems such as unbalanced workload and all-to-all communication latency. Moreover, how to extend existing dense model with MoE and how many experts to allocate in each layer remain open problems. Therefore, designing a trillion parameter sparse model with high performance and training efficiency is a significant yet challenging task.
- · System Scaling . Frameworks such as DeepSpeed 4 have been proposed to support training trillion parameter models. In practice, the main barrier often lies on limited compute budget, or more specifically the number of accelerating devices (e.g., GPU, NPU, TPU) that can be used. By utilizing techniques such as tensor parallelism [23], pipeline parallelism [24], zero redundancy optimizer [25] and rematerialization [26], practitioners can train trillion-parameter model with feasible batch sizes across thousands of accelerating devices. Alternatively, practitioners can reduce the amount of computation resources by utilizing heterogeneous computing techniques such as offloading some of the computation to host devices [27]. However, the current techniques inevitably hinder the training throughput due to slow bandwidth between the host and device as well as weak computing capabilities of CPUs compared to accelerating devices, which prevent feeding large language models with reasonably amount of data and achieving optimal performance. Therefore, how to efficiently scale the system performance with limited computation budget is critical to the performance of large language models.

In this work, we present PanGuΣ , a large language model with sparse architecture containing 1.085 trillion parameters. We develop PanGuΣ model under the framework of MindSpore 5 and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days. PanGuΣ inherent parameters from PanGuα [1] with Transformer decoder architecture and are extended via Random Routed Experts (RRE). Different from conventional MoE, RRE adopts two-level routing. At the first level experts are grouped by domain or task, and at the second level tokens are randomly and uniformly mapped to experts in each group without using any learnable gating function as in MoE. With the design of RRE, one can easily extract sub-models from the PanGuΣ for various downstream applications such as dialogue, translation, code generation or general nature language understanding. To make training system efficient and scalable, we propose Expert Computation and Storage Separation (ECSS) mechanism, which achieves 69905 tokens/s observed throughput in training 1.085 trillion PanGuΣ on cluster of 512 Ascend 910 accelerators, and reduces Host-to-Device and Device-to-Host communication as well as optimizer update computation by a large margin. As a whole, the training throughput is improved by 6.3x compared to the model of the same hyper-parameters but with MoE architecture. By consuming 329B tokens in more than 40 natural and programming languages, the sub-modal of PanGuΣ in Chinese domain significantly outperforms the previous SOTA models including PanGuα with 13B parameters and ERNIE 3.0 Titan [8] with 260B parameters over 16 downstream tasks in six categories in the zero-shot setting without any multitask finetuning or instruction tuning. We also test the performance of fine-tuned PanGuΣ on several applications domain such as dialogue, machine translation and code generation. PanGuΣ outperforms the SOTA models in the corresponding areas.

The rest of the technical report is organized as follows. Section 2 introduces the design philosophy and the architecture of PanGuΣ model. Section 3 introduces the collection and organization of the dataset. Section 4 describes system design and acceleration techniques. Section 5 presents the experimental results of PanGuΣ model.

## 2 Model



## 2.2 PanGuΣ Architecture



## 2.2.1 Overview

Figure 1: PanGuΣ architecture. The architecture is mixed by dense transformer layers and sparse transformer layers. The lower M layers are dense layers shared across different domains. The upper N transformer layers' feed-forward part are sparsely activated via Random Routed Experts (RRE). Tokens from different domains have different embeddings.

<!-- image -->

PanGuΣ adopts an auto-regressive language modeling with stacked transformer decoder layers and a query layer on the top. The PanGuΣ architecture offers a flexible design. The bottom M layers are globally shared across all the domains, and the top N layers (including the query layer) are sparsely activated according to the domains of the input data. In each RRE layers, there are K experts in G groups in total, the number of experts in each group can be different. This flexible design offers three mode.

- · Mixed mode : when M > 0 , N > 0 and K > 0 , model contains both sparse RRE layers and dense layers.

- · Dense mode : when N = 0 or K = 1 , the architecture will reduce to a dense PanGuα model.
- · Sparse mode : when M = 0 and K > 1 , the architecture will be a sparse model.

In this trillion-parameters modeling practice, We use the mixed configuration by placing the shared parameters close to the input layer (bottom) and all the sparsely activated expert parameters close to the output layer (top). In the model designing stage, we benchmark various experts placement strategies on smaller scale models and the selected strategy obtains the lowest language modeling perplexity. Our hypothesis is that bottom layers tends to learn general knowledge, while the specific knowledge is in a higher level of abstraction and is more appropriate to be learned by the top layers. In the token embedding layer, we choose to use different embedding matrices for different domains.

## 3 Dataset



## 5.3.2 Evaluation Details

Each dataset of all Chinese downstream tasks can be evaluated using either a generation-based method or a scoring-based method. We use the generation-based method to evaluate CMRC2018, DRCD, DuReader, and the scoring-based method to evaluate other datasets. For each instance, a text sequence is obtained by filling it into a manually designed template, and then the text sequence is fed into PanGuΣ for prediction to get the result. The templates we used for all datasets are shown in Table 4.

Table 4: The templates for all datasets. "/" indicates that the corresponding dataset does not contain answer choices.

| Dataset                       | Template                                                                                                                                                                                                                                                                                                                                                                | Answer Choices                                                                                                                               |
|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| CMRC2018 DRCD DuReader C3     | 阅 读 文 章 ： {Passage}\n 根 据 上 文 ， 回 答 如 下 问 题 ： {Question}\n 答 ： 阅 读 文 章 ： {Passage}\n 根 据 上 文 ， 回 答 如 下 问 题 ： {Question}\n 答 ： 阅 读 文 章 ： {Passage}\n 根 据 上 文 ， 回 答 如 下 问 题 ： {Question}\n 答 ： 问 ： {Question}\n 答 ： {Answer}\n 该 答 案 来 自 对 话 ： {Passage}                                             | / / / Answer ∈ All options                                                                                                                   |
| CMNLI OCNLI                   | {Premise} ？ {Answer} ， {Hypothesis} {Premise} ？ {Answer} ， {Hypothesis}                                                                                                                                                                                                                                                                                             | Answer ∈ { 对 , 错 , 或 许 } Answer ∈ { 对 , 错 , 或 许 }                                                                                    |
| TNEWS IFLYTEK                 | 这 是 关 于 {Answer} 的 文 章 ： {Passage} 这 是 关 于 {Answer} 的 应 用 程 序 ： {Passage}                                                                                                                                                                                                                                                                             | Answer ∈ All categories Answer ∈ All categories                                                                                              |
| AFQMC CSL                     | {Sentence1} 。 {Sentence2} 。 上 面 两个 句 子 的 语 义 是 {Answer} 摘 要 ： {Abstract} {Answer} 的 关 键 词 ： {Keywords}                                                                                                                                                                                                                                              | Answer ∈ { 不 同 的 , 相 同 的 } Answer ∈ { 含 有 错 误 , 全 部 正 确 }                                                                      |
| CLUEWSC2020                   | {Text to the left of the pronoun}{Answer}{Text to the right of the pronoun}                                                                                                                                                                                                                                                                                             | Answer ∈ All mentions                                                                                                                        |
| CHID PD CFT CMRC2017 CMRC2019 | {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} {Text to the left of the blank}{Answer}{Text to the right of the blank} | Answer ∈ All candidates Answer ∈ All words in the text Answer ∈ All words in the text Answer ∈ All words in the text Answer ∈ All candidates |

## Generation-based evaluation method

For each instance to be predicted, it is filled into the corresponding template to obtain a text sequence. After that, the text sequence is used as the input to PanGuΣ to generate the answer. We use a greedy decoding strategy to generate the answer.

## Scoring-based evaluation method

Each instance to be predicted contains multiple candidate answers. For each candidate answer, a text sequence is obtained by filling the candidate answer together with the sample into the corresponding template, and the perplexity of the text sequence is calculated by PanGuΣ . Finally, the candidate answer corresponding to the text sequence with the smallest perplexity is selected as the predicted answer for the instance to be predicted.

## 5.4.5 Natural language generation of base PanGuΣ model

For evaluating base PanGuΣ 's abilities on open-ended text generation, we present three categories of cases about character dialog, question-answering and text generation with few-shot prompt learning, which are shown in Table 12, Table 13 and Table 14 respectively. More cases of character dialog and open-end text generation are shown in Appendix A.4 and Appendix A.5.

## A.3 Open domain question-answering of dialog model

Table 2: Cases of open domain question-answering

| Question:       | 世 界 上 的 四 大 文 明 古 国 是 哪 四 个 ？                     | What are the four ancient civilizations in the world?                                     |
|-----------------|------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| EVA2.0:         | 世 界 上 的 四 大 文 明 古 国 : 世 界 上 的 四 大 文 明 古 国    | The four ancient civilizations in the world: the four ancient civilizations in the world. |
| PanGu-Bot 2.6B: | 古 巴 比 伦 、 古印 度 、 古 埃 及 、 古 埃 及                   | Ancient Babylon, Ancient India, Ancient Egypt and Ancient Egypt                           |
| PanGu- Σ :      | 古印 度 、 古 埃 及 、 古 巴 比 伦 、 中 国                      | Ancient India, Ancient Egypt, Ancient Babylon and China                                   |
| Question:       | 目 前 ， 中 国 的 人 口 大 约 有 多 少 亿 ？                     | At present, how many billions of people are there in China?                               |
| EVA2.0:         | 我 觉 得 应 该 是 100 亿 左 右                                   | I think it's about 10 billion.                                                            |
| PanGu-Bot 2.6B: | 960 万 , 占 全 国 总 人 口 的百 分 之七 十 。                    | 9.6 million, 70 percent of China's total pop- ulation.                                    |
| PanGu- Σ :      | 中 国 有 十 四 亿人 口                                           | China has a population of 1.4 billion.                                                    |
| Question:       | 美 术 中 的 ' 三 原 色 ' 指 哪 三 种 颜 色                       | What are the "three primary colors" in art?                                               |
| EVA2.0:         | 三 原 色 指 的 是 三 原 色 的 颜 色 , 指 的 是 三 原 色 的 色 。 | Yellow, red and green                                                                     |
| PanGu-Bot 2.6B: | 颜 红 、 黄 、 蓝 、 绿 、 紫                                    | Red, yellow, blue, green, purple.                                                         |
| PanGu- Σ :      | 红 、 黄 、 蓝                                                   | Red, yellow and blue.                                                                     |

