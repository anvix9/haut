## Abstract

Humans describe the physical world using natural language to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract associations, or actionable affordances. In this work we propose Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf models like CLIP into NeRF, which enable these types of open-

ended language queries in 3D. LERF learns a dense, multiscale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embeddings across training views to provide multi-view consistency and smooth the underlying language field. After optimization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding visionlanguage models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D

CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hierarchically across the volume. See the project website at: https://lerf.io .

## 1. Introduction

Neural Radiance Fields (NeRFs) [24] have emerged as a powerful technique for capturing photorealistic digital representations of intricate real-world 3D scenes. However, the immediate output of NeRFs is nothing but a colorful density field, devoid of meaning or context, which inhibits building interfaces for interacting with the resulting 3D scenes.

Natural language is an intuitive interface for interacting with a 3D scene. Consider the capture of a kitchen in Figure 1. Imagine being able to navigate this kitchen by asking where the 'utensils' are, or more specifically for a tool that you could use for 'stirring' , and even for your favorite mug with a specific logo on it - all through the comfort and familiarity of everyday conversation. This requires not only the capacity to handle natural language input queries but also the ability to incorporate semantics at multiple scales and relate to long-tail and abstract concepts.

In this work, we propose Language Embedded Radiance Fields (LERF), a novel approach that grounds language within NeRF by optimizing embeddings from an offthe-shelf vision-language model like CLIP into 3D scenes. Notably, LERF utilizes CLIP directly without the need for finetuning through datasets like COCO or reliance on mask region proposals, which limits the ability to capture a wide range of semantics. Because LERF preserves the integrity of CLIP embeddings at multiple scales, it is able to handle a broad range of language queries, including visual properties ( 'yellow' ), abstract concepts ( 'electricity' ), text ( 'boops' ), and long-tail objects ( 'waldo' ) as illustrated in Figure 1.

We construct a LERF by optimizing a language field jointly with NeRF, which takes both position and physical scale as input and outputs a single CLIP vector. During training, the field is supervised using a multi-scale feature pyramid that contains CLIP embeddings generated from image crops of training views. This allows the CLIP encoder to capture different scales of image context, thus associating the same 3D location with distinct language embeddings at different scales ( e.g . 'utensils' vs . 'wooden spoon' ). The language field can be queried at arbitrary scales during test time to obtain 3D relevancy maps. To regularize the optimized language field, self-supervised DINO [5] features are also incorporated through a shared bottleneck.

LERF offers an added benefit: since we extract CLIP embeddings from multiple views over multiple scales, the relevancy maps of text queries obtained through our 3D CLIP embedding are more localized compared to those ob-

tained via 2D CLIP embeddings. By definition, they are also 3D consistent, enabling queries directly in the 3D fields without having to render to multiple views.

LERF can be trained without significantly slowing down the base NeRF implementation. Upon completion of the training process, LERF allows for the generation of 3D relevancy maps for a wide range of language prompts in realtime. We evaluate the capabilities of LERF on a set of hand-held captured in-the-wild scenes and find it can localize both fine-grained queries relating to highly specific parts of geometry ( 'fingers' ), or abstract queries relating to multiple objects ( 'cartoon' ). LERF produces view-consistent relevancy maps in 3D across a wide range of queries and scenes, which are best viewed in videos on our website. We also provide quantitative evaluations against popular openvocab detectors LSeg [21] and OWL-ViT [25], by distilling LSeg features into 3D [20] and querying OWL-ViT from rendered novel views. Our results suggest that features in 3D from LERF can localize a wide variety of queries across in-the-wild scenes. The zero-shot capabilities of LERF leads to potential use cases in robotics, analyzing visionlanguage models, and interacting with 3D scenes. Code and data will be made available at https://lerf.io .

## 3.4. Field Architecture

Intuitively, optimizing a language embedding in 3D should not influence the distribution of density in the underlying scene representation. We capture this inductive bias in LERF by training two separate networks: one for feature vectors (DINO, CLIP), and the other for standard NeRF outputs (color, density). Gradients from L lang and L dino do not

affect the NeRF outputs, and can be viewed as jointly optimizing a language field in conjunction with a radiance field.

Werepresent both fields with a multi-resolution hashgrid [26]. The language hashgrid has two output MLPs for CLIP and DINO respectively. Scale s is passed into the CLIP MLP as an extra input in addition to the concatenated hashgrid features. We adopt the Nerfacto method from Nerfstudio [35] as the backbone for our approach, leveraging the same proposal sampling, scene contraction, and appearance embeddings

## 3.6. Implementation Details

We implement LERF in Nerfstudio [35], on top of the Nerfacto method. Proposal sampling is the same except we reduce the number of LERF samples from 48 to 24 to increase training speed. We use the OpenClip [10] ViTB/16 model trained on the LAION-2B dataset, with an image pyramid varying from s min = . 05 to s min = . 5 in 7 steps. The hashgrid used for representing language features is much larger than a typical RGB hashgrid: it has 32 layers from a resolution of 16 to 512, with a hash table size of 2 21 and feature dimension of 8. The CLIP MLP used for F lang has 3 hidden layers with width 256 before the final 512 dimension CLIP output. The DINO MLP for F DINO has 1 hidden layer of dimension 256.

We use the Adam optimizer for proposal networks and fields with weight decay 10 -9 , with an exponential learning rate scheduler from 10 -2 to 10 -3 over the first 5000 training steps. All models are trained to 30,000 steps (45 minutes), although good results can be obtained in as few as 6,000(8 minutes) as presented in the Appendix. We train on an NVIDIA A100, which takes roughly 20GB of memory total. One can interactively query in real-time within the Nerfstudio viewer. The Î» used in weighting CLIP loss is 0 . 01 , chosen empirically and ablated in Sec 4.4. When computing relevancy score, we multiply similarity by 10 as a temperature parameter within the softmax.

## 4.4. Ablations

No DINO : Removing DINO results in a qualitative deterioration in the smoothness and boundaries of relevancy maps, especially in regions with few surrounding views or little geometric separation between foreground and background. We show two illustrative examples where DINO improves the quality of relevancy maps in Fig. 5.

Single-Scale Training : We ablate multi-scale CLIP supervision from the pipeline by only training on a fixed s 0 = 15% image scale. Doing so significantly impairs LERF's ability to handle queries of all scales, failing on both large ( 'espresso machine' ) queries it doesn't have enough context for, as well as queries for which it does ( 'creamer pods' ). These results imply that multi-scale training regularizes the language field at all scales, not just ones with relevant context for a given query.

## 5. Limitations

LERF has limitations associated with both CLIP and NeRF; some are visualized in Fig. 9. Like CLIP, language queries from LERF often exhibit 'bag-of-words' behavior (i.e., 'not red' is similar to 'red' ) and struggles to capture spatial relationships between objects. LERF can be prone to false positives with queries that appear visually or semantically similar: 'zucchinis' activate on other similarlyshaped vegetables, though zucchinis are more relevant than the distractors (Fig. 9).

LERF requires known calibrated camera matrices and NeRF-quality multi-view captures, which aren't always available or easy to capture. The quality of language fields is bottlenecked by the quality of the NeRF recontsruction. In addition, because of the volumetric input to F lang, objects which are near other surfaces without side views can result in their embeddings being blurred to their surroundings since no views see the background without the object. This results in similar blurry relevancy maps to single-view CLIP (Fig. 4). In addition, we only render language embed-

dings from a single scale for a given query. Some queries could benefit from or even require incorporating context from multiple scales (eg 'table' ).

## 6. Conclusions

We present LERF, a novel method of fusing raw CLIP embeddings into a NeRF in a dense, multi-scale fashion without requiring region proposals or fine-tuning. We find that it can support a broad range of natural language queries across diverse real-world scenes, strongly outperforming pixel-aligned LSeg in supporting natural language queries. LERF is a general framework that supports any aligned multi-modal encoders, meaning it can naturally support improvements to vision-language models. Code and datasets will be released after the submission process.

## E. Experiment details

We provide a list of the labels used for the localization experiment in Tab. 3. Each label was labeled in 3-4 different

Figure 12: Prompt tuning case study : Some objects are sensitive to the prompt, with more specific wordings producing better results.

<!-- image -->

views. We provide an exhaustive list of our custom long-tail labels for the existence experiment in Tab. 4.

Figure 13: CLIP bag-of-words behavior : CLIP sometimes behaves as a bag-of-words, resulting in some adjectives not properly incorporating into queries.

<!-- image -->

Figure 14: Degradation with poor NeRF geometry : Floaters and incomplete geometry can produce unreliable rendered CLIP embeddings.

<!-- image -->

Fig.12 illustrates examples of relevancy maps improving in quality with subtly changed queries to become more and more specific. Usually this has a subtle effect on relevancy maps by refining the activation to a more localized region, for example providing progressively more descriptive queries improves the relevancy activation on the tea cup. This effect can also be drastic, for example 'Dish soap' primarily activates on a pump soap bottle, but describing 'blue dish soap' shifts focus to the correct object.

<!-- image -->

Target

<!-- image -->

Target

"USB Cable

<!-- image -->

Figure 15: Geometric separation impacts quality : Queries without much geometric separation can blur between objects and foreground-background. In the toaster case, very few viewing angles were taken because of its position, which results in a fuzzier boundary.

<!-- image -->

"Toaster

## F. Detailed Illustrations of Limitations

LERF inherits limitations from CLIP relating to language ambiguity and prompt sensitivity, as well as from NeRF's geometry representation capabilities. We present additional figures on failure cases to complement the ones provided in the main text.

Fig.10 showcases visual and language ambiguity from our usage of CLIP. Some queries get confused by unrelated regions of the scene because they appear very similar, such as the portafilter and the coffee grinder. In the refrigerator query, unrelated parts of the kitchen also activate in relevancy maps, though less strongly than refrigerator, because the CLIP embeddings of square white cabinets are more similar to a refrigerator than the canonical phrases. Sugar packets appear to be a confusing case for LERF, getting distracted in two separate scenes (teatime, espresso machine) with a tea packet and creamer pods respectively.

Fig.13 highlights another well-known undesirable property inherited from CLIP: text embeddings often behave as a bag-of-words rather than a grammatically parsed sentence. As a result, sometimes adding additional adjectives cause the output to latch onto incorrect regions ( 'mug handle' vs 'handle' or 'coffee spill' vs 'spill' .)

Fig.14 shows performance degradation when geometry is unreliable in the underlying NeRF: reflective objects like the table in the ramen scene can produce holes, which result in CLIP embeddings from multiple views incorrectly averaging. Highly transparent objects like the glass cup in the espresso scene also suffer from lack of density, since the rendering weights mostly focus on the opaque background rather than the transparent foreground.

Finally, Fig.15 shows cases where lack of geometric separation (a cable close to a table, or a toaster flush in the corner) causes the relevancy maps to blur into other surrounding objects because most views of the background contain the foreground object in front.

<!-- image -->

Figure 16: Additional results of scenes not reported fully in the main text or rendered in videos.

| Scene: Espresso Machine                     | "Brush          |                               | "Mug'               |
|---------------------------------------------|-----------------|-------------------------------|---------------------|
| "Digital scale                              |                 |                               |                     |
|                                             |                 | "Napkins "Coffee beans"       | "Tamper"            |
| Scene: Fruit Aisle "Plums Scene: Ramen bowl | "Limes "Pomelos | "Lemons "Oranges "Pork belly" | "Nectarines "Fruits |
| Scene: Table                                |                 |                               |                     |
|                                             | "Pink spiral"   |                               |                     |
| "Chopsticks                                 |                 | "Noodles on a plate           | "Noodles in a soup  |
|                                             |                 | "Airpods case "Red bag        |                     |
|                                             |                 | "QR Code                      | "Ramen              |
|                                             |                 |                               | "Packing tape roll  |
|                                             | of food"        |                               |                     |

Table 3: Labels used during detection experiments (75 total).

| Scene                                                               | Text queries                                               | Text queries                                                     | Text queries                                          | Text queries   |
|---------------------------------------------------------------------|------------------------------------------------------------|------------------------------------------------------------------|-------------------------------------------------------|----------------|
| blue hydroflask copper-bottom pot olive oil power outlet spice rack | coffee grinder dish soap paper towel roll red mug utensils | cookbooks faucet pepper mill scrub brush vegetable oil carnation | cooking tongs knives pour-over vessel sink waldo      | Kitchen        |
| big white crinkly flower eucalyptus vase                            | bouquet lily                                               | rosemary                                                         | daisy small white flowers                             | Bouquet        |
| green apple old camera quilted pumpkin rubics cube toy chair        | ice cream cone pikachu rabbit spatula toy elephant broth   | jake pink ice cream red apple tesla door handle twizzlers        | miffy porcelain hand rubber duck toy cat statue waldo | Figurines      |
| bowl glass of water pork belly                                      | green onion ramen                                          | chopsticks napkin sake cup                                       | egg nori wavy noodles                                 | Ramen          |
| bag of cookies cookies on a plate plate stuffed bear                | bear nose dall-e sheep tea in a glass                      | coffee hooves spill yellow pouf                                  | coffee mug paper napkin spoon handle                  | Teatime        |

| Scene Positive Labels                                                                                                                                                                                                                                                                                                                                    |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Figurines jake, miffy, rabbit, bunny, old camera, toy elephant, twizzlers, quilted pumpkin, tesla door handle,                                                                                                                                                                                                                                           |
| Kitchen red mug, pour-over vessel, olive oil, vegetable oil, cookbooks, waldo,                                                                                                                                                                                                                                                                           |
| Teatime stuffed bear, sheep, bear nose, coffee mug, spill, tea in a glass, cookies on a plate, bag of cookies, dall-e, hooves,coffee, yellow pouf, spoon handle, paper napkin, plate, wood, bag of food, hand sanitizer, mug,                                                                                                                            |
| Table wood texture, red bag of food, hand sanitizer bottle, airpods case, usb cable, brown paper napkins, transparent tupperware, colorful coaster, packing tape roll, hardware clamps, iphone, laptop, metal cooking tongs, mug, drinking straw, table, tea in a glass, big white crinkly flower, bouquet, carnation, daisy, eucalyptus, lily, rosemary |
| Bouquet                                                                                                                                                                                                                                                                                                                                                  |

