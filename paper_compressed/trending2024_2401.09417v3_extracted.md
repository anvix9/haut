## Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model



## Abstract

Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k

Proceedings of the 41 st International Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).

semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8 × faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248 × 1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code and models are released at https://github.com/hustvl/Vim

## 1. Introduction

Recent research advancements have led to a surge of interest in the state space model (SSM). Originating from the classic Kalman filter model (Kalman, 1960), modern SSMs excel at capturing long-range dependencies and benefit from parallel training. Some SSM-based methods, such as the linear state-space layers (LSSL) (Gu et al., 2021b), structured state space sequence model (S4) (Gu et al., 2021a), diagonal state space (DSS) (Gupta et al., 2022), and S4D (Gu et al., 2022), are proposed to process sequence data across a

wide range of tasks and modalities, particularly on modeling long-range dependencies. They are efficient in processing long sequences because of convolutional computation and near-linear computation. 2-D SSM (Baron et al., 2023), SGConvNeXt (Li et al., 2022b), and ConvSSM (Smith et al., 2023a) combine SSM with CNN or Transformer architecture to process 2-D data. The recent work, Mamba (Gu & Dao, 2023), incorporates time-varying parameters into the SSM and proposes a hardware-aware algorithm to enable very efficient training and inference. The superior scaling performance of Mamba indicates that it is a promising alternative to Transformer in language modeling. Nevertheless, a generic pure-SSM-based backbone network has not been explored for processing visual data, such as images and videos.

Vision Transformers (ViTs) have achieved great success in visual representation learning, excelling in large-scale self-supervised pre-training and high performance on downstream tasks. Compared with convolutional neural networks, the core advantage lies in that ViT can provide each image patch with data/patch-dependent global context through selfattention. This differs from convolutional networks that use the same parameters, i.e. , the convolutional filters, for all positions. Another advantage is the modality-agnostic modeling by treating an image as a sequence of patches without 2D inductive bias, which makes it the preferred architecture for multimodal applications (Bavishi et al., 2023; Li et al., 2023; Liu et al., 2023). At the same time, the self-attention mechanism in Transformers poses challenges in terms of speed and memory usage when dealing with long-range visual dependencies, e.g. , processing high-resolution images.

Motivated by the success of Mamba in language modeling, it is appealing that we can also transfer this success from language to vision, i.e. , to design a generic and efficient visual backbone with the advanced SSM method. However, there are two challenges for Mamba, i.e. , unidirectional modeling and lack of positional awareness. To address these challenges, we propose the Vision Mamba (Vim) model, which incorporates the bidirectional SSMs for data-dependent global visual context modeling and position embeddings for location-aware visual recognition. We first split the input image into patches and linearly project them as vectors to Vim. Image patches are treated as the sequence data in Vim blocks, which efficiently compresses the visual representation with the proposed bidirectional selective state space. Furthermore, the position embedding in Vim block provides the awareness for spatial information, which enables Vim to be more robust in dense prediction tasks. In the current stage, we train the Vim model on the supervised image classification task using the ImageNet dataset and then use the pretrained Vim as the backbone to perform sequential visual representation learning for downstream dense prediction tasks, i.e. , semantic segmentation, object

detection, and instance segmentation. Like Transformers, Vim can be pretrained on large-scale unsupervised visual data for better visual representation. Thanks to the better efficiency of Mamba, the large-scale pretraining of Vim can be achieved with lower computational cost.

Compared with other SSM-based models for vision tasks, Vim is a pure-SSM-based method and models images in a sequence manner, which is more promising for a generic and efficient backbone. Thanks to the bidirectional compressing modeling with positional awareness, Vim is the first pureSSM-based model to handle dense prediction tasks. Compared with the most convincing Transformer-based model, i.e. , DeiT (Touvron et al., 2021a), Vim achieves superior performance on ImageNet classification. Furthermore, Vim is more efficient in terms of GPU memory and inference time for high-resolution images. The efficiency in terms of memory and speed empowers Vim to directly perform sequential visual representation learning without relying on 2D priors (such as the 2D local window in ViTDet (Li et al., 2022c)) for high-resolution visual understanding tasks while achieving higher accuracy than DeiT.

Our main contributions can be summarized as follows:

- · We propose Vision Mamba (Vim), which incorporates bidirectional SSM for data-dependent global visual context modeling and position embeddings for locationaware visual understanding.
- · Without the need of attention, the proposed Vim has the same modeling power as ViT while it only has subquadratic-time computation and linear memory complexity. Specifically, Vim is 2.8 × faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images at the resolution of 1248 × 1248.
- · We conduct extensive experiments on ImageNet classification and dense prediction downstream tasks. The results demonstrate that Vim achieves superior performance compared to the well-established and highlyoptimized plain vision Transformer, i.e. , DeiT.

## 3. Method

The goal of Vision Mamba (Vim) is to introduce the advanced state space model (SSM), i.e. , Mamba (Gu & Dao, 2023), to computer vision. This section begins with a description of the preliminaries of SSM. It is followed by an overview of Vim. We then detail how the Vim block processes input token sequences and proceed to illustrate the architecture details of Vim. The section concludes with an

analysis of the efficiency of the proposed Vim.

## 3.1. Preliminaries

The SSM-based models, i.e. , structured state space sequence models (S4) and Mamba are inspired by the continuous system, which maps a 1-D function or sequence x ( t ) ∈ R ↦→ y ( t ) ∈ R through a hidden state h ( t ) ∈ R N . This system uses A ∈ R N × N as the evolution parameter and B ∈ R N × 1 , C ∈ R 1 × N as the projection parameters. The continuous system works as follows: h ' ( t ) = A h ( t ) + B x ( t ) and y ( t ) = C h ( t ) .

The S4 and Mamba are the discrete versions of the continuous system, which include a timescale parameter ∆ to transform the continuous parameters A , B to discrete parameters A , B . The commonly used method for transformation is zero-order hold (ZOH), which is defined as follows:

A = exp( ∆A ) , B = ( ∆A ) -1 (exp ( ∆A ) -I ) · ∆B . (1)

After the discretization of A , B , the discretized version using a step size ∆ can be rewritten as:

h t = A h t -1 + B x t , y t = C h t . (2)

At last, the models compute output through a global convolution.

K = ( CB , CAB , . . . , CA M -1 B ) , y = x ∗ K , (3)

where M is the length of the input sequence x , and K ∈ R M is a structured convolutional kernel.

## 3.4. Architecture Details

In summary, the hyper-parameters of our architecture are listed as follows: L denotes the number of blocks, D denotes the hidden state dimension, E denotes the expanded state dimension, and N denotes the SSM dimension. Following ViT (Dosovitskiy et al., 2020) and DeiT (Touvron et al., 2021b), we first employ 16 × 16 kernel size projection layer to get a 1-D sequence of non-overlapping patch embeddings. Subsequently, we directly stack L Vim blocks. By default, we set the number of blocks L to 24, SSM dimension N to 16. To align with the model sizes of DeiT series, we set the hidden state dimension D to 192 and expanded state dimension E to 384 for the tiny-size variant. For the smallsize variant, we set D to 384 and E to 768.

## 3.5. Efficiency Analysis

Traditional SSM-based methods leverage the fast Fourier transform to boost the convolution operation as shown in Eq. (3). For data-dependent methods, such as Mamba, the SSM operation in Line 11 of Algo. 1 is no longer equivalent to convolution. To address this problem, Mamba and the proposed Vim choose a modern-hardware-friendly way to ensure efficiency. The key idea of this optimization is to avoid the IO-bound and memory-bound of modern hardware accelerators (GPUs).

Figure 2: The overview of the proposed Vim model. We first split the input image into patches, and then project them into patch tokens. Last, we send the sequence of tokens to the proposed Vim encoder. To perform ImageNet classification, we concatenate an extra learnable classification token to the patch token sequence. Different from Mamba for text sequence modeling, Vim encoder processes the token sequence with both forward and backward directions.

<!-- image -->

IO-Efficiency. The high bandwidth memory (HBM) and SRAM are two important components for GPUs. Among them, SRAM has a larger bandwidth and HBM has a bigger memory size. The standard implementation of Vim's SSM operation with HBM requires the number of memory IO on the order of O ( BMEN ) . Inspired by Mamba, Vim first reads in O ( BME + EN ) bytes of memory ( ∆ o , A o , B o , C o ) from slow HBM to fast SRAM. Then, Vim gets the discrete A o , B o of a size of ( B , M , E , N ) in SRAM. Last, Vim performs SSM operations in SRAM and writes the output of a size of ( B , M , E ) back to HBM. This method can help to reduce IOs from O ( BMEN ) to O ( BME + EN ) .

Memory-Efficiency. To avoid out-of-memory problems and achieve lower memory usage when dealing with long sequences, Vim chooses the same recomputation method as Mamba. For the intermediate states of size ( B , M , E , N ) to calculate the gradient, Vim recomputes them at the network backward pass. For intermediate activations such as the output of activation functions and convolution, Vim also recomputes them to optimize the GPU memory requirement, as the activation values take a lot of memory but are fast for recomputation.

Computation-Efficiency. SSM in Vim block (Line 11 in Algo.1) and self-attention in Transformer both play a key role in providing global context adaptively. Given a visual sequence T ∈ R 1 × M × D and the default setting E = 2 D , the computation complexity of a global self-attention and SSM are:

Ω( self-attention ) = 4 MD 2 +2 M 2 D , (5)

Ω( SSM ) = 3 M (2 D ) N + M (2 D ) N , (6)

where self-attention is quadratic to sequence length M , and SSM is linear to sequence length M ( N is a fixed parameter, set to 16 by default). The computational efficiency makes Vim scalable for gigapixel applications with large sequence

lengths.

## 4.4. Ablation Study

Bidirectional SSM. We ablate the key bidirectional design of Vim, using ImageNet-1K classification and the Segmenter (Strudel et al., 2021) semantic segmentation framework on ADE20K. To fully evaluate the power of learned representation on ImageNet, we use a simple Segmenter head with only 2 layers to perform transfer learning on semantic segmentation. We study the following bidirectional strategies. None : We directly adopt the Mamba block to

Vision Mamba: Efficient Visual Representation Learning with State Space ModelTable 4: Ablation study on the bidirectional design. To ensure a fair comparison, we do not use the class token for each experiment. The default setting for Vim is marked in blue .

| Bidirectional strategy     |   ImageNet top-1 acc. |   ADE20K mIoU |
|----------------------------|-----------------------|---------------|
| None                       |                  73.2 |          32.3 |
| Bidirectional Layer        |                  70.9 |          33.6 |
| Bidirectional SSM          |                  72.8 |          33.2 |
| Bidirectional SSM + Conv1d |                  73.9 |          35.9 |

process visual sequence with only the forward direction. Bidirectional Sequence : During training, we randomly flip the visual sequence. This works like data augmentation. Bidirectional Block : We pair the stacked blocks. The first block of each pair processes visual sequence in the forward direction and the second block of each pair processes in the backward direction. Bidirectional SSM : We add an extra SSM for each block to process the visual sequence in the backward direction. Bidirectional SSM + Conv1d : Based on Bidirectional SSM, we further add a backward Conv1d before the backward SSM (Fig. 2).

As shown in Tab. 4, directly adopting the Mamba block achieves good performance in classification. However, the unnatural unidirectional manner poses challenges in downstream dense prediction. Specifically, the preliminary bidirectional strategy of using Bidirectional Block achieves 7 points lower top-1 accuracy on classification. Yet, it outperforms the vanilla unidirectional Mamba block by 1.3 mIoU on semantic segmentation. By adding extra backward SSM and Conv1d, we achieve superior classification accuracy (73.9 top-1 acc vs. 73.2 top-1 acc) and exceptional segmentation superiority (35.9 mIoU vs. 32.3 mIoU). We use the strategy of Bidirectional SSM + Conv1d as the default setting in our Vim block.

Classification Design. We ablate the classification design of Vim, benchmarking on ImageNet-1K classification. We study the following classification strategies. Mean pool : We adopt mean pooling on the output feature from the last Vim block and perform classification on this pooled feature. Max pool : We first adapt the classification head on each token of the visual sequence and then perform max pooling on the sequence to get the classification prediction result. Head class token : Following DeiT (Touvron et al., 2021b), we concatenate the class token at the head of the visual sequence and perform classification. Double class token : Based on the head class token strategy, we additionally add a class token at the tail of the visual sequence. Middle class token : We add a class token at the middle of the visual sequence and then perform classification on the final middle class token.

Table 5: Ablation study on the classification design. The default setting for Vim is marked in blue .

| Classification strategy   |   ImageNet top-1 acc. |
|---------------------------|-----------------------|
| Mean pool                 |                  73.9 |
| Max pool                  |                  73.4 |
| Head class token          |                  75.2 |
| Double class token        |                  74.3 |
| Middle class token        |                  76.1 |

As shown in Tab. 5, experiments show that the middle class token strategy can fully exploit the recurrent nature of SSM and the central object prior in ImageNet, demonstrating the best top-1 accuracy of 76.1.

## C. Extended Comparison on Hierarchical Architecture

To further compare with hierarchical architectures, we propose another variant Hier-Vim by replacing shifted local window attention in SwinTransformer with the proposed global bidirectional SSM. We detail the configuration in Tab. 6

Table 6: Detailed configurations of different variants of Hier-Vim. We provide the number of channels and blocks in 4 stages.

| Model      | #Blocks       | #Channels             | Params   |
|------------|---------------|-----------------------|----------|
| Hier-Vim-T | [2, 2, 5, 2]  | [96, 192, 384, 768]   | 30M      |
| Hier-Vim-S | [2, 2, 15, 2] | [96, 192, 384, 768]   | 50M      |
| Hier-Vim-B | [2, 2, 15, 2] | [128, 256, 512, 1024] | 89M      |

Classification on ImageNet. Following the standard training and validation protocols (Liu et al., 2021; 2024), we compare

Table 7: Comparison with hierarchical architectures on ImageNet-1K validation set.

| Method                                 | image size   | #param.   |   ImageNet top-1 acc. |
|----------------------------------------|--------------|-----------|-----------------------|
| Swin-T (Liu et al., 2021)              | 224 2        | 28M       |                  81.2 |
| FocalTransformer-T (Yang et al., 2021) | 224 2        | 29M       |                  82.2 |
| CVT-21 (Wu et al., 2021)               | 224 2        | 32M       |                  82.5 |
| MetaFormer-S35 (Yu et al., 2022)       | 224 2        | 31M       |                  81.4 |
| GFNet-H-S (Rao et al., 2021)           | 224 2        | 32M       |                  81.5 |
| Hier-Vim-T                             | 224 2        | 30M       |                  82.5 |
| Swin-S (Liu et al., 2021)              | 224 2        | 50M       |                  83.2 |
| FocalTransformer-S (Yang et al., 2021) | 224 2        | 51M       |                  83.5 |
| MetaFormer-S35 (Yu et al., 2022)       | 224 2        | 73M       |                  82.5 |
| GFNet-H-B (Rao et al., 2021)           | 224 2        | 54M       |                  82.9 |
| Hier-Vim-S                             | 224 2        | 50M       |                  83.4 |
| Swin-B (Liu et al., 2021)              | 224 2        | 88M       |                  83.5 |
| FocalTransformer-B (Yang et al., 2021) | 224 2        | 90M       |                  83.8 |
| Hier-Vim-B                             | 224 2        | 89M       |                  83.9 |

Hier-Vim with popular hierarchical architectures across tiny, small, and base model sizes in Tab. 7. The results indicate that Hier-Vim outperforms Swin Transformer by 1.3% at the tiny size, 0.2% at the small size, and 0.4% at the base size, demonstrating competitive performance against well-established and highly-optimized modern hierarchical architectures.

