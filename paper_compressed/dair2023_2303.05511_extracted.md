## Abstract

The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL · E 2, autoregressive and diffusion models became the new standard for largescale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that na¨ıvely increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.

## 1. Introduction

Recently released models, such as DALL · E 2 [74], Imagen [80], Parti [101], and Stable Diffusion [79], have ushered in a new era of image generation, achieving unprecedented levels of image quality and model flexibility. The now-dominant paradigms, diffusion models and autoregressive models, both rely on iterative inference. This is a double-edged sword, as iterative methods enable stable training with simple objectives but incur a high computational cost during inference.

Contrast this with Generative Adversarial Networks (GANs) [6,21,41,72], which generate images through a single forward pass and thus inherently efficient. While such models dominated the previous 'era' of generative modeling, scaling them requires careful tuning of the network

architectures and training considerations due to instabilities in the training procedure. As such, GANs have excelled at modeling single or multiple object classes, but scaling to complex datasets, much less an open world, has remained challenging. As a result, ultra-large models, data, and compute resources are now dedicated to diffusion and autoregressive models. In this work, we ask can GANs continue to be scaled up and potentially benefit from such resources, or have they plateaued? What prevents them from further scaling, and can we overcome these barriers?

We first experiment with StyleGAN2 [42] and observe that simply scaling the backbone causes unstable training. We identify several key issues and propose techniques to stabilize the training while increasing the model capacity. First, we effectively scale the generator's capacity by retaining a bank of filters and taking a sample-specific linear combination. We also adapt several techniques commonly used in the diffusion context and confirm that they bring similar benefits to GANs. For instance, interleaving both self-attention (image-only) and cross-attention (image-text) with the convolutional layers improves performance.

Furthermore, we reintroduce multi-scale training, finding a new scheme that improves image-text alignment and low-frequency details of generated outputs. Multi-scale training allows the GAN-based generator to use parameters in low-resolution blocks more effectively, leading to better image-text alignment and image quality. After careful tuning, we achieve stable and scalable training of a one-billionparameter GAN (GigaGAN) on large-scale datasets, such as LAION2B-en [88]. Our results are shown in Figure 1.

In addition, our method uses a multi-stage approach [14, 104]. We first generate at 64 × 64 and then upsample to 512 × 512 . These two networks are modular and robust enough to be used in a plug-and-play fashion. We show that our text-conditioned GAN-based upsampling network can be used as an efficient, higher-quality upsampler for a base diffusion model such as DALL · E 2, despite never having seen diffusion images at training time (Figures 2).

Together, these advances enable our GigaGAN to go far beyond previous GANs: 36 × larger than Style-

A portrait of a human growing colorful flowers from her hair. Hyperrealistic oil painting. Intricate details.

<!-- image -->

Figure 1. Our model, GigaGAN, shows GAN frameworks can also be scaled up for general text-to-image synthesis tasks, generating a 512px output at an interactive speed of 0.13s, and 4096px at 3.7s. Selected examples at 2K or 4K resolutions are shown. Please zoom in for more details. See Appendix C and our website for more uncurated comparisons.

<!-- image -->

<!-- image -->

<!-- image -->

A living room with a fireplace at a wood cabin. Interior design.

<!-- image -->

 blue Porsche 356 parked in front of a yellow brick wall.

Eiffel Tower, landscape photography

A painting of a majestic royal tall ship in Age of Discovery.

<!-- image -->

Isometric underwater Atlantis city with a Greek temple in a bubble.

<!-- image -->

A hot air balloon in shape of a heart. Grand Canyonlow poly bunny with cute eyes

<!-- image -->

A cube made of denim on a wooden table

<!-- image -->

A golden luxury motorcycle parked at the King's palace. 35mm f/4.5.

<!-- image -->

a cute magical flying maltipoo at light speed, fantasy concept art, bokeh, wide sky

<!-- image -->

<!-- image -->

Input artwork from AdobeStock (128px)

<!-- image -->

GigaGAN Upsampler (1024px, 0.13s)Real-ESRGAN (1024px, 0.06s)

<!-- image -->

SD Upscaler (1024px, 7.75s)Figure 2. Our GAN-based upsampler can serve in the upsampling pipeline of many text-to-image models that often generate initial outputs at low resolutions like 64px or 128px. We simulate such usage by applying our text-conditioned 8 × superresolution model on a low-res 128px artwork to obtain the 1K output, using 'Portrait of a colored iguana dressed in a hoodie'. Then our model can be re-applied to go beyond 4K. We compare our model with the text-conditioned upscaler of Stable Diffusion [78] and unconditional Real-ESRGAN [33]. Zooming in is recommended for comparison between 1K and 4K.

<!-- image -->

<!-- image -->

Figure 3. Our GAN-based upsampler , similar to Figure 2, can also be used as an off-the-shelf superresolution model for real images with a large scaling factor by providing an appropriate description of the image. We apply our text-conditioned 8 × superresolution model on a low-res 128px photo to obtain the 1K output, using 'A dog sitting in front of a mini tipi tent'. Then our model can be re-applied to go beyond 4K. We compare our model with the text-conditioned upscaler of Stable Diffusion [78] and unconditional Real-ESRGAN [33]. Zooming in is recommended for comparison between 1K and 4K.

<!-- image -->

Input photo (128px)

<!-- image -->

GigaGAN Upsampler (1024px, 0.13s)Real-ESRGAN (1024px, 0.06s)

<!-- image -->

SD Upscaler (1024px, 7.75s)

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

GAN2 [42] and 6 × larger than StyleGAN-XL [86] and XMC-GAN [103]. While our 1B parameter count is still lower than the largest recent synthesis models, such as Imagen (3.0B), DALL · E 2 (5.5B), and Parti (20B), we have not yet observed a quality saturation regarding the model size. GigaGAN achieves a zero-shot FID of 9.09 on COCO2014 dataset, lower than the FID of DALL · E 2, Parti-750M, and Stable Diffusion.

Furthermore, GigaGAN has three major practical advantages compared to diffusion and autoregressive models. First, it is orders of magnitude faster, generating a 512px image in 0.13 seconds (Figure 1). Second, it can synthesize ultra high-res images at 4k resolution in 3.66 seconds. Third, it is endowed with a controllable, latent vector space that lends itself to well-studied controllable image synthesis applications, such as style mixing (Figure 6), prompt interpolation (Figure 7), and prompt mixing (Figure 8).

In summary, our model is the first GAN-based method that successfully trains a billion-scale model on billions of real-world complex Internet images. This suggests that GANs are still a viable option for text-to-image synthesis and should be considered for future aggressive scaling. Please visit our website for additional results.

## 3. Method

We train a generator G ( z , c ) to predict an image x ∈ R H × W × 3 given a latent code z ∼ N (0 , 1) ∈ R 128 and text-conditioning signal c . We use a discriminator D ( x , c ) to judge the realism of the generated image, as compared to a sample from the training database D , which contains image-text pairs.

Figure 4. Our GigaGAN high-capacity text-to-image generator. First, we extract text embeddings using a pretrained CLIP model and a learned encoder T . The local text descriptors are fed to the generator using cross-attention. The global text descriptor, along with a latent code z , is fed to a style mapping network M to produce style code w . The style code modulates the main generator using our style-adaptive kernel selection, shown on the right. The generator outputs an image pyramid by converting the intermediate features into RGB images. To achieve higher capacity, we use multiple attention and convolution layers at each scale (Appendix A2). We also use a separate upsampler model, which is not shown in this diagram.

<!-- image -->

Our high-capacity text-to-image generator

Although GANs [6,39,41] can successfully generate realistic images on single- and multi-category datasets [13, 41,100], open-ended text-conditioned synthesis on Internet images remains challenging. We hypothesize that the current limitation stems from its reliance on convolutional layers. That is, the same convolution filters are challenged to model the general image synthesis function for all text conditioning across all locations of the image. In this light, we seek to inject more expressivity into our parameterization by dynamically selecting convolution filters based on the input conditioning and by capturing long-range dependence via the attention mechanism.

Below, we discuss our key contributions to making ConvNets more expressive (Section 3.1), followed by our designs for the generator (Section 3.2) and discriminator (Section 3.3). Lastly, we introduce a new, fast GAN-based upsampler model that can improve the inference quality and speed of our method and diffusion models such as Imagen [80] and DALL · E 2 [74].

## 4.1. Training and evaluation details

We implement GigaGAN based on the StudioGAN PyTorch library [37], following the standard FID evaluation protocol with the anti-aliasing bicubic resize function [67], unless otherwise noted. For text-to-image synthesis, we train our models on the union of LAION2B-en [88] and COYO-700M [8] datasets, with the exception of the 128-to1024 upsampler model trained on Adobe's internal Stock images. The image-text pairs are preprocessed based on CLIP score [24], image resolution, and aesthetic score [87], similar to prior work [78]. We use CLIP ViT-L/14 [71] for the pre-trained text encoder and OpenCLIP ViT-G/14 [32]

Figure 6. Style mixing . Our GAN-based architecture retains a disentangled latent space, enabling us to blend the coarse style of one sample with the fine style of another. All outputs are generated with the prompt 'A Toy sport sedan, CG art.' The corresponding latent codes are spliced together to produce a style-swapping grid.

<!-- image -->

'A modern mansion ..'

'A victorian mansion ..'

Figure 7. Prompt interpolation . GigaGAN enables smooth interpolation between prompts, as shown in the interpolation grid. The four corners are generated from the same latent z but with different text prompts. The corresponding text embeddings t and style vectors w are interpolated to create a smooth transition. The same z results in similar layouts. See Figure 8 for more precise control.

<!-- image -->

'.. in a

sunny day'

'.. in sunset'

Figure 8. Prompt mixing . GigaGAN retains a disentangled latent space, enabling us to combine the coarse style of one sample with the fine style of another. Moreover, GigaGAN can directly control the style with text prompts. Here we generate four outputs using the prompts 'a X on tabletop', shown in the 'no mixing' column. Then we re-compute the text embeddings t and the style codes w using the new prompts 'a X with the texture of Y on tabletop', such as 'a cube with the texture of crochet on tabletop', and apply them to the second half layers of the generator, achieving layout-preserving fine style control. Cross-attention mechanism automatically localizes the style to the object of interest.

<!-- image -->

for CLIP score calculation [24] except for Table 1. All our models are trained and evaluated on A100 GPUs. We include more training and evaluation details in Appendix A.

## 5. Discussion and Limitations

Our experiments provide a conclusive answer about the scalability of GANs: our new architecture can scale up to model sizes that enable text-to-image synthesis. However, the visual quality of our results is not yet comparable to production-grade models like DALL · E 2. Figure 9 shows

several instances where our method fails to produce highquality results when compared to DALL · E 2, in terms of photorealism and text-to-image alignment for the same input prompts used in their paper.

Nevertheless, we have tested capacities well beyond what is possible with a na¨ıve approach and achieved competitive results with autoregressive and diffusion models trained with similar resources while being orders of magnitude faster and enabling latent interpolation and stylization. Our GigaGAN architecture opens up a whole new design space for large-scale generative models and brings back key editing capabilities that became challenging with the transition to autoregressive and diffusion models. We expect our performance to improve with larger models, as seen in Table 1.

Acknowledgments. We thank Simon Niklaus, Alexandru Chiculita, and Markus Woodson for building the distributed training pipeline. We thank Nupur Kumari, Gaurav Parmar, Bill Peebles, Phillip Isola, Alyosha Efros, and Joonghyuk Shin for their helpful comments. We also want to thank Chenlin Meng, Chitwan Saharia, and Jiahui Yu for answering many questions about their fantastic work. We thank Kevin Duarte for discussions regarding upsampling beyond 4K. Part of this work was done while Minguk Kang was an intern at Adobe Research. Minguk Kang and Jaesik Park were supported by IITP grant funded by the government of South Korea (MSIT) (POSTECH GSAI: 2019-0-01906 and Image restoration: 2021-0-00537).

## A. Training and evaluation details



## C.1. Truncation trick at inference

Similar to the classifier guidance [15] and classifier-free guidance [28] used in diffusion models such as LDM, our GAN model can leverage the truncation trick [6, 41] at inference time.

w trunc = lerp ( w mean , w , ψ ) , (9)

where w mean is the mean of w of the entire dataset, which can be precomputed. In essence, the truncation trick lets us trade diversity for fidelity by interpolating the latent vector to the mean of the distribution and thereby making the outputs more typical. When ψ = 1 . 0 , w mean is not used, and there is no truncation. When ψ = 0 . 0 , w collapses to the mean, losing diversity.

While it is straightforward to apply the truncation trick for the unconditional case, it is less clear how to achieve this for text-conditional image generation. We find that interpolating the latent vector toward both the mean of the entire distribution as well as the mean of w conditioned on the text prompt produces desirable results.

w trunc = lerp ( w mean , c , lerp ( w mean , w , ψ ) , ψ ) , (10)

where w mean , c can be computed at inference time by sampling w = M ( z , c ) 16 times with the same c , and taking the average. This operation's overhead is negligible, as the mapping network M is computationally light compared to the synthesis network. At ψ = 1 . 0 , w trunc becomes w trunc = w , meaning no truncation. Figure A4 demonstrates the effect of our text-conditioned truncation trick.

Quantitatively, the effect of truncation is similar to the guidance technique of diffusion models. As shown in Figure A3, the CLIP score increases with more truncation, where the FID increases due to reduced diversity.

