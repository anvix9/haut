## 1 INTRODUCTION

Recently, the emergence of large language models (LLMs) [383], exempli/fied by LLaMA [299, 300], Claude [9], Gemini [7, 259] and GPT-4 [232], has ushered in a signi/ficant paradigm shift in natural language processing (NLP), achieving unprecedented progress in language understanding [116, 124], generation [373, 393] and reasoning [57, 151, 250, 326, 354]. Furthermore, the extensive factual knowledge encoded within LLMs has demonstrated considerable advancements in leveraging LLMs for information seeking [6, 246], potentially reshaping the landscape of information retrieval systems [394]. Nevertheless, in tandem with these remarkable advancements, concerns have arisen about the tendency of LLMs to generate hallucinations [15, 105], resulting in seemingly plausible yet factually unsupported content. Further compounding this issue is the capability of LLMs to generate highly convincing and human-like responses [265], which makes detecting these hallucinations particularly challenging, thereby complicating the practical deployment of LLMs, especially realworld information retrieval (IR) systems that have integrated into our daily lives like chatbots [8, 231], search engines [4, 214], and recommender systems [97, 171]. Given that the information provided by these systems can directly in/fluence decision-making, any misleading information has the potential to spread false beliefs, or even cause harm.

Notably, hallucinations in conventional natural language generation (NLG) tasks have been extensively studied [125, 136], with hallucinations de/fined as generated content that is either nonsensical or unfaithful to the provided source content. These hallucinations are categorized into two types: intrinsic hallucination , where the generated output contradicts the source content, and extrinsic hallucination , where the generated output cannot be veri/fied from the source. However, given their remarkable versatility across tasks [15, 30], understanding hallucinations in LLMs presents a unique challenge compared to models tailored for speci/fic tasks. Besides, as LLMs typically function as open-ended systems, the scope of hallucination encompasses a broader concept, predominantly manifesting factual errors. This shift necessitates a reevaluation and adjustment of the existing taxonomy of hallucinations, aiming to enhance its adaptability in the evolving landscape of LLMs.

In this survey, we propose a rede/fined taxonomy of hallucination tailored speci/fically for applications involving LLMs. We categorize hallucination into two primary types: factuality hallucination and faithfulness hallucination . Factuality hallucination emphasizes the discrepancy between generated content and veri/fiable real-world facts, typically manifesting as factual inconsistencies. Conversely, faithfulness hallucination captures the divergence of generated content from user input or the lack of self-consistency within the generated content. This category is further subdivided into instruction inconsistency, where the content deviates from the user's original instruction; context inconsistency, highlighting discrepancies from the provided context; and logical inconsistency, pointing out internal contradictions within the content. Such categorization re/fines our understanding of hallucinations in LLMs, aligning it closely with their contemporary usage.

Delving into the underlying causes of hallucinations in LLMs is essential not merely for enhancing the comprehension of these phenomena but also for informing strategies aimed at alleviating them. Recognizing the multifaceted sources of LLM hallucinations, our survey identi/fies potential contributors into three main aspects: data, training, and inference stages. This categorization allows us to span a broad spectrum of factors, providing a holistic view of the origins and mechanisms by which hallucinations may arise within LLM systems. Furthermore, we comprehensively outline a variety of e/ffective detection methods speci/fically devised for detecting hallucinations in LLMs, as well as an exhaustive overview of benchmarks related to LLM hallucinations, serving as appropriate testbeds to assess the extent of hallucinations generated by LLMs and the e/fficacy of detection methods. Beyond evaluation, signi/ficant e/fforts have been undertaken to mitigate hallucinations of

LLMs. These initiatives are comprehensively surveyed in our study, in accordance with the corresponding causes, spanning from data-related, training-related, and inference-related approaches. In addition, the e/ffectiveness of retrieval-augmented generation (RAG) in mitigating hallucinations has garnered tremendous attention within the /field. Despite the considerable potential of RAG, current systems inherently face limitations and even su/ffer from hallucinations. Accordingly, our survey undertakes an in-depth analysis of these challenges, aiming to provide valuable insights aimed at developing more robust RAG systems. We also highlight several promising avenues for future research, such as hallucinations in large vision-language models and understanding of knowledge boundaries in LLM hallucinations, paving the way for forthcoming research in the /field.

Comparing with Existing Surveys. As hallucination stands out as a major challenge in generative AI, numerous research [136, 192, 258, 298, 312, 376] has been directed towards hallucinations. While these contributions have explored LLM hallucination from various perspectives and provided valuable insights, our survey seeks to delineate their distinct contributions and the comprehensive scope they encompass. Ji et al. [136] primarily shed light on hallucinations in pre-trained models for NLG tasks, leaving LLMs outside their discussion purview. Tonmoy et al. [298] mainly focused on discussing the mitigation strategies combating LLM hallucinations. Besides, Liu et al. [192] took a broader view of LLM trustworthiness without delving into speci/fic hallucination phenomena, whereas Wang et al. [312] provided an in-depth look at factuality in LLMs. However, our work narrows down to a critical subset of trustworthiness challenges, speci/fically addressing factuality and extending the discussion to include faithfulness hallucinations. To the best of our knowledge, Zhang et al. [376] presented research closely aligned with ours, detailing LLM hallucination taxonomies, evaluation benchmarks, and mitigation strategies. However, our survey sets itself apart through a unique taxonomy and organizational structure. We present a detailed, layered classi/fication of hallucinations and conduct a more comprehensive analysis of the causes of hallucinations. Crucially, our proposed mitigation strategies are directly tied to these causes, o/ffering a targeted and coherent framework for addressing LLM hallucinations.

Organization of this Survey. In this survey, we present a comprehensive overview of the latest developments in LLM hallucinations, as shown in Fig 1. We commence by constructing a taxonomy of hallucinations in the realm of LLM (§2). Subsequently, we analyze factors contributing to LLM hallucinations in depth (§3), followed by a review of various strategies and benchmarks employed for the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches designed to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by current RAG systems (§6) and delineate potential pathways for forthcoming research (§7).

## 3.3 Hallucination from Inference

Decoding plays an important role in manifesting the capabilities of LLMs after pretraining and alignment. However, certain shortcomings in decoding strategies can lead to LLM hallucinations.

- 3.3.1 Imperfect Decoding Strategies. LLMs have demonstrated a remarkable aptitude for generating highly creative and diverse content, a pro/ficiency that is critically dependent on the pivotal role of randomness in their decoding strategies. Stochastic sampling [84, 118] is currently the prevailing decoding strategy employed by these LLMs. The rationale for incorporating randomness into decoding strategies stems from the realization that high likelihood sequences often result in surprisingly low-quality text, which is called likelihood trap [118, 209, 283, 363]. The diversity introduced by the randomness in decoding strategies comes at a cost, as it is positively correlated with an increased risk of hallucinations [59, 78]. An elevation in the sampling temperature results in a more uniform token probability distribution, increasing the likelihood of sampling tokens with lower frequencies from the tail of the distribution. Consequently, this heightened tendency to sample infrequently occurring tokens exacerbates the risk of hallucinations [5].
- 3.3.2 Over-confidence. Prior studies in conditional text generation [45, 212] have highlighted the issue of over-con/fidence which stems from an excessive focus on the partially generated content, often prioritizing /fluency at the expense of faithfully adhering to the source context. While LLMs,

primarily adopting the causal language model architecture, have gained widespread usage, the over-con/fidence phenomenon continues to persist. During the generation process, the prediction of the next word is conditioned on both the language model context and the partially generated text. However, as demonstrated in prior studies [19, 189, 307], language models often exhibit a localized focus within their attention mechanisms, giving priority to nearby words and resulting in a notable de/ficit in context attention [275]. Furthermore, this concern is further ampli/fied in LLMs that exhibit a proclivity for generating lengthy and comprehensive responses. In such cases, there is even a heightened susceptibility to the risk of instruction forgetting [46, 193]. This insu/fficient attention can directly contribute to faithfulness hallucinations, wherein the model outputs content that deviates from the original context.

3.3.3 So/f\_tmax Bo/t\_tleneck. The majority of language models utilize a softmax layer that operates on the /final layer's representation within the language model, in conjunction with a word embedding, to compute the ultimate probability associated with word prediction. Nevertheless, the e/fficacy of Softmax-based language models is impeded by a recognized limitation known as the Softmax bottleneck [342], wherein the employment of softmax in tandem with distributed word embeddings constrains the expressivity of the output probability distributions given the context which prevents LMs from outputting the desired distribution. Additionally, Chang and McCallum [38] discovered that when the desired distribution within the output word embedding space exhibits multiple modes, language models face challenges in accurately prioritizing words from all the modes as the top next words, which also introduces the risk of hallucination.

3.3.4 Reasoning Failure. Beyond the challenges with long-tail knowledge, e/ffective utilization of knowledge is inextricably linked with reasoning capabilities. For instance, in multi-hop questionanswering scenarios, even if the LLM possesses the necessary knowledge, it may struggle to produce accurate results if multiple associations exist between questions, due to its limitations in reasoning [386]. Furthermore, Berglund et al. [22] unveiled a speci/fic reasoning failure in LLMs termed the Reversal Curse . Speci/fically, while the model can correctly answer when the question is formulated as "A is B", it exhibits a failed logical deduction when asked the converse "B is A". This discrepancy in reasoning extends beyond simple deductions.

## 7 FUTURE DISCUSSION

As the /field of research on hallucinations in LLMs continues to evolve, our focus shifts towards the next horizon of inquiry. We explore prospective areas of study, notably the phenomenon of hallucinations in vision-language models (§7.1) and the challenge of delineating and understanding knowledge boundaries within LLMs (§7.2).

## 8 CONCLUSION

In this comprehensive survey, we have undertaken an in-depth examination of hallucinations within large language models, delving into the intricacies of their underlying causes, pioneering detection methodologies as well as related benchmarks, and e/ffective mitigation strategies. Although signi/ficant strides have been taken, the conundrum of hallucination in LLMs remains a compelling and ongoing concern that demands continuous investigation. Moreover, we envision this survey as a guiding beacon for researchers dedicated to advancing robust information retrieval systems and trustworthy arti/ficial intelligence. By navigating the complex landscape of hallucinations, we hope to empower these dedicated individuals with invaluable insights that drive the evolution of AI technologies toward greater reliability and safety.

