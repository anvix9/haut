## Abstract

We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for cu-

rating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base

<!-- image -->

<!-- image -->

model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/ Stability-AI/generative-models .

## 1. Introduction

Driven by advances in generative image modeling with diffusion models [38, 68, 71, 76], there has been significant recent progress on generative video models both in research [9, 42, 82, 95] and real-world applications [54, 74] Broadly, these models are either trained from scratch [41] or finetuned (partially or fully) from pretrained image models with additional temporal layers inserted [9, 32, 43, 82]. Training is often carried out on a mix of image and video datasets [41].

While research around improvements in video modeling has primarily focused on the exact arrangement of the spatial and temporal layers [9, 41, 43, 82], none of the aforementioned works investigate the influence of data selection. This is surprising, especially since the significant impact of the training data distribution on generative models is undisputed [13, 105]. Moreover, for generative image modeling, it is known that pretraining on a large and diverse dataset and finetuning on a smaller but higher quality dataset significantly improves the performance [13, 71]. Since many previous approaches to video modeling have successfully drawn on techniques from the image domain [9, 42, 43], it is noteworthy that the effect of data and training strategies, i.e., the separation of video pretraining at lower resolutions and high-quality finetuning, has yet to be studied. This work directly addresses these previously uncharted territories.

Webelieve that the significant contribution of data selection is heavily underrepresented in today's video research landscape despite being well-recognized among practitioners when training video models at scale. Thus, in contrast to previous works, we draw on simple latent video diffusion baselines [9] for which we fix architecture and training scheme and assess the effect of data curation . To this end, we first identify three different video training stages that we find crucial for good performance: text-to-image pretraining, video pretraining on a large dataset at low resolution, and high-resolution video finetuning on a much smaller dataset with higher-quality videos. Borrowing from largescale image model training [13, 64, 66], we introduce a systematic approach to curate video data at scale and present an empirical study on the effect of data curation during video

pretraining. Our main findings imply that pretraining on well-curated datasets leads to significant performance improvements that persist after high-quality finetuning.

A general motion and multi-view prior Drawing on these findings, we apply our proposed curation scheme to a large video dataset comprising roughly 600 million samples and train a strong pretrained text-to-video base model, which provides a general motion representation. We exploit this and finetune the base model on a smaller, high-quality dataset for high-resolution downstream tasks such as textto-video (see Figure 1, top row) and image-to-video, where we predict a sequence of frames from a single conditioning image (see Figure 1, mid rows). Human preference studies reveal that the resulting model outperforms state-of-the-art image-to-video models.

Furthermore, we also demonstrate that our model provides a strong multi-view prior and can serve as a base to finetune a multi-view diffusion model that generates multiple consistent views of an object in a feedforward manner and outperforms specialized novel view synthesis methods such as Zero123XL [14, 57] and SyncDreamer [58]. Finally, we demonstrate that our model allows for explicit motion control by specifically prompting the temporal layers with motion cues and also via training LoRAmodules [32, 45] on datasets resembling specific motions only, which can be efficiently plugged into the model.

To summarize, our core contributions are threefold: (i) We present a systematic data curation workflow to turn a large uncurated video collection into a quality dataset for generative video modeling. Using this workflow, we (ii) train state-of-the-art text-to-video and image-to-video models, outperforming all prior models. Finally, we (iii) probe the strong prior of motion and 3D understanding in our models by conducting domain-specific experiments. Specifically, we provide evidence that pretrained video diffusion models can be turned into strong multi-view generators, which may help overcome the data scarcity typically observed in the 3D domain [14].

## 2. Background

Most recent works on video generation rely on diffusion models [38, 84, 87] to jointly synthesize multiple consistent frames from text- or image-conditioning. Diffusion models implement an iterative refinement process by learning to gradually denoise a sample from a normal distribution and have been successfully applied to highresolution text-to-image [13, 64, 68, 71, 75] and video synthesis [9, 29, 41, 82, 95].

In this work, we follow this paradigm and train a latent [71, 92] video diffusion model [9, 23] on our video dataset. We provide a brief overview of related works which utilize latent video diffusion models (Video-LDMs)

in the following paragraph; a full discussion that includes approaches using GANs [10, 30] and autoregressive models [43] can be found in App. B. An introduction to diffusion models can be found in App. D.

## 3.3. Stage II: Curating a Video Pretraining Dataset

A systematic approach to video data curation. For multimodal image modeling, data curation is a key element of many powerful discriminative [66, 105] and generative [13, 40, 69] models. However, since there are no

(a) Initializing spatial layers from pretrained images models greatly improves performance.

<!-- image -->

(b) Video data curation boosts performance after video pretraining.

<!-- image -->

Figure 3. Effects of image-only pretraining and data curation on video-pretraining on LVD-10M : A video model with spatial layers initialized from a pretrained image model clearly outperforms a similar one with randomly initialized spatial weights as shown in Figure 3a. Figure 3b emphasizes the importance of data curation for pretraining, since training on a curated subset of LVD-10M with the filtering threshold proposed in Section 3.3 improves upon training on the entire uncurated LVD-10M .

equally powerful off-the-shelf representations available in the video domain to filter out unwanted examples, we rely on human preferences as a signal to create a suitable pretraining dataset. Specifically, we curate subsets of LVD using different methods described below and then consider the human-preference-based ranking of latent video diffusion models trained on these datasets.

More specifically, for each type of annotation introduced in Section 3.1 ( i.e ., CLIP scores, aesthetic scores, OCR detection rates, synthetic captions, optical flow scores), we start from an unfiltered, randomly sampled 9.8M-sized subset of LVD , LVD-10M , and systematically remove the bottom 12.5, 25 and 50% of examples. Note that for the synthetic captions, we cannot filter in this sense. Instead, we assess Elo rankings [21] for the different captioning methods from Section 3.1. To keep the number of total subsets tractable, we apply this scheme separately to each type of annotation. We train models with the same training hyperparameters on each of these filtered subsets and compare the results of all models within the same class of annotation with an Elo ranking [21] for human preference votes. Based on these votes, we consequently select the best-performing filtering threshold for each annotation type. The details of this study are presented and discussed in App. E. Applying this filtering approach to LVD results in a final pretraining dataset of 152M training examples, which we refer to as LVD-F , cf . Tab. 1.

Curated training data improves performance. In this section, we demonstrate that the data curation approach described above improves the training of our video diffusion models. To show this, we apply the filtering strategy described above to LVD-10M and obtain a four times smaller subset, LVD-10M-F . Next, we use it to train a baseline model that follows our standard architecture and train-

(a) User preference for LVD10M-F and WebVid [7].

<!-- image -->

Figure 4. Summarized findings of Sections 3.3 and 3.4 : Pretraining on curated datasets consistently boosts performance of generative video models during video pretraining at small (Figures 4a and 4b) and larger scales (Figures 4c and 4d). Remarkably, this performance improvement persists even after 50k steps of video finetuning on high quality data (Figure 4e).

<!-- image -->

(b) User preference for

LVD-

10M-F

and InternVid [100].

<!-- image -->

(c)

User preference at 50M

samples scales.

<!-- image -->

(d)

User preference on scal-

ing datasets.

(e) Relative ELO progression over time during Stage III.

<!-- image -->

ing schedule and evaluate the preference scores for visual quality and prompt-video alignment compared to a model trained on uncurated LVD-10M .

We visualize the results in Figure 3b, where we can see the benefits of filtering: In both categories, the model trained on the much smaller LVD-10M-F is preferred. To further show the efficacy of our curation approach, we compare the model trained on LVD-10M-F with similar video models trained on WebVid-10M [7], which is the most recognized research licensed dataset, and InternVid10M [100], which is specifically filtered for high aesthetics. Although LVD-10M-F is also four times smaller than these datasets, the corresponding model is preferred by human evaluators in both spatiotemporal quality and prompt alignment as shown in Figure 4b.

Data curation helps at scale. To verify that our data curation strategy from above also works on larger, more practically relevant datasets, we repeat the experiment above and train a video diffusion model on a filtered subset with 50M examples and a non-curated one of the same size. We conduct a human preference study and summarize the results of this study in Figure 4c, where we can see that the advantages of data curation also come into play with larger amounts of data. Finally, we show that dataset size is also a crucial factor when training on curated data in Figure 4d, where a model trained on 50M curated samples is superior to a model trained on LVD-10M-F for the same number of steps.

## 4.1. Pretrained Base Model

As discussed in Section 3.2, our video model is based on Stable Diffusion 2.1 [71] (SD 2.1). Recent works [44] show that it is crucial to adopt the noise schedule when training image diffusion models, shifting towards more noise for higher-resolution images. As a first step, we finetune the fixed discrete noise schedule from our image model towards continuous noise [87] using the network preconditioning proposed in Karras et al. [51] for images of size 256 × 384 . After inserting temporal layers, we then train the model on LVD-F on 14 frames at resolution 256 × 384 . We use the

standard EDM noise schedule [51] for 150k iterations and batch size 1536. Next, we finetune the model to generate 14 320 × 576 frames for 100k iterations using batch size 768. We find that it is important to shift the noise schedule towards more noise for this training stage, confirming results by Hoogeboom et al. [44] for image models. For further training details, see App. D. We refer to this model as our base model which can be easily finetuned for a variety of tasks as we show in the following sections. The base model has learned a powerful motion representation, for example, it significantly outperforms all baselines for zero-shot textto-video generation on UCF-101 [88] (Tab. 2). Evaluation details can be found in App. E.

## 4.2. High-Resolution Text-to-Video Model

We finetune the base text-to-video model on a high-quality video dataset of ∼ 1Msamples. Samples in the dataset generally contain lots of object motion, steady camera motion, and well-aligned captions, and are of high visual quality altogether. We finetune our base model for 50k iterations at resolution 576 × 1024 (again shifting the noise schedule towards more noise) using batch size 768. Samples in Figure 5, more can be found in App. E.

Figure 7. Applying three camera motion LoRAs ( horizontal , zooming , static ) to the same conditioning frame (on the left).

<!-- image -->

## 4.3. High Resolution Image-to-Video Model

Besides text-to-video, we finetune our base model for image-to-video generation, where the video model receives a still input image as a conditioning. Accordingly, we replace text embeddings that are fed into the base model with the CLIP image embedding of the conditioning. Additionally, we concatenate a noise-augmented [39] version of the conditioning frame channel-wise to the input of the UNet [73]. We do not use any masking techniques and simply copy the frame across the time axis. We finetune two models, one predicting 14 frames and another one predicting 25 frames; implementation and training details can be found in App. D. We occasionally found that standard vanilla classifier-free guidance [36] can lead to artifacts: too little guidance may result in inconsistency with the conditioning frame while too much guidance can result in oversaturation. Instead of using a constant guidance scale, we found it helpful to linearly increase the guidance scale across the frame axis (from small to high). Details can be found in App. D. Samples in Figure 5, more can be found in App. E.

In Section 4.5 we compare our model with state-of-theart, closed-source video generative models, in particular GEN-2 [23, 74] and PikaLabs [54], and show that our model is preferred in terms of visual quality by human voters. Details on the experiment, as well as many more image-tovideo samples, can be found in App. E.

## 5. Conclusion

We present Stable Video Diffusion (SVD), a latent video diffusion model for high-resolution, state-of-the-art text-tovideo and image-to-video synthesis. To construct its pretraining dataset, we conduct a systematic data selection and scaling study, and propose a method to curate vast amounts of video data and turn large and noisy video collection into suitable datasets for generative video models. Furthermore, we introduce three distinct stages of video model training which we separately analyze to assess their impact on the final model performance. Stable Video Diffusion provides a powerful video representation from which we finetune video models for state-of-the-art image-to-video synthesis and other highly relevant applications such as LoRAs for camera control. Finally we provide a pioneering study on multi-view finetuning of video diffusion models and show that SVD constitutes a strong 3D prior, which obtains stateof-the-art results in multi-view synthesis while using only a

fraction of the compute of previous methods.

We hope these findings will be broadly useful in the generative video modeling literature. A discussion on our work's broader impact and limitations can be found in App. A.

## A. Broader Impact and Limitations

Broader Impact: Generative models for different modalities promise to revolutionize the landscape of media creation and use. While exploring their creative applications, reducing the potential to use them for creating misinformation and harm are crucial aspects before real-world deployment. Furthermore, risk analyses need to highlight and evaluate the differences between the various existing model types, such as interpolation, text-to-video, animation, and long-form generation. Before these models are used in practice, a thorough investigation of the models themselves, their intended uses, safety aspects, associated risks, and potential biases is essential.

Limitations: While our approach excels at short video generation, it comes with some fundamental shortcomings w.r.t. long video synthesis: Although a latent approach provides efficiency benefits, generating multiple keyframes at once is expensive both during training but also inference, and future work on long video synthesis should either try a cascade of very coarse frame generation or build dedicated tokenizers for video generation. Furthermore, videos generated with our approach sometimes suffer from too little generated motion. Lastly, video diffusion models are typically slow to sample and have high VRAM requirements, and our model is no exception. Diffusion distillation methods [41, 61, 79] are promising candidates for faster synthesis.

## D. Model and Implementation Details



## D.2. Base Model Training and Architecture

As discussed in , we start the publicly available Stable Diffusion 2.1 [71] (SD 2.1) model. In the EDM-framework [51], SD 2.1 has the following preconditioning functions:

c SD2 . 1 skip ( σ ) = 1 , (5)

c SD2 . 1 out ( σ ) = -σ , (6)

c SD2 . 1 in ( σ ) = 1 √ σ 2 +1 , (7)

c SD2 . 1 noise ( σ ) = arg min j ∈ [1000] ( σ -σ j ) , (8)

(9)

where σ j +1 > σ j . The distribution over noise levels p ( σ ) used for the original SD 2.1. training is a uniform distribution over the 1000 discrete noise levels { σ j } j ∈ [1000] . One issue with the training of SD 2.1 (and in particular its noise distribution p ( σ ) ) is that even for the maximum discrete noise level σ 1000 the signal-to-noise ratio [53] is still relatively high which results in issues when, for example, generating very dark images [34, 56]. Guttenberg and CrossLabs [34] proposed offset noise , a modification of the training objective in Eq. (2) by making p ( n | σ ) non-isotropic Gaussian. In this work, we instead opt to modify the preconditioning functions and distribution over training noise levels altogether.

Image model finetuning. We replace the above preconditioning functions with

c skip ( σ ) = ( σ 2 +1 ) -1 , (10)

c out ( σ ) = -σ √ σ 2 +1 , (11)

c in ( σ ) = 1 √ σ 2 +1 , (12)

c noise ( σ ) = 0 . 25 log σ, (13)

(14)

which can be recovered in the EDM framework [51] by setting σ data = 1 ); the preconditioning functions were originally proposed in [79]. We also use the noise distribution and weighting function proposed in Karras et al. [51], namely log σ ∼ N ( P mean , P 2 std ) and λ ( σ ) = (1+ σ 2 ) σ -2 , with P mean = -1 . 2 and P std = 1 . We then finetune the neural network backbone F θ of SD2.1 for 31k iterations using this setup. For the first 1k iterations, we freeze all parameters of F θ except for the time-embedding layer and train on SD2.1's original training resolution of 512 × 512 . This allows the model to adapt to the new preconditioning functions without unnecessarily modifying the internal representations of F θ too much. Afterward, we train all layers of F θ for another 30k iterations on images of size 256 × 384 , which is the resolution used in the initial stage of video pretraining.

Video pretraining. We use the resulting model as the image backbone of our video model. We then insert temporal convolution and attention layers. In particular, we follow the exact setup from [9], inserting a total of 656M new parameters into the UNet bumping its total size (spatial and temporal layers) to 1521M parameters. We then train the resulting UNet on 14 frames on resolution 256 × 384 for 150k iters using AdamW [59] with learning rate 10 -4 and a batch size of 1536. We train the model for classifier-free guidance [36] and drop out the text-conditioning 15% of the time. Afterward, we increase the spatial resolution to 320 × 576 and train for an additional 100k iterations, using the same settings as for the lower-resolution training except for a reduced batch size of 768 and a shift of the noise distribution towards more noise, in

particular, we increase P mean = 0 . During training, the base model and the high-resolution Text/Image-to-Video models are all conditioned on the input video's frame rate and motion score. This allows us to vary the amount of motion in a generated video at inference time.

## D.3. High-Resolution Text-to-Video Model

We finetune our base model on a high-quality dataset of ∼ 1M samples at resolution 576 × 1024 . We train for 50 k iterations at a batch size of 768, learning rate 3 × 10 -5 , and set P mean = 0 . 5 and P std = 1 . 4 . Additionally, we track an exponential moving average of the weights at a decay rate of 0.9999. The final checkpoint is chosen using a combination of visual inspection and human evaluation.

## D.4. High-Resolution Image-to-Video Model

Wecan finetune our base text-to-video model for the image-to-video task. In particular, during training, we use one additional frame on which the model is conditioned. We do not use text-conditioning but rather replace text embeddings fed into the base model with the CLIP image embedding of the conditioning frame. Additionally, we concatenate a noise-augmented [39] version of the conditioning frame channel-wise to the input of the UNet [73]. In particular, we add a small amount of noise of strength log σ ∼ N ( -3 . 0 , 0 . 5 2 ) to the conditioning frame and then feed it through the standard SD 2.1 encoder. The mean of the encoder distribution is then concatenated to the input of the UNet (copied across the time axis). Initially, we finetune our base model for the image-to-video task on the base resolution ( 320 × 576 ) for 50k iterations using a batch size of 768 and learning rate 3 × 10 -5 . Since the conditioning signal is very strong, we again shift the noise distribution towards more noise, i.e., P mean = 0 . 7 and P std = 1 . 6 . Afterwards, we fintune the base image-to-video model on a high-quality dataset of ∼ 1M samples at 576 × 1024 resolution. We train two versions: one to generate 14 frames and one to generate 25 frames. We train both models for 50 k iterations at a batch size of 768, learning rate 3 × 10 -5 , and set P mean = 1 . 0 and P std = 1 . 6 . Additionally, we track an exponential moving average of the weights at a decay rate of 0.9999. The final checkpoints are chosen using a combination of visual inspection and human evaluation.

## D.5. Interpolation Model Details

Similar to the text-to-video and image-to-video models, we finetune our interpolation model starting from the base textto-video model, cf . App. D.2. To enable interpolation, we reduce the number of output frames from 14 to 5, of which we use the first and last as conditioning frames, which we feed to the UNet [73] backbone of our model via the concatconditioning-mechanism [71]. To this end, we embed these frames into the latent space of our autoencoder, resulting in two image encodings z s , z e ∈ R c × h × w , where c = 4 , h = 52 , w = 128 . To form a latent frame sequence that is of the same shape as the noise input of the UNet, i.e . R 5 × c × h × w , we use a learned mask embedding z m ∈ R c × h × w and form a latent sequence z = { z s , z m , z m , z m , z e } ∈ R 5 × c × h × w . We concatenate this sequence channel-wise with the noise input and additionally with a binary mask where 1 indicates the presence of a conditioning frame and 0 that of a mask embedding. The final input for the UNet is thus of shape (5 , 9 , 52 , 128) . In line with previous work [9, 41, 82], we use noise augmentation for the two conditioning frames, which we apply in the latent space. Moreover, we replace the CLIP text representation for the crossattention conditioning with the corresponding CLIP image representation of the start frame and end frame, which we concatenate to form a conditioning sequence of length 2.

We train the model on our high-quality dataset at spatial resolution 576 × 1024 using AdamW [59] with a learning rate of 10 -4 in combination with exponential moving averaging at decay rate 0.9999 and use a shifted noise schedule with

```
1 import torch 2 from einops import rearrange, repeat 3 4 5 def append\_dims(x: torch.Tensor, target\_dims: int) -> torch.Tensor: 6 """Appends dimensions to the end of a tensor until it has target\_dims dimensions.""" 7 dims\_to\_append = target\_dims - x.ndim 8 if dims\_to\_append < 0: 9 raise ValueError( 10 f"input has {x.ndim} dims but target\_dims is {target\_dims}, which is less" 11 ) 12 return x[(...,) + (None,) * dims\_to\_append] 13 14 15 class LinearPredictionGuider: 16 def \_\_init\_\_( 17 self, 18 max\_scale: float, 19 num\_frames: int, 20 min\_scale: float = 1.0, 21 ): 22 self.min\_scale = min\_scale 23 self.max\_scale = max\_scale 24 self.num\_frames = num\_frames 25 self.scale = torch.linspace(min\_scale, max\_scale, num\_frames).unsqueeze(0) 26 27 def \_\_call\_\_(self, x: torch.Tensor, sigma: float) -> torch.Tensor: 28 x\_u, x\_c = x.chunk(2) 29 30 x\_u = rearrange(x\_u, "(b t) ... -> b t ...", t=self.num\_frames) 31 x\_c = rearrange(x\_c, "(b t) ... -> b t ...", t=self.num\_frames) 32 scale = repeat(self.scale, "1 t -> b t", b=x\_u.shape[0]) 33 scale = append\_dims(scale, x\_u.ndim).to(x\_u.device) 34 35 return rearrange(x\_u + scale * (x\_c - x\_u), "b t ... -> (b t) ...")
```

Figure 15. PyTorch code for our novel linearly increasing guidance technique.

P mean = 1 and P std = 1 . 2 . Surprisingly, we find this model, which we train with a comparably small batch size of 256, to converge extremely fast and to yield consistent and smooth outputs after only 10k iterations. We take this as another evidence of the usefulness of the learned motion representation our base text-to-video model has learned.

## E. Experiment Details



## E.1.1 Experimental Setup

Given all models in one ablation axis ( e.g . four models of varying aesthetic or motion scores), we compare each prompt for each pair of models (1v1). For every such comparison, we collect on average three votes per task from different annotators, i.e., three each for visual quality and prompt following, respectively. Performing a complete assessment between all pairwise comparisons gives us robust and reliable signals on model performance trends and the effect of varying thresholds. Sample interfaces that the annotators interact with are shown in Figure 16. The order of prompts and the order between models are fully randomized. Frequent attention checks are in place to ensure data quality.

(a) Sample instructions for evaluating visual quality of videos.

<!-- image -->

(b) Sample instructions for evaluating the prompt following of videos.

## E.2.1 Architectural Details

Architecturally, all models trained for the presented analysis in Section 3 are identical. To insert create a temporal UNet [73] based on an existing spatial model, we follow Blattmann et al. [9] and add temporal convolution and (cross-)attention layers after each corresponding spatial layer. As a base 2D-UNet, we use the architecture from Stable Diffusion 2.1 , whose weights we further use to initialize the spatial layers for all runs except the second one presented in Figure 3a, where we intentionally skip this initialization to create a baseline for demonstrating the effect of image-pretraining. Unlike Blattmann et al. [9], we train all layers, including the spatial ones, and do not freeze the spatial layers after initialization. All models are trained with the AdamW [59] optimizer with a learning rate of 1 .e -4 and a batch size of 256 . Moreover, in contrast to our models from Section 4, we do not translate the noise process to continuous time but use the standard linear schedule used in Stable Diffusion 2.1 , including offset noise [34], in combination with the v-parameterization [37]. We omit the text-conditioning in 10% of the cases to enable classifier-free guidance [37] during inference. To generate samples for the evaluations, we use 50 steps of the deterministic DDIM sampler [86] with a classifier guidance scale of 12 for all models.

