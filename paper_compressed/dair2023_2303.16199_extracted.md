## ABSTRACT

With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT. To this end, we present LLaMA-Adapter , a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning. Specifically, a zero-initialized attention mechanism is proposed. It adopts a learnable zero gating to adaptively inject the instructional cues into LLaMA within self-attention layers, contributing to a stable training process and superior final performance. In this way, LLaMA-Adapter can generate high-quality responses to diverse language instructions, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, by incorporating an image encoder, our approach can be simply extended to a Multi-modal LLM for image-conditioned instruction following, which achieves superior multi-modal reasoning capacity on several popular benchmarks (MME, MMBench, LVLM-eHub). Furthermore, we also verify the proposed zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa, CLIP) on traditional vision and language tasks, demonstrating the effectiveness and generalizability of our approach. Code and models are released at https://github.com/OpenGVLab/LLaMA-Adapter .

## 1 INTRODUCTION

Large Language Models (LLMs) (Dai et al., 2019; Radford et al., 2019; Zhang et al., 2022; Raffel et al., 2020; Devlin et al., 2018) have stimulated widespread attention in both academia and industry. Driven by massive corpora and advanced hardware, LLMs exhibit remarkable understanding and generative ability, propelling language tasks to a higher level. Recently, significant progress has been made on instruction-following models, e.g., ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), which follow language instructions and generate contextual responses. However, the further prevalence of instruction models is largely impeded by the closed-source restriction and high development costs.

To alleviate this, Stanford Alpaca (Taori et al., 2023) proposes to fine-tune an open-source LLM, i.e., LLaMA (Touvron et al., 2023) into an instruction-following model, which is affordable and replicable. Starting from 175 human-written instruction-output pairs (Wang et al., 2022a), Alpaca leverages GPT-3.5 (Brown et al., 2020) to expand the training data to 52K in a self-instruct manner. Supervised by this, Alpaca fine-tunes the entire 7B parameters in LLaMA, producing an exceptional instruction model that performs similarly to GPT-3.5. Despite Alpaca's effectiveness, a complete fine-tuning of large-scale LLaMA is still time-consuming, computation-intensive, and cumbersome to transfer to different downstream scenarios.

Figure 1: Characteristics of LLaMA-Adapter. Our lightweight adaption method efficiently finetunes LLaMA (Touvron et al., 2023) 7B model with only 1.2M learnable parameters within one hour, which exhibits superior instruction-following and multi-modal reasoning capacity.

<!-- image -->

In this paper, we introduce LLaMA-Adapter , an efficient fine-tuning method that adapts LLaMA into a well-performed instruction-following model. Trained by Alpaca's instruction-output data, our approach freezes the entire LLaMA model, and proposes a zero-initialized attention mechanism with superior resource efficiency. Specifically, in LLaMA's higher transformer layers, we append a set of learnable adaption prompts as prefixes to the word tokens. Then, to avoid the noise from randomly initialized prompts at the early training stage, we equip the frozen self-attention layers with a learnable gating factor. The gating mechanism is initialized by zeros, and controls the feature interaction between prompt and word tokens, within the process of attention calculation. Such a strategy can first preserve the original knowledge in LLaMA, and progressively inject the new instructional signals during training. This contributes to a more stable learning process and better instruction-following capacity of the final model.

Overall, our LLaMA-Adapter exhibits four main characteristics, as shown in Figure 1.

- · 1.2M Parameters. Instead of updating the full 7B parameters, we freeze the pre-trained LLaMA and only learn the zero-initialized attention mechanism with 1.2M parameters. This, however, reveals comparable instruction-following proficiency with the 7B Alpaca.
- · One-hour Fine-tuning. Thanks to our lightweight adaption modules with zero-initialized gating, the training convergence of LLaMA-Adapter costs less than one hour on 8 A100 GPUs, which are three times faster than Alpaca.
- · Plug with Expertise. For different scenarios, it is flexible to insert their respective adapters to endow LLaMA with different expert knowledge or new modality input. Thus, it suffices to store a 1.8M adapter within each context, other than a complete copy of the 13G LLaMA.
- · Multi-modal Reasoning. Besides language instruction, our approach can also incorporate an image encoder via zero-initialized attention to become a multi-modal LLM. Compared to concurrent works (Liu et al., 2023b; Zhu et al., 2023), LLaMA-Adapter showcases higher tuning efficiency with competitive reasoning capacity on MME (Fu et al., 2023), MMBench (Liu et al., 2023c), and LVLM-eHub (Xu et al., 2023) benchmarks.

In addition to instruction tuning, our zero-initialized attention can be generalized to traditional vision and language tasks for parameter-efficient fine-tuning. We apply our approach to the pretrained ViT (Dosovitskiy et al., 2020), ReBERTa (Liu et al., 2019), and CLIP (Radford et al., 2021), respectively for fine-tuning vision, language, and vision-language models. On a wide range of downstream tasks, we demonstrate the effectiveness of our proposed method for traditional tasks.

## 4.3 ABLATION STUDY

Insertion Layers. We first investigate the number of transformer layers to be inserted by zeroinitialized attention in LLaMA-Adapter. As shown in Table 4, increasing the layer numbers introduces more parameters, but leads to a large improvement in the answering accuracy of ScienceQA's validation set. There also exists an optimal insertion number from the higher layers, since too many layers would adversely disturb the early encoding of input words. If one has limited resources to identify the best number, simply inserting into all transformer layers is generally a good solution.

Zero-initialized Attention. Our proposed zero-initialized attention is essential for the early-stage training stability and final generation capacity. As shown in Table 5, it contributes to a significant +43.08% gain on ScienceQA's validation set. In contrast, the randomly initialized baseline only achieves 40.77% accuracy, nearly the same as 'Random Choice' (Table 2's first row). In Figure 7, we plot the loss curves with and without the zero initialization. The loss of 'Zero-initialized' declines much faster at the beginning, and finally converges to zero. In contrast, the 'Random-initialized' slowly approaches 0.15, which is not fully converged and causes a large performance drop.

Table 4: Number of Insertion Layers to the pre-trained transformer of LLaMA.Table 6: Vision Model Finetuning with ViT on VTAB-1k benchmark.

|   Layers |   Params |   Val Acc. |
|----------|----------|------------|
|       10 |     0.97 |      55.95 |
|       20 |     1.37 |      73.36 |
|       30 |     1.79 |      83.85 |
|       32 |     1.83 |      81.03 |

Table 5: Effectiveness of Zero-initialized Attention in our method.

| Method     |   Natural |   Special. |   Struct. |
|------------|-----------|------------|-----------|
| Full       |     75.88 |      83.36 |     47.64 |
| Adapter    |     70.39 |      77.11 |     33.43 |
| Sidetune   |     58.21 |      68.12 |     23.41 |
| VPT        |     78.48 |      82.43 |     54.98 |
| Zero-init. |     81.74 |      84.43 |     56.75 |

Figure 7: Loss Curves of LLaMAAdapter with (blue) and without (orange) zero-initialized attention.

| Setting    |   Val Acc. |
|------------|------------|
| Rand-Init. |      40.77 |
| Zero-Init. |      83.85 |
| Gain       |      43.08 |

Table 7: Language Model Fine-tuning with RoBERTa on SQuAD benchmark.

<!-- image -->

Table 8: Vision-language Fine-tuning with CLIP on base-to-novel benchmark.

| Method     | SQuAD 1.1   | SQuAD 1.1   | SQuAD 2.0   | SQuAD 2.0   |
|------------|-------------|-------------|-------------|-------------|
|            | EM          | F1          | EM          | F1          |
| Full       | 88.9        | 94.6        | 86.5        | 89.4        |
| PT         | 1.2         | 12.0        | 50.2        | 50.2        |
| PT2        | 88.5        | 94.4        | 82.1        | 85.5        |
| Zero-init. | 88.8        | 94.6        | 83.9        | 87.2        |

| Method     |   Base |   Novel |    HM |
|------------|--------|---------|-------|
| CLIP       |  75.88 |   83.36 | 47.64 |
| CoOp       |  70.39 |   77.11 | 33.43 |
| CoCop      |  58.21 |   68.12 | 23.41 |
| MaPLe      |  78.48 |   82.43 | 54.98 |
| Zero-init. |  81.74 |   84.43 | 56.75 |

## 5 CONCLUSION

In this paper, we propose LLaMA-Adapter, an efficient adaption method for tuning instructionfollowing models. For better training stability and final performance, we introduce the zero-initialized attention mechanism with a learnable gating factor, which increasingly incorporates instructional signals, while preserving the pre-trained knowledge in LLaMA. With only 1.2M parameters and one-hour training, our approach effectively fine-tunes LLaMA with superior efficiency compared to the 7B-parameter Alpaca. LLaMA-Adapter can be generalized to image-conditioned generation as a multi-modal LLM, achieving competitive results on various visual question answering benchmarks. On traditional vision and language tasks, our zero-initialized attention also attains favorable finetuning performance, which indicates strong generalization capacity.

## A OVERVIEW

- · Section B: Detailed results of zero-shot multi-modal evaluation.
- · Section C: Additional related work.
- · Section D: Detailed results of fine-tuning traditional vision and language models.
- · Section E: Additional experiments and discussion.
- · Section F: Full comparison of instruction-following models.
- · Section G: Comparison of LLaMA-Adapter and LLaMA-I.

## E ADDITIONAL EXPERIMENT AND DISCUSSION



