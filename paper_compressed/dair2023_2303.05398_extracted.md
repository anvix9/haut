## ABSTRACT

Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose 'MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset ( 78 . 7% → 92 . 5% ) evaluated using 175B parameter GPT-based LLM.

## 1 INTRODUCTION

Recent advancements in natural language processing (NLP) can be attributed to massive scaling of Large Language Models (LLMs) Vaswani et al. (2017); Devlin et al. (2018); Raffel et al. (2020); Brown et al. (2020); Rae et al. (2021); Chowdhery et al. (2022); Thoppilan et al. (2022). A very interesting recent discovery that the LLMs are naturally good (in-context) Zero-shot or few-shot learners turned out to be very useful Brown et al. (2020); Liu et al. (2021; 2023). This led to the development of 'prompting' technique, where the user provides a small context for solving the task athand to the LLM. This conditioning of the models on a few examples is termed as few-shot prompting, while providing instructions to solve a task is known as Zero-shot prompting. Extensive research efforts are being poured into designing these prompts, either manually Schick & Schütze (2020); Reynolds & McDonell (2021) or automatically Shin et al. (2020); Gao et al. (2020). Although quite successful for single-step system-I tasks Stanovich & West (2000); Liu et al. (2023), the prompting techniques were inadequate in their performance on system-II tasks where multi-step reasoning is required Rae et al. (2021). As humans, we tend to break down a problem and attempt to solve them step-by-step. Extending this intuition to LLMs led to the development of 'chain-of-thought' (CoT) prompting technique Wei et al. (2022); Wang et al. (2022). The use of CoT has led to improved performance on a range of NLP tasks Talmor et al. (2018); Gao et al. (2020); Patel et al. (2021); Cobbe et al. (2021); Geva et al. (2021); Chowdhery et al. (2022); Srivastava et al. (2022)

In this work, we investigate Zero-shot-CoT methods for solving mathematical reasoning tasks. To the best of our knowledge, we found the recent work by Kojima et al. (2022) that proposed a Zero-shot-CoT technique to be the state-of-the-art where they demonstrated a remarkable accuracy improvement on the 'MultiArith' Roy & Roth (2016) data ( 17 . 7% → 78 . 7% ). Now, we identify two key aspects that lacks in the previous CoT prompting based SOTA, namely (1) Although, the chain-of-thought followed by the model improved the results, but there is no check on the validity of the steps followed by the chain-of-thought prompting and (2) The confidence in the predictions of LLMs are often not provided. In order to address these gap to some extent, we derive inspiration from how we humans solve a math question by breaking it down to a simpler multi-step procedure and make use of multiple ways to validate our approach at each step. Specifically, given a question Q,

Figure 1: MathPrompter flow. We outline the MathPrompter process with an example alongside.

<!-- image -->

(I) Generating Algebraic template : We first generate its corresponding Algebraic expression Qt that replaces the numerical entries by variables. (II) Math-prompts : Then, we provide multiple prompts P to the LLM that can solve Qt analytically in different ways. For eg. P can be 'Derive an Algebraic expression' or 'Write a Python function' etc. Following this procedure, we end up with P expressions that analytically solves Qt in terms of its variables. (III) Compute verification : We then evaluate the P analytical solutions by allotting multiple random values to the Qt variables. (IV) Statistical significance : If the solutions of the P analytical functions are in ' consensus ' over N ∼ 5 different variable choices, then we substitute the original values from Q to obtain the final solution. In the case where there is no definite consensus, we repeat the steps (II), (III) & (IV). Our method, MathPrompter, uses 175B parameter LLM called GPT3 DaVinci completion engine Brown et al. (2020). We were able to improve the accuracy on the MultiArith data from 78 . 7% → 92 . 5% .

## 2 METHOD

Since the LLMs are generative models, it becomes very tricky to ensure that the generated answers are accurate, especially for mathematical reasoning tasks. We take clues from the process followed by students to solve arithmetic problems. We narrowed down a few steps that students take in order to verify their solutions, namely

- · Compliance with known results : By comparing the solution to a known result, one can assess its accuracy and make necessary adjustments. This is particularly useful when the question is a standard problem with a well-established solution.
- · Multi-verification : By approaching a problem from multiple perspectives and comparing the results helps to confirm the validity of the solution and ensure that it is both sound and accurate.
- · Cross-checking : The process of solving a problem is just as necessary as the final answer. Verifying the correctness of the intermediate steps of the process provide a clear understanding of the thought process behind the solution.
- · Compute verification : Utilizing a calculator or computer to perform arithmetic calculations can assist in verifying the accuracy of the final answer.

## 3.1 DATASET

We evaluate MathPrompter on MultiArith dataset Roy & Roth (2016), which is a subset of the Math World Problem Repository Koncel-Kedziorski et al. (2016). This dataset is a collection of mathematical problems that are specifically designed to test the ability of machine learning models to perform complex arithmetic operations and reasoning. These problems demand the application of multiple arithmetic operations and logical reasoning to be sucessfully solved.

## 4 CONCLUSIONS & DISCUSSIONS

We introduced MathPrompter, a novel approach that improves LLM performance on mathematical reasoning problems. It also addresses an important concern of building the user trust to some extent in the LLM predictions. We translated our intuition on how students solve arithmetic problems to a LLM model by utilizing the Zero-shot chain-of-thought prompting technique. MathPrompter incorporates ideas like cross-checking the intermediate steps and solving the same math problem using multiple approaches in its design. We empirically show that our model is comparable to SOTA Few-shot-CoT models as well as the larger Zero-shot-CoT models that have 540B parameters. In future, we plan to further evaluate performance on additional datasets and explore incorporating additional prompts into MathPrompter.

## 5 LIMITATION

One of the limitations of our work is that while we are running the MathPrompter multiple times in different ways to increase the accuracy of our results, this does not always guarantee the correctness of the output. Both Algebraic and Pythonic expressions have the potential to produce the incorrect results, even if the prompt outputs match each other. This is the fail case as shown in the last row of Table 2. Increasing the number of prompts will mitigate this issue. We are currently investigating techniques that can address this issue in a more principled manner.

