# The Llama 3 Herd of Models

# Research questions
Q1: What is the primary goal of the research paper presented by Tethys AI?

A1: The primary goal of this research paper is to present a new set of foundation models called Llama 3 that natively support multilinguality, coding, reasoning, and tool usage.

Q2: What are the key improvements made in developing Llama 3 compared to prior versions of Llama models?

A2: The improvements made include enhanced data quality, increased data quantity, and more rigorous post-training data filtering approaches.

Q3: What scaling laws were followed in the development of Llama 3?

A3: Llama 3 is trained using a larger scale than previous Llama models, with its flagship model being pre-trained on 15.6T text tokens, and this approach outperforms smaller models at similar inference budgets.

Q4: What design choices were made to maximize the ability to scale the model development process?

A4: The team opted for a standard dense Transformer model architecture and a relatively simple post-training procedure based on supervised finetuning, rejection sampling, and direct preference optimization, rather than more complex reinforcement learning algorithms.

Contribution: Developing Llama 3 demonstrates that substantial further improvements in high-quality foundation models are possible by focusing on data quality, scale, and simplicity. The public release of the flagship model aims to accelerate research, improve model safety, and promote open development towards achieving artificial general intelligence (AGI).

## Problem Statement, Methods and Main Results

• Introducing a foundation model (Llama 3) that supports multilinguality, coding, reasoning, and tool usage.
• Developing repository-level data construction during pre-training to boost cross-file code generation capabilities.
• Conducting extensive evaluations demonstrating superiority over existing open-source models.

#### Keywords: Multilingual Model, Dense Transformer Architecture, Foundation Models, Pre-training and Post-Training Stages, Reinforcement Learning


### [Link to paper](https://arxiv.org/abs/2407.21783v3)
