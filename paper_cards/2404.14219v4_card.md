# Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone

# Research questions
Q1: Can a small-sized language model, such as phi-3-mini, trained on a large dataset, achieve near-par performance with models like ChatGPT despite having significantly fewer parameters?

Q2: How does the use of a novel training data curation approach, combined with fine-tuning and parameter scaling techniques, enable the development of compact yet highly capable language models?

Contribution: We introduce phi-3-mini, a 3.8 billion parameter language model that rivals models like ChatGPT despite being small enough to fit on a phone, by leveraging advanced data-driven machine learning techniques and optimized training datasets.

## Problem Statement, Methods and Main Results

  + Advanced code-focused large language models, including DeepSeek-Coder-Base and DeepSeek-Coder-Instruct.
  + Repository-level data construction during pre-training to boost cross-file code generation capabilities.
  + Evaluating extensive benchmarks and demonstrating superiority over existing open-source models.

#### Keywords: Natural Language Processing, Large Language Models, Scaling Laws in Machine Learning


### [Link to paper](https://arxiv.org/abs/2404.14219v4)
