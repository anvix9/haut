# Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone

# Research questions
Q1: Can a large language model be trained with smaller amounts of data and still achieve comparable performance to models that require massive amounts of data?

Q2: Is it possible to design a language model small enough to fit on a phone, yet powerful enough to rival the capabilities of larger models like ChatGPT?

Q3: What role does dataset curation play in enabling the development of compact language models with impressive performance?

Contribution: We introduce phi-3-mini, a 3.8 billion parameter language model that achieves state-of-the-art performance on several benchmarks despite being small enough to fit on a phone, solely by leveraging an optimized training dataset.

## Problem Statement, Methods and Main Results

	+ Demonstrated the feasibility of scaling up language models while maintaining robustness and performance.
	+ Introduced compact language models that achieve impressive performance despite small size, enabling deployment on resource-constrained devices.

#### Keywords: LLM, Large Language Models, Scalable LLMs, Data-Driven Machine Learning, Model Optimization
### [Link to paper](https://arxiv.org/abs/2404.14219v4)
