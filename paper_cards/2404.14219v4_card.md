# Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone

# Research questions
Q1: Why can a large language model (LLM) be built small enough to fit on a phone yet achieve impressive performance comparable to state-of-the-art models like ChatGPT?

Contribution: We introduce phi-3-mini, a 3.8 billion parameter LLM trained on 3.3 trillion tokens, which achieves near-par performance with larger models such as Mixtral 8x7B and GPT-3.5 despite its small size, highlighting the potential for data-driven machine learning to achieve significant reductions in model size without sacrificing performance.

## Problem Statement, Methods and Main Results

  • Introducing DeepSeek-Coder-Base and DeepSeek-Coder-Instruct for advanced code-focused LLMs.
  • Developing methods for cross-file code generation capabilities.
  • Conducting evaluations showcasing superiority over existing open-source models.

#### Keywords: Natural Language Processing, Large Language Models (LLMs), Scaling Laws, Data-Driven Machine Learning, Model Compression


### [Link to paper](https://arxiv.org/abs/2404.14219v4)
