# Direct Preference Optimization: Your Language Model is Secretly a Reward Model

# Research questions
Q1: Can a simple binary cross-entropy objective be used instead of reinforcement learning (RL) to optimize the policy for large unsupervised language models (LMs)?

Q2: Is it possible to directly optimize a language model's policy without explicit reward modeling or sampling from the LM, given human preferences?

Q3: Can a stable and computationally lightweight algorithm, called Direct Preference Optimization (DPO), be developed that eliminates the need for RL during fine-tuning and performs at least as well as existing methods in tasks such as sentiment modulation, summarization, and dialogue?

## Problem Statement, Methods and Main Results

  • Introduction of specialized Large Language Models (LLMs) for coding.
  • Development of repository-level data construction during pre-training.
  • Demonstrated superiority over existing open-source models in various benchmarks.

#### Keywords: Direct Preference Optimization, Reward Model Optimization, Implicit Reward Function, Closed-Form Policy Extraction, Simple Classification Loss


### [Link to paper](https://arxiv.org/abs/2305.18290v3)
