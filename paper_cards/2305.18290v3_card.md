# Direct Preference Optimization: Your Language Model is Secretly a Reward Model

# Research questions
**Research Question:**

1. Can a simple, RL-free algorithm optimize large unsupervised language models (LMs) to adhere to human preferences without explicit reward modeling or reinforcement learning?
2. How can a binary cross-entropy objective be used to directly optimize the policy best satisfying human preferences, allowing for closed-form extraction of the optimal policy?

**Motivations:**

The authors aim to address the limitations of existing methods that rely on reinforcement learning (RL) from human feedback (RLHF). They seek to develop an efficient and stable algorithm for fine-tuning language models with human preferences, which is essential for building safe, performant, and controllable AI systems.

**Problems:**

The main problem addressed in this paper is the difficulty of achieving precise control over the behavior of large unsupervised LMs due to their completely unsupervised nature. Existing methods often rely on reinforcement learning from human feedback (RLHF), which can be complex and unstable.

**Contributions:**

The authors propose Direct Preference Optimization (DPO), a simple RL-free algorithm that directly optimizes for the policy best satisfying human preferences using a binary cross-entropy objective. DPO enables closed-form extraction of the optimal policy, eliminating the need for sampling from the LM during fine-tuning or significant hyperparameter tuning.

**Implicit Questions:**

* Can we develop an efficient and stable algorithm for fine-tuning language models with human preferences?
* How can we balance the complexity of RLHF methods with the need for simplicity and stability in AI systems?

## Problem Statement, Methods and Main Results
**
    + Provided efficient and stable algorithm for fine-tuning LMs without reinforcement learning or extensive hyperparameter tuning.
    + Demonstrated outperformance of DPO over existing methods in sentiment control, summarization, and single-turn dialogue tasks.

#### Keywords: Reinforcement Learning, Direct Preference Optimization, Human Feedback, Language Model Training, Reward Modeling, Sentiment Modulation
### [Link to paper](https://arxiv.org/abs/2305.18290v3)
