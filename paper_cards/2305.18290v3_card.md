# Direct Preference Optimization: Your Language Model is Secretly a Reward Model

# Research questions
Q1: Can a large-scale unsupervised language model (LM) be trained to have precise control of its behavior while still retaining its broad world knowledge and some reasoning skills?

Q2: How can existing methods for fine-tuning language models with human feedback, which rely on reinforcement learning from human feedback (RLHF), be simplified and made more stable?

Contribution: Direct Preference Optimization (DPO) is a new algorithm that enables the training of large-scale unsupervised LMs to align with human preferences without explicit reward modeling or reinforcement learning. DPO can directly optimize a language model policy using a simple binary cross-entropy objective, allowing for the extraction of an optimal policy in closed form and eliminating the need for sampling from the LM during fine-tuning or significant hyperparameter tuning.

## Problem Statement, Methods and Main Results

    • Introducing DPO, a new algorithm for aligning large-scale unsupervised LMs with human preferences.
    • Reducing hyperparameter tuning requirements and enabling the extraction of an optimal policy in closed form.

#### Keywords: Topic extraction failed


### [Link to paper](https://arxiv.org/abs/2305.18290v3)
