# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

# Research questions
Research Question:

Q1: Can a large-scale, open-source platform that leverages crowdsourced, pairwise human preferences effectively evaluate the performance of Large Language Models (LLMs) in assessing user preferences?
Q2: What are the limitations and challenges associated with existing LLM benchmarks, and how can they be addressed by introducing an open, live evaluation platform based on human preference?

Contribution:

The authors aim to address the limitations of current LLM benchmarks, which often fail to capture nuanced and diverse aspects of these models. They introduce Chatbot Arena, a platform that allows users to anonymously ask questions, receive answers from two anonymous LLMs, and vote for their preferred response. The platform's primary contributions are:

1. Building a large-scale, crowd-sourced live LLM evaluation platform with over 240K votes.
2. Conducting an in-depth analysis of the collected data to validate the diversity and quality of user prompts and preference votes.
3. Developing an efficient sampling algorithm that actively chooses which model pairs to show, improving sample efficiency.
4. Releasing a human preference dataset with over 100K pairwise votes for future research use.

The authors aim to provide a more accurate representation of real-world LLM applications by leveraging crowdsourced, pairwise human preferences and developing scalable, incremental, and efficient ranking systems.

## Problem Statement, Methods and Main Results

  • Large-scale, open-source LLM evaluation platform (Chatbot Arena)
  • In-depth analysis of collected data for validation and diversity
  • Efficient sampling algorithm for model pairs

#### Keywords: Chatbot Arena, LLMs evaluation, crowd-sourced evaluations, human preferences, ranking systems, Statistical methods
### [Link to paper](https://arxiv.org/abs/2403.04132v1)
