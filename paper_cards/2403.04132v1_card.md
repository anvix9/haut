# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

# Research questions
Q1: Can a large-scale, open-platform evaluation method based on human preferences effectively assess the alignment of large language models (LLMs) with human preferences in real-world, open-ended tasks?

Q2: How can crowdsourced, live LLM evaluation platforms like Chatbot Arena ensure the quality and diversity of user prompts and preference votes to accurately evaluate model performance? 

Q3: What are the key challenges in creating a scalable, incremental, and efficient ranking system for evaluating a large number of models, and how does Chatbot Arena address these challenges?

Q4: How can statistical techniques, such as those described by Bradley & Terry (1952) and Vovk & Wang (2021), be used to estimate model rankings in a sample-efficient manner while retaining statistical validity?

Q5: What insights do the analysis of prompt diversity, quality, vote quality, and human feedback provide for understanding the strengths and weaknesses of Chatbot Arena and its potential as a benchmark platform?

## Problem Statement, Methods and Main Results

* Development of specialized LLMs for coding, including the DeepSeek-Coder series
* Introducing repository-level data construction during pre-training
* Conducting extensive evaluations of code-focused LLMs against various benchmarks

#### Keywords: Human Preference, Crowdsourcing, Natural Language Processing, Machine Learning Evaluation, Large Language Models


### [Link to paper](https://arxiv.org/abs/2403.04132v1)
