# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

        # Research questions
        Q1: How can a large-scale crowd-sourced evaluation platform that utilizes human preferences effectively assess the performance of Large Language Models (LLMs) in real-world, open-ended tasks, and what are the key benefits of using such an approach? 

Contribution: Developing Chatbot Arena, a benchmarking platform for LLMs that features crowdsourced, pairwise human preferences, to evaluate their performance in real-world scenarios.

        ## Problem Statement, Methods and Main Results
        
  • Developing the DeepSeek-Coder series of open-source code models with comprehensive understanding of coding languages and syntax.
  • Establishing Chatbot Arena, a benchmarking platform for Large Language Models (LLMs) that features crowdsourced, pairwise human preferences.

        #### Keywords: Natural Language Processing, Crowdsourcing, Human Preference Evaluation, Large Language Models (LLMs), Ranking Systems, Machine Learning for NLP


        ### [Link to paper](https://arxiv.org/abs/2403.04132v1)
        