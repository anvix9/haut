# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

# Research questions
Q1: How can a large-scale, open platform like Chatbot Arena effectively evaluate Large Language Models (LLMs) based on human preferences in real-world scenarios?

Contribution: Developing an efficient sampling algorithm to actively choose model pairs and ensure statistical validity while improving sample efficiency.

The main research question this paper addresses is the need for a more comprehensive evaluation method that accurately assesses LLMs' performance in real-world, open-ended tasks by leveraging human preferences. The authors aim to create a scalable, open platform (Chatbot Arena) that can gather diverse and fresh user prompts and accurately measure LLMs' performance through crowdsourced votes.

By developing an efficient sampling algorithm and employing statistical techniques, the study aims to address the limitations of current benchmarks, which often rely on static datasets and ground-truth-based evaluations. The goal is to establish a robust foundation for the credibility of Chatbot Arena as a benchmarking platform for LLMs, particularly in evaluating user preferences.

The paper's focus on developing an open-source and open-accessible platform highlights the importance of making evaluation methods more transparent, inclusive, and replicable, which will likely have a significant impact on the research community and industry as a whole.

## Problem Statement, Methods and Main Results
**
  * Developed repository-level data construction during pre-training to boost cross-file code generation capabilities.
  * Conducted extensive evaluations of code LLMs against various benchmarks.
  * Introduced specialized Large Language Models (LLMs) for coding, including DeepSeek-Coder series.

#### Keywords: Large Language Models (LLMs), Crowdsourced Evaluation, Human Preference-Based Benchmarking, Chatbot Arena Platform, Ranking Systems


### [Link to paper](https://arxiv.org/abs/2403.04132v1)
