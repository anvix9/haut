# Improved Baselines with Visual Instruction Tuning

# Research questions
Based on the provided passage, here is a clear and concise summary of the research question:

Q1: What are the optimal design choices for training Large Multimodal Models (LMMs) towards the goal of general-purpose assistants?

Q2: How can LMMs be scaled to higher resolutions while maintaining data efficiency?

Q3: What are the compositional capabilities of LMMs, and how can they generalize to novel tasks and domains?

Contribution: This study aims to address these open problems in visual instruction tuning and provides a systematic investigation into the design choices of LMMs under the LLaVA framework. The authors propose a simple yet effective approach to balance multitask learning and scaling for large multimodal models, which achieves state-of-the-art results on a broad range of benchmarks while being data-efficient.

Limitation: Despite promising results, the study highlights limitations of the current approach, including prolonged training for high-resolution images, lack of multiple-image understanding, and limited problem-solving capabilities in certain fields.

## Problem Statement, Methods and Main Results

  • Provides a simple yet effective approach to balance multitask learning and scaling for large multimodal models
  • Explores open problems in visual instruction tuning and LMMs

#### Keywords: Large Multimodal Models, Visual Instruction Tuning, LLaVA, Data Efficiency, Compositional Capabilities
### [Link to paper](https://arxiv.org/abs/2310.03744v2)
