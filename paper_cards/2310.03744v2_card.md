# Improved Baselines with Visual Instruction Tuning

# Research questions
Research Problem: 
Q1: Can Large Multimodal Models (LMMs) be effectively trained using simple modifications to their architectures and training data, while maintaining state-of-the-art performance on various benchmarks?

Contribution:
A systematic study is presented to investigate the design choices of LMMs in a controlled setting. The authors show that two simple improvements, namely, an MLP cross-modal connector and incorporating academic task-related data such as VQA, are orthogonal to the framework of LLaVA and lead to better multimodal understanding capabilities. Furthermore, they explore open problems in visual instruction tuning, including scaling to higher resolution inputs, compositional capabilities, and model hallucination.

The authors also present a simple yet effective approach to balance multitask learning and data scaling for large multimodal models, achieving state-of-the-art results on 11 benchmarks using only public data. Their findings pave the way for more robust and capable systems for LMMs.

## Problem Statement, Methods and Main Results

  - Establish stronger baselines for large multimodal models using simple improvements.
  - Address open problems in LMMs, including scaling to higher resolution inputs and model hallucination.
  - Provide more robust and capable systems for LMMs.

#### Keywords: Topic extraction failed


### [Link to paper](https://arxiv.org/abs/2310.03744v2)
