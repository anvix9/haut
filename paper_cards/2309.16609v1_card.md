# Qwen Technical Report

# Research questions
Q1: What are the primary capabilities of the proposed Large Language Model (LLM) series QWEN?

Q2: How do the LLMs outperform existing models on benchmark tasks, specifically in coding and mathematics?

Q3: What techniques are used for alignment and fine-tuning of the chat models in the QWEN series?

Contribution: The QWEN series presents a comprehensive language model series that encompasses distinct models with varying parameter counts, including base pretrained language models, chat models finetuned with human alignment techniques, and specialized models in coding and math.

## Problem Statement, Methods and Main Results
**
* Introducing DeepSeek-Coder series of open-source code models.
* Developing repository-level data construction during pre-training.
* Conducting extensive evaluations against existing benchmarks, demonstrating superiority over open-source models.

#### Keywords: Natural Language Processing, Reinforcement Learning (RLHF), Chat Models, Large Language Models (LLMs), Specialized Models for Coding and Mathematics


### [Link to paper](https://arxiv.org/abs/2309.16609v1)
