# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

# Research questions
Q1: How do selective state space models address the trade-off between efficiency and effectiveness in sequence modeling?

Contribution: Selective state space models (SSMs) improve on prior work by incorporating a selection mechanism, which enables context-aware reasoning while scaling linearly in sequence length. This allows SSMs to focus on relevant information and selectively forget irrelevant information along the sequence dimension, addressing the trade-off between efficiency and effectiveness.

A foundational problem in sequence modeling is how to compress context into a smaller state. While efficient models have small states, effective models require a state that contains all necessary information from the context. Selective SSMs propose that a fundamental principle for building sequence models is selectivity, which enables context-aware ability to focus on or filter out inputs into a sequential state.

The proposed selective SSM architecture achieves this by introducing a simple selection mechanism based on parameterizing SSM parameters as functions of the input. This allows the model to selectively propagate or forget information along the sequence length dimension depending on the current token, addressing the limitation of prior models.

## Problem Statement, Methods and Main Results

• Introduction of selective structured state space models (SSMs) with a new selection mechanism for content-based reasoning
• Development of Mamba, a simplified end-to-end neural network architecture using SSMs

#### Keywords: Structured State Space Models, Selective State Spaces, Deep Sequence Modeling, Recurrent Neural Networks, Hardware-Aware Parallel Algorithm


### [Link to paper](https://arxiv.org/abs/2312.00752v2)
