# Mistral 7B

# Research questions
Q1: What is the primary goal of designing a balanced language model that can deliver high-performance while maintaining efficiency in the rapidly evolving domain of Natural Language Processing (NLP)? 

Contribution: The authors introduce Mistral 7B, a 7-billion-parameter language model engineered for superior performance and efficiency, which achieves this balance by leveraging grouped-query attention (GQA) and sliding window attention (SWA), and demonstrates its effectiveness in outperforming the best open 13B model and the best released 34B model across various benchmarks.

## Problem Statement, Methods and Main Results

* Introducing Mistral 7B, a high-performance language model with superior efficiency and balance

#### Keywords: Mistral 7B, Grouped-Query Attention, Sliding Window Attention, Efficient Language Models, Low-Memory Inference


### [Link to paper](https://arxiv.org/abs/2310.06825v1)
