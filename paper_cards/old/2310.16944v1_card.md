# Zephyr: Direct Distillation of LM Alignment

# Research questions
Q1: Can a small open large language model (LLM) be aligned with user intent solely through distillation using preference data from AI Feedback?

Contribution: We aim to produce a smaller LLM that is aligned to user intent through distilled direct preference optimization (dDPO), which requires only a few hours of training without human annotation, and achieve performance comparable to 70B-parameter chat models aligned with human feedback.

This question highlights the main research problem addressed by this study, which focuses on developing an approach for aligning small open LLMs with user intent through distillation. The authors aim to overcome the limitations of existing methods, such as the need for human annotation and sampling, to create a more efficient and effective alignment process.

## Problem Statement, Methods and Main Results
**
• Introduced DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, advanced code-focused LLMs
• Developed repository-level data construction during pre-training for cross-file code generation capabilities
• Conducted extensive evaluations of the code LLMs against various benchmarks

#### Keywords: Distilled Direct Preference Optimization, Natural Language Processing, Intent Alignment, Pre-training, Chat Models


### [Link to paper](https://arxiv.org/abs/2310.16944v1)
