# Qwen Technical Report

# Research questions
Q1: Can large language models like QWEN-CHAT be trained using reinforcement learning from human feedback (RLHF) to produce responses preferred by humans? 
Contribution: In this work, we develop a comprehensive series of large language models, QWEN, including base pretrained language models and chat models finetuned with human alignment techniques, such as RLHF. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, while the chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter.

## Problem Statement, Methods and Main Results
**
* Development of advanced large language models (LLMs) for coding, including QWEN-CHAT and DeepSeek-Coder series
* Superior performance on downstream tasks, including code completion and utilization of a code interpreter
* Open-source, high-quality models for researchers and developers to advance the field

#### Keywords: Natural Language Processing, Reinforcement Learning from Human Feedback (RLHF), Code Generation, Multimodal Models, Specialized Models for Coding and Mathematics, Large Language Models (LLMs)


### [Link to paper](https://arxiv.org/abs/2309.16609v1)
