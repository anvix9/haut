# DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence

# Research questions
Q1: Can open-source large language models (LLMs) effectively close the performance gap with closed-source models in software development tasks?

Contribution: The researchers introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained on a high-quality project-level code corpus and utilizing a fill-in-the-blank task to enhance code generation capabilities.

## Problem Statement, Methods and Main Results

    • Introduction of DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B.
    • Development of repository-level data construction during pre-training.
    • Extensive evaluations of the code LLMs against various benchmarks.

#### Keywords: Natural Language Processing, Code Generation, Large Language Models (LLMs), Open-Source Models, Pre-training, Fill-In-Middle (FIM) approach, Context Window, Zero-Shot Instruction Capabilities


### [Link to paper](https://arxiv.org/abs/2401.14196v2)
