# VMamba: Visual State Space Model

# Research questions
Q1: How does the proposed VMamba architecture overcome the limitations of existing vision backbone networks, particularly in terms of input scalability and computational complexity?

Contribution: Designing computationally efficient network architectures persists as an ongoing necessity in computer vision. In this paper, we transplant Mamba, a state-space language model, into VMamba, a vision backbone that works in linear time complexity.

Note: This contribution clearly states the problem that VMamba aims to solve (computational inefficiency) and the solution proposed by the authors (VMamba's architecture).

## Problem Statement, Methods and Main Results

• Introduced advanced code-focused large language models DeepSeek-Coder-Base and DeepSeek-Coder-Instruct.
• Developed repository-level data construction, boosting cross-file code generation capabilities.
• Demonstrated superiority of code LLMs in extensive evaluations.

#### Keywords: Visual Representation Learning, Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), State Space Models (SSMs), 2D Selective Scan (SS2D), Linear Time Complexity, Efficient Computer Vision Architectures


### [Link to paper](https://arxiv.org/abs/2401.10166v3)
