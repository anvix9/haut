# Qwen2 Technical Report

# Research questions
Q1: What is the primary performance comparison being made between the proposed large language models (LLMs) and existing LLMs across diverse benchmarks?

Contribution: This report introduces Qwen2, a new series of large language models that surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across various benchmarks in language understanding, generation, multilingual capabilities, coding, mathematics, and reasoning.

## Problem Statement, Methods and Main Results

  • Introduction of Qwen2 series and its variants with improved performance in various tasks.
  • Release of model weights on public platforms for facilitating community-driven innovation and accessibility.
  • Development of robust multilingual capabilities across approximately 30 languages.

#### Keywords: Large Language Models, Transformer Architecture, Next-Token Prediction, Mixture-of-Experts (MoE) Model, Multilingual Capabilities, Instruction-Tuning, Fine-Tuning with DPO


### [Link to paper](https://arxiv.org/abs/2407.10671v4)
