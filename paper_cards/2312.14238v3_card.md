# InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks

# Research questions
Q1: What vision-language foundation model does the authors propose in this paper?

Q2: The proposed model, InternVL, aligns the large-scale vision encoder with LLMs by leveraging web-scale image-text data from various sources and a progressive alignment strategy.

Contribution:
The contribution of the authors can be summarized as follows:

1. Presentation of a novel vision-language foundation model, InternVL, that bridges the gap between vision foundation models and LLMs.
2. Introduction of a progressive image-text alignment strategy for efficient training of large-scale visionlanguage foundation models.
3. Comparative study of the proposed model with state-of-the-art vision foundation models and VLLMs.

The authors propose a novel approach to aligning the representation of large-scale vision encoders with LLMs, addressing limitations such as disparity in parameter scales, inconsistent representations, and inefficient connections between the vision encoder and language middleware.

## Problem Statement, Methods and Main Results
**
1. Novel vision-language foundation model, InternVL
2. Progressive image-text alignment strategy for efficient training of large-scale vision-language models
3. Comparative study of InternVL with state-of-the-art vision foundation models and VLLMs

#### Keywords: Multimodal AGI, Vision-Language Foundation Model, Large Language Models (LLMs), Cross-Modal Alignment, Progressive Image-Text Alignment


### [Link to paper](https://arxiv.org/abs/2312.14238v3)
