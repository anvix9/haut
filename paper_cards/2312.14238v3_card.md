# InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks

        # Research questions
        Research Question: Q1: Can a large-scale vision-language foundation model that aligns the representation of a scaled-up vision encoder with an LLM achieve state-of-the-art performance on various generic visual-linguistic tasks, including image classification, video classification, image-text retrieval, and multi-modal dialogue systems?

Contribution: The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models has not kept pace with LLMs. To address this gap, we design a large-scale vision-language foundation model, InternVL, which scales up the vision encoder to 6 billion parameters and progressively aligns it with the LLM.

        ## Problem Statement, Methods and Main Results
         
    • Designing InternVL, a large-scale vision-language foundation model that bridges the gap between vision foundation models and large language models (LLMs).
    • Developing repository-level data construction during pre-training.
    • Conducting extensive evaluations on various benchmarks.

        #### Keywords: Large Language Models, Vision-Language Foundation Models, Multimodal AI, Contrastive Learning, Generative Supervision


        ### [Link to paper](https://arxiv.org/abs/2312.14238v3)
        