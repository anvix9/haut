# InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks

# Research questions
Q1: How can a large-scale vision-language foundation model (InternVL) be effectively aligned with a large language model (LLM) to achieve state-of-the-art performance on various visual and vision-language tasks?

Contribution: The proposed InternVL model demonstrates strong performance on a wide range of generic visuallinguistic tasks, including visual perception tasks, visionlanguage tasks, and multi-modal dialogue, by aligning the large-scale vision encoder with LLMs for the first time.

## Problem Statement, Methods and Main Results
**
• Introduced InternVL, a large-scale vision-language foundation model that aligns a large-scale vision encoder with LLMs for state-of-the-art performance on visual-linguistic tasks.
• Demonstrated the effectiveness of repository-level data construction during pre-training for cross-file code generation capabilities.
• Conducted extensive evaluations against various benchmarks to showcase the superiority of DeepSeek-Coder models.

#### Keywords: Multimodal AGI, Large Language Models, Vision-Language Foundation Models, Conversational AI, Multi-Modal Dialogue Systems


### [Link to paper](https://arxiv.org/abs/2312.14238v3)
