# Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

# Research questions
Q1: What is the performance of Gemini 1.5 models on long-context retrieval tasks across modalities?

A: Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k).

Q2: How do Gemini 1.5 Pro and Gemini 1.5 Flash perform compared to other LLMs on core capabilities?

A: Gemini 1.5 Pro surpasses Gemini 1.0 Ultra in a wide array of benchmarks, requiring significantly less compute to train, while Gemini 1.5 Flash performs uniformly better than 1.0 Pro, even at a similar level to 1.0 Ultra on several benchmarks.

Q3: What are the multimodal capabilities of Gemini 1.5 models?

A: The model is capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio, with continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens.

Q4: What are the surprising new capabilities of Gemini 1.5 models?

A: The model can learn to translate a new language from a single set of linguistic documentation, and demonstrate speech recognition for a new language in context, breaking grounds on long-context automatic speech recognition, long-context video understanding, in-context planning and unstructured multimodal data analytics tasks.

Contribution: In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, achieving near-perfect recall on long-context retrieval tasks across modalities, improving the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matching or surpassing Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.

## Problem Statement, Methods and Main Results
**
Introduce the Gemini 1.5 series, representing the next generation of multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context.

#### Keywords: Multimodal Long-Context Models, Large Language Model Long-Context Capabilities, Natural Language Processing with Extremely Long Contexts, Long-Form Mixed-Modality Inputs, Reinforcement Learning for Multimodal Reasoning


### [Link to paper](https://arxiv.org/abs/2403.05530v5)
