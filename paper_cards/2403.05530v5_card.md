# Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

# Research questions
Q1: Can large language models recall and reason over fine-grained information from millions of tokens in long documents, videos, and audio with near-perfect recall up to at least 10M tokens?

Contribution: The authors introduce the Gemini 1.5 family of multimodal models, representing a generational leap in model performance and training efficiency, capable of recalling and reasoning over fine-grained information from millions of tokens.

In this report, the researchers describe the development and evaluation of the Gemini 1.5 Pro and Gemini 1.5 Flash models, which surpass previous state-of-the-art models like Gemini 1.0 Ultra and demonstrate exceptional performance in various multimodal benchmarks, including long-context retrieval tasks, text, vision, audio, and video understanding.

## Problem Statement, Methods and Main Results

* Development of the Gemini 1.5 family of multimodal models with improved performance and efficiency in long-context tasks.
* Demonstration of significant advancements in multimodal modeling, including non-long-context tasks and learning from new information without prior training data.

#### Keywords: Topic extraction failed


### [Link to paper](https://arxiv.org/abs/2403.05530v5)
