# Scaling Rectified Flow Transformers for High-Resolution Image Synthesis

# Research questions
Based on the passage, the main research question being addressed can be summarized as follows:

Q1: How can rectified flow models for text-to-image synthesis be improved through noise sampling techniques, and what are the benefits of these improvements?

Q2: Can a novel transformer-based architecture that incorporates learnable streams for both image and text tokens improve the performance of text-to-image synthesis models, and how does it compare to existing architectures?

Q3: How can rectified flow models be scaled up to larger model sizes while maintaining their performance, and what are the implications for their use in real-world applications?

Contribution:
The primary contributions of this work are:

1. Improved noise samplers for rectified flow models that enhance performance over previously known samplers.
2. A novel, scalable architecture for text-to-image synthesis that enables bi-directional mixing between text and image token streams within the network.
3. A scaling study demonstrating predictable scaling trends in validation loss and improved performance with lower validation loss.

The research aims to address the limitations of existing diffusion formulations for high-resolution images and videos, particularly in terms of sampling efficiency and scalability, by introducing new noise samplers and a novel architecture that takes into account the multi-modal nature of the text-to-image task. The study demonstrates improvements over state-of-the-art models and achieves competitive performance with proprietary models, making it an important contribution to the field of generative modeling.

## Problem Statement, Methods and Main Results
 Novel noise samplers, scalable transformer-based architecture, and improved scalability.

diffusion models, rectified flow, text-to-image synthesis, generative modeling, neural networks

#### Keywords: Diffusion Models, Rectified Flow, Text-to-Image Synthesis, Generative Modeling, Neural Networks, Transformer Architecture
### [Link to paper](https://arxiv.org/abs/2403.03206v1)
