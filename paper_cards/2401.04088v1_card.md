# Mixtral of Experts

# Research questions
Q1: What is the specific architecture of Mixtral that allows it to utilize a subset of its parameters while achieving state-of-the-art performance?

Q2: How does Mixtral's use of a mixture-of-experts network with open weights enable faster inference speed at low batch-sizes and higher throughput at large batch-sizes compared to existing models?

Contribution: We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. 

 Q3: What improvements does Mixtral - Instruct demonstrate over other state-of-the-art models in human evaluation benchmarks, particularly in terms of biases and sentiment profile?

## Problem Statement, Methods and Main Results
**
* Introduce Mixtral 8x7B, a Sparse Mixture of Experts language model
* Demonstrate effectiveness of SMoE technique in reducing parameter count while maintaining performance

#### Keywords: Sparse Mixture of Experts, Natural Language Processing, Decoder-only Models, Multilingual Benchmarking, Feedforward Block Architecture


### [Link to paper](https://arxiv.org/abs/2401.04088v1)
