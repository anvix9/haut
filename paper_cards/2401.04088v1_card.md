# Mixtral of Experts

# Research questions
Research Problem:
Q1: Can a sparse mixture-of-experts network, Mixtral 8x7B, achieve state-of-the-art performance among open-source models while reducing parameters and increasing inference speed compared to existing large language model architectures like Llama 2 70B?

## Problem Statement, Methods and Main Results
**
• Introducing Mixtral, a highly efficient and powerful language model.
• Improving open-source code modeling through repository-level data construction.
• Conducting extensive evaluations against various benchmarks.

#### Keywords: Sparse Mixture of Experts, Natural Language Processing, Multilingual Understanding, Reinforcement Learning (DPO), Transformer Architecture


### [Link to paper](https://arxiv.org/abs/2401.04088v1)
