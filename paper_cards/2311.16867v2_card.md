# The Falcon Series of Open Language Models

# Research questions
Q1: What is the main problem or motivation behind the development of the Falcon series of pre-trained language models?

A1: The primary motivation is to accelerate the development of open-source large language models, which have the potential to transform various industries and improve human civilization.

Q2: How does the Falcon series differ from existing state-of-the-art models in terms of scalability and data requirements?

A2: The Falcon series achieves competitive performance across scale while utilizing a diverse high-quality corpus predominantly assembled from web data, demonstrating performance scalability, data scalability, and hardware scalability.

Q3: What contributions do the authors make with this paper and the Falcon series?

A3: Contributions include public documentation of the pretraining process, open release of large models (Falcon-7 / 40 / 180B) and a high-quality web dataset extract, and detailed evaluations and method descriptions to facilitate further research and development.

Q4: What are some key features and architectures of the Falcon series?

A4: The Falcon series consists of three causal decoder-only models trained on up to 3.5 trillion tokens, utilizing multigroup attention and custom tooling for efficient distributed training and data preprocessing.

Overall, the study aims to address scalability challenges in large language models by exploring various aspects of pretraining, data quality, and model architecture, ultimately fostering open research and collaboration in this field.

## Problem Statement, Methods and Main Results

  • Public documentation of pretraining process and open release of large models
  • High-quality web dataset extract
  • Detailed evaluations and method descriptions for further research

Large Language Models, Causal Decoding, Distributed Training, Transformer-based Models

#### Keywords: Large Language Models, Falcon Series, Pretraining, Transformer-based Models, Causal Decoding, Distributed Training, Hardware Scalability, Data Scalability, Emergent Capabilities
### [Link to paper](https://arxiv.org/abs/2311.16867v2)
