# The Falcon Series of Open Language Models

# Research questions
Q1: What is the primary objective of the Falcon series, and what are its contributions to the field of large language models?

Q2: How do the authors scale up pretraining for increasingly large models, and what is the architecture of the Falcon series based on?

Q3: What specific techniques and interventions does the authors employ to improve inference scalability, memory efficiency, and performance in the Falcon series?

## Problem Statement, Methods and Main Results

• Advanced code-focused large language models (DeepSeek-Coder-Base and DeepSeek-Coder-Instruct)
• Developed repository-level data construction during pre-training
• Conducted extensive evaluations of the code LLMs

#### Keywords: Large Language Models, Causal Decoder-Only Models, Pretraining on Web Data, Transformer-Based Recipes, Open-Source Language Models


### [Link to paper](https://arxiv.org/abs/2311.16867v2)
