# The Falcon Series of Open Language Models

# Research questions
Q1: What pretraining scale-up method has been developed using a combination of high-quality web data and hardware scalability?

Contribution: We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data.

## Problem Statement, Methods and Main Results
**
* Introducing the Falcon series of causal decoder-only models for NLP tasks.
* Developing custom distributed training codebase and pretraining data pipeline.
* Demonstrating competitive performance on various NLP benchmarks.

#### Keywords: Large Language Models, Pretraining, Distributed Training, Transformers, Scaling Law


### [Link to paper](https://arxiv.org/abs/2311.16867v2)
