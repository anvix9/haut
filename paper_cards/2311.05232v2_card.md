# A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

# Research questions
Q1: What is the proposed taxonomy of hallucination in large language models (LLMs), and how does it categorize hallucination into factuality and faithfulness types?

Q2: What are the main factors contributing to hallucinations in LLMs, and how do they relate to data, training, and inference stages?

Q3: What are some effective detection methods for LLM hallucinations, and what is the purpose of comprehensive benchmarks related to LLM hallucinations?

Q4: How does the proposed survey address the limitations of retrieval-augmented generation (RAG) systems in mitigating hallucinations, and what promising avenues for future research are highlighted?

Q5: What is the main contribution of this survey, and how does it aim to empower researchers with invaluable insights to drive the evolution of AI technologies toward greater reliability and safety?

## Problem Statement, Methods and Main Results
**
* Introduce a novel taxonomy for categorizing LLM hallucinations.
* Develop comprehensive benchmarks for detecting and mitigating LLM hallucinations.
* Present effective mitigation strategies, including model-based and rule-based approaches.

#### Keywords: Hallucination, Natural Language Processing (NLP), Large Language Models, Factuality, Faithfulness, Information Retrieval


### [Link to paper](https://arxiv.org/abs/2311.05232v2)
