# A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

# Research questions
Q1: What is the primary problem being addressed by this research survey, specifically in the context of Large Language Models (LLMs)?

Q2: How do the motivations behind the study and the current limitations faced by LLMs impact their potential for reliable deployment in real-world information retrieval systems?

Q3: What are the key questions being asked by this research, based on the introduction and abstract?

Contributions:
The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing, but is also prone to hallucinations, generating plausible yet nonfactual content. This phenomenon raises concerns over the reliability of LLMs in real-world information retrieval systems.

This survey aims to provide a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations, including factors contributing to hallucinations, detection methods, mitigation strategies, and future research directions.

## Problem Statement, Methods and Main Results
 Large Language Models (LLMs), Hallucination, Factuality, Faithfulness, Detection, Mitigation.

#### Keywords: LLM Hallucination, Factuality, Faithfulness, LLM Detection, RAG Mitigations
### [Link to paper](https://arxiv.org/abs/2311.05232v2)
