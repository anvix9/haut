# Qwen2 Technical Report

# Research questions
Q1: What are the key technical characteristics and capabilities of the Qwen2 series of large language models (LLMs) developed by the Qwen team?

Q2: How does the Qwen2 series compare to other LLMs in terms of performance, particularly its predecessor Qwen1.5 and proprietary models such as GPT-4o?

Contribution: The Qwen2 series comprises foundational and instruction-tuned language models based on the Transformer architecture, trained using next-token prediction, with a range of parameter counts from 0.5 to 72 billion.

## Problem Statement, Methods and Main Results

• Introduction of foundational and instruction-tuned language models based on the Transformer architecture.
• Development of repository-level data construction during pre-training, which boosts cross-file code generation capabilities.

#### Keywords: Natural Language Processing, Large Language Models, Transformer Architecture, Mixture-of-Experts (MoE) Model, Fine-tuning, Direct Preference Optimization (DPO), Multilingual Capabilities


### [Link to paper](https://arxiv.org/abs/2407.10671v4)
