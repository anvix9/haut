# Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond

# Research questions
Q1: What are the limitations of current Large Vision Language Models (LVLMs) and how do they compare to proprietary models in terms of training and optimization?

Q2: How can LVLMs be improved to support fine-grained visual understanding, such as object grounding and text reading, while still leveraging the benefits of their large language model baseline?

Contribution: In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models designed to overcome the limitations of current LVLMs by introducing a new visual receptor, a 3-stage training pipeline, and fine-grained visual understanding capabilities.

## Problem Statement, Methods and Main Results
 
  - Introduces the Qwen-VL series of large-scale vision-language models to overcome LVLM limitations.

#### Keywords: Vision-Language Models, Large Language Models, Qwen-VL Series, Image Captioning, Question Answering, Visual Grounding
### [Link to paper](https://arxiv.org/abs/2308.12966v3)
