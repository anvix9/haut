# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

# Research questions
Q1: What class of architectures improves upon the limitations of existing attention-based models by enabling selective data selection based on input-dependent parameters?

A: Selective State Space Models (SSMs).

Q2: How does the proposed architecture, Mamba, address the efficiency- effectiveness trade-off in sequence modeling, particularly in the context of long-range dependencies and complex data modalities?

A: Mamba addresses this trade-off by incorporating a selection mechanism that allows for input-dependent selection, reducing the need for explicit context compression, and leveraging hardware-aware parallelization to scale linearly in sequence length.

Q3: What are the key observations made about the computation problem of SSMs, particularly with regard to recurrent and convolutional modes, and how do they inform the design of Mamba?

A: The naive recurrent computation has a higher constant factor than the convolutional computation, but the recurrent mode can use fewer FLOPs for long sequences and smaller state dimensions. To address the sequential nature of recurrence and large memory usage, Mamba leverages properties of modern accelerators to materialize the state in more efficient levels of the memory hierarchy.

Q4: How does Mamba's selection mechanism contribute to its overall performance on various domains, including language modeling, audio, and genomics?

A: The selection mechanism enables context-dependent reasoning while scaling linearly in sequence length, allowing Mamba to achieve state-of-the-art results on diverse domains. Specifically, it improves upon the performance of strong Transformer models on tasks such as language modeling, audio waveform modeling, and DNA sequence modeling.

Q5: What are the implications of Mamba's architecture for building foundation models for different domains, particularly in emerging modalities requiring long context?

A: Mamba's selective state space model backbone has broad applications in building foundation models for various domains, especially those requiring long-range dependencies. Its ability to scale linearly in sequence length and enable context-dependent reasoning makes it a strong candidate as a general sequence model backbone.

## Problem Statement, Methods and Main Results
**
*   Implementation of various sequence models, including Transformer and Mamba.
*   Training details with improved training recipe.
*   Evaluation metrics on real data across multiple modalities.

#### Keywords: Structured State Space Models, Selectivity, Recurrent Neural Networks, Convolutional Neural Networks, Hardware-Aware Parallel Algorithm


### [Link to paper](https://arxiv.org/abs/2312.00752v2)
