# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

# Research questions
Q1: What type of sequence modeling architectures are currently widely used as the backbone of foundation models, and what limitations do they face?

Answer: Foundation models are predominantly based on the Transformer architecture and its core attention layer, but this comes with drawbacks such as quadratic scaling with respect to the window length and an inability to model anything outside of a finite window.

Q2: What type of sequence modeling architectures have emerged as promising alternatives to address these limitations, particularly in domains involving continuous signal data?

Answer: Structured state space models (SSMs) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks and convolutional neural networks and have principled mechanisms for modeling long-range dependencies.

Q3: What is the primary limitation of prior SSMs, and how do they address it?

Answer: Prior SSMs struggle to efficiently select data in an input-dependent manner, which is crucial for content-aware reasoning. To address this limitation, we propose a new class of selective state space models that incorporates a selection mechanism.

Q4: How does the proposed selective scan algorithm for SSMs overcome the limitations of prior recurrent and convolutional computations?

Answer: The selective scan algorithm leverages properties of modern accelerators to materialize the state only in more efficient levels of the memory hierarchy, reducing memory IOs and achieving a significant speedup compared to standard implementations.

Q5: What are the key benefits of the proposed Mamba architecture, and how does it compare to existing models?

Answer: The Mamba architecture combines the design of prior SSM architectures with a simplified end-to-end neural network architecture, resulting in fast inference, linear scaling in sequence length, and high-quality performance across several modalities.

Q6: What are the potential applications of the proposed selective state space models, particularly in emerging modalities requiring long context?

Answer: The proposed Mamba architecture is a strong candidate to be a general sequence model backbone, with broad applications in various domains such as genomics, audio, and video.

## Problem Statement, Methods and Main Results
**

* Introduce selective state space models (SSMs) for content-aware reasoning and scaling linearly in sequence length.
* Develop a hardware-aware parallel algorithm for recurrent mode to address computational inefficiency.
* Propose the Mamba architecture as a strong candidate general sequence model backbone. 
NLP, SSMs, Transformers

#### Keywords: Structured State Space Models, Selective SSMs, Mamba Architecture, Hardware-Aware Parallelism, Reconstitution, Attention-free Model
### [Link to paper](https://arxiv.org/abs/2312.00752v2)
