# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

        # Research questions
        Q1: Can selective state space models (SSMs) achieve the modeling power of Transformer-based architectures while scaling linearly in sequence length? 

Contribution: Researchers propose a new class of selective state space models, which can select data in an input-dependent manner and improve upon prior work on several axes to achieve the modeling power of Transformers. They introduce a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, allowing for faster inference times and linear scaling in sequence length.

        ## Problem Statement, Methods and Main Results
        **
    * Improved performance across multiple domains with gains ranging from 8% to 71%.
    * Achieved state-of-the-art results in language modeling, outperforming larger Transformer models.

        #### Keywords: Structured State Space Models, Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Hardware-Aware Parallel Algorithm, Linear Time Inference


        ### [Link to paper](https://arxiv.org/abs/2312.00752v2)
        