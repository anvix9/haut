# VMamba: Visual State Space Model

# Research questions
Based on the provided passage, here is a concise summary of the main research question addressed by the authors:

**Research Question:**

Q1: How can we develop an efficient vision backbone network for visual representation learning that preserves the benefits of self-attention mechanisms while achieving linear time complexity?

Q2: Can we bridge the gap between ordered 1D scanning and non-sequential 2D traversal in vision data, facilitating the extension of selective State Space Models (SSMs) from natural language processing tasks to computer vision applications?

The authors aim to address these questions by introducing a novel architecture called VMamba, which integrates SSM-based blocks with a novel 2D Selective Scan (SS2D) module. The contributions of this study include:

* Proposing VMamba, an efficient vision backbone network with linear time complexity
* Introducing SS2D to facilitate the extension of selective SSMs from NLP tasks to computer vision applications
* Demonstrating promising performance across various visual tasks and exhibiting linear growth in computational complexity

The authors also highlight limitations in their study, including the need for further research on compatibility with existing pre-training methods, exploration of large-scale architectures, and fine-grained search of hyperparameters.

## Problem Statement, Methods and Main Results
**
* Development of the efficient vision backbone network VMamba with linear time complexity
* Introduction of the Selective Scan (SS2D) module to facilitate extension of SSMs from NLP tasks to computer vision applications

#### Keywords: Vision Transformer, VMamba, SS2D, State Space Model, Natural Language Processing
### [Link to paper](https://arxiv.org/abs/2401.10166v3)
