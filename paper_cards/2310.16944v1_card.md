# Zephyr: Direct Distillation of LM Alignment

# Research questions
Q1: Can a smaller language model be developed using distillation and AI feedback that aligns with user intent without human annotation?

Q2: Does utilizing preference data from an ensemble of teacher models through distilled direct preference optimization improve the alignment property in small open LLMs?

Contribution:
We aim to develop a smaller language model aligned to user intent, previously explored in research using techniques such as distillation and supervised fine-tuning.

Research Question: 
Q3: Can a 7B parameter model achieve performance comparable to 70B-parameter chat models aligned with human feedback by utilizing AI feedback through distilled direct preference optimization?

## Problem Statement, Methods and Main Results

• Developed a smaller language model aligned to user intent using distillation and AI feedback.
• Introduced the 'fill-in-the-blank' pre-training objective, context window extension, and Fill-In-Middle (FIM) approach for code completion capabilities.

#### Keywords: Natural Language Processing, Distillation (in NLP), Direct Preference Optimization (DPO), Intent Alignment, Chat Models


### [Link to paper](https://arxiv.org/abs/2310.16944v1)
