# Code Llama: Open Foundation Models for Code

# Research questions
Q1: Can a large language model be specialized for code generation and infilling, enabling applications such as real-time completion in source code editors or docstring generation?

Q2: How can a large language model be fine-tuned to operate on very large contexts with a moderate impact on performance on standard coding benchmarks?

Contribution: We release Code Llama, a family of large language models for code generation and infilling derived from Llama 2 (Touvron et al., 2023b) and released under the same custom permissive license. Our approach is based on gradually specializing and increasing the capabilities of Llama 2 models by applying a cascade of training and fine-tuning steps, including code-training, infilling, long input contexts, and instruction fine-tuning.

## Problem Statement, Methods and Main Results

• Introduced Code Llama-Base and Code Llama-Instruct, advanced code-focused large language models.
• Developed repository-level data construction during pre-training for cross-file code generation capabilities.
• Demonstrated zero-shot instruction following ability for programming tasks.

#### Keywords: Natural Language Processing, Code Generation, Large Language Models, Specialized Models, Infilling Capabilities, Contextual Understanding


### [Link to paper](https://arxiv.org/abs/2308.12950v3)
