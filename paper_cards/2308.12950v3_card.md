# Code Llama: Open Foundation Models for Code

# Research questions
Q1: What is the primary goal of developing a large language model (LLM) for code generation and infilling, specifically in relation to its applications and capabilities?

Q2: How does the Code Llama model address the limitations of existing LLMs for code-related tasks, such as handling long input contexts, instruction fine-tuning, and autoregressive training and fine-tuning?

Contribution: We release Code Llama , a family of large language models for code generation and infilling derived from Llama 2 (Touvron et al., 2023b) and released under the same custom permissive license. Our approach is based on gradually specializing and increasing the capabilities of Llama 2 models by applying a cascade of training and fine-tuning steps to address limitations in existing code-related tasks.

Summary: The study aims to develop a large language model that can effectively generate and fill code, addressing limitations such as handling long input contexts, instruction fine-tuning, and autoregressive training and fine-tuning. Code Llama addresses these challenges by proposing a specialization pipeline that includes infilling-capable models, which enable applications like real-time completion in source code editors or docstring generation.

## Problem Statement, Methods and Main Results

	+ Developing a family of large language models for code generation and infilling.
	+ Proposing a specialization pipeline that includes infilling-capable models.

#### Keywords: Code Generation, Large Language Models, LLM Fine-Tuning, Instruction Following, Infilling, Computational Linguistics
### [Link to paper](https://arxiv.org/abs/2308.12950v3)
