# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model

# Research questions
Q1: Can a generic vision backbone based on state space models (SSMs) overcome the computational and memory constraints in processing Transformer-style understanding for high-resolution images?

Contribution: Recently, we proposed Vision Mamba (Vim), which incorporates bidirectional SSMs for data-dependent global visual context modeling and position embeddings for location-aware visual recognition. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency.

Q2: How does the proposed Vision Mamba (Vim) model compare in terms of speed and memory usage when performing batch inference to extract features on high-resolution images?

Contribution: The results demonstrate that Vim is 2.8 × faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248 × 1248.

Q3: What are the limitations of existing SSM-based models for vision tasks, and how does the proposed Vision Mamba (Vim) model address these limitations?

Contribution: Prior state space models for vision tasks use hybrid architecture or equivalent global 2D convolutional kernel. Vim, on the other hand, learns visual representation in the sequence modeling manner and does not introduce imagespecific inductive biases.

Q4: Can the proposed Vision Mamba (Vim) model be used for unsupervised tasks such as mask image modeling pretraining, and what are its potential applications?

Contribution: The proposed Vim architecture is suitable for unsupervised tasks such as mask image modeling pretraining. Future works include exploring the usefulness of Vim for analyzing high-resolution medical images, remote sensing images, and long videos.

Q5: What are the future directions for research on Vision Mamba (Vim) and its applications in computer vision?

Contribution: The proposed Vim model has great potential to be the next-generation vision backbone. Future works include exploring the usefulness of Vim for analyzing high-resolution medical images, remote sensing images, and long videos, as well as developing multimodal tasks such as CLIP-style pretraining.

## Problem Statement, Methods and Main Results
 Exploring the usefulness of Vim for analyzing high-resolution medical images, remote sensing images, and long videos.

Note that both papers report similar results, but with some minor differences in details (e.g., specific experiment setups used).

#### Keywords: State Space Models, Vision Transformers, Bidirectional Mamba, Efficient Visual Backbones, Computer Vision, Sequence Modeling


### [Link to paper](https://arxiv.org/abs/2401.09417v3)
