# Efficient Memory Management for Large Language Model Serving with PagedAttention

# Research questions
Q1: What attention algorithm inspired by classical virtual memory and paging techniques is proposed to address the challenges of managing key-value (KV) cache memory in large language models (LLMs)?

A1: PagedAttention.

Q2: How does vLLM, a distributed LLM serving engine built on top of PagedAttention, achieve near-zero waste in KV cache memory?

A2: vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention, which enables efficient memory sharing at the granularity of blocks across different sequences associated with the same request or even across different requests.

Q3: What is the primary goal of proposing vLLM, a high-throughput distributed LLM serving engine on top of PagedAttention?

A3: To achieve near-zero waste in KV cache memory and improve the throughput of popular LLMs by 2-4 times compared to state-of-the-art systems.

Q4: What improvements are demonstrated in the evaluation of vLLM, compared to FasterTransformer and Orca?

A4: VLLM substantially outperforms the previous state-of-the-art solutions in terms of throughput improvements, with better performance on longer sequences, larger models, and more complex decoding algorithms.

## Problem Statement, Methods and Main Results
**

* Introduction of DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, novel attention algorithm called PagedAttention.
* Development of a centralized scheduler and KV cache manager with PagedAttention for efficient memory management.
* Demonstration of near-zero waste in KV cache memory and improved LLM throughput.

#### Keywords: Attention Mechanism, Virtual Memory, Memory Fragmentation, Paged Attention, Distributed LLM Serving


### [Link to paper](https://arxiv.org/abs/2309.06180v1)
