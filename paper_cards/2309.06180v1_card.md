# Efficient Memory Management for Large Language Model Serving with PagedAttention

# Research questions
Q1: Can large language models (LLMs) efficiently serve requests with minimal waste in key-value (KV) cache memory?

Contribution: Yes, we propose PagedAttention, an attention algorithm inspired by operating system's virtual memory and paging techniques, to manage the KV cache memory e/fficiently. We build vLLM, a high-throughput distributed LLM serving engine on top of PagedAttention, which achieves near-zero waste in KV cache memory and improves LLM serving throughput by 2-4 Ã— compared to state-of-the-art systems.

## Problem Statement, Methods and Main Results
**
* Introduction of PagedAttention and vLLM for efficient LLM serving
* Improved memory management through the use of paged attention
* Increased throughput and reduced waste in KV cache memory

#### Keywords: Attention Mechanism, Memory Fragmentation, Virtual Memory, Page-level Memory Management, Paging, High-Throughput Serving


### [Link to paper](https://arxiv.org/abs/2309.06180v1)
