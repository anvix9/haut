# DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence

        # Research questions
        Q1: Can large open-source code models be developed that can effectively perform code infilling and generation comparable to or surpassing closed-source models like Codex and GPT-3.5? 

Contribution: The authors introduce the DeepSeek-Coder series, a range of open-source code models (DeepSeek-Coder-Base and DeepSeek-Coder-Instruct) with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens sourced from 87 programming languages.

        ## Problem Statement, Methods and Main Results
        
  • Introducing DeepSeek-CoderBase and DeepSeek-Coder-Instruct, advanced code-focused large language models.
  • Developing repository-level data construction during pre-training for cross-file code generation capabilities.

        #### Keywords: Code Generation, Large Language Models (LLMs), Open-Source Models, Natural Language Understanding, Programming Languages


        ### [Link to paper](https://arxiv.org/abs/2401.14196v2)
        