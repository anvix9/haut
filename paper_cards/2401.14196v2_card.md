# DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence

# Research questions
Q1: What are the primary challenges faced by researchers and developers in utilizing large language models for software development tasks?

A: The major challenge lies in the performance gap between open-source models and closed-source models, with the former being inaccessible to many researchers and developers due to their proprietary nature.

Q2: How do the authors address this challenge by developing the DeepSeek-Coder series of open-source code models?

A: The authors introduce a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive understanding of coding languages and syntax.

Q3: What specific enhancements and innovations does the DeepSeek-Coder series bring to the field of software development?

A: The authors develop several innovative techniques, including the "fill-in-the-blank" pre-training objective, the extension of the context window to 16K tokens, and the incorporation of the Fill-In-Middle (FIM) approach, which significantly bolster the models' code completion capabilities.

Q4: What are the main contributions of the authors in this study?

A: The authors make several key contributions, including:

* Introducing DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, advanced code-focused large language models.
* Developing repository-level data construction during pre-training, which significantly boosts cross-file code generation capabilities.
* Conducting extensive evaluations of the code LLMs against various benchmarks, demonstrating their superiority over existing open-source models.

Contribution: The authors' work introduces a series of specialized Large Language Models (LLMs) for coding, including the DeepSeek-Coder series, which provides significant advancements in open-source code modeling.

## Problem Statement, Methods and Main Results
 Advanced code-focused large language models (DeepSeek-Coder-Base and DeepSeek-Coder-Instruct), repository-level data construction, extensive evaluations.

#### Keywords: DeepSeek-Coder, code intelligence, natural language processing, code generation, reinforcement learning, code understanding
### [Link to paper](https://arxiv.org/abs/2401.14196v2)
