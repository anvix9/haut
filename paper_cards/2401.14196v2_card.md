# DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence

# Research questions
Q1: What are the limitations of current large language models (LLMs) in software development, specifically with regards to their accessibility and performance compared to closed-source models?

Q2: How do the proposed DeepSeek-Coder series models address these limitations by introducing open-source code models trained on a comprehensive project-level code corpus, incorporating FIM approach, and employing larger context windows?

Contribution: The authors introduce the DeepSeek-Coder series of open-source code models, which outperform existing closed-source models in terms of performance and accessibility, addressing the limitations of current LLMs in software development.

## Problem Statement, Methods and Main Results
**
* Introduces DeepSeek-Coder series of open-source code models with improved performance and accessibility.
* Develops novel pre-training objective and extended context window for enhanced code generation capabilities.
* Demonstrates the potential of Large Language Models (LLMs) in coding tasks.

#### Keywords: Large Language Models, Code Generation, Open-Source Models, Pretraining, Reinforcement Learning, Token Prediction Loss, Natural Language Understanding


### [Link to paper](https://arxiv.org/abs/2401.14196v2)
