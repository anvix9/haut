# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

# Research questions
Q1: How can a large-scale crowd-sourced evaluation platform that utilizes human preferences effectively assess the performance of Large Language Models (LLMs) in real-world, open-ended tasks, and what are the key benefits of using such an approach? 

Contribution: Developing Chatbot Arena, a benchmarking platform for LLMs that features crowdsourced, pairwise human preferences, to evaluate their performance in real-world scenarios.

## Problem Statement, Methods and Main Results
 Large Language Models, chatbots, human preferences, crowdsourcing, pairwise comparison.

#### Keywords: Natural Language Processing, Crowdsourcing, Human Preference Evaluation, Large Language Models (LLMs), Ranking Systems, Machine Learning for NLP


### [Link to paper](https://arxiv.org/abs/2403.04132v1)
        