# LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention

# Research questions
Q1: What is the primary goal of developing an efficient fine-tuning method for large-scale instruction-following models like LLaMA?

## Problem Statement, Methods and Main Results

• Introduced DeepSeek-Coder-Base and DeepSeek-Coder-Instruct for code-focused LLMs.
• Developed repository-level data construction during pre-training for cross-file code generation capabilities.
• Conducted extensive evaluations of the code LLMs against various benchmarks.

#### Keywords: Natural Language Processing, Attention Mechanism, Fine-tuning, Instruction-following Models, Multi-modal Reasoning


### [Link to paper](https://arxiv.org/abs/2303.16199)
        