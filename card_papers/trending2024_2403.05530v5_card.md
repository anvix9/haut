# Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

# Research questions
Q1: How do Gemini 1.5 Pro and Gemini 1.5 Flash models handle extremely long contexts, and what benefits do they provide compared to existing large language models (LLMs)? 

 Contribution: We present our latest multimodal models from the Gemini line: Gemini 1.5 Pro and Gemini 1.5 Flash. They are members of Gemini 1.5, a new family of highly-capable multimodal models which incorporates our latest innovations in sparse and dense scaling as well as major advances in training, distillation and serving infrastructure that allow it to push the boundary of efficiency, reasoning, planning, multi-linguality, function calling and long-context performance.

Gemini 1.5 Pro continues this trend by extending language model context lengths by over an order of magnitude. Scaling to millions of tokens, we find a continued improvement in predictive performance (Section 5.2.1.1), near perfect recall ( > 99%) on synthetic retrieval tasks (Figure 1 and Section 5.2.1.2), and a host of surprising new capabilities like in-context learning from entire long documents and multimodal content (Section 5.2.2).

Gemini 1.5 Flash, while being smaller and way more efficient and faster to serve, maintains high levels of performance even as its context window increases.

Q1: Do Gemini 1.5 Pro and Gemini 1.5 Flash break the boundaries in multimodal long-context capabilities for language models compared to existing systems?

Contribution: Yes. Gemini 1.5 models are built to handle extremely long contexts; they have the ability to recall and reason over fine-grained information from up to at least 10M tokens.

 Gemini 1.5 Pro surpasses Gemini 1.0 Pro and 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train.
 
Gemini 1.5 Flash performs uniformly better compared to 1.0 Pro and even performs at a similar level to 1.0 Ultra on several benchmarks.

Q1: How do Gemini 1.5 models perform on synthetic and real-world tasks, particularly in multimodal long-context capabilities?

Contribution: We conduct experiments on both synthetic and real-world tasks. In synthetic 'needle-in-a-haystack' tasks inspired by Kamradt (2023) that probe how reliably the model can recall information amidst distractor context, we find that both Gemini 1.5 Pro and Gemini 1.5 Flash achieve near-perfect ( > 99%) 'needle' recall up to multiple millions of tokens of 'haystack' in all modalities.

In more realistic multimodal long-context benchmarks which require retrieval and reasoning over multiple parts of the context, we see Gemini 1.5 Pro outperforming all competing models across all modalities even when these models are augmented with external retrieval methods.

Moreover, we showcase the in-context learning abilities of both Gemini 1.5 Pro and Gemini 1.5 Flash enabled by very long context: for example, learning to translate a new language from a single set of linguistic documentation.

## Problem Statement, Methods and Main Results
 Training with JAX and ML Pathways on TPUs, online distillation from Gemini to develop dense Transformer-based Gemini 1.5 Flash model.

**Results**
- Significant improvements in long-context reasoning and downstream performance.
- Near-perfect recall on multi-modal versions of needle-in-a-haystack benchmark.
- Ability to perform realistic long-context tasks such as long-document QA from 700k-word material and long-video QA from 40-105 minute videos.
- In-context learn capability for translating from English to Kalamang, a low-resource language with fewer than 200 speakers.
- Superior performance on multi-modal core capabilities compared to the Gemini 1.0 series.

**Computational Methods**
- Training with JAX and ML Pathways on TPUs.
- Online distillation from Gemini to develop dense Transformer-based Gemini 1.5 Flash model.
- Automatic parallelization using GSPMD partitioner for faster training.

#### Keywords: Multimodal Models, Long-Context Capabilities, Large Language Models (LLMs), N-gram Models, Transformer Architecture


### [Link to paper](https://arxiv.org/abs/2403.05530v5)
        