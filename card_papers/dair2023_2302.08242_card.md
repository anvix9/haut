# Tuning computer vision models with task rewards

# Research questions
Q1: Can reward optimization with reinforcement learning (RL) be applied to improve alignment between model predictions and intended usage in computer vision tasks?

Contribution: The authors demonstrate that tuning a pre-trained model with a reward function using REINFORCE works out-of-the-box for multiple computer vision tasks, such as object detection, panoptic segmentation, and image colorization.

## Problem Statement, Methods and Main Results

  + Introducing DeepSeek-Coder-Base and DeepSeek-Coder-Instruct.
  + Development of repository-level data construction during pre-training.
  + Extensive evaluations against various benchmarks. 
Large Language Models, Code Modeling, Computer Vision

#### Keywords: Topic extraction failed


### [Link to paper](https://arxiv.org/abs/2302.08242)
        