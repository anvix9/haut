# Active Prompting with Chain-of-Thought for Large Language Models

# Research questions
Q1: What is the research problem being addressed by the proposed Active-Prompt method, which aims to improve the performance of large language models (LLMs) on complex reasoning tasks by judiciously selecting the most helpful questions for annotation?

Contribution: The increasing scale of LLMs brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks.

The key problem is how to determine which questions are the most important and helpful for annotation, reducing the human engineering workload. By leveraging uncertainty and introducing several metrics to characterize the uncertainty among the model's predictions on each question, the proposed Active-Prompt method aims to improve the performance of LLMs by selectively annotating the most uncertain questions.

## Problem Statement, Methods and Main Results
**
* Introduces an active learning method for LLMs on complex reasoning tasks.
* Develops a new approach for adaptively selecting task-specific example prompts.
* Improves performance and efficiency of annotation and fine-tuning for LLMs.

#### Keywords: Chain-of-thought Prompting, Active Learning, Uncertainty-based Active Selection, Reasoning in Large Language Models


### [Link to paper](https://arxiv.org/abs/2302.12246)
        