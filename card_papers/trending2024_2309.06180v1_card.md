# Efficient Memory Management for Large Language Model Serving with PagedAttention

# Research questions
Q1: How do existing large language models (LLMs) achieve high-throughput serving with efficient memory management, and what are the limitations of current approaches?

Q2: Can LLM serving systems efficiently manage the KV cache memory to reduce internal fragmentation and external fragmentation, leading to improved serving performance?

A1: Existing LLM serving systems struggle to manage the KV cache memory efficiently due to their reliance on contiguous memory allocation for tensors, resulting in significant internal and external memory fragmentation.

A2: No.

## Problem Statement, Methods and Main Results

• Novel approach to managing LLM serving systems.
• Efficient memory management techniques.
• Improved throughput and low latency performance.

#### Keywords: Attention, Virtual Memory, Memory Fragmentation, Paged Attention, Efficient Cache Management, Distributed LLM Serving


### [Link to paper](https://arxiv.org/abs/2309.06180v1)
        