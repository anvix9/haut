# CoLT5: Faster Long-Range Transformers with Conditional Computation

# Research questions
Q1: Can a Transformer model be optimized to efficiently process long documents while maintaining its quality by identifying and devoting more computation to the most important tokens in both feedforward and attention layers? 

Contribution: The authors propose COLT5, a new family of models that combines architecture improvements for both attention and feedforward layers to enable fast processing of long inputs.

## Problem Statement, Methods and Main Results
**

* Introduction of COLT5 model that combines architecture improvements for feedforward and attention layers
* Development of repository-level data construction during pre-training for cross-file code generation capabilities
* Conducting extensive evaluations of code LLMs against various benchmarks

#### Keywords: Efficient Transformer, Conditional Computation, Long-Range Input Processing, Feedforward Layer Optimization, Attention Mechanism Improvement


### [Link to paper](https://arxiv.org/abs/2303.09752)
        