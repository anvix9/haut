# Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models

# Research questions
Q1: Can Text2Room â€  , a method for generating roomscale textured 3D meshes from a given text prompt, create room-scale 3D structure and texture that are dense and coherent across outward-facing viewpoints without relying on pre-existing 3D training data?

Contribution: 
- Generating 3D meshes of room-scale indoor scenes with compelling textures and geometry from any text input.
- A method that leverages 2D text-to-image models and monocular depth estimation to lift frames into 3D in an iterative scene generation. 
- A two-stage tailored viewpoint selection that samples camera poses from optimal positions to first create the room layout and furniture and then close any remaining holes, creating a watertight mesh.

## Problem Statement, Methods and Main Results

- Generates 3D meshes of room-scale indoor scenes with compelling textures and geometry.
- Aims to address the challenge of generating large-scale 3D models from text without pre-existing 3D training data.
- Advances state-of-the-art in 3D model generation from text with a novel iterative fusion strategy.

#### Keywords: Text-to-Image Synthesis, 3D Mesh Generation, Monocular Depth Estimation, Iterative Optimization, Viewpoint Selection


### [Link to paper](https://arxiv.org/abs/2303.11989)
        