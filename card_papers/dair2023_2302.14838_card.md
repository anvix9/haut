# EvoPrompting: Language Models for Code-Level Neural Architecture Search

# Research questions
Research Problem: Q1: Can evolutionary search methods improve the performance of large language models (LMs) in generating novel and effective neural architectures through iterative in-context prompting and prompt-tuning?

Contribution: Given that LMs have shown impressive performance on various natural language processing tasks, yet struggle with creating novel solutions to complex problems, we propose a method called EVOPROMPTING, which uses evolutionary search to create and curate data to improve LM in-context prompting examples.

## Problem Statement, Methods and Main Results

  • Introduces EVOPROMPTING, a method for enhancing the in-context capabilities of pre-trained LMs using evolutionary techniques.
  • Demonstrates the effectiveness of EVOPROMPTING in discovering novel state-of-the-art architectures that optimize for both accuracy and model size.

#### Keywords: Evolutionary Prompt Engineering, Natural Language Processing (NLP), Deep Learning Architecture Search, Reinforcement Learning (RL), Soft Prompt Tuning


### [Link to paper](https://arxiv.org/abs/2302.14838)
        