# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

# Research questions
Q1: Can selective state space models (SSMs) achieve the modeling power of Transformer-based architectures while scaling linearly in sequence length? 

Contribution: Researchers propose a new class of selective state space models, which can select data in an input-dependent manner and improve upon prior work on several axes to achieve the modeling power of Transformers. They introduce a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, allowing for faster inference times and linear scaling in sequence length.

## Problem Statement, Methods and Main Results
 
        + Novel selective state space models with context-dependent reasoning capabilities
        + Hardware-aware algorithm for linear scaling in sequence length
        + State-of-the-art performance across multiple domains

#### Keywords: Structured State Space Models, Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Hardware-Aware Parallel Algorithm, Linear Time Inference


### [Link to paper](https://arxiv.org/abs/2312.00752v2)
        