# PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing

# Research questions
Q1: Can Large Language Models with Trillion Parameters Achieve Optimal Performance when Trained on Limited Compute Budget and Distributed Across Multiple Accelerating Devices?

Contribution: This work presents PanGuΣ , a large language model with sparse architecture, that achieves state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. By utilizing Random Routed Experts (RRE) and Expert Computation Storage Separation (ECSS), the training throughput is improved by 6.3x compared to the model of the same hyper-parameters but with dense Transformer architecture, demonstrating the potential for efficient scaling of large language models on limited compute budget and distributed across multiple accelerating devices.

## Problem Statement, Methods and Main Results
**
* Development of PanGuΣ, a state-of-the-art sparse language model for Chinese NLP tasks.
* Improved training throughput through heterogeneous computing.
* Demonstration of efficient scaling of large language models on limited compute budget.

#### Keywords: Large Language Models, Distributed Training, Sparse Architectures, Expert Computation and Storage Separation, Random Routed Experts (RRE)


### [Link to paper](https://arxiv.org/abs/2303.10845)
        