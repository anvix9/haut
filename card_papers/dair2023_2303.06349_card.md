# Resurrecting Recurrent Neural Networks for Long Sequences

# Research questions
Q1: Can deep Linear Recurrent Unit (LRU) architectures recover the performance and efficiency of deep continuous-time state-space models (SSMs) using vanilla deep RNNs?

Contribution:
The researchers demonstrate that by modifying a vanilla RNN architecture, specifically introducing linear recurrences, complex diagonal recurrent matrices, stable exponential parameterization, and normalization, they can match the performance of deep SSMs on long-range reasoning tasks while maintaining computational efficiency.

## Problem Statement, Methods and Main Results
 Large Language Models, Open-source models, Software development.

#### Keywords: Recurrent Neural Networks, Deep State-Space Models, Linear Recurrent Units, Attention Mechanisms, Long-Range Reasoning


### [Link to paper](https://arxiv.org/abs/2303.06349)
        