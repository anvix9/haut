# Modular Deep Learning

# Research questions
This is a comprehensive survey on modular deep learning, covering various aspects of the field. Here's a summary of the key points:

**Definition and Benefits**

* Modularity is defined as functional specialization of the components of a system.
* Modularity provides benefits such as positive transfer, compositionality, and parameter efficiency.

**Module Implementations**

* There are several ways to implement modules, including:
	+ Sparse subnetworks
	+ Adapter layers
	+ Prefix tuning
	+ Human-engineered prompts

Each implementation has its trade-offs between efficiency and performance.

**Routing Functions**

* Routing functions control the flow of information to the modules.
* There are two types of routing:
	+ Fixed routing, where module allocation is manually defined based on expert knowledge.
	+ Learned routing, which learns a parameterized routing function during training.

**Aggregation Functions**

* Aggregation functions combine the outputs of multiple modules.
* Common aggregation methods include:
	+ Interpolation
	+ Attention mechanism
	+ Simple averaging

**Applications**

* Modular deep learning has applications in various areas, including:
	+ Natural Language Processing (NLP)
	+ Computer Vision
	+ Speech Processing
	+ Hierarchical Reinforcement Learning
	+ Programme Induction
	+ Causal Discovery and Inference

**Key Takeaways**

* Modularity is a desirable property of neural networks, which can lead to improved performance and efficiency.
* Routing functions are crucial in controlling the flow of information to the modules.
* Aggregation functions combine the outputs of multiple modules to produce a final output.

Overall, this survey provides a comprehensive overview of modular deep learning, highlighting its benefits, implementation techniques, and applications across various domains.

## Problem Statement, Methods and Main Results

• Introducing DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, advanced code-focused LLMs.
• Developing repository-level data construction during pre-training for cross-file code generation capabilities.
• Extensive evaluations demonstrating superiority over existing open-source models.

#### Keywords: Topic extraction failed


### [Link to paper](https://arxiv.org/abs/2302.11529)
        