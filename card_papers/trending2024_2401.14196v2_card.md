# DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence

# Research questions
Q1: Can large open-source code models be developed that can effectively perform code infilling and generation comparable to or surpassing closed-source models like Codex and GPT-3.5? 

Contribution: The authors introduce the DeepSeek-Coder series, a range of open-source code models (DeepSeek-Coder-Base and DeepSeek-Coder-Instruct) with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens sourced from 87 programming languages.

## Problem Statement, Methods and Main Results
**
  - Introducing DeepSeek-Coder series, advanced open-source code-focused large language models with sizes from 1.3B to 33B.
  - Developing repository-level data construction during pre-training, which significantly boosts cross-file code generation capabilities.
  - Conducting extensive evaluations of the code LLMs against various benchmarks, demonstrating their superiority over existing open-source models.

#### Keywords: Code Generation, Large Language Models (LLMs), Open-Source Models, Natural Language Understanding, Programming Languages


### [Link to paper](https://arxiv.org/abs/2401.14196v2)
        