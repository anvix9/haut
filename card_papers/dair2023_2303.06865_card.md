# FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU

# Research questions
Q1: How can FlexGen efficiently run large language models (LLMs) with limited GPU memory while maintaining high throughput?

A1: Contribution 1: FlexGen formally defines a search space of possible offloading strategies and develops a linear programming-based search algorithm to optimize the throughput within the search space, capturing computation order with I/O complexity within 2 Ã— optimality.

## Problem Statement, Methods and Main Results
 Large Language Models, Open-Source Models, Software Development, Code Completion, Quantization Techniques, Sparse Attention Approximation.

#### Keywords: High-Throughput Generation, Offloading Strategies, Compression Techniques, Distributed Pipeline Parallelism, Latency-Tolerant Batch Processing


### [Link to paper](https://arxiv.org/abs/2303.06865)
        