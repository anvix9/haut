# Improved Baselines with Visual Instruction Tuning

# Research questions
Research Question:

Q1: What are the optimal design choices for large multimodal models (LMMs) in visual instruction tuning, considering training data, model architecture, and resamplers?

Contribution:
We present the first systematic study on the design choices of LMMs in a controlled setting under the LLaVA framework. Our findings reveal that the fully-connected vision-language connector in LLaVA is surprisingly powerful and data-efficient, achieving state-of-the-art results across 11 benchmarks with simple modifications such as using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts. We also explore open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, providing insights into the optimal design choices for large multimodal models in visual instruction tuning.

Note: The research question is not directly asking about a specific problem but rather about the optimal design choices for LMMs. However, based on the context provided, we can infer that one of the main contributions of this study is to provide guidance on how to train LMMs effectively and efficiently.

## Problem Statement, Methods and Main Results

• Introducing the fully-connected vision-language connector in LLaVA, achieving state-of-the-art results.
• Providing insights into optimal design choices for large multimodal models in visual instruction tuning.
• Exploring open problems in LMMs and shedding light on future research directions.

#### Keywords: Topic extraction failed


### [Link to paper](https://arxiv.org/abs/2310.03744v2)
        