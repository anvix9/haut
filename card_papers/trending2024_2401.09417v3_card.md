# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model

# Research questions
Q1: Can a generic vision backbone that utilizes state space models (SSMs) achieve superior performance in image classification and dense prediction tasks compared to well-established vision transformers like DeiT?

Contribution: Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models.

The results demonstrate that Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. Specifically, Vim is 2.8 × faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248 × 1248.

Q2: Can the proposed Vision Mamba (Vim) model achieve superior performance in image classification tasks while reducing computation complexity compared to well-established vision transformers like DeiT?

Contribution: Without the need of attention, the proposed Vim has the same modeling power as ViT while it only has subquadratic-time computation and linear memory complexity. Specifically, Vim is 2.8 × faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images at the resolution of 1248 × 1248.

Q3: Can the proposed Vision Mamba (Vim) model be used for unsupervised tasks such as mask image modeling pretraining, and has it been shown that it enables multimodal tasks such as CLIP-style pretraining?

Contribution: Like Transformers, Vim can be pretrained on large-scale unsupervised visual data for better visual representation. Thanks to the better efficiency of Mamba, the large-scale pretraining of Vim can be achieved with lower computational cost.

Q4: What are the future directions for the proposed Vision Mamba (Vim) model, and what downstream tasks can it be used for?

Contribution: In future works, Vim with the bidirectional SSM modeling with position embeddings is suitable for unsupervised tasks such as mask image modeling pretraining and the similar architecture with Mamba enables multimodal tasks such as CLIP-style pretraining. Based on the pretrained Vim weights, exploring the usefulness of Vim for analyzing high-resolution medical images, remote sensing images, and long videos, which can be regarded as downstream tasks, is very straightforward.

## Problem Statement, Methods and Main Results

- Exploring the usefulness of Vim for analyzing high-resolution medical images, remote sensing images, and long videos as downstream tasks.
- Ablating different strategies for the bidirectional design in the proposed architecture to evaluate their effects on performance.

#### Keywords: Visual Transformers, State Space Models (SSMs), Bidirectional State Space Models, Positional Awareness, Generic Vision Backbones, Efficient Deep Learning, Hardware-Aware Designs


### [Link to paper](https://arxiv.org/abs/2401.09417v3)
        