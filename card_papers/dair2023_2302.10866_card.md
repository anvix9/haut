# Hyena Hierarchy: Towards Larger Convolutional Language Models

# Research questions
Q1: Can a subquadratic replacement for the attention operator match the quality of state-of-the-art models while reducing computational cost?

Contribution: We propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating.

## Problem Statement, Methods and Main Results

    * Development of specialized Large Language Models for coding.
    * Advances in open-source code modeling through data-driven approaches and repository-level data construction.
    * Demonstrates superiority over existing models in various benchmarks.

#### Keywords: Hyena, Subquadratic Attention, Efficient NLP Architectures, Long Convolutional Networks, Data-Controlled Gating


### [Link to paper](https://arxiv.org/abs/2302.10866)
        