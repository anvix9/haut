# Gemma: Open Models Based on Gemini Research and Technology

# Research questions
Research Problem: 

Q1: Can pre-trained and fine-tuned language model architectures achieve state-of-the-art performance on a wide range of domains, while also ensuring safety and responsible development methodologies for large-scale models like Gemma? 

Contribution: We present Gemma, a family of open models based on Google's Gemini models that demonstrate strong generalist capabilities in text domains alongside state-of-the-art understanding and reasoning skills at scale. The release of both pre-trained and fine-tuned checkpoints, as well as an open-source codebase for inference and serving, aims to enable thorough research and investigation into the impact of current instruction tuning regimes and promote the development of increasingly safe and responsible model development methodologies.

## Problem Statement, Methods and Main Results
 
    * Introduction of the Gemma family of open models based on Google's Gemini models.
    * Release of pre-trained and fine-tuned checkpoints and an open-source codebase for inference and serving.
    * Promoting safe and responsible model development methodologies.

#### Keywords: Transformers, Large Language Models (LLMs), Neural Networks, Deep Learning Methods, Sequence Models, Natural Language Processing


### [Link to paper](https://arxiv.org/abs/2403.08295v4)
        