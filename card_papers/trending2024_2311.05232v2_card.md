# A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

# Research questions
Q1: What is the proposed categorization of LLM hallucinations, and how does it distinguish itself from existing approaches?

Contribution: Recently, we propose a refined taxonomy of hallucination tailored specifically for applications involving large language models (LLMs). We categorize hallucination into two primary types: factuality hallucination and faithfulness hallucination.

Q2: What are the underlying causes of LLM hallucinations, and how do they relate to the proposed mitigation strategies?

Contribution: Our survey identifies potential contributors into three main aspects: data, training, and inference stages. We comprehensively outline a variety of effective detection methods specifically devised for detecting hallucinations in LLMs.

Q3: How does the proposed taxonomy of hallucinations differ from existing approaches, such as those discussed by Ji et al., Tonmoy et al., Liu et al., Wang et al., and Zhang et al.?

Contribution: Our survey sets itself apart through a unique taxonomy and organizational structure, presenting a detailed, layered classification of hallucinations and conducting a more comprehensive analysis of the causes of hallucinations.

Q4: What is the main objective of the proposed mitigation strategies for LLM hallucinations, and how do they relate to the underlying causes identified in the survey?

Contribution: Our proposed mitigation strategies are directly tied to the causes identified, offering a targeted and coherent framework for addressing LLM hallucinations.

Q5: What is the significance of the proposed comprehensive survey on understanding and mitigating LLM hallucinations, and what insights does it offer for researchers dedicated to advancing robust information retrieval systems and trustworthy artificial intelligence?

Contribution: Our survey aims to provide invaluable insights that drive the evolution of AI technologies toward greater reliability and safety by navigating the complex landscape of hallucinations.

## Problem Statement, Methods and Main Results

    + Introducing a unique taxonomy and organizational structure for categorizing hallucination
    + Developing targeted and coherent mitigation strategies for addressing LLM hallucinations

#### Keywords: Natural Language Processing, Large Language Models, Hallucinations, Information Retrieval Systems, Trustworthiness Challenges


### [Link to paper](https://arxiv.org/abs/2311.05232v2)
        