# Eliciting Latent Predictions from Transformers with the Tuned Lens

# Research questions
Q1: How do the internal representations of transformer models evolve layer by layer during the prediction process, and what insights can be gained from analyzing these evolutions?

Contribution: We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary.

## Problem Statement, Methods and Main Results
**
  • Development of DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, advanced code-focused LLMs.
  • Repository-level data construction during pre-training for cross-file code generation capabilities.
  • Extensive evaluation of code LLMs against various benchmarks.

#### Keywords: Natural Language Processing, Transformer Models, Iterative Inference, Early Exiting Techniques, Unconventional Training Methods


### [Link to paper](https://arxiv.org/abs/2303.08112)
        