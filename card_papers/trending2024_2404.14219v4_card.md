# Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone

# Research questions
Q1: Why is it possible to train a language model small enough to fit on a phone, yet rival the capabilities of large models like ChatGPT?

Contribution: We introduce phi-3-mini , a 3.8 billion parameter language model trained on a scaled-up version of publicly available web data and synthetic data, achieving performance comparable to larger models despite its compact size, thus demonstrating the potential of carefully curated training data in reducing model size without sacrificing capability.

## Problem Statement, Methods and Main Results

  • Introducing phi-3-mini, a compact language model for software development tasks
  • Demonstrating the effectiveness of scaling up language models and novel methodologies
  • Advancements in multimodal modeling and optimization techniques

#### Keywords: Large Language Models, Scaling Laws, Transfer Learning, Data Curation, Compact Model Optimization


### [Link to paper](https://arxiv.org/abs/2404.14219v4)
        