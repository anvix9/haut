# Context-faithful Prompting for Large Language Models

# Research questions
Research Question: 
Q1: Can large language models (LLMs) accurately capture contextual information in knowledge-driven NLP tasks, particularly when faced with conflicting knowledge or situations where predictions require abstention? 

Contribution:  Improving the contextual faithfulness of LLMs using carefully designed prompting strategies, specifically opinion-based prompts and counterfactual demonstrations.

## Problem Statement, Methods and Main Results

  - Improving LLMs' contextual faithfulness using carefully designed prompting strategies (opinion-based prompts and counterfactual demonstrations)
  - Developing practical solutions for enhancing LLMs' contextual understanding without requiring additional training.

#### Keywords: Natural Language Processing, Knowledge Acquisition, Contextual Faithfulness, Prompt Engineering


### [Link to paper](https://arxiv.org/abs/2303.11315)
        