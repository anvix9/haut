# PaLM-E: An Embodied Multimodal Language Model

# Research questions
Q1: Can a large language model be effectively grounded in the real world to solve complex tasks such as robotic manipulation planning, visual question answering, and captioning?

Q2: What are the limitations of current state-of-the-art visual-language models when applied to embodied reasoning tasks, and how can they be overcome by incorporating multi-modal information and novel architectural ideas?

## Problem Statement, Methods and Main Results
**
* Introduced a series of specialized LLMs for coding, including the DeepSeek-Coder series.
* Developed repository-level data construction during pre-training.
* Conducted extensive evaluations of code LLMs against various benchmarks.

#### Keywords: Multi-modal Models, Visual-Language Models, Embodied Language Models, Robotics Reinforcement Learning, Grounding in AI


### [Link to paper](https://arxiv.org/abs/2303.03378)
        