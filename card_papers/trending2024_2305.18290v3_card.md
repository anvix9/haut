# Direct Preference Optimization: Your Language Model is Secretly a Reward Model

# Research questions
Q1: Can a simple classification objective be used to directly optimize a large unsupervised language model (LM) to align with human preferences without the need for reinforcement learning from human feedback (RLHF)?

Contribution: Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms but is simple to implement and straightforward to train, achieves this by fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.

## Problem Statement, Methods and Main Results
**
* Development of advanced code-focused large language models (LLMs).
* Introduction of repository-level data construction for cross-file code generation capabilities.
* Demonstration of superiority over existing open-source models.

#### Keywords: Direct Preference Optimization (DPO), Reinforcement Learning from Human Feedback (RLHF), Binary Cross-Entropy Objective, Implicit Reward Modeling, Language Model Training


### [Link to paper](https://arxiv.org/abs/2305.18290v3)
        