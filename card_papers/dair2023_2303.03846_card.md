# Larger language models do in-context learning differently

# Research questions
Q1: How do large language models learn to override semantic priors when presented with flipped labels versus semantically-unrelated label ICL (SUL-ICL) settings?

Contribution: We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings.

## Problem Statement, Methods and Main Results

* Insights into how in-context learning behavior changes with model scale.
* Emergent abilities of large language models to perform linear classification in SUL-ICL settings.
* Strengthening the understanding of ICL's impact on semantic priors and input-label mappings.

#### Keywords: In-Context Learning, Semantic Priors, Input-Label Mappings, Language Models, Model Scale


### [Link to paper](https://arxiv.org/abs/2303.03846)
        