# Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection

# Research questions
Q1: Can Indirect Prompt Injection (IPI) attacks be used to remotely compromise Large Language Models (LLMs) integrated into applications without a direct interface, potentially leading to data theft, worming, and other security risks?

Contribution: Researchers have demonstrated that IPI attacks can be used to override original instructions and controls in LLM-integrated applications, enabling adversaries to exploit these systems by strategically injecting prompts into data likely to be retrieved. The study reveals new attack vectors, including full compromise of the model at inference time, remote control, persistent compromise, theft of data, and denial of service, and provides a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities.

## Problem Statement, Methods and Main Results

  + Introduction of Indirect Prompt Injection (IPI) as a novel attack vector against Large Language Models (LLMs).
  + Development of a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities of IPI attacks.
  + Demonstration of the practical feasibility of IPI attacks on real-world systems, highlighting potential risks associated with LLM-integrated applications.

#### Keywords: Indirect Prompt Injection, Prompt Injection (PI), Large Language Models, Adversarial Prompting, Computer Security Perspective


### [Link to paper](https://arxiv.org/abs/2302.12173)
        